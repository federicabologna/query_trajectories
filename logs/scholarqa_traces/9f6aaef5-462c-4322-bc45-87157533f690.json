{
    "query": "What were the main technical innovations introduced in each major LLaVA variant (LLaVA 1.5, LLaVA-NeXT, and LLaVA-OneVision) that drove their performance improvements?",
    "user_id": "lib_user",
    "task_id": "9f6aaef5-462c-4322-bc45-87157533f690",
    "timestamp": "2025-06-23T22:35:29.857114",
    "n_retrieval": 256,
    "n_retrieved": 252,
    "n_candidates": 20,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.256149,
    "decomposed_query": {
        "rewritten_query": "Technical innovations in LLaVA 1.5, LLaVA-NeXT, and LLaVA-OneVision that drove performance improvements.",
        "keyword_query": "LLaVA 1.5 LLaVA-NeXT LLaVA-OneVision technical innovations performance improvements",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010443,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
            "venue": "",
            "year": 2025,
            "reference_count": 30,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.23859, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292400830",
                    "name": "S. Riggi"
                },
                {
                    "authorId": "2042077694",
                    "name": "T. Cecconello"
                },
                {
                    "authorId": "2352941747",
                    "name": "A. Pilzer"
                },
                {
                    "authorId": "2352939581",
                    "name": "S. Palazzo"
                },
                {
                    "authorId": "2299008238",
                    "name": "N. Gupta"
                },
                {
                    "authorId": "2298907506",
                    "name": "A. Hopkins"
                },
                {
                    "authorId": "2258840598",
                    "name": "C. Trigilio"
                },
                {
                    "authorId": "2349648144",
                    "name": "G. Umana"
                }
            ],
            "abstract": "The advent of next-generation radio telescopes is set to transform radio astronomy by producing massive data volumes that challenge traditional processing methods. Deep learning techniques have shown strong potential in automating radio analysis tasks, yet are often constrained by the limited availability of large annotated datasets. Recent progress in self-supervised learning has led to foundational radio vision models, but adapting them for new tasks typically requires coding expertise, limiting their accessibility to a broader astronomical community. Text-based AI interfaces offer a promising alternative by enabling task-specific queries and example-driven learning. In this context, Large Language Models (LLMs), with their remarkable zero-shot capabilities, are increasingly used in scientific domains. However, deploying large-scale models remains resource-intensive, and there is a growing demand for AI systems that can reason over both visual and textual data in astronomical analysis. This study explores small-scale Vision-Language Models (VLMs) as AI assistants for radio astronomy, combining LLM capabilities with vision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio images from multiple surveys, enriched with 38k image-caption pairs from the literature. The fine-tuned models show clear improvements over base models in radio-specific tasks, achieving ~30% F1-score gains in extended source detection, but they underperform pure vision models and exhibit ~20% drop on general multimodal tasks. Inclusion of caption data and LoRA fine-tuning enhances instruction-following and helps recover ~10% accuracy on standard benchmarks. This work lays the foundation for future advancements in radio VLMs, highlighting their potential and limitations, such as the need for better multimodal alignment, higher-quality datasets, and mitigation of catastrophic forgetting.",
            "corpus_id": 277452239,
            "sentences": [
                {
                    "corpus_id": "277452239",
                    "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
                    "text": "LLaVA (Large Language and Vision Assistant) (Liu et al., 2023) is a state-of-the-art multimodal model that integrates both visual and textual understanding, combining the capabilities of large language models (LLMs) with vision processing abilities. Its primary function is to interpret and generate responses to input that includes both images and text, making it ideal for tasks like visual question answering (VQA), image captioning, and other vision-language tasks. Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks. Various variants or specialization of the first LLaVA models have been produced so far. For example, TinyLLaVA (Zhou et al., 2024;Jia et al., 2024) is a compact refactored variant of the original LLaVA 1.5 model, designed to enable easier inclusion of alternative light vision and LLM models, thus significantly reducing overall model size and resource requirements. LLaVA-Med (Li et al., 2023a) is a specialized variant of the LLaVA model designed to assist in medical image analysis and diagnostics by fine-tuning its multimodal capabilities on medical datasets such as X-rays, MRIs, and other healthcare-related visual data.",
                    "score": 0.47229667376940565,
                    "section_title": "Model overview",
                    "char_start_offset": 10302,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1241
                        },
                        {
                            "start": 1242,
                            "end": 1550
                        },
                        {
                            "start": 1551,
                            "end": 1638
                        },
                        {
                            "start": 1639,
                            "end": 1917
                        },
                        {
                            "start": 1918,
                            "end": 2178
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.908203125
                }
            ],
            "relevance_judgement": 0.908203125,
            "relevance_judgment_input_expanded": "# Title: Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks\n# Venue: \n# Authors: S. Riggi, T. Cecconello, A. Pilzer, S. Palazzo, N. Gupta, A. Hopkins, C. Trigilio, G. Umana\n## Abstract\nThe advent of next-generation radio telescopes is set to transform radio astronomy by producing massive data volumes that challenge traditional processing methods. Deep learning techniques have shown strong potential in automating radio analysis tasks, yet are often constrained by the limited availability of large annotated datasets. Recent progress in self-supervised learning has led to foundational radio vision models, but adapting them for new tasks typically requires coding expertise, limiting their accessibility to a broader astronomical community. Text-based AI interfaces offer a promising alternative by enabling task-specific queries and example-driven learning. In this context, Large Language Models (LLMs), with their remarkable zero-shot capabilities, are increasingly used in scientific domains. However, deploying large-scale models remains resource-intensive, and there is a growing demand for AI systems that can reason over both visual and textual data in astronomical analysis. This study explores small-scale Vision-Language Models (VLMs) as AI assistants for radio astronomy, combining LLM capabilities with vision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio images from multiple surveys, enriched with 38k image-caption pairs from the literature. The fine-tuned models show clear improvements over base models in radio-specific tasks, achieving ~30% F1-score gains in extended source detection, but they underperform pure vision models and exhibit ~20% drop on general multimodal tasks. Inclusion of caption data and LoRA fine-tuning enhances instruction-following and helps recover ~10% accuracy on standard benchmarks. This work lays the foundation for future advancements in radio VLMs, highlighting their potential and limitations, such as the need for better multimodal alignment, higher-quality datasets, and mitigation of catastrophic forgetting.\n## Model overview\nLLaVA (Large Language and Vision Assistant) (Liu et al., 2023) is a state-of-the-art multimodal model that integrates both visual and textual understanding, combining the capabilities of large language models (LLMs) with vision processing abilities. Its primary function is to interpret and generate responses to input that includes both images and text, making it ideal for tasks like visual question answering (VQA), image captioning, and other vision-language tasks. Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks. Various variants or specialization of the first LLaVA models have been produced so far. For example, TinyLLaVA (Zhou et al., 2024;Jia et al., 2024) is a compact refactored variant of the original LLaVA 1.5 model, designed to enable easier inclusion of alternative light vision and LLM models, thus significantly reducing overall model size and resource requirements. LLaVA-Med (Li et al., 2023a) is a specialized variant of the LLaVA model designed to assist in medical image analysis and diagnostics by fine-tuning its multimodal capabilities on medical datasets such as X-rays, MRIs, and other healthcare-related visual data.",
            "reference_string": "[277452239 | Riggi et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
            "venue": "",
            "year": 2024,
            "reference_count": 65,
            "citation_count": 10,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.01818, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118051520",
                    "name": "Qizhe Zhang"
                },
                {
                    "authorId": "2292408664",
                    "name": "Aosong Cheng"
                },
                {
                    "authorId": "2331417542",
                    "name": "Ming Lu"
                },
                {
                    "authorId": "2275104296",
                    "name": "Renrui Zhang"
                },
                {
                    "authorId": "2333364107",
                    "name": "Zhiyong Zhuo"
                },
                {
                    "authorId": "2268711797",
                    "name": "Jiajun Cao"
                },
                {
                    "authorId": "2333442704",
                    "name": "Shaobo Guo"
                },
                {
                    "authorId": "2331326229",
                    "name": "Qi She"
                },
                {
                    "authorId": "2332857566",
                    "name": "Shanghang Zhang"
                }
            ],
            "abstract": "Large vision-language models (LVLMs) generally contain significantly more visual tokens than their textual counterparts, resulting in a considerable computational burden. Recent efforts have been made to tackle this issue by pruning visual tokens early within the language model. Most existing works use attention scores between text and visual tokens to assess the importance of visual tokens. However, in this study, we first analyze the text-visual attention in the language model and find that this score is not an ideal indicator for token pruning. Based on the analysis, We propose VisPruner, a plug-and-play method that utilizes visual cues for more effective token pruning in LVLMs. Specifically, we first use visual attention to select a limited number of significant tokens. Then, we remove duplicate tokens from the remaining ones based on their similarity. By retaining diverse tokens alongside the initially selected important tokens, we maximally preserve the visual information of the input image. Experimental results demonstrate that our VisPruner sustains strong performance across various VLM architectures and reduction ratios, significantly outperforming existing methods based on text-visual attention. Notably, without any training, VisPruner can reduce the FLOPs of LLaVA-1.5-7B by 91% and inference latency by 75%, while maintaining comparable performance. Our code is available at https://github.com/Theia-4869/VisPruner.",
            "corpus_id": 274437586,
            "sentences": [
                {
                    "corpus_id": "274437586",
                    "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
                    "text": "LLaVA-1.5 [34]. LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection. The overall architecture remains consistent with LLaVA: the visual encoder encodes continuous video frames individually, and the representations are concatenated as inputs to the LLM. After joint training, Video-LLaVA is capable of understanding both image and video data. Qwen-VL [3]. Qwen-VL is another widely used opensource vision-language model. Similar to LLaVA, it includes a visual encoder (OpenCLIP) and a text decoder (Qwen LLM).",
                    "score": 0.4559431707743645,
                    "section_title": "Model architectures",
                    "char_start_offset": 36898,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 15
                        },
                        {
                            "start": 16,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 315
                        },
                        {
                            "start": 316,
                            "end": 426
                        },
                        {
                            "start": 427,
                            "end": 513
                        },
                        {
                            "start": 514,
                            "end": 707
                        },
                        {
                            "start": 708,
                            "end": 724
                        },
                        {
                            "start": 725,
                            "end": 749
                        },
                        {
                            "start": 750,
                            "end": 895
                        },
                        {
                            "start": 896,
                            "end": 999
                        },
                        {
                            "start": 1000,
                            "end": 1139
                        },
                        {
                            "start": 1140,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1381
                        },
                        {
                            "start": 1382,
                            "end": 1478
                        },
                        {
                            "start": 1479,
                            "end": 1563
                        },
                        {
                            "start": 1564,
                            "end": 1747
                        },
                        {
                            "start": 1748,
                            "end": 1836
                        },
                        {
                            "start": 1837,
                            "end": 1849
                        },
                        {
                            "start": 1850,
                            "end": 1914
                        },
                        {
                            "start": 1915,
                            "end": 2003
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 10,
                            "end": 14,
                            "matchedPaperCorpusId": "263672058"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88232421875
                }
            ],
            "relevance_judgement": 0.88232421875,
            "relevance_judgment_input_expanded": "# Title: Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs\n# Venue: \n# Authors: Qizhe Zhang, Aosong Cheng, Ming Lu, Renrui Zhang, Zhiyong Zhuo, Jiajun Cao, Shaobo Guo, Qi She, Shanghang Zhang\n## Abstract\nLarge vision-language models (LVLMs) generally contain significantly more visual tokens than their textual counterparts, resulting in a considerable computational burden. Recent efforts have been made to tackle this issue by pruning visual tokens early within the language model. Most existing works use attention scores between text and visual tokens to assess the importance of visual tokens. However, in this study, we first analyze the text-visual attention in the language model and find that this score is not an ideal indicator for token pruning. Based on the analysis, We propose VisPruner, a plug-and-play method that utilizes visual cues for more effective token pruning in LVLMs. Specifically, we first use visual attention to select a limited number of significant tokens. Then, we remove duplicate tokens from the remaining ones based on their similarity. By retaining diverse tokens alongside the initially selected important tokens, we maximally preserve the visual information of the input image. Experimental results demonstrate that our VisPruner sustains strong performance across various VLM architectures and reduction ratios, significantly outperforming existing methods based on text-visual attention. Notably, without any training, VisPruner can reduce the FLOPs of LLaVA-1.5-7B by 91% and inference latency by 75%, while maintaining comparable performance. Our code is available at https://github.com/Theia-4869/VisPruner.\n## Model architectures\nLLaVA-1.5 [34]. LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection. The overall architecture remains consistent with LLaVA: the visual encoder encodes continuous video frames individually, and the representations are concatenated as inputs to the LLM. After joint training, Video-LLaVA is capable of understanding both image and video data. Qwen-VL [3]. Qwen-VL is another widely used opensource vision-language model. Similar to LLaVA, it includes a visual encoder (OpenCLIP) and a text decoder (Qwen LLM).",
            "reference_string": "[274437586 | Zhang et al. | 2024 | Citations: 10]"
        },
        {
            "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model",
            "venue": "Information Fusion",
            "year": 2024,
            "reference_count": 89,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.03034, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2082462168",
                    "name": "Dawei Dai"
                },
                {
                    "authorId": "2329189750",
                    "name": "Xu Long"
                },
                {
                    "authorId": "2136494548",
                    "name": "Yutang Li"
                },
                {
                    "authorId": "2310835404",
                    "name": "Yuanhui Zhang"
                },
                {
                    "authorId": "2147222435",
                    "name": "Shuy Xia"
                }
            ],
            "abstract": "Human-scene vision-language tasks are increasingly prevalent in diverse social applications, yet recent advancements predominantly rely on models specifically tailored to individual tasks. Emerging research indicates that large vision-language models (VLMs) can enhance performance across various downstream vision-language understanding tasks. However, general-domain models often underperform in specialized fields. This study introduces a domain-specific Large Vision-Language Model, Human-Scene Vision-Language Model (HumanVLM), designed to provide a foundation for human-scene Vision-Language tasks. Specifically, (1) we create a large-scale human-scene multimodal image-text dataset (HumanCaption-10M) sourced from the Internet to facilitate domain-specific alignment; (2) develop a captioning approach for human-centered images, capturing human faces, bodies, and backgrounds, and construct a high-quality Human-Scene image-text dataset (HumanCaptionHQ, about 311k pairs) that contain as much detailed information as possible about human; (3) Using HumanCaption-10M and HumanCaptionHQ, we train a HumanVLM. In the experiments, we then evaluate our HumanVLM across varous downstream tasks, where it demonstrates superior overall performance among multimodal models of comparable scale, particularly excelling in human-related tasks and significantly outperforming similar models, including Qwen2VL and ChatGPT-4o. HumanVLM, alongside the data introduced, will stimulate the research in human-around fields.",
            "corpus_id": 273821149,
            "sentences": [
                {
                    "corpus_id": "273821149",
                    "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model",
                    "text": "Liu at el. introduced an end-to-end trained large vision-language assistant (LLaVA [51]) on instruction-following data for general purpose visual and language understanding, which gained widespread attention upon release. Subsequent research has further enhanced LLaVA's performance. For instance, LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios. LLaVA-Interactive [53] serves as a comprehensive demonstration platform, incorporating features such as image chatting, segmentation, and generation and editing capabilities, significantly expanding LLaVA's original functionalities. MoE-LLaVA [54], a sparse LVLM architecture based on Mixture of Experts (MoE), was developed to tackle performance degradation in multimodal sparse learning. MG-LLaVA [55] enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, allowing it to handle features at various resolutions and object centers. \n\nLLaVA has set new standards for efficiency and effectiveness in multimodal learning and has quickly been adapted across various domains. For example, LLaVA-based models, including LLaVA-Med [18], PathChat [19], QUILT-LLaVA [56], PA-LLaVA [57], have been designed for medical image understanding, where they outperform traditional methods. Zheng et al. [58] developed the first large-scale open-source dataset, MMTab, to address the multimodal table understanding problem and trained a multifunctional tableformat LLM called Table-LLaVA. In the power sector, Wang et al. [21] proposed Power-LLaVA, a large vision-language assistant designed for reliable inspection of power transmission lines, showcasing strong capabilities in this field. In the food domain, Fnu Mohbat et al. [20] introduced LLaVA-Chef, trained on a carefully selected recipe dataset, enabling it to recognize ingredients and generate detailed recipes. In this study, we aim to construct a unified multimodal Vision-Language Model for human-scene tasks.",
                    "score": 0.4260461346019444,
                    "section_title": "Various Vision-Language Applications",
                    "char_start_offset": 8442,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 1034
                        },
                        {
                            "start": 1037,
                            "end": 1173
                        },
                        {
                            "start": 1174,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1573
                        },
                        {
                            "start": 1574,
                            "end": 1775
                        },
                        {
                            "start": 1776,
                            "end": 1957
                        },
                        {
                            "start": 1958,
                            "end": 2058
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1227,
                            "end": 1231,
                            "matchedPaperCorpusId": "258999820"
                        },
                        {
                            "start": 1242,
                            "end": 1246,
                            "matchedPaperCorpusId": "270437603"
                        },
                        {
                            "start": 1260,
                            "end": 1264,
                            "matchedPaperCorpusId": "266149936"
                        },
                        {
                            "start": 1389,
                            "end": 1393,
                            "matchedPaperCorpusId": "270391758"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8681640625
                }
            ],
            "relevance_judgement": 0.8681640625,
            "relevance_judgment_input_expanded": "# Title: HumanVLM: Foundation for Human-Scene Vision-Language Model\n# Venue: Information Fusion\n# Authors: Dawei Dai, Xu Long, Yutang Li, Yuanhui Zhang, Shuy Xia\n## Abstract\nHuman-scene vision-language tasks are increasingly prevalent in diverse social applications, yet recent advancements predominantly rely on models specifically tailored to individual tasks. Emerging research indicates that large vision-language models (VLMs) can enhance performance across various downstream vision-language understanding tasks. However, general-domain models often underperform in specialized fields. This study introduces a domain-specific Large Vision-Language Model, Human-Scene Vision-Language Model (HumanVLM), designed to provide a foundation for human-scene Vision-Language tasks. Specifically, (1) we create a large-scale human-scene multimodal image-text dataset (HumanCaption-10M) sourced from the Internet to facilitate domain-specific alignment; (2) develop a captioning approach for human-centered images, capturing human faces, bodies, and backgrounds, and construct a high-quality Human-Scene image-text dataset (HumanCaptionHQ, about 311k pairs) that contain as much detailed information as possible about human; (3) Using HumanCaption-10M and HumanCaptionHQ, we train a HumanVLM. In the experiments, we then evaluate our HumanVLM across varous downstream tasks, where it demonstrates superior overall performance among multimodal models of comparable scale, particularly excelling in human-related tasks and significantly outperforming similar models, including Qwen2VL and ChatGPT-4o. HumanVLM, alongside the data introduced, will stimulate the research in human-around fields.\n## Various Vision-Language Applications\nLiu at el. introduced an end-to-end trained large vision-language assistant (LLaVA [51]) on instruction-following data for general purpose visual and language understanding, which gained widespread attention upon release. Subsequent research has further enhanced LLaVA's performance. For instance, LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios. LLaVA-Interactive [53] serves as a comprehensive demonstration platform, incorporating features such as image chatting, segmentation, and generation and editing capabilities, significantly expanding LLaVA's original functionalities. MoE-LLaVA [54], a sparse LVLM architecture based on Mixture of Experts (MoE), was developed to tackle performance degradation in multimodal sparse learning. MG-LLaVA [55] enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, allowing it to handle features at various resolutions and object centers. \n\nLLaVA has set new standards for efficiency and effectiveness in multimodal learning and has quickly been adapted across various domains. For example, LLaVA-based models, including LLaVA-Med [18], PathChat [19], QUILT-LLaVA [56], PA-LLaVA [57], have been designed for medical image understanding, where they outperform traditional methods. Zheng et al. [58] developed the first large-scale open-source dataset, MMTab, to address the multimodal table understanding problem and trained a multifunctional tableformat LLM called Table-LLaVA. In the power sector, Wang et al. [21] proposed Power-LLaVA, a large vision-language assistant designed for reliable inspection of power transmission lines, showcasing strong capabilities in this field. In the food domain, Fnu Mohbat et al. [20] introduced LLaVA-Chef, trained on a carefully selected recipe dataset, enabling it to recognize ingredients and generate detailed recipes. In this study, we aim to construct a unified multimodal Vision-Language Model for human-scene tasks.",
            "reference_string": "[273821149 | Dai et al. | 2024 | Citations: 2]"
        },
        {
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 156,
            "citation_count": 867,
            "influential_citation_count": 201,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.03326, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2310709478",
                    "name": "Bo Li"
                },
                {
                    "authorId": "2145784327",
                    "name": "Yuanhan Zhang"
                },
                {
                    "authorId": "2325209062",
                    "name": "Dong Guo"
                },
                {
                    "authorId": "2310650738",
                    "name": "Renrui Zhang"
                },
                {
                    "authorId": "2310758205",
                    "name": "Feng Li"
                },
                {
                    "authorId": "2267467406",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2300086932",
                    "name": "Kaichen Zhang"
                },
                {
                    "authorId": "2315071527",
                    "name": "Yanwei Li"
                },
                {
                    "authorId": "2315193840",
                    "name": "Ziwei Liu"
                },
                {
                    "authorId": "2264692022",
                    "name": "Chunyuan Li"
                }
            ],
            "abstract": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.",
            "corpus_id": 271719914,
            "sentences": [
                {
                    "corpus_id": "271719914",
                    "title": "LLaVA-OneVision: Easy Visual Task Transfer",
                    "text": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.",
                    "score": 0.5166824859576965,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84716796875
                },
                {
                    "corpus_id": "271719914",
                    "title": "LLaVA-OneVision: Easy Visual Task Transfer",
                    "text": "It is a core aspiration in AI to build general-purpose assistants with Large Multimodal Models (LMM) [67]. LLaVA-OneVision is an open model, continuing to advance the line of research in building large vision-and-language assistant (LLaVA) [83] that can follow diverse instructions to complete a variety of computer vision tasks in the wild. As a cost-efficient recipe, it is typically developed by connecting vision encoders with large language models (LLM) using a simple connection module. \n\nThe first LLaVA model [83] demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors similar to GPT-4V on previously unseen images and instructions for the first time. LLaVA-1.5 [81] significantly expands and improves the capabilities by incorporating more academicrelated instruction data, achieving SoTA performance on a dozens of benchmarks with a data-efficient recipe. LLaVA-NeXT [82] inherits this property, further pushing performance boundaries through three key techniques: AnyRes for handling high-resolution images, expanding high-quality instruction data, and utilizing the best open LLM available at the time. \n\nLLaVA-NeXT provides an extendable and scalable prototype, which facilitates several parallel explorations, reported in the LLaVA-NeXT blog series [82,169,65,64,68]: https://llava-vl.github.io/blog/ \u2022 The Video blog [169] shows that the image-only-trained LLaVA-NeXT model is surprisingly strong on video tasks with zero-shot modality transfer, due to the design of AnyRes to digest any vision signals as a sequence of images. \u2022 The Stronger blog [65] demonstrates the LLM model scaling succuss of this cost-efficient strategy. By simply scaling up the LLM, it achieves performance comparable to GPT-4V on selected benchmarks. \n\n\u2022 The Ablation blog [64] summarizes our empirical exploration except the visual instruction data itself, including the choice of architectures (scaling of LLM & vision encoder), visual representations (resolution & #tokens), as well as training strategies (trainable modules & high-quality data) in the pursuit of data scaling success.",
                    "score": 0.54266803324375,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 106
                        },
                        {
                            "start": 107,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 492
                        },
                        {
                            "start": 495,
                            "end": 690
                        },
                        {
                            "start": 691,
                            "end": 896
                        },
                        {
                            "start": 897,
                            "end": 1145
                        },
                        {
                            "start": 1148,
                            "end": 1345
                        },
                        {
                            "start": 1346,
                            "end": 1573
                        },
                        {
                            "start": 1574,
                            "end": 1674
                        },
                        {
                            "start": 1675,
                            "end": 1773
                        },
                        {
                            "start": 1776,
                            "end": 2111
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 701,
                            "end": 705,
                            "matchedPaperCorpusId": "263672058"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82470703125
                },
                {
                    "corpus_id": "271719914",
                    "title": "LLaVA-OneVision: Easy Visual Task Transfer",
                    "text": "\u2022 The Ablation blog [64] summarizes our empirical exploration except the visual instruction data itself, including the choice of architectures (scaling of LLM & vision encoder), visual representations (resolution & #tokens), as well as training strategies (trainable modules & high-quality data) in the pursuit of data scaling success. \u2022 The Interleave blog [68] describes the strategies to extend and improve the capability in new scenarios including multi-image, multi-frame (video) and multi-view (3D), while maintaining the single-image performance. \n\nThese explorations, conducted within a fixed compute budget, aimed to offer useful insights along the way as we navigate the project, rather than push performance limits. During the process, we have also been accumulating and curating a large collection of the high-quality datasets from January to June. By consolidating these insights and execute the experiments with \"yolo run\" on newly accumulated larger datasets, we introduce LLaVA-OneVision. We implement the new model with the available compute, without extensively de-risking individual components. This leaves room for further improvements in capabilities through additional data and model scaling following our recipe, Please see the detailed development timeline in Section A. In particular, our paper makes the following contributions: \n\n\u2022 Large multimodal models. We develop LLaVA-OneVision, a family of open large multimodal models (LMMs) that improves the performance boundaries of open LMMs in three important vision settings, including single-image, multi-image, and video scenarios. \u2022 Emerging Capabilities with Task Transfer. Our design in modeling and data representations allow task transfer across different scenarios, suggesting a simple approach to yield new emgerging capabilities. In particular, LLaVA-OneVision demonstrate strong video understanding through task transfer from images. \u2022 Open-source. To pave the way towards building a general-purpose visual assistant, we release the following assets to the public: the generated multimodal instruction data, the codebase, the model checkpoints, and a visual chat demo.",
                    "score": 0.4017377753501298,
                    "section_title": "Introduction",
                    "char_start_offset": 1791,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 553
                        },
                        {
                            "start": 556,
                            "end": 726
                        },
                        {
                            "start": 727,
                            "end": 860
                        },
                        {
                            "start": 861,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1113
                        },
                        {
                            "start": 1114,
                            "end": 1354
                        },
                        {
                            "start": 1357,
                            "end": 1383
                        },
                        {
                            "start": 1384,
                            "end": 1607
                        },
                        {
                            "start": 1608,
                            "end": 1651
                        },
                        {
                            "start": 1652,
                            "end": 1813
                        },
                        {
                            "start": 1814,
                            "end": 1918
                        },
                        {
                            "start": 1919,
                            "end": 1933
                        },
                        {
                            "start": 1934,
                            "end": 2153
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62890625
                },
                {
                    "corpus_id": "271719914",
                    "title": "LLaVA-OneVision: Easy Visual Task Transfer",
                    "text": "In addition to reporting the LLaVA-OneVision's capabilities across various benchmarks, we also observe the emerging behaviors of the proposed model with task transfer and composition, paving a promising way to generalize to tackle real-world computer vision tasks in the wild. We illustrate several emerging capabilities using examples as below. 9. Given an image and a target video, the model constructs a coherent and vivid narrative for the video, detailing elements such as characters, actions, background settings, and scene specifics. This task leverages both single-image analysis and video comprehension. It is hypothesized that this ability is generalized from the composition of single-image editing instruction task and video detailed description task. \n\nS5: Video-to-Video Difference (Transfer from multi-image and video). Understanding differences in images is a common ability in recent large multimodal models (LMMs), but our models extend this capability to videos. Table 10 showcases LLaVA-OneVision's ability to analyze differences between two video sequences with the same beginning frame but different endings. The model provides a detailed comparison, describing characters, actions, and scene changes. In Table 11, LLaVA-OneVision's describe the differences one by one between videos with a similar background but different main object in the foreground. This task leverages spot the difference in the multi-image analysis to generalize to video scenarios. \n\nS6: Multi-camera Video Understanding in Self-driving (Transfer from single-image and multiimage to video). Understanding videos in a normal aspect ratio is straightforward, what about the videos with multi-views? In Table 12, we observe that LLaVA-OneVision could analyze and interprets multi-camera video footage from self-driving cars. Given video showing four camera views, the model describes each view in detail and plans the ego car's next move. This task combines multi-panel comprehension, video detailed description, and spatial-temporal reasoning. \n\nS7: Composed Sub-video Understanding (Transfer from multi-image to video). Besides multiview video, we see our model generalize to vertical videos with two sub-scenes. Table 13 demonstrates LLaVA-OneVision's ability to understand and describe the content and layout of a composed subvideo.",
                    "score": 0.42552379092656034,
                    "section_title": "Emerging Capabilities with Task Transfer",
                    "char_start_offset": 27257,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 276
                        },
                        {
                            "start": 277,
                            "end": 345
                        },
                        {
                            "start": 346,
                            "end": 540
                        },
                        {
                            "start": 541,
                            "end": 612
                        },
                        {
                            "start": 613,
                            "end": 763
                        },
                        {
                            "start": 766,
                            "end": 834
                        },
                        {
                            "start": 835,
                            "end": 981
                        },
                        {
                            "start": 982,
                            "end": 1130
                        },
                        {
                            "start": 1131,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1376
                        },
                        {
                            "start": 1377,
                            "end": 1478
                        },
                        {
                            "start": 1481,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1693
                        },
                        {
                            "start": 1694,
                            "end": 1818
                        },
                        {
                            "start": 1819,
                            "end": 1932
                        },
                        {
                            "start": 1933,
                            "end": 2038
                        },
                        {
                            "start": 2041,
                            "end": 2115
                        },
                        {
                            "start": 2116,
                            "end": 2208
                        },
                        {
                            "start": 2209,
                            "end": 2330
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.4951171875
                },
                {
                    "corpus_id": "271719914",
                    "title": "LLaVA-OneVision: Easy Visual Task Transfer",
                    "text": "The SoTA proprietary LMMs, such as GPT-4V [109], GPT-4o [110], Gemini [131] and Claude-3.5 [3], exhibit excellent performance in versertile vision scenarios, including single-image, multi-image and video settings. In the open research community, existing works typically develop models tailored to each individual scenario separately. Specifically, most focus on pushing the performance limits in single-image scenarios [26,83,173,73,164,35], only a few recent papers have begun to explore multi-image scenarios [70,47]. While video LMMs excel in video understanding, they often do so at the expense of image performance [72,76]. It is rare to have a single open model that reports excellent performance in all three scenarios. LLaVA-OneVision aims to fill this gap by demonstrating state-of-the-art performance across a broad range of tasks, and showcasing interesting emerging capabilities through cross-scenario task transfer and composition. \n\nTo the best of our knowledge, LLaVA-NeXT-Interleave [68] is the first attempt to report good performance in all three scenarios, LLaVA-OneVision inherits its training recipe and data for improved performance. Other versatial open LMMs with potentials to excel include VILA [77], InternLM-XComposer-2.5 [162]. Unfortunately, their results are not fully evaluated and reported; we compare with them in the experiments. In addition to building systems with versatial capabilities, LLaVA-OneVision is benefited from large-scale high-quality data training, including model-synthesized knowledge and the new collection of diverse instruction tuning data. For the former, we inherit all the knowledge learning data in [64]. For the latter, our are motivated by FLAN [136,88,145]. The data collection process is con-current with Idefics2 [63] and Cambrian-1 [133], but we focus on a smaller but more carefully curated collection of datasets. A similar conclusion is observed: a large amount of visual instruction tuning data can significantly improve performance.",
                    "score": 0.4506260947016788,
                    "section_title": "Related Work",
                    "char_start_offset": 3961,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 213
                        },
                        {
                            "start": 214,
                            "end": 334
                        },
                        {
                            "start": 335,
                            "end": 520
                        },
                        {
                            "start": 521,
                            "end": 629
                        },
                        {
                            "start": 630,
                            "end": 727
                        },
                        {
                            "start": 728,
                            "end": 945
                        },
                        {
                            "start": 948,
                            "end": 1156
                        },
                        {
                            "start": 1157,
                            "end": 1256
                        },
                        {
                            "start": 1257,
                            "end": 1364
                        },
                        {
                            "start": 1365,
                            "end": 1596
                        },
                        {
                            "start": 1597,
                            "end": 1664
                        },
                        {
                            "start": 1665,
                            "end": 1720
                        },
                        {
                            "start": 1721,
                            "end": 1881
                        },
                        {
                            "start": 1882,
                            "end": 2003
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 621,
                            "end": 625,
                            "matchedPaperCorpusId": "265466723"
                        },
                        {
                            "start": 1221,
                            "end": 1225,
                            "matchedPaperCorpusId": "266174746"
                        },
                        {
                            "start": 1712,
                            "end": 1715,
                            "matchedPaperCorpusId": "256415991"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.466552734375
                },
                {
                    "corpus_id": "271719914",
                    "title": "LLaVA-OneVision: Easy Visual Task Transfer",
                    "text": "-Bo Li contributes to maintaining the LLaVA-OneVision codebase, conducting the large-scale training of the LLaVA-OneVision models of all stages (including the stage with single-image, multi-image, and video data), based on upon our previous LLaVA-NeXT series. He contributes significantly to the single-image development such as LLaVA-NeXT-Ablations [64], high-quality recpationing, as well as collection and curation of the single-image data mixture. \n\n-Yuanhan Zhang contributes to a series of works in LLaVA-NeXT-Video [169], including video training and inference codebase, an effective pipeline for high-quality video data generation, and all the video training data. \n\n-Dong Guo contributes to collection and curation of the single-image data mixture and consistently provides technical support throughout the project. \n\n-Feng Li, Renrui Zhang, and Hao Zhang contribute to LLaVA-NeXT-Interleave [68], including the multi-image instruction data mixture, the multi-image evaluation benchmarks, and the early prototype of LLaVA-OneVision, i.e., a joint training stage with single-image, multi-image, and videos. They also contribute to the collection and curation of the single-image data mixture. \n\n-Kaichen Zhang maintains the training codebase and contributes to the integration of LLaVA-OneVision model into LMMs-Eval's evaluation pipeline. \n\n-Yanwei Li contributes to revising the paper. \n\n-Ziwei Liu makes valuable suggestions throughout the projects. \n\n-Chunyuan Li initiates and leads the series of projects, designs the roadmap and milestones, drives the excution, as well as leads the the paper writing.",
                    "score": 0.6353682871620958,
                    "section_title": "B Author Contributions",
                    "char_start_offset": 38848,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 451
                        },
                        {
                            "start": 454,
                            "end": 672
                        },
                        {
                            "start": 675,
                            "end": 824
                        },
                        {
                            "start": 827,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1200
                        },
                        {
                            "start": 1203,
                            "end": 1347
                        },
                        {
                            "start": 1350,
                            "end": 1395
                        },
                        {
                            "start": 1398,
                            "end": 1460
                        },
                        {
                            "start": 1463,
                            "end": 1616
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.447265625
                }
            ],
            "relevance_judgement": 0.84716796875,
            "relevance_judgment_input_expanded": "# Title: LLaVA-OneVision: Easy Visual Task Transfer\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li\n## Abstract\nWe present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.\n## Introduction\nIt is a core aspiration in AI to build general-purpose assistants with Large Multimodal Models (LMM) [67]. LLaVA-OneVision is an open model, continuing to advance the line of research in building large vision-and-language assistant (LLaVA) [83] that can follow diverse instructions to complete a variety of computer vision tasks in the wild. As a cost-efficient recipe, it is typically developed by connecting vision encoders with large language models (LLM) using a simple connection module. \n\nThe first LLaVA model [83] demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors similar to GPT-4V on previously unseen images and instructions for the first time. LLaVA-1.5 [81] significantly expands and improves the capabilities by incorporating more academicrelated instruction data, achieving SoTA performance on a dozens of benchmarks with a data-efficient recipe. LLaVA-NeXT [82] inherits this property, further pushing performance boundaries through three key techniques: AnyRes for handling high-resolution images, expanding high-quality instruction data, and utilizing the best open LLM available at the time. \n\nLLaVA-NeXT provides an extendable and scalable prototype, which facilitates several parallel explorations, reported in the LLaVA-NeXT blog series [82,169,65,64,68]: https://llava-vl.github.io/blog/ \u2022 The Video blog [169] shows that the image-only-trained LLaVA-NeXT model is surprisingly strong on video tasks with zero-shot modality transfer, due to the design of AnyRes to digest any vision signals as a sequence of images. \u2022 The Stronger blog [65] demonstrates the LLM model scaling succuss of this cost-efficient strategy. By simply scaling up the LLM, it achieves performance comparable to GPT-4V on selected benchmarks. \n\n\u2022 The Ablation blog [64] summarizes our empirical exploration except the visual instruction data itself, including the choice of architectures (scaling of LLM & vision encoder), visual representations (resolution & #tokens), as well as training strategies (trainable modules & high-quality data) in the pursuit of data scaling success.\n...\n\u2022 The Ablation blog [64] summarizes our empirical exploration except the visual instruction data itself, including the choice of architectures (scaling of LLM & vision encoder), visual representations (resolution & #tokens), as well as training strategies (trainable modules & high-quality data) in the pursuit of data scaling success. \u2022 The Interleave blog [68] describes the strategies to extend and improve the capability in new scenarios including multi-image, multi-frame (video) and multi-view (3D), while maintaining the single-image performance. \n\nThese explorations, conducted within a fixed compute budget, aimed to offer useful insights along the way as we navigate the project, rather than push performance limits. During the process, we have also been accumulating and curating a large collection of the high-quality datasets from January to June. By consolidating these insights and execute the experiments with \"yolo run\" on newly accumulated larger datasets, we introduce LLaVA-OneVision. We implement the new model with the available compute, without extensively de-risking individual components. This leaves room for further improvements in capabilities through additional data and model scaling following our recipe, Please see the detailed development timeline in Section A. In particular, our paper makes the following contributions: \n\n\u2022 Large multimodal models. We develop LLaVA-OneVision, a family of open large multimodal models (LMMs) that improves the performance boundaries of open LMMs in three important vision settings, including single-image, multi-image, and video scenarios. \u2022 Emerging Capabilities with Task Transfer. Our design in modeling and data representations allow task transfer across different scenarios, suggesting a simple approach to yield new emgerging capabilities. In particular, LLaVA-OneVision demonstrate strong video understanding through task transfer from images. \u2022 Open-source. To pave the way towards building a general-purpose visual assistant, we release the following assets to the public: the generated multimodal instruction data, the codebase, the model checkpoints, and a visual chat demo.\n\n## Related Work\nThe SoTA proprietary LMMs, such as GPT-4V [109], GPT-4o [110], Gemini [131] and Claude-3.5 [3], exhibit excellent performance in versertile vision scenarios, including single-image, multi-image and video settings. In the open research community, existing works typically develop models tailored to each individual scenario separately. Specifically, most focus on pushing the performance limits in single-image scenarios [26,83,173,73,164,35], only a few recent papers have begun to explore multi-image scenarios [70,47]. While video LMMs excel in video understanding, they often do so at the expense of image performance [72,76]. It is rare to have a single open model that reports excellent performance in all three scenarios. LLaVA-OneVision aims to fill this gap by demonstrating state-of-the-art performance across a broad range of tasks, and showcasing interesting emerging capabilities through cross-scenario task transfer and composition. \n\nTo the best of our knowledge, LLaVA-NeXT-Interleave [68] is the first attempt to report good performance in all three scenarios, LLaVA-OneVision inherits its training recipe and data for improved performance. Other versatial open LMMs with potentials to excel include VILA [77], InternLM-XComposer-2.5 [162]. Unfortunately, their results are not fully evaluated and reported; we compare with them in the experiments. In addition to building systems with versatial capabilities, LLaVA-OneVision is benefited from large-scale high-quality data training, including model-synthesized knowledge and the new collection of diverse instruction tuning data. For the former, we inherit all the knowledge learning data in [64]. For the latter, our are motivated by FLAN [136,88,145]. The data collection process is con-current with Idefics2 [63] and Cambrian-1 [133], but we focus on a smaller but more carefully curated collection of datasets. A similar conclusion is observed: a large amount of visual instruction tuning data can significantly improve performance.\n\n## Emerging Capabilities with Task Transfer\nIn addition to reporting the LLaVA-OneVision's capabilities across various benchmarks, we also observe the emerging behaviors of the proposed model with task transfer and composition, paving a promising way to generalize to tackle real-world computer vision tasks in the wild. We illustrate several emerging capabilities using examples as below. 9. Given an image and a target video, the model constructs a coherent and vivid narrative for the video, detailing elements such as characters, actions, background settings, and scene specifics. This task leverages both single-image analysis and video comprehension. It is hypothesized that this ability is generalized from the composition of single-image editing instruction task and video detailed description task. \n\nS5: Video-to-Video Difference (Transfer from multi-image and video). Understanding differences in images is a common ability in recent large multimodal models (LMMs), but our models extend this capability to videos. Table 10 showcases LLaVA-OneVision's ability to analyze differences between two video sequences with the same beginning frame but different endings. The model provides a detailed comparison, describing characters, actions, and scene changes. In Table 11, LLaVA-OneVision's describe the differences one by one between videos with a similar background but different main object in the foreground. This task leverages spot the difference in the multi-image analysis to generalize to video scenarios. \n\nS6: Multi-camera Video Understanding in Self-driving (Transfer from single-image and multiimage to video). Understanding videos in a normal aspect ratio is straightforward, what about the videos with multi-views? In Table 12, we observe that LLaVA-OneVision could analyze and interprets multi-camera video footage from self-driving cars. Given video showing four camera views, the model describes each view in detail and plans the ego car's next move. This task combines multi-panel comprehension, video detailed description, and spatial-temporal reasoning. \n\nS7: Composed Sub-video Understanding (Transfer from multi-image to video). Besides multiview video, we see our model generalize to vertical videos with two sub-scenes. Table 13 demonstrates LLaVA-OneVision's ability to understand and describe the content and layout of a composed subvideo.\n\n## B Author Contributions\n-Bo Li contributes to maintaining the LLaVA-OneVision codebase, conducting the large-scale training of the LLaVA-OneVision models of all stages (including the stage with single-image, multi-image, and video data), based on upon our previous LLaVA-NeXT series. He contributes significantly to the single-image development such as LLaVA-NeXT-Ablations [64], high-quality recpationing, as well as collection and curation of the single-image data mixture. \n\n-Yuanhan Zhang contributes to a series of works in LLaVA-NeXT-Video [169], including video training and inference codebase, an effective pipeline for high-quality video data generation, and all the video training data. \n\n-Dong Guo contributes to collection and curation of the single-image data mixture and consistently provides technical support throughout the project. \n\n-Feng Li, Renrui Zhang, and Hao Zhang contribute to LLaVA-NeXT-Interleave [68], including the multi-image instruction data mixture, the multi-image evaluation benchmarks, and the early prototype of LLaVA-OneVision, i.e., a joint training stage with single-image, multi-image, and videos. They also contribute to the collection and curation of the single-image data mixture. \n\n-Kaichen Zhang maintains the training codebase and contributes to the integration of LLaVA-OneVision model into LMMs-Eval's evaluation pipeline. \n\n-Yanwei Li contributes to revising the paper. \n\n-Ziwei Liu makes valuable suggestions throughout the projects. \n\n-Chunyuan Li initiates and leads the series of projects, designs the roadmap and milestones, drives the excution, as well as leads the the paper writing.",
            "reference_string": "[271719914 | Li et al. | 2024 | Citations: 867]"
        },
        {
            "title": "Revisiting Multi-Modal LLM Evaluation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.05334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316020150",
                    "name": "Jian Lu"
                },
                {
                    "authorId": "2315987232",
                    "name": "Shikhar Srivastava"
                },
                {
                    "authorId": "2315947134",
                    "name": "Junyu Chen"
                },
                {
                    "authorId": "153677280",
                    "name": "Robik Shrestha"
                },
                {
                    "authorId": "47309247",
                    "name": "Manoj Acharya"
                },
                {
                    "authorId": "33315685",
                    "name": "Kushal Kafle"
                },
                {
                    "authorId": "2303258758",
                    "name": "Christopher Kanan"
                }
            ],
            "abstract": "With the advent of multi-modal large language models (MLLMs), datasets used for visual question answering (VQA) and referring expression comprehension have seen a resurgence. However, the most popular datasets used to evaluate MLLMs are some of the earliest ones created, and they have many known problems, including extreme bias, spurious correlations, and an inability to permit fine-grained analysis. In this paper, we pioneer evaluating recent MLLMs (LLaVA 1.5, LLaVA-NeXT, BLIP2, InstructBLIP, GPT-4V, and GPT-4o) on datasets designed to address weaknesses in earlier ones. We assess three VQA datasets: 1) TDIUC, which permits fine-grained analysis on 12 question types; 2) TallyQA, which has simple and complex counting questions; and 3) DVQA, which requires optical character recognition for chart understanding. We also study VQDv1, a dataset that requires identifying all image regions that satisfy a given query. Our experiments reveal the weaknesses of many MLLMs that have not previously been reported. Our code is integrated into the widely used LAVIS framework for MLLM evaluation, enabling the rapid assessment of future MLLMs. Project webpage: https://kevinlujian.github.io/MLLM_Evaluations/",
            "corpus_id": 271855335,
            "sentences": [
                {
                    "corpus_id": "271855335",
                    "title": "Revisiting Multi-Modal LLM Evaluation",
                    "text": "LLaVA-NeXT [24] is an improved version of LLaVA 1.5, with a focus on enhanced visual reasoning, optical character recognition (OCR), and multi-modal document understanding. LLaVA-NeXT scales the input image resolution of input images by 4\u00d7, up to 1344 \u00d7 336 compared to 336 \u00d7 336 in LLaVA 1.5 to enhance its ability to grasp finer-grained visual cues. LLaVA-NeXT is also trained on a more diverse and realistic visual instruction-tuning dataset (ShareGPT-4V and LAION-GPT-V), as well as a range of OCR, document, and chart datasets. We evaluate the 7B parameter version of LLaVA-NeXT. \n\nGPT-4o/GPT-4V [25,26] are closed-weight MLLMs created by OpenAI that enable users to leverage the capability of GPT-4 scale LLMs to analyze visual inputs. GPT-4V is a powerful generalist multi-modal model and can process arbitrarily interleaved image-text data. GPT-4V can perform many visual-language tasks well, including spatial understanding, object localization, and object counting [26]. GPT-4o is reportedly an end-to-end text, vision, and audio multi-modal model, where multi-modal tokens are processed within the same network. GPT-4o has also been reported to improve linguistic and multi-modal understanding. Given that these are closed-source MLLMs, we use the API provided by OpenAI for our evaluations.",
                    "score": 0.36326457561378367,
                    "section_title": "Introduction",
                    "char_start_offset": 3542,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 351
                        },
                        {
                            "start": 352,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 584
                        },
                        {
                            "start": 587,
                            "end": 741
                        },
                        {
                            "start": 742,
                            "end": 848
                        },
                        {
                            "start": 849,
                            "end": 980
                        },
                        {
                            "start": 981,
                            "end": 1122
                        },
                        {
                            "start": 1123,
                            "end": 1205
                        },
                        {
                            "start": 1206,
                            "end": 1302
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.724609375
                }
            ],
            "relevance_judgement": 0.724609375,
            "relevance_judgment_input_expanded": "# Title: Revisiting Multi-Modal LLM Evaluation\n# Venue: arXiv.org\n# Authors: Jian Lu, Shikhar Srivastava, Junyu Chen, Robik Shrestha, Manoj Acharya, Kushal Kafle, Christopher Kanan\n## Abstract\nWith the advent of multi-modal large language models (MLLMs), datasets used for visual question answering (VQA) and referring expression comprehension have seen a resurgence. However, the most popular datasets used to evaluate MLLMs are some of the earliest ones created, and they have many known problems, including extreme bias, spurious correlations, and an inability to permit fine-grained analysis. In this paper, we pioneer evaluating recent MLLMs (LLaVA 1.5, LLaVA-NeXT, BLIP2, InstructBLIP, GPT-4V, and GPT-4o) on datasets designed to address weaknesses in earlier ones. We assess three VQA datasets: 1) TDIUC, which permits fine-grained analysis on 12 question types; 2) TallyQA, which has simple and complex counting questions; and 3) DVQA, which requires optical character recognition for chart understanding. We also study VQDv1, a dataset that requires identifying all image regions that satisfy a given query. Our experiments reveal the weaknesses of many MLLMs that have not previously been reported. Our code is integrated into the widely used LAVIS framework for MLLM evaluation, enabling the rapid assessment of future MLLMs. Project webpage: https://kevinlujian.github.io/MLLM_Evaluations/\n## Introduction\nLLaVA-NeXT [24] is an improved version of LLaVA 1.5, with a focus on enhanced visual reasoning, optical character recognition (OCR), and multi-modal document understanding. LLaVA-NeXT scales the input image resolution of input images by 4\u00d7, up to 1344 \u00d7 336 compared to 336 \u00d7 336 in LLaVA 1.5 to enhance its ability to grasp finer-grained visual cues. LLaVA-NeXT is also trained on a more diverse and realistic visual instruction-tuning dataset (ShareGPT-4V and LAION-GPT-V), as well as a range of OCR, document, and chart datasets. We evaluate the 7B parameter version of LLaVA-NeXT. \n\nGPT-4o/GPT-4V [25,26] are closed-weight MLLMs created by OpenAI that enable users to leverage the capability of GPT-4 scale LLMs to analyze visual inputs. GPT-4V is a powerful generalist multi-modal model and can process arbitrarily interleaved image-text data. GPT-4V can perform many visual-language tasks well, including spatial understanding, object localization, and object counting [26]. GPT-4o is reportedly an end-to-end text, vision, and audio multi-modal model, where multi-modal tokens are processed within the same network. GPT-4o has also been reported to improve linguistic and multi-modal understanding. Given that these are closed-source MLLMs, we use the API provided by OpenAI for our evaluations.",
            "reference_string": "[271855335 | Lu et al. | 2024 | Citations: 3]"
        },
        {
            "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 82,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.05067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313875208",
                    "name": "Jia-Xin Zhao"
                },
                {
                    "authorId": "2342467513",
                    "name": "Boyuan Sun"
                },
                {
                    "authorId": "2339423925",
                    "name": "Xiang Chen"
                },
                {
                    "authorId": "2339268195",
                    "name": "Xihan Wei"
                },
                {
                    "authorId": "2339266488",
                    "name": "Qibin Hou"
                }
            ],
            "abstract": "In this paper, we introduce LLaVA-Octopus, a novel video multimodal large language model. LLaVA-Octopus adaptively weights features from different visual projectors based on user instructions, enabling us to leverage the complementary strengths of each projector. We observe that different visual projectors exhibit distinct characteristics when handling specific tasks. For instance, some projectors excel at capturing static details, while others are more effective at processing temporal information, and some are better suited for tasks requiring temporal coherence. By dynamically adjusting feature weights according to user instructions, LLaVA-Octopus dynamically selects and combines the most suitable features, significantly enhancing the model's performance in multimodal tasks. Experimental results demonstrate that LLaVA-Octopus achieves excellent performance across multiple benchmarks, especially in tasks such as video question answering, long video understanding, and comprehensive multi-choices benchmarks, highlighting its broad application potential.",
            "corpus_id": 275405668,
            "sentences": [
                {
                    "corpus_id": "275405668",
                    "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
                    "text": "Currently, multimodal large language models can be categorized into community models and proprietary models. Proprietary models [3,[52][53][54]64] often achieve better performance but are not open-sourced. Meanwhile, community models [17,23,27,29,30,35,38,39,80,81,85,91], which have seen rapid performance improvements, are garnering increasing attention due to their open-source nature, including model architecture, weights, and even training data. LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously. \n\nBased on the ideas of LLaVA, several variant series have emerged, such as the mPLUG-owl series. mPLUG-owl [80] introduces a new paradigm for training large language models through modularity, and the latest version, mPLUG-owl3 [78], can even understand 2-hour movie videos. BLIP-2 [31] uses Q-Former [88] to connect the visual and linguistic modalities. In BLIP-3 [76], Q-Former is replaced by more scalable visual token samplers, such as perceptual resamplers. We observe that numerous methods have explored various visual projectors. However, to the best of our knowledge, we are the first to classify these projectors and analyze their complementarity.",
                    "score": 0.37910050337997114,
                    "section_title": "Multimodal Large Language Model",
                    "char_start_offset": 6442,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 108
                        },
                        {
                            "start": 109,
                            "end": 205
                        },
                        {
                            "start": 206,
                            "end": 451
                        },
                        {
                            "start": 452,
                            "end": 724
                        },
                        {
                            "start": 725,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 1066
                        },
                        {
                            "start": 1069,
                            "end": 1164
                        },
                        {
                            "start": 1165,
                            "end": 1342
                        },
                        {
                            "start": 1343,
                            "end": 1422
                        },
                        {
                            "start": 1423,
                            "end": 1530
                        },
                        {
                            "start": 1531,
                            "end": 1604
                        },
                        {
                            "start": 1605,
                            "end": 1724
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1350,
                            "end": 1354,
                            "matchedPaperCorpusId": "256390509"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66552734375
                }
            ],
            "relevance_judgement": 0.66552734375,
            "relevance_judgment_input_expanded": "# Title: LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding\n# Venue: arXiv.org\n# Authors: Jia-Xin Zhao, Boyuan Sun, Xiang Chen, Xihan Wei, Qibin Hou\n## Abstract\nIn this paper, we introduce LLaVA-Octopus, a novel video multimodal large language model. LLaVA-Octopus adaptively weights features from different visual projectors based on user instructions, enabling us to leverage the complementary strengths of each projector. We observe that different visual projectors exhibit distinct characteristics when handling specific tasks. For instance, some projectors excel at capturing static details, while others are more effective at processing temporal information, and some are better suited for tasks requiring temporal coherence. By dynamically adjusting feature weights according to user instructions, LLaVA-Octopus dynamically selects and combines the most suitable features, significantly enhancing the model's performance in multimodal tasks. Experimental results demonstrate that LLaVA-Octopus achieves excellent performance across multiple benchmarks, especially in tasks such as video question answering, long video understanding, and comprehensive multi-choices benchmarks, highlighting its broad application potential.\n## Multimodal Large Language Model\nCurrently, multimodal large language models can be categorized into community models and proprietary models. Proprietary models [3,[52][53][54]64] often achieve better performance but are not open-sourced. Meanwhile, community models [17,23,27,29,30,35,38,39,80,81,85,91], which have seen rapid performance improvements, are garnering increasing attention due to their open-source nature, including model architecture, weights, and even training data. LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously. \n\nBased on the ideas of LLaVA, several variant series have emerged, such as the mPLUG-owl series. mPLUG-owl [80] introduces a new paradigm for training large language models through modularity, and the latest version, mPLUG-owl3 [78], can even understand 2-hour movie videos. BLIP-2 [31] uses Q-Former [88] to connect the visual and linguistic modalities. In BLIP-3 [76], Q-Former is replaced by more scalable visual token samplers, such as perceptual resamplers. We observe that numerous methods have explored various visual projectors. However, to the best of our knowledge, we are the first to classify these projectors and analyze their complementarity.",
            "reference_string": "[275405668 | Zhao et al. | 2025 | Citations: 5]"
        },
        {
            "title": "Intriguing Properties of Large Language and Vision Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 87,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.04751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2119566888",
                    "name": "Young-Jun Lee"
                },
                {
                    "authorId": "9726578",
                    "name": "ByungSoo Ko"
                },
                {
                    "authorId": "2118626092",
                    "name": "Han-Gyu Kim"
                },
                {
                    "authorId": "2296345851",
                    "name": "Yechan Hwang"
                },
                {
                    "authorId": "2260282476",
                    "name": "Ho-Jin Choi"
                }
            ],
            "abstract": "Recently, large language and vision models (LLVMs) have received significant attention and development efforts due to their remarkable generalization performance across a wide range of tasks requiring perception and cognitive abilities. A key factor behind their success is their simple architecture, which consists of a vision encoder, a projector, and a large language model (LLM). Despite their achievements in advanced reasoning tasks, their performance on fundamental perception-related tasks (e.g., MMVP) remains surprisingly low. This discrepancy raises the question of how LLVMs truly perceive images and exploit the advantages of the vision encoder. To address this, we systematically investigate this question regarding several aspects: permutation invariance, robustness, math reasoning, alignment preserving and importance, by evaluating the most common LLVM's families (i.e., LLaVA) across 10 evaluation benchmarks. Our extensive experiments reveal several intriguing properties of current LLVMs: (1) they internally process the image in a global manner, even when the order of visual patch sequences is randomly permuted; (2) they are sometimes able to solve math problems without fully perceiving detailed numerical information; (3) the cross-modal alignment is overfitted to complex reasoning tasks, thereby, causing them to lose some of the original perceptual capabilities of their vision encoder; (4) the representation space in the lower layers (<25%) plays a crucial role in determining performance and enhancing visual understanding. Lastly, based on the above observations, we suggest potential future directions for building better LLVMs and constructing more challenging evaluation benchmarks.",
            "corpus_id": 273186838,
            "sentences": [
                {
                    "corpus_id": "273186838",
                    "title": "Intriguing Properties of Large Language and Vision Models",
                    "text": "\u2022 LLaVA-1.5 (Liu et al., 2024a) incorporates academic task-oriented datasets to enhance performance in VQA tasks and features an MLP vision-language connector, which improves upon the original linear layer utilized in LLaVA (Liu et al., 2024c). It uses CLIP ViT-L/14 (Radford et al., 2021) with a 336px resolution as its vision encoder, resulting in a total of (336/14) 2 = 576 visual tokens. LLaVA-1.5 is built on Vicuna with either 7B or 13B parameters. The training dataset includes 558K samples for pre-training and 665K for fine-tuning, totaling 1.2M image-text pairs from publicly available datasets \n\n\u2022 LLaVA-NeXT (Liu et al., 2024b) (also known as LLaVA-1.6) enhances visual reasoning, OCR, and world knowledge, offering four times higher image resolution (up to 1344x336) and improved performance in visual conversations. Its architecture includes a CLIP ViT-L/14 as a vision encoder, paired with Vicuna models ranging from 7B to 34B as a backbone language model. It utilizes 1.3M visual instruction tuning data samples for training, maintaining efficiency with approximately one day of training on 32 A100 GPUs. The architecture's high resolution and dynamic grid scheme improve detailed image processing capabilities. \n\n\u2022 LLaVA-OneVision (Li et al., 2024b) is a LVLM designed for task transfer across singleimage, multi-image, and video scenarios, with strong capabilities in video understanding through image-to-video task transfer. Its architecture consists of a Qwen2 language model (Yang et al., 2024) with 8B to 72B parameters, and the SigLIP vision encoder (Zhai et al., 2023), which processes images at a base resolution of 384x384, producing 729 visual tokens. The model employs a 2-layer MLP projector.",
                    "score": 0.5377326672011338,
                    "section_title": "B DESCRIPTION OF EVALUATION LVLMS",
                    "char_start_offset": 39162,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 244
                        },
                        {
                            "start": 245,
                            "end": 392
                        },
                        {
                            "start": 393,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 605
                        },
                        {
                            "start": 608,
                            "end": 666
                        },
                        {
                            "start": 667,
                            "end": 830
                        },
                        {
                            "start": 831,
                            "end": 972
                        },
                        {
                            "start": 973,
                            "end": 1121
                        },
                        {
                            "start": 1122,
                            "end": 1228
                        },
                        {
                            "start": 1231,
                            "end": 1444
                        },
                        {
                            "start": 1445,
                            "end": 1679
                        },
                        {
                            "start": 1680,
                            "end": 1722
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 12,
                            "end": 31,
                            "matchedPaperCorpusId": "263672058"
                        },
                        {
                            "start": 224,
                            "end": 243,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 267,
                            "end": 289,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 1249,
                            "end": 1266,
                            "matchedPaperCorpusId": "260334888"
                        },
                        {
                            "start": 1574,
                            "end": 1593,
                            "matchedPaperCorpusId": "257767223"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6640625
                }
            ],
            "relevance_judgement": 0.6640625,
            "relevance_judgment_input_expanded": "# Title: Intriguing Properties of Large Language and Vision Models\n# Venue: arXiv.org\n# Authors: Young-Jun Lee, ByungSoo Ko, Han-Gyu Kim, Yechan Hwang, Ho-Jin Choi\n## Abstract\nRecently, large language and vision models (LLVMs) have received significant attention and development efforts due to their remarkable generalization performance across a wide range of tasks requiring perception and cognitive abilities. A key factor behind their success is their simple architecture, which consists of a vision encoder, a projector, and a large language model (LLM). Despite their achievements in advanced reasoning tasks, their performance on fundamental perception-related tasks (e.g., MMVP) remains surprisingly low. This discrepancy raises the question of how LLVMs truly perceive images and exploit the advantages of the vision encoder. To address this, we systematically investigate this question regarding several aspects: permutation invariance, robustness, math reasoning, alignment preserving and importance, by evaluating the most common LLVM's families (i.e., LLaVA) across 10 evaluation benchmarks. Our extensive experiments reveal several intriguing properties of current LLVMs: (1) they internally process the image in a global manner, even when the order of visual patch sequences is randomly permuted; (2) they are sometimes able to solve math problems without fully perceiving detailed numerical information; (3) the cross-modal alignment is overfitted to complex reasoning tasks, thereby, causing them to lose some of the original perceptual capabilities of their vision encoder; (4) the representation space in the lower layers (<25%) plays a crucial role in determining performance and enhancing visual understanding. Lastly, based on the above observations, we suggest potential future directions for building better LLVMs and constructing more challenging evaluation benchmarks.\n## B DESCRIPTION OF EVALUATION LVLMS\n\u2022 LLaVA-1.5 (Liu et al., 2024a) incorporates academic task-oriented datasets to enhance performance in VQA tasks and features an MLP vision-language connector, which improves upon the original linear layer utilized in LLaVA (Liu et al., 2024c). It uses CLIP ViT-L/14 (Radford et al., 2021) with a 336px resolution as its vision encoder, resulting in a total of (336/14) 2 = 576 visual tokens. LLaVA-1.5 is built on Vicuna with either 7B or 13B parameters. The training dataset includes 558K samples for pre-training and 665K for fine-tuning, totaling 1.2M image-text pairs from publicly available datasets \n\n\u2022 LLaVA-NeXT (Liu et al., 2024b) (also known as LLaVA-1.6) enhances visual reasoning, OCR, and world knowledge, offering four times higher image resolution (up to 1344x336) and improved performance in visual conversations. Its architecture includes a CLIP ViT-L/14 as a vision encoder, paired with Vicuna models ranging from 7B to 34B as a backbone language model. It utilizes 1.3M visual instruction tuning data samples for training, maintaining efficiency with approximately one day of training on 32 A100 GPUs. The architecture's high resolution and dynamic grid scheme improve detailed image processing capabilities. \n\n\u2022 LLaVA-OneVision (Li et al., 2024b) is a LVLM designed for task transfer across singleimage, multi-image, and video scenarios, with strong capabilities in video understanding through image-to-video task transfer. Its architecture consists of a Qwen2 language model (Yang et al., 2024) with 8B to 72B parameters, and the SigLIP vision encoder (Zhai et al., 2023), which processes images at a base resolution of 384x384, producing 729 visual tokens. The model employs a 2-layer MLP projector.",
            "reference_string": "[273186838 | Lee et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 73,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.09445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308224151",
                    "name": "Xiaoda Yang"
                },
                {
                    "authorId": "2350336954",
                    "name": "JunYu Lu"
                },
                {
                    "authorId": "2220747584",
                    "name": "Hongshun Qiu"
                },
                {
                    "authorId": "2350180388",
                    "name": "Sijing Li"
                },
                {
                    "authorId": "2349632427",
                    "name": "Hao Li"
                },
                {
                    "authorId": "72890649",
                    "name": "Shengpeng Ji"
                },
                {
                    "authorId": "2349737557",
                    "name": "Xudong Tang"
                },
                {
                    "authorId": "2349670795",
                    "name": "Jiayang Xu"
                },
                {
                    "authorId": "2329894630",
                    "name": "Jiaqi Duan"
                },
                {
                    "authorId": "2112347676",
                    "name": "Ziyue Jiang"
                },
                {
                    "authorId": "2349737916",
                    "name": "Cong Lin"
                },
                {
                    "authorId": "2328348412",
                    "name": "Sihang Cai"
                },
                {
                    "authorId": "2266912737",
                    "name": "Zejian Xie"
                },
                {
                    "authorId": "2352067468",
                    "name": "Zhuoyang Song"
                },
                {
                    "authorId": "2266803682",
                    "name": "Songxin Zhang"
                }
            ],
            "abstract": "Vision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures have emerged as a pivotal paradigm in multimodal understanding, offering a powerful framework for integrating visual and linguistic information. However, the increasing complexity and diversity of tasks present significant challenges in coordinating load balancing across heterogeneous visual experts, where optimizing one specialist's performance often compromises others' capabilities. To address task heterogeneity and expert load imbalance, we propose Astrea, a novel multi-expert collaborative VLM architecture based on progressive pre-alignment. Astrea introduces three key innovations: 1) A heterogeneous expert coordination mechanism that integrates four specialized models (detection, segmentation, classification, captioning) into a comprehensive expert matrix covering essential visual comprehension elements; 2) A dynamic knowledge fusion strategy featuring progressive pre-alignment to harmonize experts within the VLM latent space through contrastive learning, complemented by probabilistically activated stochastic residual connections to preserve knowledge continuity; 3) An enhanced optimization framework utilizing momentum contrastive learning for long-range dependency modeling and adaptive weight allocators for real-time expert contribution calibration. Extensive evaluations across 12 benchmark tasks spanning VQA, image captioning, and cross-modal retrieval demonstrate Astrea's superiority over state-of-the-art models, achieving an average performance gain of +4.7\\%. This study provides the first empirical demonstration that progressive pre-alignment strategies enable VLMs to overcome task heterogeneity limitations, establishing new methodological foundations for developing general-purpose multimodal agents.",
            "corpus_id": 276938164,
            "sentences": [
                {
                    "corpus_id": "276938164",
                    "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
                    "text": "Mainstream Vision-Language Models (VLMs) are typically based on the Transformer architecture of deep learning and achieve multimodal understanding by integrating visual and linguistic features. For example, the mPLUG [29], the Qwen2-VL [58], and the LLaVA-OneVision [28] are all based on the Transformer architecture and incorporate pretraining along with self-supervised learning. mPLUG introduces an effective and efficient vision-language architecture with novel cross-modal skip connections, enabling time-consuming self-attention on the visual side through the creation of inter-layer shortcuts [29]. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, enabling the model to dynamically process images of different resolutions into varying numbers of visual tokens. This allows the model to generate more efficient and accurate visual representations [58]. LLaVA-OneVision simultaneously pushes the performance boundaries of open-domain Large Multimodal Models (LMMs) across three key computer vision scenarios-single image, multiple images, and video-while enabling powerful transfer learning across different modalities and scenarios, resulting in the emergence of new capabilities [28]. InternLM-XComposer-2.5 [70] and MoE-LLaVA [34] both demonstrate strong performance in enhancing the model's multitask adaptability and computational efficiency. InternLM-XComposer-2.5 supports long-context inputs and outputs, maintaining strong text-image understanding capabilities while also expanding the use of additional LoRA parameters for text-image synthesis [70]. MoE-LLaVA employs a mechanism of a mixture of experts, dynamically selecting the most relevant \"expert\" sub-models during inference based on the input. This significantly reduces computational costs while maintaining high performance [34].",
                    "score": 0.40590184160660747,
                    "section_title": "Large Vision-Language Model",
                    "char_start_offset": 4766,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 193
                        },
                        {
                            "start": 194,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 605
                        },
                        {
                            "start": 606,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 869
                        },
                        {
                            "start": 870,
                            "end": 1202
                        },
                        {
                            "start": 1203,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1575
                        },
                        {
                            "start": 1576,
                            "end": 1727
                        },
                        {
                            "start": 1728,
                            "end": 1815
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66064453125
                }
            ],
            "relevance_judgement": 0.66064453125,
            "relevance_judgment_input_expanded": "# Title: Astrea: A MOE-based Visual Understanding Model with Progressive Alignment\n# Venue: arXiv.org\n# Authors: Xiaoda Yang, JunYu Lu, Hongshun Qiu, Sijing Li, Hao Li, Shengpeng Ji, Xudong Tang, Jiayang Xu, Jiaqi Duan, Ziyue Jiang, Cong Lin, Sihang Cai, Zejian Xie, Zhuoyang Song, Songxin Zhang\n## Abstract\nVision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures have emerged as a pivotal paradigm in multimodal understanding, offering a powerful framework for integrating visual and linguistic information. However, the increasing complexity and diversity of tasks present significant challenges in coordinating load balancing across heterogeneous visual experts, where optimizing one specialist's performance often compromises others' capabilities. To address task heterogeneity and expert load imbalance, we propose Astrea, a novel multi-expert collaborative VLM architecture based on progressive pre-alignment. Astrea introduces three key innovations: 1) A heterogeneous expert coordination mechanism that integrates four specialized models (detection, segmentation, classification, captioning) into a comprehensive expert matrix covering essential visual comprehension elements; 2) A dynamic knowledge fusion strategy featuring progressive pre-alignment to harmonize experts within the VLM latent space through contrastive learning, complemented by probabilistically activated stochastic residual connections to preserve knowledge continuity; 3) An enhanced optimization framework utilizing momentum contrastive learning for long-range dependency modeling and adaptive weight allocators for real-time expert contribution calibration. Extensive evaluations across 12 benchmark tasks spanning VQA, image captioning, and cross-modal retrieval demonstrate Astrea's superiority over state-of-the-art models, achieving an average performance gain of +4.7\\%. This study provides the first empirical demonstration that progressive pre-alignment strategies enable VLMs to overcome task heterogeneity limitations, establishing new methodological foundations for developing general-purpose multimodal agents.\n## Large Vision-Language Model\nMainstream Vision-Language Models (VLMs) are typically based on the Transformer architecture of deep learning and achieve multimodal understanding by integrating visual and linguistic features. For example, the mPLUG [29], the Qwen2-VL [58], and the LLaVA-OneVision [28] are all based on the Transformer architecture and incorporate pretraining along with self-supervised learning. mPLUG introduces an effective and efficient vision-language architecture with novel cross-modal skip connections, enabling time-consuming self-attention on the visual side through the creation of inter-layer shortcuts [29]. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, enabling the model to dynamically process images of different resolutions into varying numbers of visual tokens. This allows the model to generate more efficient and accurate visual representations [58]. LLaVA-OneVision simultaneously pushes the performance boundaries of open-domain Large Multimodal Models (LMMs) across three key computer vision scenarios-single image, multiple images, and video-while enabling powerful transfer learning across different modalities and scenarios, resulting in the emergence of new capabilities [28]. InternLM-XComposer-2.5 [70] and MoE-LLaVA [34] both demonstrate strong performance in enhancing the model's multitask adaptability and computational efficiency. InternLM-XComposer-2.5 supports long-context inputs and outputs, maintaining strong text-image understanding capabilities while also expanding the use of additional LoRA parameters for text-image synthesis [70]. MoE-LLaVA employs a mechanism of a mixture of experts, dynamically selecting the most relevant \"expert\" sub-models during inference based on the input. This significantly reduces computational costs while maintaining high performance [34].",
            "reference_string": "[276938164 | Yang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.19552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305201677",
                    "name": "Hosam Elgendy"
                },
                {
                    "authorId": "2265656556",
                    "name": "Ahmed Sharshar"
                },
                {
                    "authorId": "2292596478",
                    "name": "Ahmed Aboeitta"
                },
                {
                    "authorId": "2292603520",
                    "name": "Yasser Ashraf"
                },
                {
                    "authorId": "2327863134",
                    "name": "Mohsen Guizani"
                }
            ],
            "abstract": "Detecting temporal changes in geographical landscapes is critical for applications like environmental monitoring and urban planning. While remote sensing data is abundant, existing vision-language models (VLMs) often fail to capture temporal dynamics effectively. This paper addresses these limitations by introducing an annotated dataset of video frame pairs to track evolving geographical patterns over time. Using fine-tuning techniques like Low-Rank Adaptation (LoRA), quantized LoRA (QLoRA), and model pruning on models such as Video-LLaVA and LLaVA-NeXT-Video, we significantly enhance VLM performance in processing remote sensing temporal changes. Results show significant improvements, with the best performance achieving a BERT score of 0.864 and ROUGE-1 score of 0.576, demonstrating superior accuracy in describing land-use transformations.",
            "corpus_id": 273638057,
            "sentences": [
                {
                    "corpus_id": "273638057",
                    "title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing",
                    "text": "Pre-training VLMs is typically computationally intensive and timeconsuming. Consequently, fine-tuning presents an effective alternative that preserves most of the model's parameters while enhancing performance on downstream tasks. Fine-tuned models can often outperform the original general models, utilizing fewer computing resources and requiring less training time [25]. This advantage motivates the use of Parameter-Efficient Fine-Tuning (PEFT) methods for tasks involving geographical change detection. \n\nIn our work, we focus on fine-tuning two distinct models that have demonstrated a robust understanding of temporal data through video processing within the VLM framework for question-answering and captioning. The first model, LLaVA-NeXT [20], was introduced in early 2024, offering improved reasoning and world knowledge compared to other large models. It exhibits data efficiency comparable to SOTA models such as LLaVA-1.5 [19], while delivering higher image resolution and enhanced visual conversation capabilities. Shortly after the release of LLaVA-NeXT, a video variant was introduced, named LLaVA-NeXT-Video, which has demonstrated strong performance in zero-shot video tasks. \n\nThe second model utilized for comparison is Video-LLaVA [15], which excels in understanding visual language for downstream tasks and surpasses many existing video language models across various  benchmarks. Both projects have multiple variations based on the number of parameters for the models. For simplicity, we have chosen to use the 7B parameter variation from both models. The 7B variations can be fine-tuned with PEFT techniques on a single GPU, making them particularly well-suited for our dataset.",
                    "score": 0.3588745246307341,
                    "section_title": "Model Fine-tuning",
                    "char_start_offset": 11891,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 75
                        },
                        {
                            "start": 76,
                            "end": 230
                        },
                        {
                            "start": 231,
                            "end": 373
                        },
                        {
                            "start": 374,
                            "end": 507
                        },
                        {
                            "start": 510,
                            "end": 718
                        },
                        {
                            "start": 719,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1193
                        },
                        {
                            "start": 1196,
                            "end": 1402
                        },
                        {
                            "start": 1403,
                            "end": 1491
                        },
                        {
                            "start": 1492,
                            "end": 1574
                        },
                        {
                            "start": 1575,
                            "end": 1702
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 368,
                            "end": 372,
                            "matchedPaperCorpusId": "268157336"
                        },
                        {
                            "start": 935,
                            "end": 939,
                            "matchedPaperCorpusId": "263672058"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6494140625
                }
            ],
            "relevance_judgement": 0.6494140625,
            "relevance_judgment_input_expanded": "# Title: GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing\n# Venue: arXiv.org\n# Authors: Hosam Elgendy, Ahmed Sharshar, Ahmed Aboeitta, Yasser Ashraf, Mohsen Guizani\n## Abstract\nDetecting temporal changes in geographical landscapes is critical for applications like environmental monitoring and urban planning. While remote sensing data is abundant, existing vision-language models (VLMs) often fail to capture temporal dynamics effectively. This paper addresses these limitations by introducing an annotated dataset of video frame pairs to track evolving geographical patterns over time. Using fine-tuning techniques like Low-Rank Adaptation (LoRA), quantized LoRA (QLoRA), and model pruning on models such as Video-LLaVA and LLaVA-NeXT-Video, we significantly enhance VLM performance in processing remote sensing temporal changes. Results show significant improvements, with the best performance achieving a BERT score of 0.864 and ROUGE-1 score of 0.576, demonstrating superior accuracy in describing land-use transformations.\n## Model Fine-tuning\nPre-training VLMs is typically computationally intensive and timeconsuming. Consequently, fine-tuning presents an effective alternative that preserves most of the model's parameters while enhancing performance on downstream tasks. Fine-tuned models can often outperform the original general models, utilizing fewer computing resources and requiring less training time [25]. This advantage motivates the use of Parameter-Efficient Fine-Tuning (PEFT) methods for tasks involving geographical change detection. \n\nIn our work, we focus on fine-tuning two distinct models that have demonstrated a robust understanding of temporal data through video processing within the VLM framework for question-answering and captioning. The first model, LLaVA-NeXT [20], was introduced in early 2024, offering improved reasoning and world knowledge compared to other large models. It exhibits data efficiency comparable to SOTA models such as LLaVA-1.5 [19], while delivering higher image resolution and enhanced visual conversation capabilities. Shortly after the release of LLaVA-NeXT, a video variant was introduced, named LLaVA-NeXT-Video, which has demonstrated strong performance in zero-shot video tasks. \n\nThe second model utilized for comparison is Video-LLaVA [15], which excels in understanding visual language for downstream tasks and surpasses many existing video language models across various  benchmarks. Both projects have multiple variations based on the number of parameters for the models. For simplicity, we have chosen to use the 7B parameter variation from both models. The 7B variations can be fine-tuned with PEFT techniques on a single GPU, making them particularly well-suited for our dataset.",
            "reference_string": "[273638057 | Elgendy et al. | 2024 | Citations: 3]"
        },
        {
            "title": "When Less is Enough: Adaptive Token Reduction for Efficient Image Representation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 32,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.16660, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2351602796",
                    "name": "Eduard Allakhverdov"
                },
                {
                    "authorId": "2266389312",
                    "name": "Elizaveta Goncharova"
                },
                {
                    "authorId": "2331330907",
                    "name": "Andrey Kuznetsov"
                }
            ],
            "abstract": "Vision encoders typically generate a large number of visual tokens, providing information-rich representations but significantly increasing computational demands. This raises the question of whether all generated tokens are equally valuable or if some of them can be discarded to reduce computational costs without compromising quality. In this paper, we introduce a new method for determining feature utility based on the idea that less valuable features can be reconstructed from more valuable ones. We implement this concept by integrating an autoencoder with a Gumbel-Softmax selection mechanism, that allows identifying and retaining only the most informative visual tokens. To validate our approach, we compared the performance of the LLaVA-NeXT model, using features selected by our method with randomly selected features. We found that on OCR-based tasks, more than 50% of the visual context can be removed with minimal performance loss, whereas randomly discarding the same proportion of features significantly affects the model capabilities. Furthermore, in general-domain tasks, even randomly retaining only 30% of tokens achieves performance comparable to using the full set of visual tokens. Our results highlight a promising direction towards adaptive and efficient multimodal pruning that facilitates scalable and low-overhead inference without compromising performance.",
            "corpus_id": 277244459,
            "sentences": [
                {
                    "corpus_id": "277244459",
                    "title": "When Less is Enough: Adaptive Token Reduction for Efficient Image Representation",
                    "text": "The LLaVA-OneVision model includes a built-in compression mechanism, which reduces the size of the visual feature tensor through interpolation. In the LLaVA-OneVision implementation, interpolation compression is activated from a predefined reference size, and the compression ratio varies depending on the input image dimensions. For example, larger documents typically result in compression ratios of 1.7 to 2. \n\nSince our main goal is to demonstrate that our proposed selector effectively identifies informative visual features, we disabled the default compression mechanism in LLaVA-OneVision instead of combining two distinct compression methods. \n\nDuring preliminary experiments, we noticed that if the visual context exceeds the internally fixed context length of the model, the performance tends to degrade. Therefore, we limited our experiments to the range p \u2208 [0.1, 0.2, 0.3, 0.4, 0.5, 0.6 \u2248 1 1.7 ], ensuring that the length of the retained visual features does not exceed approximately 1  1.7 the original input length. Similar to the LLaVA-NeXT evaluations, the results for LLaVA-OneVision are divided into two categories, as depicted in Figs. 6 and 7. \n\nFor OCR-based benchmarks involving high-resolution images (Fig. 6), our trained selector consistently outperforms random selection, proving its ability to effectively preserve informative visual features. \n\nIn contrast, for non-OCR benchmarks (Fig. 7), the results are very similar to those observed previously with LLaVA-NeXT (Fig. 5), indicating that the performance is largely unaffected by the choice of selection method in these sce-",
                    "score": 0.4662800880335053,
                    "section_title": "LLaVA-OneVision: Our Selector vs. Random Selector",
                    "char_start_offset": 16867,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 329
                        },
                        {
                            "start": 330,
                            "end": 411
                        },
                        {
                            "start": 414,
                            "end": 650
                        },
                        {
                            "start": 653,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 1031
                        },
                        {
                            "start": 1032,
                            "end": 1165
                        },
                        {
                            "start": 1168,
                            "end": 1372
                        },
                        {
                            "start": 1375,
                            "end": 1606
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6474609375
                },
                {
                    "corpus_id": "277244459",
                    "title": "When Less is Enough: Adaptive Token Reduction for Efficient Image Representation",
                    "text": "To evaluate the performance of the proposed feature selector, we integrated it with vision encoders used as backbones in two multimodal models: LLaVA-NeXT (visual encoder of CLIP [26]) and LLaVA-OneVision (visual encoder of SigLIP [36]). Our primary goal was to investigate whether the selector described in Sec. 4.1.1 can be applied directly (i.e., without further fine-tuning) to existing multimodal architectures, and to determine whether selecting informative features positively impacts model inference. To this end, we evaluated both models augmented with our trained feature selector under different pruning factors, comparing their performance to a baseline using random feature selection with the same pruning factors across multiple multimodal benchmarks.",
                    "score": 0.35691653343547025,
                    "section_title": "Experiments",
                    "char_start_offset": 14040,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 765
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.48291015625
                }
            ],
            "relevance_judgement": 0.6474609375,
            "relevance_judgment_input_expanded": "# Title: When Less is Enough: Adaptive Token Reduction for Efficient Image Representation\n# Venue: arXiv.org\n# Authors: Eduard Allakhverdov, Elizaveta Goncharova, Andrey Kuznetsov\n## Abstract\nVision encoders typically generate a large number of visual tokens, providing information-rich representations but significantly increasing computational demands. This raises the question of whether all generated tokens are equally valuable or if some of them can be discarded to reduce computational costs without compromising quality. In this paper, we introduce a new method for determining feature utility based on the idea that less valuable features can be reconstructed from more valuable ones. We implement this concept by integrating an autoencoder with a Gumbel-Softmax selection mechanism, that allows identifying and retaining only the most informative visual tokens. To validate our approach, we compared the performance of the LLaVA-NeXT model, using features selected by our method with randomly selected features. We found that on OCR-based tasks, more than 50% of the visual context can be removed with minimal performance loss, whereas randomly discarding the same proportion of features significantly affects the model capabilities. Furthermore, in general-domain tasks, even randomly retaining only 30% of tokens achieves performance comparable to using the full set of visual tokens. Our results highlight a promising direction towards adaptive and efficient multimodal pruning that facilitates scalable and low-overhead inference without compromising performance.\n## Experiments\nTo evaluate the performance of the proposed feature selector, we integrated it with vision encoders used as backbones in two multimodal models: LLaVA-NeXT (visual encoder of CLIP [26]) and LLaVA-OneVision (visual encoder of SigLIP [36]). Our primary goal was to investigate whether the selector described in Sec. 4.1.1 can be applied directly (i.e., without further fine-tuning) to existing multimodal architectures, and to determine whether selecting informative features positively impacts model inference. To this end, we evaluated both models augmented with our trained feature selector under different pruning factors, comparing their performance to a baseline using random feature selection with the same pruning factors across multiple multimodal benchmarks.\n\n## LLaVA-OneVision: Our Selector vs. Random Selector\nThe LLaVA-OneVision model includes a built-in compression mechanism, which reduces the size of the visual feature tensor through interpolation. In the LLaVA-OneVision implementation, interpolation compression is activated from a predefined reference size, and the compression ratio varies depending on the input image dimensions. For example, larger documents typically result in compression ratios of 1.7 to 2. \n\nSince our main goal is to demonstrate that our proposed selector effectively identifies informative visual features, we disabled the default compression mechanism in LLaVA-OneVision instead of combining two distinct compression methods. \n\nDuring preliminary experiments, we noticed that if the visual context exceeds the internally fixed context length of the model, the performance tends to degrade. Therefore, we limited our experiments to the range p \u2208 [0.1, 0.2, 0.3, 0.4, 0.5, 0.6 \u2248 1 1.7 ], ensuring that the length of the retained visual features does not exceed approximately 1  1.7 the original input length. Similar to the LLaVA-NeXT evaluations, the results for LLaVA-OneVision are divided into two categories, as depicted in Figs. 6 and 7. \n\nFor OCR-based benchmarks involving high-resolution images (Fig. 6), our trained selector consistently outperforms random selection, proving its ability to effectively preserve informative visual features. \n\nIn contrast, for non-OCR benchmarks (Fig. 7), the results are very similar to those observed previously with LLaVA-NeXT (Fig. 5), indicating that the performance is largely unaffected by the choice of selection method in these sce-",
            "reference_string": "[277244459 | Allakhverdov et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Repurposing the scientific literature with vision-language models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 47,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.19546, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2328643882",
                    "name": "Anton Alyakin"
                },
                {
                    "authorId": "2325728846",
                    "name": "Jaden Stryker"
                },
                {
                    "authorId": "1941269137",
                    "name": "D. Alber"
                },
                {
                    "authorId": "2248607147",
                    "name": "Karl L. Sangwon"
                },
                {
                    "authorId": "2347532646",
                    "name": "Brandon Duderstadt"
                },
                {
                    "authorId": "2269042919",
                    "name": "Akshay V. Save"
                },
                {
                    "authorId": "5861736",
                    "name": "David B. Kurland"
                },
                {
                    "authorId": "2331397171",
                    "name": "Spencer Frome"
                },
                {
                    "authorId": "2347581153",
                    "name": "Shrutika Singh"
                },
                {
                    "authorId": "2347550668",
                    "name": "Jeff Zhang"
                },
                {
                    "authorId": "2321510385",
                    "name": "Eunice Yang"
                },
                {
                    "authorId": "2347589607",
                    "name": "Ki Yun Park"
                },
                {
                    "authorId": "6068765",
                    "name": "C. Orillac"
                },
                {
                    "authorId": "49558131",
                    "name": "Aly A. Valliani"
                },
                {
                    "authorId": "2321514213",
                    "name": "Sean Neifert"
                },
                {
                    "authorId": "2148770758",
                    "name": "Albert Liu"
                },
                {
                    "authorId": "2273924323",
                    "name": "Aneek Patel"
                },
                {
                    "authorId": "2219711063",
                    "name": "Christopher Livia"
                },
                {
                    "authorId": "2291574490",
                    "name": "Darryl Lau"
                },
                {
                    "authorId": "2347528853",
                    "name": "Ilya Laufer"
                },
                {
                    "authorId": "47034244",
                    "name": "P. Rozman"
                },
                {
                    "authorId": "39243010",
                    "name": "E. Hidalgo"
                },
                {
                    "authorId": "145327270",
                    "name": "H. Riina"
                },
                {
                    "authorId": "2281737315",
                    "name": "Rui Feng"
                },
                {
                    "authorId": "2249367961",
                    "name": "T. Hollon"
                },
                {
                    "authorId": "8340776",
                    "name": "Yindalon Aphinyanagphongs"
                },
                {
                    "authorId": "6473601",
                    "name": "J. Golfinos"
                },
                {
                    "authorId": "2347529122",
                    "name": "Laura Snyder"
                },
                {
                    "authorId": "2334000180",
                    "name": "Eric C. Leuthardt"
                },
                {
                    "authorId": "2247415145",
                    "name": "Douglas Kondziolka"
                },
                {
                    "authorId": "2181708076",
                    "name": "E. Oermann"
                }
            ],
            "abstract": "Leading vision-language models (VLMs) are trained on general Internet content, overlooking scientific journals' rich, domain-specific knowledge. Training on specialty-specific literature could yield high-performance, task-specific tools, enabling generative AI to match generalist models in specialty publishing, educational, and clinical tasks. We created NeuroPubs, a multimodal dataset of 23,000 Neurosurgery Publications articles (134M words, 78K image-caption pairs). Using NeuroPubs, VLMs generated publication-ready graphical abstracts (70% of 100 abstracts) and board-style questions indistinguishable from human-written ones (54% of 89,587 questions). We used these questions to train CNS-Obsidian, a 34B-parameter VLM. In a blinded, randomized controlled trial, our model demonstrated non-inferiority to then state-of-the-art GPT-4o in neurosurgical differential diagnosis (clinical utility, 40.62% upvotes vs. 57.89%, p=0.1150; accuracy, 59.38% vs. 65.79%, p=0.3797). Our pilot study demonstrates how training generative AI models on specialty-specific journal content - without large-scale internet data - results in high-performance academic and clinical tools, enabling domain-tailored AI across diverse fields.",
            "corpus_id": 276647578,
            "sentences": [
                {
                    "corpus_id": "276647578",
                    "title": "Repurposing the scientific literature with vision-language models",
                    "text": "LLaVA (Large Language and Vision Assistant) combines vision and language processing by aligning CLIP-derived image features with language embeddings, enabling interactive image understanding. We built on LLaVA-Next's improvements -including its multilayer projection, patch-based processing of large images, and enhanced pre-training (Fig. S6a). As a starting point, we used the 34B-parameter version of LLaVA-Next, built on Nous Hermes 2 Yi-34B, available on HuggingFace Transformers.",
                    "score": 0.4076992264536705,
                    "section_title": "Model Architecture and Training Vision-Language Model Backbone",
                    "char_start_offset": 22355,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 345
                        },
                        {
                            "start": 346,
                            "end": 485
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6435546875
                }
            ],
            "relevance_judgement": 0.6435546875,
            "relevance_judgment_input_expanded": "# Title: Repurposing the scientific literature with vision-language models\n# Venue: arXiv.org\n# Authors: Anton Alyakin, Jaden Stryker, D. Alber, Karl L. Sangwon, Brandon Duderstadt, Akshay V. Save, David B. Kurland, Spencer Frome, Shrutika Singh, Jeff Zhang, Eunice Yang, Ki Yun Park, C. Orillac, Aly A. Valliani, Sean Neifert, Albert Liu, Aneek Patel, Christopher Livia, Darryl Lau, Ilya Laufer, P. Rozman, E. Hidalgo, H. Riina, Rui Feng, T. Hollon, Yindalon Aphinyanagphongs, J. Golfinos, Laura Snyder, Eric C. Leuthardt, Douglas Kondziolka, E. Oermann\n## Abstract\nLeading vision-language models (VLMs) are trained on general Internet content, overlooking scientific journals' rich, domain-specific knowledge. Training on specialty-specific literature could yield high-performance, task-specific tools, enabling generative AI to match generalist models in specialty publishing, educational, and clinical tasks. We created NeuroPubs, a multimodal dataset of 23,000 Neurosurgery Publications articles (134M words, 78K image-caption pairs). Using NeuroPubs, VLMs generated publication-ready graphical abstracts (70% of 100 abstracts) and board-style questions indistinguishable from human-written ones (54% of 89,587 questions). We used these questions to train CNS-Obsidian, a 34B-parameter VLM. In a blinded, randomized controlled trial, our model demonstrated non-inferiority to then state-of-the-art GPT-4o in neurosurgical differential diagnosis (clinical utility, 40.62% upvotes vs. 57.89%, p=0.1150; accuracy, 59.38% vs. 65.79%, p=0.3797). Our pilot study demonstrates how training generative AI models on specialty-specific journal content - without large-scale internet data - results in high-performance academic and clinical tools, enabling domain-tailored AI across diverse fields.\n## Model Architecture and Training Vision-Language Model Backbone\nLLaVA (Large Language and Vision Assistant) combines vision and language processing by aligning CLIP-derived image features with language embeddings, enabling interactive image understanding. We built on LLaVA-Next's improvements -including its multilayer projection, patch-based processing of large images, and enhanced pre-training (Fig. S6a). As a starting point, we used the 34B-parameter version of LLaVA-Next, built on Nous Hermes 2 Yi-34B, available on HuggingFace Transformers.",
            "reference_string": "[276647578 | Alyakin et al. | 2025 | Citations: 0]"
        },
        {
            "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.11066, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2173939633",
                    "name": "Tingyu Qu"
                },
                {
                    "authorId": "2112132080",
                    "name": "Mingxiao Li"
                },
                {
                    "authorId": "1704728",
                    "name": "T. Tuytelaars"
                },
                {
                    "authorId": "1802161",
                    "name": "M. Moens"
                }
            ],
            "abstract": "Recent advances in multimodal Large Language Models (LLMs) have shown great success in understanding multi-modal contents. For video understanding tasks, training-based video LLMs are difficult to build due to the scarcity of high-quality, curated video-text paired data. In contrast, paired image-text data are much easier to obtain, and there is substantial similarity between images and videos. Consequently, extending image LLMs for video understanding tasks presents an appealing alternative. Developing effective strategies for compressing visual tokens from multiple frames is a promising way to leverage the powerful pre-trained image LLM. In this work, we explore the limitations of the existing compression strategies for building a training-free video LLM. The findings lead to our method TS-LLaVA, which constructs visual tokens through a Thumbnail-and-Sampling strategy. Given a video, we select few equidistant frames from all input frames to construct a Thumbnail image as a detailed visual cue, complemented by Sampled visual tokens from all input frames. Our method establishes the new state-of-the-art performance among training-free video LLMs on various benchmarks. Notably, our 34B model outperforms GPT-4V on the MVBench benchmark, and achieves performance comparable to the 72B training-based video LLM, Video-LLaMA2, on the challenging MLVU benchmark. Code is available at https://github.com/tingyu215/TS-LLaVA.",
            "corpus_id": 274131643,
            "sentences": [
                {
                    "corpus_id": "274131643",
                    "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models",
                    "text": "Image LLMs aim to bridge the representation space between the vision encoder [33] and LLM [1,6,12,37] using image-text data. The BLIP family, including BLIP-2 [14] and InstructBLIP [7], employs a Querying Transformer (QFormer) to connect vision and language modalities via learnable queries in cross-attention modules. QFormer is also adopted by follow-up works, such as QWen-VL [2] and mPLUG-Owl [46]. QWen-VL incorporates interleaved image-text data in a three-stage training pipeline, while mPLUG-Owl adopts a modularized learning approach. The LLaVA family, in contrast, uses an MLP for connecting the vision encoder and LLM. LLaVA [21] leverages GPT-4 [31] generated visual instruction data for fine-tuning. Subsequent versions, LLaVA-v1.5 [22] and LLaVA-v1.6(NeXT) [23] further improve LLaVA's performance through better data, higher resolution and stronger LLM. As a concurrent work, MiniGPT-4 [54] aligns a frozen LLM with frozen ViT [8] and QFormer using a trainable linear layer to enable multimodal instruction following. \n\nTraining-based video LLMs are further trained on massive video data to endow image LLMs or LLMs with video understanding capabilities. Video-ChatGPT [26] uses LLaVA as its backbone, applying separate temporal and spatial pooling to visual features. The model further utilizes 100k video instruction tuning data. VideoChat [16] leverages QFormer for video token compression, and VideoChat2 [17] refines this approach with improved vision-language alignment and instruction tuning. It also introduces the multitask video understanding benchmark, MVBench. Video-LLaVA [20] learns a shared projector for image and video encoders. Video-LLaMA [49] and Video-LLaMA2 [5] incorporate video, audio and language modalities to support tasks oriented toward video and audio.",
                    "score": 0.40107972514250273,
                    "section_title": "Related Works",
                    "char_start_offset": 3113,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 124
                        },
                        {
                            "start": 125,
                            "end": 318
                        },
                        {
                            "start": 319,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 543
                        },
                        {
                            "start": 544,
                            "end": 629
                        },
                        {
                            "start": 630,
                            "end": 712
                        },
                        {
                            "start": 713,
                            "end": 770
                        },
                        {
                            "start": 771,
                            "end": 868
                        },
                        {
                            "start": 869,
                            "end": 1032
                        },
                        {
                            "start": 1035,
                            "end": 1169
                        },
                        {
                            "start": 1170,
                            "end": 1283
                        },
                        {
                            "start": 1284,
                            "end": 1346
                        },
                        {
                            "start": 1347,
                            "end": 1514
                        },
                        {
                            "start": 1515,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1660
                        },
                        {
                            "start": 1661,
                            "end": 1797
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 77,
                            "end": 81,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 159,
                            "end": 163,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 181,
                            "end": 184,
                            "matchedPaperCorpusId": "258615266"
                        },
                        {
                            "start": 636,
                            "end": 640,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 745,
                            "end": 749,
                            "matchedPaperCorpusId": "263672058"
                        },
                        {
                            "start": 942,
                            "end": 945,
                            "matchedPaperCorpusId": "225039882"
                        },
                        {
                            "start": 1184,
                            "end": 1188,
                            "matchedPaperCorpusId": "259108333"
                        },
                        {
                            "start": 1424,
                            "end": 1428,
                            "matchedPaperCorpusId": "265466214"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64013671875
                },
                {
                    "corpus_id": "274131643",
                    "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models",
                    "text": "Video-LLaMA [49] and Video-LLaMA2 [5] incorporate video, audio and language modalities to support tasks oriented toward video and audio. LLaVA-NeXT-Video [52] fine-tunes LLaVA-NeXT on video data, with a variant that applies DPO [34] for improved performance. LITA [11] employs a slow-fast design [9,41] to capture spatial and temporal information more effectively. \n\nTraining-free video LLMs extend image LLMs for video understanding without requiring additional fine-tuning on video data. As a pioneering approach, IG-VLM [13] constructs a grid-view image from video frames, which is then fed directly into a frozen image LLM with a specially designed prompt. While promising, the image grid approach has limitations, such as reduced resolution and the limited number of frames it can include, which we further discuss in the next section. FreeVA [40] explores various temporal aggregation methods, but similarly uses a limited number of frames. The current state-of-the-art SF-LLaVA [45] adopts the slow-fast design, which is proven to be effective in action recognition [9,41], and in LITA, as mentioned earlier. SF-LLaVA designs a slow pathway compressing fewer frames, and a fast pathway heavily compressing more frames. While both SF-LLaVA and our method use a twostream design, our Thumbnail-and-Sampling strategy leads to significantly better performance on various video understanding benchmarks, while maintaining a better token efficiency. We extend the discussion in Sec. 4.",
                    "score": 0.3948997097959168,
                    "section_title": "Related Works",
                    "char_start_offset": 4774,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 137,
                            "end": 258
                        },
                        {
                            "start": 259,
                            "end": 364
                        },
                        {
                            "start": 367,
                            "end": 489
                        },
                        {
                            "start": 490,
                            "end": 660
                        },
                        {
                            "start": 661,
                            "end": 840
                        },
                        {
                            "start": 841,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1115
                        },
                        {
                            "start": 1116,
                            "end": 1225
                        },
                        {
                            "start": 1226,
                            "end": 1450
                        },
                        {
                            "start": 1451,
                            "end": 1486
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 228,
                            "end": 232,
                            "matchedPaperCorpusId": "258959321"
                        },
                        {
                            "start": 296,
                            "end": 299,
                            "matchedPaperCorpusId": "54463801"
                        },
                        {
                            "start": 1073,
                            "end": 1076,
                            "matchedPaperCorpusId": "54463801"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.625
                }
            ],
            "relevance_judgement": 0.64013671875,
            "relevance_judgment_input_expanded": "# Title: TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models\n# Venue: arXiv.org\n# Authors: Tingyu Qu, Mingxiao Li, T. Tuytelaars, M. Moens\n## Abstract\nRecent advances in multimodal Large Language Models (LLMs) have shown great success in understanding multi-modal contents. For video understanding tasks, training-based video LLMs are difficult to build due to the scarcity of high-quality, curated video-text paired data. In contrast, paired image-text data are much easier to obtain, and there is substantial similarity between images and videos. Consequently, extending image LLMs for video understanding tasks presents an appealing alternative. Developing effective strategies for compressing visual tokens from multiple frames is a promising way to leverage the powerful pre-trained image LLM. In this work, we explore the limitations of the existing compression strategies for building a training-free video LLM. The findings lead to our method TS-LLaVA, which constructs visual tokens through a Thumbnail-and-Sampling strategy. Given a video, we select few equidistant frames from all input frames to construct a Thumbnail image as a detailed visual cue, complemented by Sampled visual tokens from all input frames. Our method establishes the new state-of-the-art performance among training-free video LLMs on various benchmarks. Notably, our 34B model outperforms GPT-4V on the MVBench benchmark, and achieves performance comparable to the 72B training-based video LLM, Video-LLaMA2, on the challenging MLVU benchmark. Code is available at https://github.com/tingyu215/TS-LLaVA.\n## Related Works\nImage LLMs aim to bridge the representation space between the vision encoder [33] and LLM [1,6,12,37] using image-text data. The BLIP family, including BLIP-2 [14] and InstructBLIP [7], employs a Querying Transformer (QFormer) to connect vision and language modalities via learnable queries in cross-attention modules. QFormer is also adopted by follow-up works, such as QWen-VL [2] and mPLUG-Owl [46]. QWen-VL incorporates interleaved image-text data in a three-stage training pipeline, while mPLUG-Owl adopts a modularized learning approach. The LLaVA family, in contrast, uses an MLP for connecting the vision encoder and LLM. LLaVA [21] leverages GPT-4 [31] generated visual instruction data for fine-tuning. Subsequent versions, LLaVA-v1.5 [22] and LLaVA-v1.6(NeXT) [23] further improve LLaVA's performance through better data, higher resolution and stronger LLM. As a concurrent work, MiniGPT-4 [54] aligns a frozen LLM with frozen ViT [8] and QFormer using a trainable linear layer to enable multimodal instruction following. \n\nTraining-based video LLMs are further trained on massive video data to endow image LLMs or LLMs with video understanding capabilities. Video-ChatGPT [26] uses LLaVA as its backbone, applying separate temporal and spatial pooling to visual features. The model further utilizes 100k video instruction tuning data. VideoChat [16] leverages QFormer for video token compression, and VideoChat2 [17] refines this approach with improved vision-language alignment and instruction tuning. It also introduces the multitask video understanding benchmark, MVBench. Video-LLaVA [20] learns a shared projector for image and video encoders. Video-LLaMA [49] and Video-LLaMA2 [5] incorporate video, audio and language modalities to support tasks oriented toward video and audio.\n...\nVideo-LLaMA [49] and Video-LLaMA2 [5] incorporate video, audio and language modalities to support tasks oriented toward video and audio. LLaVA-NeXT-Video [52] fine-tunes LLaVA-NeXT on video data, with a variant that applies DPO [34] for improved performance. LITA [11] employs a slow-fast design [9,41] to capture spatial and temporal information more effectively. \n\nTraining-free video LLMs extend image LLMs for video understanding without requiring additional fine-tuning on video data. As a pioneering approach, IG-VLM [13] constructs a grid-view image from video frames, which is then fed directly into a frozen image LLM with a specially designed prompt. While promising, the image grid approach has limitations, such as reduced resolution and the limited number of frames it can include, which we further discuss in the next section. FreeVA [40] explores various temporal aggregation methods, but similarly uses a limited number of frames. The current state-of-the-art SF-LLaVA [45] adopts the slow-fast design, which is proven to be effective in action recognition [9,41], and in LITA, as mentioned earlier. SF-LLaVA designs a slow pathway compressing fewer frames, and a fast pathway heavily compressing more frames. While both SF-LLaVA and our method use a twostream design, our Thumbnail-and-Sampling strategy leads to significantly better performance on various video understanding benchmarks, while maintaining a better token efficiency. We extend the discussion in Sec. 4.",
            "reference_string": "[274131643 | Qu et al. | 2024 | Citations: 2]"
        },
        {
            "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 59,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.20811, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261886750",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2348096687",
                    "name": "Jingyun Hua"
                },
                {
                    "authorId": "2348609405",
                    "name": "Weihong Lin"
                },
                {
                    "authorId": "2145784116",
                    "name": "Yuanxing Zhang"
                },
                {
                    "authorId": "2257136363",
                    "name": "Fuzheng Zhang"
                },
                {
                    "authorId": "2292205089",
                    "name": "Jianlong Wu"
                },
                {
                    "authorId": "2323902668",
                    "name": "Di Zhang"
                },
                {
                    "authorId": "2284688853",
                    "name": "Liqiang Nie"
                }
            ],
            "abstract": "Recent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce a two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in a standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. \\textbf{HAICTrain} comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, \\textbf{HAICBench} includes 412 manually annotated video-caption pairs and 2,000 QA pairs, for a comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.",
            "corpus_id": 276724729,
            "sentences": [
                {
                    "corpus_id": "276724729",
                    "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models",
                    "text": "MLLMs generally process multi-modal input information and generate language output. Kosmos (Huang et al., 2023) introduced an end-to-end framework that integrated visual inputs with LLM from cohesive training. Flamingo (Alayrac et al., 2022) and InstructBLIP (Dai et al., 2023) merged visual and linguistic features through crossattention and a Q-Former module, respectively. MiniGPT-4 (Zhu et al., 2024) and LLaVA (Liu et al., 2023) simplified the integration by linearly projecting visual features directly into the LLM embedding space. \n\nRecent studies focused on different aspects to enhance the above early attempts in MLLMs. Cosmos-2 (Peng et al., 2024) and NeXT-GPT (Wu et al., 2024) have expanded MLLM applications to broader multi-modal tasks. LLaVA-1.5 (Liu et al., 2024) explored adding high-quality multitask training data, and scaling up the resolution and LLM size to boost MLLM performance. LLaVA-OneVision (Li et al., 2024a) explored to unify dynamic image resolution, multi-image, and video into a unified input format.",
                    "score": 0.3792031331441435,
                    "section_title": "Multi-modal Large Language Models",
                    "char_start_offset": 7702,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 83
                        },
                        {
                            "start": 84,
                            "end": 209
                        },
                        {
                            "start": 210,
                            "end": 375
                        },
                        {
                            "start": 376,
                            "end": 538
                        },
                        {
                            "start": 541,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 752
                        },
                        {
                            "start": 753,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 1036
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 219,
                            "end": 241,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 673,
                            "end": 690,
                            "matchedPaperCorpusId": "261696650"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.591796875
                }
            ],
            "relevance_judgement": 0.591796875,
            "relevance_judgment_input_expanded": "# Title: HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models\n# Venue: arXiv.org\n# Authors: Xiao Wang, Jingyun Hua, Weihong Lin, Yuanxing Zhang, Fuzheng Zhang, Jianlong Wu, Di Zhang, Liqiang Nie\n## Abstract\nRecent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce a two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in a standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. \\textbf{HAICTrain} comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, \\textbf{HAICBench} includes 412 manually annotated video-caption pairs and 2,000 QA pairs, for a comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.\n## Multi-modal Large Language Models\nMLLMs generally process multi-modal input information and generate language output. Kosmos (Huang et al., 2023) introduced an end-to-end framework that integrated visual inputs with LLM from cohesive training. Flamingo (Alayrac et al., 2022) and InstructBLIP (Dai et al., 2023) merged visual and linguistic features through crossattention and a Q-Former module, respectively. MiniGPT-4 (Zhu et al., 2024) and LLaVA (Liu et al., 2023) simplified the integration by linearly projecting visual features directly into the LLM embedding space. \n\nRecent studies focused on different aspects to enhance the above early attempts in MLLMs. Cosmos-2 (Peng et al., 2024) and NeXT-GPT (Wu et al., 2024) have expanded MLLM applications to broader multi-modal tasks. LLaVA-1.5 (Liu et al., 2024) explored adding high-quality multitask training data, and scaling up the resolution and LLM size to boost MLLM performance. LLaVA-OneVision (Li et al., 2024a) explored to unify dynamic image resolution, multi-image, and video into a unified input format.",
            "reference_string": "[276724729 | Wang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "PAVE: Patching and Adapting Video Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 81,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.19794, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333863752",
                    "name": "Zhuoming Liu"
                },
                {
                    "authorId": "2321684254",
                    "name": "Yiquan Li"
                },
                {
                    "authorId": "2342927060",
                    "name": "Khoi Duc Nguyen"
                },
                {
                    "authorId": "1828787912",
                    "name": "Yiwu Zhong"
                },
                {
                    "authorId": "2333742645",
                    "name": "Yin Li"
                }
            ],
            "abstract": "Pre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D information) remains challenging. In this paper, we present PAVE, a flexible framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals, such as audio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters, referred to as\"patches,\"which add a small number of parameters and operations to a base model without modifying its architecture or pre-trained weights. In doing so, PAVE can effectively adapt the pre-trained base model to support diverse downstream tasks, including audio-visual question answering, 3D reasoning, multi-view video recognition, and high frame rate video understanding. Across these tasks, PAVE significantly enhances the performance of the base model, surpassing state-of-the-art task-specific models while incurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE supports multi-task learning and generalizes well across different Video LLMs. Our code is available at https://github.com/dragonlzm/PAVE.",
            "corpus_id": 277313523,
            "sentences": [
                {
                    "corpus_id": "277313523",
                    "title": "PAVE: Patching and Adapting Video Large Language Models",
                    "text": "Video large language models. Recent advances in instruction tuning with visual and text data [38][39][40] have led to a surge of interest in developing Video LLMs. Many of these models share a common design, where visual features are extracted using a pre-trained visual encoder, projected into the text latent space of an LLM, and subsequently processed by the pre-trained LLM to generate responses. Video-ChatGPT [45] introduces instruction tuning into the video domain. Video-LLaVA [36] improves model performance with better text-aligned video features [83], while VideoChat2 [33] resorts to increasing the quality and quantity of the video instruction tuning set. Recent vision-LLM models like LLaVA-NeXT [39], LLaVA-OneVision [28], LLaVA-Video [79], Qwen2-VL [60], and mPlug-Owl3 [71] consider multi-stage training with both video and image, which substantially improves the model performance. Recent works in VideoLLM focus on long video understanding. [12,25,56,64,65,76] propose to use Q-former [32] or text-query-based cross-attention to compress vision tokens, while others [47,63] resort to state-space models [18]. \n\nResearchers also extend the instructional tuning into different video sub-domains. For instance, CAT [72] focuses on audio-visual understanding, while Scene-LLM [15] and LLaVA-3D [84] address 3D QA tasks. Built on these developments, our work specifically focuses on adapting pre-trained Video LLMs to downstream tasks with side-channel signals, aiming to significantly extend the capabilities of these models. \n\nAdaptation of vision foundation models. Adapting vision foundation models to downstream tasks has received significant attention. Prior works have studied learning lightweight adapters [4,6,8,35,52,58], prepending learnable input tokens (e.g. prompts) [24,82], or in-context learning [3,62,68,78]. Recently, adapter-and promptbased methods have been explored for Video LLMs.",
                    "score": 0.38508348408171966,
                    "section_title": "Related Works",
                    "char_start_offset": 5040,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 28
                        },
                        {
                            "start": 29,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 400
                        },
                        {
                            "start": 401,
                            "end": 472
                        },
                        {
                            "start": 473,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 899
                        },
                        {
                            "start": 900,
                            "end": 959
                        },
                        {
                            "start": 960,
                            "end": 1127
                        },
                        {
                            "start": 1130,
                            "end": 1212
                        },
                        {
                            "start": 1213,
                            "end": 1334
                        },
                        {
                            "start": 1335,
                            "end": 1540
                        },
                        {
                            "start": 1543,
                            "end": 1582
                        },
                        {
                            "start": 1583,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1785
                        },
                        {
                            "start": 1786,
                            "end": 1840
                        },
                        {
                            "start": 1841,
                            "end": 1917
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 93,
                            "end": 97,
                            "matchedPaperCorpusId": "263672058"
                        },
                        {
                            "start": 101,
                            "end": 105,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 415,
                            "end": 419,
                            "matchedPaperCorpusId": "259108333"
                        },
                        {
                            "start": 485,
                            "end": 489,
                            "matchedPaperCorpusId": "265281544"
                        },
                        {
                            "start": 557,
                            "end": 561,
                            "matchedPaperCorpusId": "263608698"
                        },
                        {
                            "start": 580,
                            "end": 584,
                            "matchedPaperCorpusId": "265466214"
                        },
                        {
                            "start": 964,
                            "end": 967,
                            "matchedPaperCorpusId": "266362624"
                        },
                        {
                            "start": 973,
                            "end": 976,
                            "matchedPaperCorpusId": "268889590"
                        },
                        {
                            "start": 1004,
                            "end": 1008,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 1085,
                            "end": 1089,
                            "matchedPaperCorpusId": "270123263"
                        },
                        {
                            "start": 1231,
                            "end": 1235,
                            "matchedPaperCorpusId": "268264651"
                        },
                        {
                            "start": 1728,
                            "end": 1731,
                            "matchedPaperCorpusId": "261100811"
                        },
                        {
                            "start": 1731,
                            "end": 1733,
                            "matchedPaperCorpusId": "249097890"
                        },
                        {
                            "start": 1733,
                            "end": 1735,
                            "matchedPaperCorpusId": "248834106"
                        },
                        {
                            "start": 1735,
                            "end": 1738,
                            "matchedPaperCorpusId": "252918870"
                        },
                        {
                            "start": 1741,
                            "end": 1744,
                            "matchedPaperCorpusId": "245123737"
                        },
                        {
                            "start": 1795,
                            "end": 1799,
                            "matchedPaperCorpusId": "247618727"
                        },
                        {
                            "start": 1799,
                            "end": 1802,
                            "matchedPaperCorpusId": "237386023"
                        },
                        {
                            "start": 1830,
                            "end": 1833,
                            "matchedPaperCorpusId": "254246343"
                        },
                        {
                            "start": 1833,
                            "end": 1836,
                            "matchedPaperCorpusId": "270045544"
                        },
                        {
                            "start": 1836,
                            "end": 1839,
                            "matchedPaperCorpusId": "256416477"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57421875
                }
            ],
            "relevance_judgement": 0.57421875,
            "relevance_judgment_input_expanded": "# Title: PAVE: Patching and Adapting Video Large Language Models\n# Venue: arXiv.org\n# Authors: Zhuoming Liu, Yiquan Li, Khoi Duc Nguyen, Yiwu Zhong, Yin Li\n## Abstract\nPre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D information) remains challenging. In this paper, we present PAVE, a flexible framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals, such as audio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters, referred to as\"patches,\"which add a small number of parameters and operations to a base model without modifying its architecture or pre-trained weights. In doing so, PAVE can effectively adapt the pre-trained base model to support diverse downstream tasks, including audio-visual question answering, 3D reasoning, multi-view video recognition, and high frame rate video understanding. Across these tasks, PAVE significantly enhances the performance of the base model, surpassing state-of-the-art task-specific models while incurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE supports multi-task learning and generalizes well across different Video LLMs. Our code is available at https://github.com/dragonlzm/PAVE.\n## Related Works\nVideo large language models. Recent advances in instruction tuning with visual and text data [38][39][40] have led to a surge of interest in developing Video LLMs. Many of these models share a common design, where visual features are extracted using a pre-trained visual encoder, projected into the text latent space of an LLM, and subsequently processed by the pre-trained LLM to generate responses. Video-ChatGPT [45] introduces instruction tuning into the video domain. Video-LLaVA [36] improves model performance with better text-aligned video features [83], while VideoChat2 [33] resorts to increasing the quality and quantity of the video instruction tuning set. Recent vision-LLM models like LLaVA-NeXT [39], LLaVA-OneVision [28], LLaVA-Video [79], Qwen2-VL [60], and mPlug-Owl3 [71] consider multi-stage training with both video and image, which substantially improves the model performance. Recent works in VideoLLM focus on long video understanding. [12,25,56,64,65,76] propose to use Q-former [32] or text-query-based cross-attention to compress vision tokens, while others [47,63] resort to state-space models [18]. \n\nResearchers also extend the instructional tuning into different video sub-domains. For instance, CAT [72] focuses on audio-visual understanding, while Scene-LLM [15] and LLaVA-3D [84] address 3D QA tasks. Built on these developments, our work specifically focuses on adapting pre-trained Video LLMs to downstream tasks with side-channel signals, aiming to significantly extend the capabilities of these models. \n\nAdaptation of vision foundation models. Adapting vision foundation models to downstream tasks has received significant attention. Prior works have studied learning lightweight adapters [4,6,8,35,52,58], prepending learnable input tokens (e.g. prompts) [24,82], or in-context learning [3,62,68,78]. Recently, adapter-and promptbased methods have been explored for Video LLMs.",
            "reference_string": "[277313523 | Liu et al. | 2025 | Citations: 1]"
        },
        {
            "title": "SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 37,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10627, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237599228",
                    "name": "Ziyu Guo"
                },
                {
                    "authorId": "2350073299",
                    "name": "Ray Zhang"
                },
                {
                    "authorId": "2280286298",
                    "name": "Hao Chen"
                },
                {
                    "authorId": "2350025512",
                    "name": "Jialin Gao"
                },
                {
                    "authorId": "2293242031",
                    "name": "Dongzhi Jiang"
                },
                {
                    "authorId": "2254323687",
                    "name": "Jiaze Wang"
                },
                {
                    "authorId": "2274486861",
                    "name": "Pheng-Ann Heng"
                }
            ],
            "abstract": "The rapid advancement of Large Multi-modal Models (LMMs) has enabled their application in scientific problem-solving, yet their fine-grained capabilities remain under-explored. In this paper, we introduce SciVerse, a multi-modal scientific evaluation benchmark to thoroughly assess LMMs across 5,735 test instances in five distinct versions. We aim to investigate three key dimensions of LMMs: scientific knowledge comprehension, multi-modal content interpretation, and Chain-of-Thought (CoT) reasoning. To unveil whether LMMs possess sufficient scientific expertise, we first transform each problem into three versions containing different levels of knowledge required for solving, i.e., Knowledge-free, -lite, and -rich. Then, to explore how LMMs interpret multi-modal scientific content, we annotate another two versions, i.e., Vision-rich and -only, marking more question information from texts to diagrams. Comparing the results of different versions, SciVerse systematically examines the professional knowledge stock and visual perception skills of LMMs in scientific domains. In addition, to rigorously assess CoT reasoning, we propose a new scientific CoT evaluation strategy, conducting a step-wise assessment on knowledge and logical errors in model outputs. Our extensive evaluation of different LMMs on SciVerse reveals critical limitations in their scientific proficiency and provides new insights into future developments. Project page: https://sciverse-cuhk.github.io",
            "corpus_id": 276961504,
            "sentences": [
                {
                    "corpus_id": "276961504",
                    "title": "SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems",
                    "text": "Recent advances in multi-modal AI have been marked by significant developments in LMMs, which combine the capabilities of LLMs and vision models to process diverse visual inputs. While proprietary models like GPT-4V (OpenAI, 2023c), Claude (Anthropic, 2024), Gemini (Gemini Team, 2023), and GPT-4o (OpenAI, 2024a) have shown remarkable visual reasoning abilities, their closed nature has spurred the development of open-source alternatives. Early opensource LMMs like LLaVA (Liu et al., 2023b) and MiniGPT-4 (Zhu et al., 2023) paired CLIP-based image encoders (Radford et al., 2021) with LLMs for multi-modal instruction tuning. Later models such as LLaVA-NeXT (Li et al., 2024a), LLaVA-OneVision (Li et al., 2024b), ShareGPT4V (Chen et al., 2023b), InternVL (Chen et al., 2024b), SPHINX (Lin et al., 2023), and Qwen-VL (Qwen Team, 2024) expanded these capabilities through broader training datasets and advanced training strategies. In this paper, we aim to comprehensively evaluate their fine-grained capabilities in scientific domains, guiding the future developments of LMMs.",
                    "score": 0.3721490776415851,
                    "section_title": "Large Multi-modal Models (LMMs)",
                    "char_start_offset": 17461,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 628
                        },
                        {
                            "start": 629,
                            "end": 933
                        },
                        {
                            "start": 934,
                            "end": 1079
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 560,
                            "end": 582,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 759,
                            "end": 779,
                            "matchedPaperCorpusId": "266521410"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5654296875
                }
            ],
            "relevance_judgement": 0.5654296875,
            "relevance_judgment_input_expanded": "# Title: SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems\n# Venue: arXiv.org\n# Authors: Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, Pheng-Ann Heng\n## Abstract\nThe rapid advancement of Large Multi-modal Models (LMMs) has enabled their application in scientific problem-solving, yet their fine-grained capabilities remain under-explored. In this paper, we introduce SciVerse, a multi-modal scientific evaluation benchmark to thoroughly assess LMMs across 5,735 test instances in five distinct versions. We aim to investigate three key dimensions of LMMs: scientific knowledge comprehension, multi-modal content interpretation, and Chain-of-Thought (CoT) reasoning. To unveil whether LMMs possess sufficient scientific expertise, we first transform each problem into three versions containing different levels of knowledge required for solving, i.e., Knowledge-free, -lite, and -rich. Then, to explore how LMMs interpret multi-modal scientific content, we annotate another two versions, i.e., Vision-rich and -only, marking more question information from texts to diagrams. Comparing the results of different versions, SciVerse systematically examines the professional knowledge stock and visual perception skills of LMMs in scientific domains. In addition, to rigorously assess CoT reasoning, we propose a new scientific CoT evaluation strategy, conducting a step-wise assessment on knowledge and logical errors in model outputs. Our extensive evaluation of different LMMs on SciVerse reveals critical limitations in their scientific proficiency and provides new insights into future developments. Project page: https://sciverse-cuhk.github.io\n## Large Multi-modal Models (LMMs)\nRecent advances in multi-modal AI have been marked by significant developments in LMMs, which combine the capabilities of LLMs and vision models to process diverse visual inputs. While proprietary models like GPT-4V (OpenAI, 2023c), Claude (Anthropic, 2024), Gemini (Gemini Team, 2023), and GPT-4o (OpenAI, 2024a) have shown remarkable visual reasoning abilities, their closed nature has spurred the development of open-source alternatives. Early opensource LMMs like LLaVA (Liu et al., 2023b) and MiniGPT-4 (Zhu et al., 2023) paired CLIP-based image encoders (Radford et al., 2021) with LLMs for multi-modal instruction tuning. Later models such as LLaVA-NeXT (Li et al., 2024a), LLaVA-OneVision (Li et al., 2024b), ShareGPT4V (Chen et al., 2023b), InternVL (Chen et al., 2024b), SPHINX (Lin et al., 2023), and Qwen-VL (Qwen Team, 2024) expanded these capabilities through broader training datasets and advanced training strategies. In this paper, we aim to comprehensively evaluate their fine-grained capabilities in scientific domains, guiding the future developments of LMMs.",
            "reference_string": "[276961504 | Guo et al. | 2025 | Citations: 7]"
        },
        {
            "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 70,
            "citation_count": 16,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.00876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294440519",
                    "name": "Wenxuan Huang"
                },
                {
                    "authorId": "2333365484",
                    "name": "Zijie Zhai"
                },
                {
                    "authorId": "2259478606",
                    "name": "Yunhang Shen"
                },
                {
                    "authorId": "2334027321",
                    "name": "Shaoshen Cao"
                },
                {
                    "authorId": "2334050626",
                    "name": "Fei Zhao"
                },
                {
                    "authorId": "2333462877",
                    "name": "Xiangfeng Xu"
                },
                {
                    "authorId": "2333868375",
                    "name": "Zheyu Ye"
                },
                {
                    "authorId": "2257475538",
                    "name": "Shaohui Lin"
                }
            ],
            "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision understanding, reasoning, and interaction. However, the inference computation and memory increase progressively with the generation of output tokens during decoding, directly affecting the efficacy of MLLMs. Existing methods attempt to reduce the vision context redundancy to achieve efficient MLLMs. Unfortunately, the efficiency benefits of the vision context reduction in the prefill stage gradually diminish during the decoding stage. To address this problem, we proposed a dynamic vision-language context sparsification framework Dynamic-LLaVA, which dynamically reduces the redundancy of vision context in the prefill stage and decreases the memory and computation overhead of the generated language context during decoding. Dynamic-LLaVA designs a tailored sparsification inference scheme for different inference modes, i.e., prefill, decoding with and without KV cache, to achieve efficient inference of MLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by $\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation process of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption under decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead when decoding with KV cache, due to the vision-language context sparsification. Extensive experiments also demonstrate that Dynamic-LLaVA achieves efficient inference for MLLMs with negligible understanding and generation ability degradation or even performance gains compared to the full-context inference baselines. Code is available at https://github.com/Osilly/dynamic_llava .",
            "corpus_id": 274437635,
            "sentences": [
                {
                    "corpus_id": "274437635",
                    "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification",
                    "text": "Vision understanding. As demonstrated in Tab. 1, our proposed Dynamic-LLaVA framework achieves superior performance compared to SoTA vision context sparsification methods in most benchmarks while reducing image tokens by approximately 80%. Additionally, the Dynamic-LLaVA framework reduces FLOPs by more than 70% for official LLaVA-1.5 base-  lines in the prefill stage, with only minimal performance degradation across most benchmarks. \n\nEspecially, for the LLaVA-1.5 with 7B and 13B parameters, the Dynamic-LLaVA even shows performance improvements on the SciQA, POPE, MME, and MMBench benchmarks. For instance, on the SciQA benchmark, the 7B and 13B Dynamic-LLaVA models achieve performance gains of +2.3% and +0.8%, respectively, compared to the original LLaVA-1.5. More comparison of SoTA vision context sparsification methods are presented in Appendix A.5.1. As shown in Tab. 2, for efficient vision projector methods for MLLMs, Dynamic-LLaVA exclusively utilizes the original MLP of LLaVA-1.5 as the vision projector. This approach surpasses those that modify the vision projector across most benchmarks, achieving an average performance that exceeds the best alternative methods by 0.9%. Furthermore, our Dynamic-LLaVA framework can be integrated with other efficient vision projector methods. For instance, our combination with the TokenPacker projector, Dynamic-LLaVA-TokenPacker, achieves significant reductions in vision tokens by an additional 60% on top of the already reduced count by efficient vision projector methods, with only a minimal loss in performance. Compared to the official LLaVA-TokenPacker method, our Dynamic-LLaVA-TokenPacker incurs only a minimal performance loss of 0.3% and 0.1% (for 7B and 13B models) while using fewer vision tokens (57 vs. 144). Moreover, we observe that the sparsification of the output text tokens does not impede the comprehension ability of MLLMs.",
                    "score": 0.4003311066031888,
                    "section_title": "MAIN RESULTS",
                    "char_start_offset": 25218,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 21
                        },
                        {
                            "start": 22,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 436
                        },
                        {
                            "start": 439,
                            "end": 599
                        },
                        {
                            "start": 600,
                            "end": 769
                        },
                        {
                            "start": 770,
                            "end": 864
                        },
                        {
                            "start": 865,
                            "end": 1024
                        },
                        {
                            "start": 1025,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1301
                        },
                        {
                            "start": 1302,
                            "end": 1576
                        },
                        {
                            "start": 1577,
                            "end": 1783
                        },
                        {
                            "start": 1784,
                            "end": 1906
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56298828125
                }
            ],
            "relevance_judgement": 0.56298828125,
            "relevance_judgment_input_expanded": "# Title: Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification\n# Venue: International Conference on Learning Representations\n# Authors: Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaoshen Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, Shaohui Lin\n## Abstract\nMultimodal Large Language Models (MLLMs) have achieved remarkable success in vision understanding, reasoning, and interaction. However, the inference computation and memory increase progressively with the generation of output tokens during decoding, directly affecting the efficacy of MLLMs. Existing methods attempt to reduce the vision context redundancy to achieve efficient MLLMs. Unfortunately, the efficiency benefits of the vision context reduction in the prefill stage gradually diminish during the decoding stage. To address this problem, we proposed a dynamic vision-language context sparsification framework Dynamic-LLaVA, which dynamically reduces the redundancy of vision context in the prefill stage and decreases the memory and computation overhead of the generated language context during decoding. Dynamic-LLaVA designs a tailored sparsification inference scheme for different inference modes, i.e., prefill, decoding with and without KV cache, to achieve efficient inference of MLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by $\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation process of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption under decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead when decoding with KV cache, due to the vision-language context sparsification. Extensive experiments also demonstrate that Dynamic-LLaVA achieves efficient inference for MLLMs with negligible understanding and generation ability degradation or even performance gains compared to the full-context inference baselines. Code is available at https://github.com/Osilly/dynamic_llava .\n## MAIN RESULTS\nVision understanding. As demonstrated in Tab. 1, our proposed Dynamic-LLaVA framework achieves superior performance compared to SoTA vision context sparsification methods in most benchmarks while reducing image tokens by approximately 80%. Additionally, the Dynamic-LLaVA framework reduces FLOPs by more than 70% for official LLaVA-1.5 base-  lines in the prefill stage, with only minimal performance degradation across most benchmarks. \n\nEspecially, for the LLaVA-1.5 with 7B and 13B parameters, the Dynamic-LLaVA even shows performance improvements on the SciQA, POPE, MME, and MMBench benchmarks. For instance, on the SciQA benchmark, the 7B and 13B Dynamic-LLaVA models achieve performance gains of +2.3% and +0.8%, respectively, compared to the original LLaVA-1.5. More comparison of SoTA vision context sparsification methods are presented in Appendix A.5.1. As shown in Tab. 2, for efficient vision projector methods for MLLMs, Dynamic-LLaVA exclusively utilizes the original MLP of LLaVA-1.5 as the vision projector. This approach surpasses those that modify the vision projector across most benchmarks, achieving an average performance that exceeds the best alternative methods by 0.9%. Furthermore, our Dynamic-LLaVA framework can be integrated with other efficient vision projector methods. For instance, our combination with the TokenPacker projector, Dynamic-LLaVA-TokenPacker, achieves significant reductions in vision tokens by an additional 60% on top of the already reduced count by efficient vision projector methods, with only a minimal loss in performance. Compared to the official LLaVA-TokenPacker method, our Dynamic-LLaVA-TokenPacker incurs only a minimal performance loss of 0.3% and 0.1% (for 7B and 13B models) while using fewer vision tokens (57 vs. 144). Moreover, we observe that the sparsification of the output text tokens does not impede the comprehension ability of MLLMs.",
            "reference_string": "[274437635 | Huang et al. | 2024 | Citations: 16]"
        },
        {
            "title": "From Training-Free to Adaptive: Empirical Insights into MLLMs' Understanding of Detection Information",
            "venue": "",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 15,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.17981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2281947967",
                    "name": "Qirui Jiao"
                },
                {
                    "authorId": "49025612",
                    "name": "Daoyuan Chen"
                },
                {
                    "authorId": "2237943131",
                    "name": "Yilun Huang"
                },
                {
                    "authorId": "2237607166",
                    "name": "Yaliang Li"
                },
                {
                    "authorId": "2266438880",
                    "name": "Ying Shen"
                }
            ],
            "abstract": "Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. Vision detection models excel at recognizing fine-grained image details, prompting researchers to use them to enhance MLLMs. One effective strategy is to infuse detection information in text format, which has proven simple and effective. However, most studies utilize this method without training, leaving the potential of adaptive training largely unexplored. Adaptive training could significantly enhance MLLMs' comprehension of unique inputs while filtering out irrelevant information. This paper addresses the crucial question: How does training impact MLLMs' understanding of infused textual detection information? We systematically experiment with various representative models to evaluate the effects of training-free, retraining, and fine-tuning strategies. We also examine the influence of training on MLLMs' original abilities and the interchangeability of detection models. Our findings indicate that fine-tuning a pre-trained MLLM to incorporate textual detection information delivers superior results compared to training-free and retraining methods, improving performance by 6.71% across 10 widely recognized benchmarks. Furthermore, fine-tuning enables MLLMs to retain performance enhancements even when detection models are swapped, indicating improved understanding of formatted textual data. We release our codes to support further exploration of fusion strategies for vision detection models and the enhancement of MLLMs' fine-grained multimodal capabilities.",
            "corpus_id": 267335231,
            "sentences": [
                {
                    "corpus_id": "267335231",
                    "title": "From Training-Free to Adaptive: Empirical Insights into MLLMs' Understanding of Detection Information",
                    "text": "D.1 How LLaVA-1.5 Represents Other MLLMs? \n\nIn the main body of our paper, we exclusively select LLaVA-1.5 1 for experimentation, considering it representative of the majority of state-of-the-art models. In this section, we will illustrate this choice from the following two aspects: \n\n(1) The representativeness of LLaVA-1.5. We choose LLaVA-1.5 as we are in a highly dynamic field and it is representative enough of most state-of-the-art MLLMs. The advanced MLLMs typically consist of three main modules: an image encoder, an input projector, and a LLM Backbone. LLaVA-1.5 adheres to this structure. \n\nImages are first processed through the image encoder and the input projector. Most advanced MLLMs typically include a dedicated branch like this for processing image features into analogous image token sequences. Next, the image tokens are concatenated with text tokens that represent input sentences and fed into the LLM. Specifically, the text tokens representing our detection information can be directly concatenated with the extracted image tokens and used during MLLM's training and inference. In other words, as long as the MLLM conforms to this structure, the additional detection information can be processed similarly before being inputted into the LLM, specifically being concatenated with image features at the embedding level. The way they are infused into the backbone LLM is similar. Therefore, the results of experiments conducted on LLaVA-1.5 can be applied to other MLLMs with similar structures. \n\nFurthermore, LLaVA-1.5 has proven to be highly successful, spawning numerous outstanding works. We conduct our study based on LLaVA-1.5, enabling the application of our experimental findings to the subsequent works of LLaVA-1.5. Consequently, we are positioned to contribute to the open-source community. \n\n(2) The empirical support on Qwen-VL. To better illustrate the versatility of our work, we also conduct experiments on another MLLM, Qwen-VL.",
                    "score": 0.3605834149316388,
                    "section_title": "D Model Architecture Rationale",
                    "char_start_offset": 39759,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 41
                        },
                        {
                            "start": 44,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 283
                        },
                        {
                            "start": 286,
                            "end": 326
                        },
                        {
                            "start": 327,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 564
                        },
                        {
                            "start": 565,
                            "end": 601
                        },
                        {
                            "start": 604,
                            "end": 681
                        },
                        {
                            "start": 682,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1103
                        },
                        {
                            "start": 1104,
                            "end": 1343
                        },
                        {
                            "start": 1344,
                            "end": 1402
                        },
                        {
                            "start": 1403,
                            "end": 1518
                        },
                        {
                            "start": 1521,
                            "end": 1616
                        },
                        {
                            "start": 1617,
                            "end": 1749
                        },
                        {
                            "start": 1750,
                            "end": 1825
                        },
                        {
                            "start": 1828,
                            "end": 1865
                        },
                        {
                            "start": 1866,
                            "end": 1969
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53515625
                }
            ],
            "relevance_judgement": 0.53515625,
            "relevance_judgment_input_expanded": "# Title: From Training-Free to Adaptive: Empirical Insights into MLLMs' Understanding of Detection Information\n# Venue: \n# Authors: Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen\n## Abstract\nDespite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. Vision detection models excel at recognizing fine-grained image details, prompting researchers to use them to enhance MLLMs. One effective strategy is to infuse detection information in text format, which has proven simple and effective. However, most studies utilize this method without training, leaving the potential of adaptive training largely unexplored. Adaptive training could significantly enhance MLLMs' comprehension of unique inputs while filtering out irrelevant information. This paper addresses the crucial question: How does training impact MLLMs' understanding of infused textual detection information? We systematically experiment with various representative models to evaluate the effects of training-free, retraining, and fine-tuning strategies. We also examine the influence of training on MLLMs' original abilities and the interchangeability of detection models. Our findings indicate that fine-tuning a pre-trained MLLM to incorporate textual detection information delivers superior results compared to training-free and retraining methods, improving performance by 6.71% across 10 widely recognized benchmarks. Furthermore, fine-tuning enables MLLMs to retain performance enhancements even when detection models are swapped, indicating improved understanding of formatted textual data. We release our codes to support further exploration of fusion strategies for vision detection models and the enhancement of MLLMs' fine-grained multimodal capabilities.\n## D Model Architecture Rationale\nD.1 How LLaVA-1.5 Represents Other MLLMs? \n\nIn the main body of our paper, we exclusively select LLaVA-1.5 1 for experimentation, considering it representative of the majority of state-of-the-art models. In this section, we will illustrate this choice from the following two aspects: \n\n(1) The representativeness of LLaVA-1.5. We choose LLaVA-1.5 as we are in a highly dynamic field and it is representative enough of most state-of-the-art MLLMs. The advanced MLLMs typically consist of three main modules: an image encoder, an input projector, and a LLM Backbone. LLaVA-1.5 adheres to this structure. \n\nImages are first processed through the image encoder and the input projector. Most advanced MLLMs typically include a dedicated branch like this for processing image features into analogous image token sequences. Next, the image tokens are concatenated with text tokens that represent input sentences and fed into the LLM. Specifically, the text tokens representing our detection information can be directly concatenated with the extracted image tokens and used during MLLM's training and inference. In other words, as long as the MLLM conforms to this structure, the additional detection information can be processed similarly before being inputted into the LLM, specifically being concatenated with image features at the embedding level. The way they are infused into the backbone LLM is similar. Therefore, the results of experiments conducted on LLaVA-1.5 can be applied to other MLLMs with similar structures. \n\nFurthermore, LLaVA-1.5 has proven to be highly successful, spawning numerous outstanding works. We conduct our study based on LLaVA-1.5, enabling the application of our experimental findings to the subsequent works of LLaVA-1.5. Consequently, we are positioned to contribute to the open-source community. \n\n(2) The empirical support on Qwen-VL. To better illustrate the versatility of our work, we also conduct experiments on another MLLM, Qwen-VL.",
            "reference_string": "[267335231 | Jiao et al. | 2024 | Citations: 15]"
        },
        {
            "title": "ChartLlama: A Multimodal LLM for Chart Understanding and Generation",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 36,
            "citation_count": 105,
            "influential_citation_count": 23,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.16483, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268710741",
                    "name": "Yucheng Han"
                },
                {
                    "authorId": "2265070657",
                    "name": "C. Zhang"
                },
                {
                    "authorId": "2268639237",
                    "name": "Xin Chen"
                },
                {
                    "authorId": "2268572680",
                    "name": "Xu Yang"
                },
                {
                    "authorId": "2243288284",
                    "name": "Zhibin Wang"
                },
                {
                    "authorId": "2261357816",
                    "name": "Gang Yu"
                },
                {
                    "authorId": "2107058893",
                    "name": "Bin Fu"
                },
                {
                    "authorId": "2268645398",
                    "name": "Hanwang Zhang"
                }
            ],
            "abstract": "Multi-modal large language models have demonstrated impressive performances on most vision-language tasks. However, the model generally lacks the understanding capabilities for specific domain data, particularly when it comes to interpreting chart figures. This is mainly due to the lack of relevant multi-modal instruction tuning datasets. In this article, we create a high-quality instruction-tuning dataset leveraging GPT-4. We develop a multi-step data generation process in which different steps are responsible for generating tabular data, creating chart figures, and designing instruction tuning data separately. Our method's flexibility enables us to generate diverse, high-quality instruction-tuning data consistently and efficiently while maintaining a low resource expenditure. Additionally, it allows us to incorporate a wider variety of chart and task types not yet featured in existing datasets. Next, we introduce ChartLlama, a multi-modal large language model that we've trained using our created dataset. ChartLlama outperforms all prior methods in ChartQA, Chart-to-text, and Chart-extraction evaluation benchmarks. Additionally, ChartLlama significantly improves upon the baseline in our specially compiled chart dataset, which includes new chart and task types. The results of ChartLlama confirm the value and huge potential of our proposed data generation method in enhancing chart comprehension.",
            "corpus_id": 265466206,
            "sentences": [
                {
                    "corpus_id": "265466206",
                    "title": "ChartLlama: A Multimodal LLM for Chart Understanding and Generation",
                    "text": "To elucidate our training strategies, we provide some clarification about the modifications in LLaVA-1.5 [17], and introduce its essential model architectures. \n\nVision encoder: LLaVA-1.5 incorporates CLIP's vision encoder [25]. The primary distinction is that LLaVA-1.5 employs ViT-L/14@336px, while LLaVA uses ViT-L/14@224px. Another notable alteration concerns the image processor. Eschewing traditional center cropping, LLaVA-1.5 adopts padding as an image pre-processing technique, ensuring that all information in the provided image can be apprehended. Projection layer: In LLaVA-1.5, the initial single linear layer is substituted with a two-layer MLP, resulting in improved performance. \n\nLora Layer: Based on experiments in [17,19], implementing Lora [6] layers is sufficient to achieve performance comparable to full fine-tuning strategies. For the original LLaVA [18], Lora layers with a Lora rank of 64 suffice, whereas for LLaVA-1.5 [17], the Lora rank needs to exceed 128.",
                    "score": 0.36517913129969826,
                    "section_title": "A. Model architecture",
                    "char_start_offset": 28238,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 162,
                            "end": 228
                        },
                        {
                            "start": 229,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 384
                        },
                        {
                            "start": 385,
                            "end": 558
                        },
                        {
                            "start": 559,
                            "end": 694
                        },
                        {
                            "start": 697,
                            "end": 850
                        },
                        {
                            "start": 851,
                            "end": 986
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 760,
                            "end": 763,
                            "matchedPaperCorpusId": "235458009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51611328125
                }
            ],
            "relevance_judgement": 0.51611328125,
            "relevance_judgment_input_expanded": "# Title: ChartLlama: A Multimodal LLM for Chart Understanding and Generation\n# Venue: arXiv.org\n# Authors: Yucheng Han, C. Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, Hanwang Zhang\n## Abstract\nMulti-modal large language models have demonstrated impressive performances on most vision-language tasks. However, the model generally lacks the understanding capabilities for specific domain data, particularly when it comes to interpreting chart figures. This is mainly due to the lack of relevant multi-modal instruction tuning datasets. In this article, we create a high-quality instruction-tuning dataset leveraging GPT-4. We develop a multi-step data generation process in which different steps are responsible for generating tabular data, creating chart figures, and designing instruction tuning data separately. Our method's flexibility enables us to generate diverse, high-quality instruction-tuning data consistently and efficiently while maintaining a low resource expenditure. Additionally, it allows us to incorporate a wider variety of chart and task types not yet featured in existing datasets. Next, we introduce ChartLlama, a multi-modal large language model that we've trained using our created dataset. ChartLlama outperforms all prior methods in ChartQA, Chart-to-text, and Chart-extraction evaluation benchmarks. Additionally, ChartLlama significantly improves upon the baseline in our specially compiled chart dataset, which includes new chart and task types. The results of ChartLlama confirm the value and huge potential of our proposed data generation method in enhancing chart comprehension.\n## A. Model architecture\nTo elucidate our training strategies, we provide some clarification about the modifications in LLaVA-1.5 [17], and introduce its essential model architectures. \n\nVision encoder: LLaVA-1.5 incorporates CLIP's vision encoder [25]. The primary distinction is that LLaVA-1.5 employs ViT-L/14@336px, while LLaVA uses ViT-L/14@224px. Another notable alteration concerns the image processor. Eschewing traditional center cropping, LLaVA-1.5 adopts padding as an image pre-processing technique, ensuring that all information in the provided image can be apprehended. Projection layer: In LLaVA-1.5, the initial single linear layer is substituted with a two-layer MLP, resulting in improved performance. \n\nLora Layer: Based on experiments in [17,19], implementing Lora [6] layers is sufficient to achieve performance comparable to full fine-tuning strategies. For the original LLaVA [18], Lora layers with a Lora rank of 64 suffice, whereas for LLaVA-1.5 [17], the Lora rank needs to exceed 128.",
            "reference_string": "[265466206 | Han et al. | 2023 | Citations: 105]"
        },
        {
            "title": "From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 53,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.06260, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2360628692",
                    "name": "Muzhi Dai"
                },
                {
                    "authorId": "2202454665",
                    "name": "Jiashuo Sun"
                },
                {
                    "authorId": "2303887466",
                    "name": "Zhiyuan Zhao"
                },
                {
                    "authorId": "2349818978",
                    "name": "Shixuan Liu"
                },
                {
                    "authorId": "2349405181",
                    "name": "Rui Li"
                },
                {
                    "authorId": "2275029277",
                    "name": "Junyu Gao"
                },
                {
                    "authorId": "2296723295",
                    "name": "Xuelong Li"
                }
            ],
            "abstract": "Aligning large vision-language models (LVLMs) with human preferences is challenging due to the scarcity of fine-grained, high-quality, and multimodal preference data without human annotations. Existing methods relying on direct distillation often struggle with low-confidence data, leading to suboptimal performance. To address this, we propose CAREVL, a novel method for preference reward modeling by reliably using both high- and low-confidence data. First, a cluster of auxiliary expert models (textual reward models) innovatively leverages image captions as weak supervision signals to filter high-confidence data. The high-confidence data are then used to fine-tune the LVLM. Second, low-confidence data are used to generate diverse preference samples using the fine-tuned LVLM. These samples are then scored and selected to construct reliable chosen-rejected pairs for further training. CAREVL achieves performance improvements over traditional distillation-based methods on VL-RewardBench and MLLM-as-a-Judge benchmark, demonstrating its effectiveness. The code will be released soon.",
            "corpus_id": 276903685,
            "sentences": [
                {
                    "corpus_id": "276903685",
                    "title": "From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models",
                    "text": "Table 1 summarizes the performance of our approach on the VL-RewardBench, evaluated across five metrics: General, Hallucination, Reasoning, Overall Accuracy, and Macro Average Accuracy. Our method is applied to three backbone models (LLaVA-OneVision-7B, Qwen2-VL-7B, and Llama-3.2-11B-Vision), and the results demonstrate substantial improvements over all the baseline versions and other competitive models. \n\nFor the LLaVA-OneVision backbone, the baseline model (LLaVA-OneVision-7B) achieves an Overall Accuracy of 29.6 and a Macro Average Accuracy of 36.5. In contrast, our enhanced version of LLaVA-OneVision-7B yields an Overall Accuracy of 68.7 and a Macro Average Accuracy of 67.4, accompanied by a significant boost in the General (72.8 vs. 32.2) and Hallucination (67.9 vs. 20.1) metrics. Compared with LLaVA-Critic-7b, which is also trained as a reward model based on LLaVA-OneVision-7B, CAREVL achieves higher performance across all metrics of VL-RewardBench while using less training data (75k vs. 113k), indicating the effectiveness of our method. \n\nSimilarly, for Qwen2-VL-7B, CAREVL elevates the Overall Accuracy from 28.3 to 67.8 and Macro Average Accuracy from 33.9 to 65.6. For Llama-3.2-11B-Vision, CAREVL gets the highest Overall Accuracy. Compared to IXC-2.5-Reward, one of the top-performing LVLM reward models on VL-RewardBench, CAREVL exhibits distinct strengths, with both models excelling in different metrics and showcasing complementary advantages. Notably, while IXC-2.5-Reward leverages both open-source and in-house data, CAREVL is trained exclusively on open-source data. Furthermore, CAREVL provides interpretable justifications for preference judgments, whereas IXC-2.5-Reward outputs only numerical scores. These results indicate that our method robustly aligns LVLM outputs with human judgments.",
                    "score": 0.36021106881941967,
                    "section_title": "Evaluation on VL-RewardBench",
                    "char_start_offset": 16231,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 407
                        },
                        {
                            "start": 410,
                            "end": 558
                        },
                        {
                            "start": 559,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 787
                        },
                        {
                            "start": 788,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 1059
                        },
                        {
                            "start": 1062,
                            "end": 1190
                        },
                        {
                            "start": 1191,
                            "end": 1258
                        },
                        {
                            "start": 1259,
                            "end": 1475
                        },
                        {
                            "start": 1476,
                            "end": 1505
                        },
                        {
                            "start": 1506,
                            "end": 1602
                        },
                        {
                            "start": 1603,
                            "end": 1709
                        },
                        {
                            "start": 1710,
                            "end": 1740
                        },
                        {
                            "start": 1741,
                            "end": 1830
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.513671875
                }
            ],
            "relevance_judgement": 0.513671875,
            "relevance_judgment_input_expanded": "# Title: From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models\n# Venue: arXiv.org\n# Authors: Muzhi Dai, Jiashuo Sun, Zhiyuan Zhao, Shixuan Liu, Rui Li, Junyu Gao, Xuelong Li\n## Abstract\nAligning large vision-language models (LVLMs) with human preferences is challenging due to the scarcity of fine-grained, high-quality, and multimodal preference data without human annotations. Existing methods relying on direct distillation often struggle with low-confidence data, leading to suboptimal performance. To address this, we propose CAREVL, a novel method for preference reward modeling by reliably using both high- and low-confidence data. First, a cluster of auxiliary expert models (textual reward models) innovatively leverages image captions as weak supervision signals to filter high-confidence data. The high-confidence data are then used to fine-tune the LVLM. Second, low-confidence data are used to generate diverse preference samples using the fine-tuned LVLM. These samples are then scored and selected to construct reliable chosen-rejected pairs for further training. CAREVL achieves performance improvements over traditional distillation-based methods on VL-RewardBench and MLLM-as-a-Judge benchmark, demonstrating its effectiveness. The code will be released soon.\n## Evaluation on VL-RewardBench\nTable 1 summarizes the performance of our approach on the VL-RewardBench, evaluated across five metrics: General, Hallucination, Reasoning, Overall Accuracy, and Macro Average Accuracy. Our method is applied to three backbone models (LLaVA-OneVision-7B, Qwen2-VL-7B, and Llama-3.2-11B-Vision), and the results demonstrate substantial improvements over all the baseline versions and other competitive models. \n\nFor the LLaVA-OneVision backbone, the baseline model (LLaVA-OneVision-7B) achieves an Overall Accuracy of 29.6 and a Macro Average Accuracy of 36.5. In contrast, our enhanced version of LLaVA-OneVision-7B yields an Overall Accuracy of 68.7 and a Macro Average Accuracy of 67.4, accompanied by a significant boost in the General (72.8 vs. 32.2) and Hallucination (67.9 vs. 20.1) metrics. Compared with LLaVA-Critic-7b, which is also trained as a reward model based on LLaVA-OneVision-7B, CAREVL achieves higher performance across all metrics of VL-RewardBench while using less training data (75k vs. 113k), indicating the effectiveness of our method. \n\nSimilarly, for Qwen2-VL-7B, CAREVL elevates the Overall Accuracy from 28.3 to 67.8 and Macro Average Accuracy from 33.9 to 65.6. For Llama-3.2-11B-Vision, CAREVL gets the highest Overall Accuracy. Compared to IXC-2.5-Reward, one of the top-performing LVLM reward models on VL-RewardBench, CAREVL exhibits distinct strengths, with both models excelling in different metrics and showcasing complementary advantages. Notably, while IXC-2.5-Reward leverages both open-source and in-house data, CAREVL is trained exclusively on open-source data. Furthermore, CAREVL provides interpretable justifications for preference judgments, whereas IXC-2.5-Reward outputs only numerical scores. These results indicate that our method robustly aligns LVLM outputs with human judgments.",
            "reference_string": "[276903685 | Dai et al. | 2025 | Citations: 1]"
        },
        {
            "title": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 49,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.11187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2174869132",
                    "name": "Leqi Shen"
                },
                {
                    "authorId": "2350347193",
                    "name": "Guoqiang Gong"
                },
                {
                    "authorId": "2350871279",
                    "name": "Tao He"
                },
                {
                    "authorId": "2319431168",
                    "name": "Yifeng Zhang"
                },
                {
                    "authorId": "3340237",
                    "name": "Pengzhang Liu"
                },
                {
                    "authorId": "2243033701",
                    "name": "Sicheng Zhao"
                },
                {
                    "authorId": "2323657703",
                    "name": "Guiguang Ding"
                }
            ],
            "abstract": "Video Large Language Models have demonstrated strong video understanding capabilities, yet their practical deployment is hindered by substantial inference costs caused by redundant video tokens. Existing pruning techniques fail to fully exploit the spatiotemporal redundancy inherent in video data. To bridge this gap, we perform a systematic analysis of video redundancy from two perspectives: temporal context and visual context. Leveraging these insights, we propose Dynamic Density Pruning for Fast Video LLMs termed FastVID. Specifically, FastVID dynamically partitions videos into temporally ordered segments to preserve temporal structure and applies a density-based token pruning strategy to maintain essential visual information. Our method significantly reduces computational overhead while maintaining temporal and visual integrity. Extensive evaluations show that FastVID achieves state-of-the-art performance across various short- and long-video benchmarks on leading Video LLMs, including LLaVA-OneVision and LLaVA-Video. Notably, on LLaVA-OneVision-7B, FastVID effectively prunes $\\textbf{90.3%}$ of video tokens, reduces FLOPs to $\\textbf{8.3%}$, and accelerates the prefilling stage by $\\textbf{7.1}\\times$, while maintaining $\\textbf{98.0%}$ of the original accuracy. The code is available at https://github.com/LunarShen/FastVID.",
            "corpus_id": 277043430,
            "sentences": [
                {
                    "corpus_id": "277043430",
                    "title": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models",
                    "text": "These benchmarks cover a wide range of video complexities and durations, ensuring a comprehensive evaluation. Notably, FastVID prunes 90% of video tokens while maintaining 98.0% of LLaVA-OneVision's original performance across all benchmarks. \n\nThe main contributions are summarized as follows:",
                    "score": 0.4026189711934046,
                    "section_title": "Introduction",
                    "char_start_offset": 4402,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 109
                        },
                        {
                            "start": 110,
                            "end": 242
                        },
                        {
                            "start": 245,
                            "end": 294
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5126953125
                },
                {
                    "corpus_id": "277043430",
                    "title": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models",
                    "text": "Video LLMs. With the rapid advancement of LMMs [1,6,32] and MLLMs [2,15,[19][20][21]33], there has been growing interest in Video LLMs. Existing Video LLMs can be broadly categorized based on how they process video tokens: general Video LLMs and Video LLMs with trainingtime compression. General Video LLMs [5,14,17,18,35,43] directly process raw video tokens or apply pooling. Video-LLaVA [17] leverages shared projection layers to obtain unified visual representations. LLaVA-OneVision [14] demonstrates strong video understanding through task transfer from images. LLaVA-Video [43] creates a high-quality synthetic dataset for video instruction-following. To better capture the spatiotemporal structure of video, some models [35,43] introduce additional designs for video positional information. Qwen2-VL [35] propose M-RoPE to decompose rotary embedding into temporal, height, and width components. LLaVA-Video 1 introduces newline tokens to distinguish spatial and temporal positions effectively. \n\nVideo LLMs with training-time compression [13,22,28,29,37,45] aim to significantly reduce the number of video tokens, enabling longer video sequences. PLLaVA [37] introduces an adaptive average structure pooling to extend image LLMs. Video-ChatGPT [22] extracts both spatial and temporal features through temporal and spatial pooling respectively. Chat-UniVi [13] progressively clusters visual tokens and provides multi-scale features. MovieChat [29] introduces a memory management mechanism to enhance long video understanding. LongVU [28] employs crossmodal query and inter-frame dependencies to adaptively reduce video redundancy. Apollo [45] explores scaling consistency and uses the Perceiver Resampler [12]. \n\nHowever, general Video LLMs remain the dominant paradigm, with LLaVA-OneVision being widely adopted due to its adaptability and superior performance. Moreover, Video LLMs with training-time compression often exhibit lower performance compared to their general counterparts.",
                    "score": 0.3608709337475924,
                    "section_title": "Related Work",
                    "char_start_offset": 4713,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 11
                        },
                        {
                            "start": 12,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 287
                        },
                        {
                            "start": 288,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 567
                        },
                        {
                            "start": 568,
                            "end": 658
                        },
                        {
                            "start": 659,
                            "end": 798
                        },
                        {
                            "start": 799,
                            "end": 902
                        },
                        {
                            "start": 903,
                            "end": 1001
                        },
                        {
                            "start": 1004,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1237
                        },
                        {
                            "start": 1238,
                            "end": 1351
                        },
                        {
                            "start": 1352,
                            "end": 1439
                        },
                        {
                            "start": 1440,
                            "end": 1532
                        },
                        {
                            "start": 1533,
                            "end": 1637
                        },
                        {
                            "start": 1638,
                            "end": 1717
                        },
                        {
                            "start": 1720,
                            "end": 1869
                        },
                        {
                            "start": 1870,
                            "end": 1993
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 66,
                            "end": 69,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 69,
                            "end": 72,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 72,
                            "end": 76,
                            "matchedPaperCorpusId": "263672058"
                        },
                        {
                            "start": 80,
                            "end": 84,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 1046,
                            "end": 1050,
                            "matchedPaperCorpusId": "265157455"
                        },
                        {
                            "start": 1050,
                            "end": 1053,
                            "matchedPaperCorpusId": "259108333"
                        },
                        {
                            "start": 1056,
                            "end": 1059,
                            "matchedPaperCorpusId": "260333927"
                        },
                        {
                            "start": 1252,
                            "end": 1256,
                            "matchedPaperCorpusId": "259108333"
                        },
                        {
                            "start": 1363,
                            "end": 1367,
                            "matchedPaperCorpusId": "265157455"
                        },
                        {
                            "start": 1450,
                            "end": 1454,
                            "matchedPaperCorpusId": "260333927"
                        },
                        {
                            "start": 1712,
                            "end": 1716,
                            "matchedPaperCorpusId": "232110866"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.451171875
                }
            ],
            "relevance_judgement": 0.5126953125,
            "relevance_judgment_input_expanded": "# Title: FastVID: Dynamic Density Pruning for Fast Video Large Language Models\n# Venue: arXiv.org\n# Authors: Leqi Shen, Guoqiang Gong, Tao He, Yifeng Zhang, Pengzhang Liu, Sicheng Zhao, Guiguang Ding\n## Abstract\nVideo Large Language Models have demonstrated strong video understanding capabilities, yet their practical deployment is hindered by substantial inference costs caused by redundant video tokens. Existing pruning techniques fail to fully exploit the spatiotemporal redundancy inherent in video data. To bridge this gap, we perform a systematic analysis of video redundancy from two perspectives: temporal context and visual context. Leveraging these insights, we propose Dynamic Density Pruning for Fast Video LLMs termed FastVID. Specifically, FastVID dynamically partitions videos into temporally ordered segments to preserve temporal structure and applies a density-based token pruning strategy to maintain essential visual information. Our method significantly reduces computational overhead while maintaining temporal and visual integrity. Extensive evaluations show that FastVID achieves state-of-the-art performance across various short- and long-video benchmarks on leading Video LLMs, including LLaVA-OneVision and LLaVA-Video. Notably, on LLaVA-OneVision-7B, FastVID effectively prunes $\\textbf{90.3%}$ of video tokens, reduces FLOPs to $\\textbf{8.3%}$, and accelerates the prefilling stage by $\\textbf{7.1}\\times$, while maintaining $\\textbf{98.0%}$ of the original accuracy. The code is available at https://github.com/LunarShen/FastVID.\n## Introduction\nThese benchmarks cover a wide range of video complexities and durations, ensuring a comprehensive evaluation. Notably, FastVID prunes 90% of video tokens while maintaining 98.0% of LLaVA-OneVision's original performance across all benchmarks. \n\nThe main contributions are summarized as follows:\n\n## Related Work\nVideo LLMs. With the rapid advancement of LMMs [1,6,32] and MLLMs [2,15,[19][20][21]33], there has been growing interest in Video LLMs. Existing Video LLMs can be broadly categorized based on how they process video tokens: general Video LLMs and Video LLMs with trainingtime compression. General Video LLMs [5,14,17,18,35,43] directly process raw video tokens or apply pooling. Video-LLaVA [17] leverages shared projection layers to obtain unified visual representations. LLaVA-OneVision [14] demonstrates strong video understanding through task transfer from images. LLaVA-Video [43] creates a high-quality synthetic dataset for video instruction-following. To better capture the spatiotemporal structure of video, some models [35,43] introduce additional designs for video positional information. Qwen2-VL [35] propose M-RoPE to decompose rotary embedding into temporal, height, and width components. LLaVA-Video 1 introduces newline tokens to distinguish spatial and temporal positions effectively. \n\nVideo LLMs with training-time compression [13,22,28,29,37,45] aim to significantly reduce the number of video tokens, enabling longer video sequences. PLLaVA [37] introduces an adaptive average structure pooling to extend image LLMs. Video-ChatGPT [22] extracts both spatial and temporal features through temporal and spatial pooling respectively. Chat-UniVi [13] progressively clusters visual tokens and provides multi-scale features. MovieChat [29] introduces a memory management mechanism to enhance long video understanding. LongVU [28] employs crossmodal query and inter-frame dependencies to adaptively reduce video redundancy. Apollo [45] explores scaling consistency and uses the Perceiver Resampler [12]. \n\nHowever, general Video LLMs remain the dominant paradigm, with LLaVA-OneVision being widely adopted due to its adaptability and superior performance. Moreover, Video LLMs with training-time compression often exhibit lower performance compared to their general counterparts.",
            "reference_string": "[277043430 | Shen et al. | 2025 | Citations: 2]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "271719914",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "text": "-Bo Li contributes to maintaining the LLaVA-OneVision codebase, conducting the large-scale training of the LLaVA-OneVision models of all stages (including the stage with single-image, multi-image, and video data), based on upon our previous LLaVA-NeXT series. He contributes significantly to the single-image development such as LLaVA-NeXT-Ablations [64], high-quality recpationing, as well as collection and curation of the single-image data mixture. \n\n-Yuanhan Zhang contributes to a series of works in LLaVA-NeXT-Video [169], including video training and inference codebase, an effective pipeline for high-quality video data generation, and all the video training data. \n\n-Dong Guo contributes to collection and curation of the single-image data mixture and consistently provides technical support throughout the project. \n\n-Feng Li, Renrui Zhang, and Hao Zhang contribute to LLaVA-NeXT-Interleave [68], including the multi-image instruction data mixture, the multi-image evaluation benchmarks, and the early prototype of LLaVA-OneVision, i.e., a joint training stage with single-image, multi-image, and videos. They also contribute to the collection and curation of the single-image data mixture. \n\n-Kaichen Zhang maintains the training codebase and contributes to the integration of LLaVA-OneVision model into LMMs-Eval's evaluation pipeline. \n\n-Yanwei Li contributes to revising the paper. \n\n-Ziwei Liu makes valuable suggestions throughout the projects. \n\n-Chunyuan Li initiates and leads the series of projects, designs the roadmap and milestones, drives the excution, as well as leads the the paper writing.",
            "score": 0.6353682871620958,
            "section_title": "B Author Contributions",
            "char_start_offset": 38848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1200
                },
                {
                    "start": 1203,
                    "end": 1347
                },
                {
                    "start": 1350,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1616
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447265625
        },
        {
            "corpus_id": "273098427",
            "title": "Video Instruction Tuning With Synthetic Data",
            "text": "Note that the training set for LLaVA-Video includes six datasets: LLaVA-Video-178K, LLaVA-Hound (Zhang et al., 2024d), NExT-QA (Xiao et al., 2021), ActivityNet-QA (Yu et al., 2019), PerceptionTest (P\u0203tr\u0203ucean et al., 2023), and image data from LLaVA-OneVision (Li et al., 2024c). In this section, we conduct ablation studies to assess the impact of each dataset. We separately fine-tune the LLaVA-OneVision (SI) model for each experimental setting, progressively adding datasets to the baseline. We use a video representation defined by V = (64, 679, 1, 2) The results are presented in Table 3. Initially, we used a basic model trained solely on the LLaVA-Hound dataset as our baseline. Compared to this baseline, adding the LLaVA-Video-178K dataset significantly improved performance, enhancing scores in both in-domain and out-of-domain tasks. Specifically, we observed a 31.9-point increase in NExT-QA scores and a 9.1-point rise in VideoMME scores. Furthermore, including the PerceptionTest dataset significantly enhanced its associated task. Additionally, integrating high-quality image data provided modest benefits on EgoSchema.",
            "score": 0.596690286593443,
            "section_title": "DATASET ABLATION",
            "char_start_offset": 22316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1135
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 146,
                    "matchedPaperCorpusId": "234763093"
                },
                {
                    "start": 163,
                    "end": 179,
                    "matchedPaperCorpusId": "69645185"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32666015625
        },
        {
            "corpus_id": "276421413",
            "title": "MMRC: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation",
            "text": "LLaVA-V1.5-7B, LLaVA-V1.5-13B, MiniCPM-8B, LLaVA-Next-0.5B, LLaVA-Next-7B, Qwen2VL-2B, Qwen2VL-7B, LLaVA-OneVision-0.5B, LLaVA-OneVision-7B, VILA1.5-3B, VILA1.5-8B, mplug-Ow3-1B, mplug-Ow3-2B, mplug-Ow3-7B use 16-bit floating-point precision, while Qwen2VL-72B and LLaVA-OneVision-72B use 4-bit quantization. Their output length is limited to a maximum of 256 tokens. The models utilize the default values for Temperature, Top-k Sampling, and Top-p Sampling as specified in their Hugging Face inference code.",
            "score": 0.5669677216029406,
            "section_title": "G Experimental Settings",
            "char_start_offset": 30570,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 508
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02044677734375
        },
        {
            "corpus_id": "276929450",
            "title": "QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long Video Comprehension",
            "text": "Notably, QuoTA can adapt to a given token budget. \n\nWe conduct experiments across diverse video understanding benchmarks, including Video-MME [7], MLVU [51], LongVideoBench [37], VNBench [49], MVBench [15], and NeXT-QA [39]. Results demonstrate that LLaVA-Video-7B [48] and LLaVA-OneVision-7B [13], augmented with QuoTA in a plug-and-play manner, achieve an average performance improvement of 3.2% and 2.5% across all six benchmarks while maintaining equivalent computational requirements to their original baseline. Furthermore, as shown in Figure 1, QuoTA outperforms recent state-of-theart approaches AIM [50] and FrameFusion [9] across varying visual token budgets when applied to LLaVA-Video-7B [48] and LLaVA-OneVision-7B [13], while maintaining consistent performance improvements over the baseline regardless of token budget, which is set to 64 frames with 196 tokens per frame (12,544 visual",
            "score": 0.5639023357482043,
            "section_title": "Introduction",
            "char_start_offset": 4540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 52,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 900
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 219,
                    "end": 223,
                    "matchedPaperCorpusId": "234763093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.276123046875
        },
        {
            "corpus_id": "275118957",
            "title": "MVTamperBench: Evaluating Robustness of Vision-Language Models",
            "text": "Model Families and Versions Figure 13 provides a comprehensive taxonomy of Large Multimodal Models (LMMs), showcasing their diversity across families, versions, and first-generation re-  This taxonomy highlights the wide range of LMMs currently available, organized as follows: \n\n\u2022 InternVL Family: Spanning models from Intern-VL1.1 (Chen et al., 2024c;Gao et al., 2024;Chen et al., 2024b) to the advanced Intern-VL2.5- MPO (Chen et al., 2024d), this family emphasizes efficiency and adaptability for diverse tampering and multimodal tasks. \n\nSuccessive iterations demonstrate marked improvements, particularly in handling temporal disruptions such as Dropping and Substitution. The InternVL2-5-8B models showcases its ability to handle fine-grained spatiotemporal reasoning, highlighting its dominance in high-resource benchmarks. \n\n\u2022 LLaVA Family: Starting with LLaVa-NEXT (Li et al., 2024d), which struggled across most benchmarks, this family has evolved with models like LLaVa-OneVision (Li et al., 2024b) and LLaVa-Video (Zhang et al., 2024), demonstrating significant improvements in task-specific video understanding through op- timized pretraining and alignment techniques. Despite the advancements, LLaVa-OneVision & LLaVa-Video continues to face challenges in handling complex temporal disruptions, unlike Chat-UniVi, which has emerged as a robust alternative. \n\n\u2022 VILA Family: The VILA series, including VILA-1.5 (Lin et al., 2024b)   For low-performing models (Figure 14a), Llava-OneVision exhibits consistently weak performance across tampering types, even at larger parameter sizes (e.g., Llava-OneVision-72B). This suggests potential architectural and training data limitations, particularly for temporal coherence tasks. Interestingly, Qwen2-VL-7B underperforms significantly compared to its larger counterpart, Qwen2-VL-72B, which achieves a notable improvement. This indicates that increasing model size, combined with its training paradigm, positively impacts robustness for this family.",
            "score": 0.5436255104490364,
            "section_title": "Overview of Large Multimodality Models (LMMs)",
            "char_start_offset": 20007,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 280,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 1425,
                    "end": 1444,
                    "matchedPaperCorpusId": "266174746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280029296875
        },
        {
            "corpus_id": "271719914",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "text": "It is a core aspiration in AI to build general-purpose assistants with Large Multimodal Models (LMM) [67]. LLaVA-OneVision is an open model, continuing to advance the line of research in building large vision-and-language assistant (LLaVA) [83] that can follow diverse instructions to complete a variety of computer vision tasks in the wild. As a cost-efficient recipe, it is typically developed by connecting vision encoders with large language models (LLM) using a simple connection module. \n\nThe first LLaVA model [83] demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors similar to GPT-4V on previously unseen images and instructions for the first time. LLaVA-1.5 [81] significantly expands and improves the capabilities by incorporating more academicrelated instruction data, achieving SoTA performance on a dozens of benchmarks with a data-efficient recipe. LLaVA-NeXT [82] inherits this property, further pushing performance boundaries through three key techniques: AnyRes for handling high-resolution images, expanding high-quality instruction data, and utilizing the best open LLM available at the time. \n\nLLaVA-NeXT provides an extendable and scalable prototype, which facilitates several parallel explorations, reported in the LLaVA-NeXT blog series [82,169,65,64,68]: https://llava-vl.github.io/blog/ \u2022 The Video blog [169] shows that the image-only-trained LLaVA-NeXT model is surprisingly strong on video tasks with zero-shot modality transfer, due to the design of AnyRes to digest any vision signals as a sequence of images. \u2022 The Stronger blog [65] demonstrates the LLM model scaling succuss of this cost-efficient strategy. By simply scaling up the LLM, it achieves performance comparable to GPT-4V on selected benchmarks. \n\n\u2022 The Ablation blog [64] summarizes our empirical exploration except the visual instruction data itself, including the choice of architectures (scaling of LLM & vision encoder), visual representations (resolution & #tokens), as well as training strategies (trainable modules & high-quality data) in the pursuit of data scaling success.",
            "score": 0.54266803324375,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1773
                },
                {
                    "start": 1776,
                    "end": 2111
                }
            ],
            "ref_mentions": [
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82470703125
        },
        {
            "corpus_id": "273186838",
            "title": "Intriguing Properties of Large Language and Vision Models",
            "text": "\u2022 LLaVA-1.5 (Liu et al., 2024a) incorporates academic task-oriented datasets to enhance performance in VQA tasks and features an MLP vision-language connector, which improves upon the original linear layer utilized in LLaVA (Liu et al., 2024c). It uses CLIP ViT-L/14 (Radford et al., 2021) with a 336px resolution as its vision encoder, resulting in a total of (336/14) 2 = 576 visual tokens. LLaVA-1.5 is built on Vicuna with either 7B or 13B parameters. The training dataset includes 558K samples for pre-training and 665K for fine-tuning, totaling 1.2M image-text pairs from publicly available datasets \n\n\u2022 LLaVA-NeXT (Liu et al., 2024b) (also known as LLaVA-1.6) enhances visual reasoning, OCR, and world knowledge, offering four times higher image resolution (up to 1344x336) and improved performance in visual conversations. Its architecture includes a CLIP ViT-L/14 as a vision encoder, paired with Vicuna models ranging from 7B to 34B as a backbone language model. It utilizes 1.3M visual instruction tuning data samples for training, maintaining efficiency with approximately one day of training on 32 A100 GPUs. The architecture's high resolution and dynamic grid scheme improve detailed image processing capabilities. \n\n\u2022 LLaVA-OneVision (Li et al., 2024b) is a LVLM designed for task transfer across singleimage, multi-image, and video scenarios, with strong capabilities in video understanding through image-to-video task transfer. Its architecture consists of a Qwen2 language model (Yang et al., 2024) with 8B to 72B parameters, and the SigLIP vision encoder (Zhai et al., 2023), which processes images at a base resolution of 384x384, producing 729 visual tokens. The model employs a 2-layer MLP projector.",
            "score": 0.5377326672011338,
            "section_title": "B DESCRIPTION OF EVALUATION LVLMS",
            "char_start_offset": 39162,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1722
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 31,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 224,
                    "end": 243,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 267,
                    "end": 289,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1249,
                    "end": 1266,
                    "matchedPaperCorpusId": "260334888"
                },
                {
                    "start": 1574,
                    "end": 1593,
                    "matchedPaperCorpusId": "257767223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6640625
        },
        {
            "corpus_id": "277321635",
            "title": "Beyond Intermediate States: Explaining Visual Redundancy through Language",
            "text": "We conduct token-centric and context-centric experiments on three MLLMs: LLaVA-1.5, LLaVA-Next, and LLaVA-OneVision, subsequently constructing separate redundancy codebooks for each model. For LLaVA-Next and LLaVA-OneVision, the evaluation is conducted without splitting input images into sub-images, retaining only the base image features.",
            "score": 0.5371336452595677,
            "section_title": "Visual Redundancy Analysis Details",
            "char_start_offset": 27795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 340
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2493896484375
        },
        {
            "corpus_id": "277667536",
            "title": "MM-IFEngine: Towards Multimodal Instruction Following",
            "text": "We present the performance comparison results of various MLLMs on our MM-IFEval in Tab. 3, including both proprietary MLLMs such as GPT-4o [13] and Claude-3.5 [1] and open-source MLLMs such as LLaVA-Next [21], LLaVA-OneVision [16], InternVL [6,7], and Qwen2-VL [41].",
            "score": 0.5354125430297214,
            "section_title": "Leaderboard of MM-IFEval",
            "char_start_offset": 24604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05633544921875
        },
        {
            "corpus_id": "273351277",
            "title": "Automatically Generating Visual Hallucination Test Cases for Multimodal Large Language Models",
            "text": "LLaVA-1.5 [2] CLIP-ViT-L/14 [27] 2-layer MLP Llama2-7B [9] InstructBLIP [33] EVA-CLIP ViT-g/14 [29] Q-Former [4] Vicuna-7B [11] Qwen-VL-Chat [8] OpenCLIP ViT-bigG [30] 1-layer Cross-Attention Qwen-7B [3] LLaVA-NEXT [6] CLIP-ViT-L/14 2-layer MLP Llama3-8B [7] LLaVA-OneVision [1] SigLIP ViT-SO400M/14 [32] 2-layer MLP Qwen2-7B [10] Cambrian-1 [5] CLIP ViT-L/14 Spatial Visual Aggregator [5] Llama3-8B SigLIP ViT-SO400M/14 OpenCLIP ConvNeXt-XXL [31] DINOv2 ViT-L/14 [28] GPT-4o [38] --- In contrast, POPE focuses on VQA test cases related to existence VH, specifically identifying whether an object is present in an image. Table 1 summarizes the key statistics of these datasets. Note that for VHTest and POPE, a single image can be used in multiple VH test cases. MLLMs: In our experiments, we evaluate seven MLLMs in total. In particular, six of these models are opensource, including LLaVA-1.5 [2], InstructBLIP [33], Qwen-VL-Chat [8], LLaVA-NeXT [6], LLaVA-OneVision [1], and Cambrian-1 [5], alongside one closed-source model, GPT-4o [38]. These MLLMs demonstrate state-of-theart performance across various VQA benchmarks and have diverse model architectures. Table 2 shows details of these MLLMs.",
            "score": 0.5258055421094048,
            "section_title": "MLLM Vision Encoder Connector LLM",
            "char_start_offset": 18187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1199
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 13,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 72,
                    "end": 76,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 95,
                    "end": 99,
                    "matchedPaperCorpusId": "253510587"
                },
                {
                    "start": 109,
                    "end": 112,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 300,
                    "end": 304,
                    "matchedPaperCorpusId": "257767223"
                },
                {
                    "start": 443,
                    "end": 447,
                    "matchedPaperCorpusId": "245837420"
                },
                {
                    "start": 895,
                    "end": 898,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 913,
                    "end": 917,
                    "matchedPaperCorpusId": "258615266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03662109375
        },
        {
            "corpus_id": "276725366",
            "title": "VideoA11y: Method and Dataset for Accessible Video Description",
            "text": "7.1.1 Baseline Models. \n\nWe assessed four open-source models, including Video-LLaVA-7B [43], VILA1.5-40B [46], LLaVA-NeXT-Video-32B [89], and LLaVA-OneVision-72B [40]. To ensure a fair comparison, we adhered to the original settings for all models, including the number of frames and inference hyperparameters. We then used each MLLM to generate descriptions for the VideoA11y-40K test set without incorporating VideoA11y.",
            "score": 0.5206678259986581,
            "section_title": "Benchmarking VideoA11y",
            "char_start_offset": 38797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 5
                },
                {
                    "start": 6,
                    "end": 22
                },
                {
                    "start": 25,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 422
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 109,
                    "matchedPaperCorpusId": "266174746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12939453125
        },
        {
            "corpus_id": "271719914",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "text": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.",
            "score": 0.5166824859576965,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84716796875
        },
        {
            "corpus_id": "277321635",
            "title": "Beyond Intermediate States: Explaining Visual Redundancy through Language",
            "text": "However, for LLaVA-Next and LLaVA-OneVision, we observe that they often refuse to generate responses when provided with  only a single visual token. To address this issue, we repeat v 1\u00d7d single 24 times ( \u221a 576) for LLaVA-Next and 27 times \u221a 729 for LLaVA-OneVision 3 . We then append a special image newline token after the repeated visual tokens, following the official setting. This operation constructs a \"synthesized image line\" that only contains the visual information of the original single visual token, leading to more reliable results.",
            "score": 0.5113452379700069,
            "section_title": "Single Visual Token Input Experiment Details",
            "char_start_offset": 31976,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 547
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1536865234375
        },
        {
            "corpus_id": "276482898",
            "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation",
            "text": "We compare recent open-source VLMs with a similar scale (7B), including PaliGemma-3B (Beyer et al., 2024), BLIP-3-4B (Xue et al., 2024), Cambrian-7B (Tong et al., 2024), LLaVA-1.5-7B (Liu et al., 2023), LLaVA-Next-8B (Liu et al., 2024), LLaVA OneVision-7B (Li et al., 2024), Pixtral-12B (Agrawal et al., 2024), Llama 3.2 V (Meta, 2024). We also include proprietary models: GPT-4V (OpenAI, 2023), Gemini-1.5-Flash (Team, 2024), and Claude-3 Opus (Anthropic, 2024a). \n\nImplementation Details. We train our model on TPU v3-128 with a batch size of 32. Our bestperforming model is trained for 60K steps, taking about 30 hours. The checkpoints with the highest validation performance are retained for testing.",
            "score": 0.5040259623208729,
            "section_title": "Baselines.",
            "char_start_offset": 14899,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 704
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08465576171875
        },
        {
            "corpus_id": "276929450",
            "title": "QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long Video Comprehension",
            "text": "We evaluate QuoTA implemented within LLaVA-Video [48] and LLaVA-OneVision [13] at 7B-scale, maintaining equivalent computational constraints (total visual tokens budget N t ) as the baseline across three long video understanding benchmarks: LongVideoBench [37], MLVU [51] and Video-MME [7]. The empirical outcomes presented in Table 1 demonstrate that QuoTA integration into LLaVA-Video-7B [48], yielding improvements of 0.8%, 1.1%, and 2.6% on LongVideoBench [37], MLVU [51], and Video-MME [7] (w/o subtitles), respectively. Notably, substantial performance enhancements manifest in extended-duration video (spanning 30-60 minutes) within Video-MME [7] (47.7% \u2192 52.2% for LLaVA-OneVision [13], and 51.8% \u2192 55.7% for LLaVA-Video [48]) under \"w/o subtitles\" conditions, substantiating that our query-oriented token assignment methodology together with dynamic frame sampling strategy effectively mitigates information redundancy (particularly pronounced in long videos) while accentuating salient content, thereby facilitating enhanced model activation and comprehension of complex visual narratives. Furthermore, we conduct evaluations of QuoTA on two conventional video understanding benchmarks, MVBench [15] and NeXT-QA [39], alongside the specifically constructed Needle-In-A-Haystack video benchmark VBNench [49], as illustrated in Table 2. Particularly noteworthy is the substantial enhancement observed on VNBench [49] (44.7% \u2192 49.3% for LLaVA-OneVision [13] [37], MLVU [51] and Video-MME [7]. By applying QuoTA to LLaVA-Video-7B [48], we observed an average performance improvement of 1.5% across three long video understanding benchmarks while setting new state-of-the-art. * denotes using the same visual token budget as the baseline. Models in parentheses represent the baselines they used.",
            "score": 0.5013174640383969,
            "section_title": "Main Results",
            "char_start_offset": 17987,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1800
                }
            ],
            "ref_mentions": [
                {
                    "start": 1205,
                    "end": 1209,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 1222,
                    "end": 1226,
                    "matchedPaperCorpusId": "234763093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.458740234375
        },
        {
            "corpus_id": "275324269",
            "title": "HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding",
            "text": "The quantitative results for frame-level, within-event-level, cross-event-level, and long-term-level tasks, as shown in Table II, highlight several key insights. 1) Specialized models with more parameters, such as LLaVA-Video, LLaVA-OneVision, and QWen2-VL (each with 72 billion parameters), exhibit significantly improved performance, achieving the highest overall scores. However, 2) larger commercial models like Gemini 1.5 Pro and GPT-4o do not perform as well in long-video understanding tasks, with Gemini 1.5 Pro scoring significantly lower at 62.41 compared to LLaVA-Video's score at 78.93. 3) Models that accept fewer frames, such as Claude 3.5 Sonnet, which processes only 20 frames, show a marked decline in understanding and performance, underscoring the importance of frame count in video comprehension. 4) The performance of different models varies across task levels. For frame-level tasks, LLaVA-Video and LLaVA-OneVision excel, while within-eventlevel and cross-event-level tasks present greater challenges,    with LLaVA-Video maintaining strong performance. Longterm-level tasks, requiring deep understanding of temporal dependencies, are the most challenging, with LLaVA-Video achieving the highest score, demonstrating its superior capability in handling complex, long-term video understanding. \n\nThe radar chart in Fig. 4 illustrates the performance of various models across different QA tasks, highlighting several key insights. 1) Different models excel in different areas, with varying performance all tasks for ecah model. For instance, LLaVA-Video-72B and LLaVA-OneVision-72B show strong performance in tasks such as character, object, plot and scene understanding, while models like GPT-4o performs better in the speed task than some other tasks. 2) All models struggle with tasks requiring deep understanding, such as timeorder, temporal-relationship, counting, and camera-direction, indicating a general weakness in these areas. This suggests that while some models are adept at specific tasks, there is still a significant challenge in achieving comprehensive understanding across all QA tasks, particularly those involving complex temporal and spatial reasoning.",
            "score": 0.5009237154336238,
            "section_title": "B. Results & Analysis",
            "char_start_offset": 13711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2194
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1402587890625
        },
        {
            "corpus_id": "277104362",
            "title": "Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided Visual Tokenizer and Manufacturing Process",
            "text": "Our experiments mainly focus on how industrial knowledge boosts LMMs in IAD tasks through our method, including common results from general LMMs and LMM-based IAD methods and ablation experiments of different indus-trial contexts. Implementation Details Our method is implemented on both LLaVA-1.6 (the earlier version of LLaVA-NeXT [22]) and LLaVA-OneVision-ov (the checkpoint after the \"onevision stage\" of LLaVA-OneVision [16]), referring to them as Triad-llava-1.6 and Triad-ov, respectively. We build our approach on the LLaVA architecture by integrating our expert-guided region-of-interest (EG-RoI) tokenizer. Specifically, in the LLaVA-1.6 version, the original AnyRes module is replaced with the EG-RoI module. In contrast, for LLaVA-OneVision, we append the suspicious regions to the output of the AnyRes module to fully leverage its native anomaly detection capabilities. \n\nTo preserve the generalization of the base models, we supplement our IAD-related instruction data by sampling 12K pairs from the original fine-tuning datasets of both LLaVA-1.6 and LLaVA-OneVision. Moreover, since LLaVA-1.6 lacks inherent multi-image processing capabilities, we constructed a simple dual-image caption dataset using the COCO subset [21] from the ShareGPT4V dataset [5]. This dataset is exclusively used in the 1-shot setting to provide basic multi-image support. Given the distinct objectives of 0-/1-shot anomaly detection, we offer separate versions of Triad for each setting, with data organized by CoT-M with specific instruction. For the 1-shot version of Triad-llava-1.6, we integrate the zero-shot model with the one-shot model using a Confidence Voting Mechanism (see Sec. E.1) to mitigate its multi-image inability. \n\nBoth Triad-llava-1.6 and Triad-ov were trained on 4\u00d7A800 80G GPUs with a mega-batch size of 128.",
            "score": 0.4938515779401178,
            "section_title": "Experiments",
            "char_start_offset": 13160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1726
                },
                {
                    "start": 1729,
                    "end": 1825
                }
            ],
            "ref_mentions": [
                {
                    "start": 1234,
                    "end": 1238,
                    "matchedPaperCorpusId": "14113767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.454833984375
        },
        {
            "corpus_id": "273098155",
            "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
            "text": "To evaluate the consistency between LLaVA-Critic and GPT-4o [32] in pointwise scoring across different evaluation scenarios, as described in Sec. 4.2, we select 7 popular multimodal benchmarks and collect candidate responses from 13 commonly used LMMs alongside their GPT-4o evaluations, resulting in a total of 14174 examples (see details in Appendix B.2). LLaVA-Critic is then tasked with providing judgments on theses samples. We report Pearson correlation to measure the degree of alignment with GPT-4o in instance-level scoring. \n\nWe conduct experiments based on three different baseline models: LLaVA-NeXT (LLaMA-8B) [15,26], LLaVA-OneVision-7B, and LLaVA-OneVision-72B. As shown in Table 2, LLaVA-Critic variants significantly outperform their corresponding baseline models across all models and benchmarks. (i) Data scaling. By comparing the performance between v0.5 and full data trained LLaVA-Critic-7B, it concludes the necessity of larger size and diversity of instruction in training data. (ii) Model scaling. The best performance in terms of Pearson-r is achieved by LLaVA-Critic-72B with an average score of 0.754, which significantly outperforms the LLaVA-OV-72B baseline (0.634). This indicates that LLaVA-Critic-72B already possesses pointwise scoring capabilities that are quite aligned with GPT-4o. Despite a considerable reduction in model size, LLaVA-Critic-7B retains very strong point-wise scoring capabilities. With a score of 0.732, its shows minimal performance decline compared to LLaVA-Critic-72B, and significantly outperforms other advanced open-source LMMs of similar size, such as Qwen2-VL (0.352) and LLaMA3.2-Vision (0.359). This presents an advantage for deploying and utilizing LLaVA-Critic in resource-constrained environments. We also provide Kendall's Tau results in Appendix C.2 to assess model-level ranking, which reveal similar patterns and conclusions.",
            "score": 0.492043969170563,
            "section_title": "In-domain Pointwise Scoring",
            "char_start_offset": 20669,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1897
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2000732421875
        },
        {
            "corpus_id": "277955804",
            "title": "VideoPASTA: 7K Preference Pairs That Matter for Video-LLM Alignment",
            "text": "These results highlight VideoPASTA's ability to elevate smaller models to SOTA performance through targeted and dataefficient training. \n\nOutperforming Preference-Optimized Models. Compared to preference-optimized models, VideoPASTA combined with Qwen2.5VL outperforms LLaVA-Hound-DPO (Zhang et al., 2024c) and i-SRT (Ahn et al., 2024a) on all seven benchmarks and surpasses LLaVA-Video-TPO (Li et al., 2025) on six out of seven benchmarks (except NeX-TQA) by a significant margin. On the other hand, VideoPASTA paired with LLaVA-OneVision and InternVL2.5 outperforms LLaVA-Video-TPO on NeXTQA. These improvements highlight that our multi-dimensional approach, which tackles critical failure modes in Video-LLMs, addresses fundamental challenges in prior work while requiring minimal resources.",
            "score": 0.4900172632632088,
            "section_title": "Results",
            "char_start_offset": 16839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 138,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 794
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1278076171875
        },
        {
            "corpus_id": "270559956",
            "title": "On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning",
            "text": "Table 1 shows resource usage during inference for LLaVA and LLaVA-NeXT models, evaluated on the DocVQA and ChartQA test sets (Mathew et al., 2021;Masry et al., 2022). The LLaVA-1.5 models demonstrate manageable computational costs, operable on a single V100 GPU. However, LLaVA-NeXT models, with up to 2.9K tokens, present significant challenges. Testing on an NVIDIA V100 32GB reveals that LLaVA-NeXT-13/34B cannot be accommodated on a single GPU. These findings emphasize the challenges of larger models, especially in resource-constrained environments.",
            "score": 0.4879290943735285,
            "section_title": "Benchmarking Baseline Models",
            "char_start_offset": 7105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 555
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 146,
                    "matchedPaperCorpusId": "220280200"
                },
                {
                    "start": 146,
                    "end": 165,
                    "matchedPaperCorpusId": "247593713"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1373291015625
        },
        {
            "corpus_id": "273098427",
            "title": "Video Instruction Tuning With Synthetic Data",
            "text": "We conduct two ablation studies to further analyze our dataset and training strategy. As shown in Table 4, we compared three datasets where the language annotations are from GPT-4V/GPT-4o. For each experiment, we fine-tune the LLaVA-OneVision (SI) model separately on each specific dataset setting, utilizing a video representation defined by V = (64, 679, 1, 2). \n\nTwo group of experiments are considered to assess the data quality of LLaVA-Video-178K compare to LLaVA-Hound and ShareGPT4Video. In the first group, to compare LLaVA-Video-178K with LLaVA-Hound, we randomly selected 900K open-ended questions to match the number in LLaVA-Hound. We included all captions and did not sample the multiple-choice questions. In the second group, comparing LLaVA-Video-178K to ShareGPT4Video, we randomly sampled 40K video captions to align with those in ShareGPT4Video. \n\nSince ShareGPT4Video lacks open-ended and multiple-choice questions, we supplemented with annotations from NExT-QA, PerceptionTest, and ActivityNet-QA. In the first group of Table 4, we compare LLaVA-Video-178K with LLaVA-Hound. \n\nAlthough LLaVA-Hound has more captions than LLaVA-Video-178K, our results are still better. As shown in Table 1, despite LLaVA-Hound annotates more videos, its quality is limited due to two main issues: (1) Static video: Its primary video source is WebVid (Bain et al., 2021), which tends to have relatively static content. (2) Sparse sampling: Although it includes data sources with dynamic videos, its sampling rate of 10 frames per video leads to annotations that do not fully capture the complete plot of the video. This underscores that the quality of video instruction-following data is more important than its quantity. Additionally, the second experiment group in Table 4 shows that the model trained with LLaVA-Video-178K outperforms that of ShareGPT4Video, highlighting the superiority of our data's quality.",
            "score": 0.48284459788231293,
            "section_title": "DATASET COMPARISON",
            "char_start_offset": 23474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 363
                },
                {
                    "start": 366,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 1354,
                    "end": 1373,
                    "matchedPaperCorpusId": "232478955"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.219970703125
        },
        {
            "corpus_id": "273098427",
            "title": "Video Instruction Tuning With Synthetic Data",
            "text": "We fine-tune LLaVA-OneVision (SI) on the joint dataset of video and image data. Specifically, we added video data from the LLaVA-Video-178K dataset and four public datasets: ActivityNet-QA (Yu et al., 2019), NExT-QA (Xiao et al., 2021), PerceptionTest (P\u0203tr\u0203ucean et al., 2023), and LLaVA-Hound-255K (Zhang et al., 2024d), focusing on videos shorter than three minutes. These datasets were selected to improve our model's performance, contributing to a total of 1.6 million videolanguage samples, which include 193,510 video descriptions, 1,241,412 open-ended questions, and 215,625 multiple-choice questions. Remarkably, 92.2% of the video descriptions, 77.4% of the openended questions, and 90.9% of the multiple-choice questions were newly annotated. Additionally, we used 1.1 million image-language pairs from the LLaVA-OneVision model (Li et al., 2024c). We consider the same video representation configurations for the training and inference stages. On 128 NVIDIA H100 GPUs, the video representations for LLaVA-Video-7B and LLaVA-Video-72B are V = (64, 679, 1, 2) and V = (64, 679, 3, 2), respectively. \n\nIn Table 2, we compare the performance of different models on various video benchmarks. The 72B model performs as well as the commercial, closed-source model Gemini-1.5-Flash (Team et al., 2023), highlighting the effectiveness of open-source efforts in achieving comparable results. The LLaVA-Video-7B model outperforms the previous top model, LLaVA-OV-7B, in seven out of ten datasets. Analysis of individual datasets shows some noteworthy trends.",
            "score": 0.4786436060914149,
            "section_title": "OVERALL RESULTS",
            "char_start_offset": 19725,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1559
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 206,
                    "matchedPaperCorpusId": "69645185"
                },
                {
                    "start": 216,
                    "end": 234,
                    "matchedPaperCorpusId": "234763093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.362060546875
        },
        {
            "corpus_id": "270559937",
            "title": "VELOCITI: Benchmarking Video-Language Compositional Reasoning with Strict Entailment",
            "text": "We evaluate multiple open Video-LLMs: PLLaVA [48], Video-LLaVA (V-LLaVA) [28], Owl-Con [4], Qwen2-VL (QVL) [45], and LLaVA-OneVision (OV) [23]; closed models Gemini-1.5-Flash (Gem-1.5F), Gemini-1.5-Pro (Gem-1.5P) [10], GPT-4o [1]; and humans. Due to compute and cost constraints, we evaluate QVL-72B (at native video resolution), closed models, and humans on a subset of 150 samples from each test (created once by random selection). More details in Appendix A.6.",
            "score": 0.47535582385888175,
            "section_title": "Models.",
            "char_start_offset": 17069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 463
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 90,
                    "matchedPaperCorpusId": "265281387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043212890625
        },
        {
            "corpus_id": "273098155",
            "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
            "text": "The selected response models are listed below: \n\nGPT-4o [32], Claude3-Sonnet [2], LLaVA-NeXT (LLaMA-8B) [26], LLaVA-NeXT (Vicuna-7B) [26], LLaVA-OneVision-7B [16], LLaVA-RLHF-13B [38], LLaVA-v1.5-7B [25], LLaVA-v1.5-13B [25], InstructBLIP-Vicuna-7B [9], InternVL2-8B [8], Phi-3-Vision-128k-Instruct [1], fuyu-8B [5] and Qwen-VL-Chat [3]  We analyze the impact of different data components in the LLaVA-Critic-113k dataset for critic training. Results are reported in Table 8, presenting the following insights: 1) Pointwise and pairwise critic data are mutually beneficial.",
            "score": 0.47534395820543274,
            "section_title": "B.1. Evaluation Prompts For Critic Training",
            "char_start_offset": 34642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 49,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 573
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 220,
                    "end": 224,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03753662109375
        },
        {
            "corpus_id": "277452180",
            "title": "When 'YES' Meets 'BUT': Can Large Models Comprehend Contradictory Humor Through Comparative Reasoning?",
            "text": "Our experiments include both cutting-edge proprietary and open-source VLMs and LLMs, enabling a comprehensive evaluation across diverse architectures. For commercial VLMs, we use GPT-4o (gpt-4o-2024-08-06) and GPT-4-Vision-turbo (gpt-4-turbo-2024-04-09) 3 . Among open-source VLMs, our selection includes LLaVA-Next, covering 7B, 13B, and 72B parameter sizes [59], as well as LLaVA-1.5 in 7B and 13B variants [68]. We also incorporate CogVLM2 [69], Qwen2-VL with 7B and 72B versions [55], and LLaVA-OneVision, which is available in 7B and 72B configurations [70]. \n\nFor LLMs, we use the Llama 3 instruction variant in both 8B and 70B sizes [17], GPT-4 (gpt-4-0613), DeepSeek-R1-70B 3. https://platform.openai.com/docs/models/ (DeepSeek-R1-Distill-Llama-70B), and Qwen2.5, available in 7B and 72B versions.",
            "score": 0.4725924808646815,
            "section_title": "APPENDIX C EXPERIMENTS DETAILS C.1 Model Details",
            "char_start_offset": 58760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 805
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0762939453125
        },
        {
            "corpus_id": "277452239",
            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
            "text": "LLaVA (Large Language and Vision Assistant) (Liu et al., 2023) is a state-of-the-art multimodal model that integrates both visual and textual understanding, combining the capabilities of large language models (LLMs) with vision processing abilities. Its primary function is to interpret and generate responses to input that includes both images and text, making it ideal for tasks like visual question answering (VQA), image captioning, and other vision-language tasks. Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks. Various variants or specialization of the first LLaVA models have been produced so far. For example, TinyLLaVA (Zhou et al., 2024;Jia et al., 2024) is a compact refactored variant of the original LLaVA 1.5 model, designed to enable easier inclusion of alternative light vision and LLM models, thus significantly reducing overall model size and resource requirements. LLaVA-Med (Li et al., 2023a) is a specialized variant of the LLaVA model designed to assist in medical image analysis and diagnostics by fine-tuning its multimodal capabilities on medical datasets such as X-rays, MRIs, and other healthcare-related visual data.",
            "score": 0.47229667376940565,
            "section_title": "Model overview",
            "char_start_offset": 10302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2178
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.908203125
        },
        {
            "corpus_id": "277321635",
            "title": "Beyond Intermediate States: Explaining Visual Redundancy through Language",
            "text": "We follow the official evaluation toolkits for single image VQA, (MME, POPE, SEED, MMBench, and Re-alWorldQA), multi-image VQA (Mantis-test and MUIR-Bench) and video QA (MVBench). For the POPE benchmark, we report the averaged results across the random, popular, and adversarial subsets. For image captioning, we use the pycocoevalcap package to compute quantitative metrics. \n\nControlling the Number of Visual Tokens. During inference, the number of retained visual tokens R is controlled by the r threshold parameter in Algorithm 2. To ensure a fair comparison under the same visual token budget, we adjust r threshold so that the average number of visual tokens across all test samples is the same as that of LLaVA-Prumerge and FastV. Hyperparameters. For LLaVA-1.5 and LLaVA-Next, the thresholds \u03c4 prob , \u03c4 out , \u03c4 in , and \u03c4 jsd are set to 0.1, 8, 64, and 2e-3, respectively. For LLaVA-OneVision, these thresholds are set to 0.08, 3, 16 and 1.5e-3, respectively. These hyperparameter configurations result in 454, 969, and 310 redundant prototypes for LLaVA-1.5, LLaVA-Next, and LLaVA-OneVision, respectively. In the single-input experiment, the top-1 probability is computed among the top 50 ranked candidates. in the cascaded leave-one-out experiment, the JSD value is calculated among the top 20 ranked candidates. The results of the random baseline are obtained from three independent runs with different random seeds.",
            "score": 0.4722664198690715,
            "section_title": "Detailed Experiment Setting",
            "char_start_offset": 40311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1427
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.227783203125
        },
        {
            "corpus_id": "276574826",
            "title": "Forgotten Polygons: Multimodal Large Language Models are Shape-Blind",
            "text": "Models Evaluated: In our experiments, we evaluate 13 diverse MLLMs. We consider (1) general open-source models, including LLaVA-1.5-7B (Liu et al., 2024), LLaVA-Next-7B (Li et al., 2024b), LLaVA-OneVision-7B (Li et al., 2024a), Qwen2-VL-7B (Wang et al., 2024b), InternVL-8B (Wang et al., 2024b), Molmo-7B (Deitke et al., 2024), and DeepSeek's Janus Pro-7B (Chen et al., 2025); (2) math-specialized open-source models, including Math-LLaVA-13B (Shi et al., 2024), G-LLaVA-7B (Gao et al., 2023), and Math-PUMA-7B (Zhuang et al., 2024); and (3) closed-source models, including GPT-4-Turbo and GPT-4o. See Table 6 in Appendix A for full evaluation details.",
            "score": 0.4706201205076048,
            "section_title": "Experiments",
            "char_start_offset": 9090,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 652
                }
            ],
            "ref_mentions": [
                {
                    "start": 511,
                    "end": 532,
                    "matchedPaperCorpusId": "266976992"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06280517578125
        },
        {
            "corpus_id": "277104198",
            "title": "MDocAgent: A Multi-Modal Multi-Agent Framework for Document Understanding",
            "text": "\u2022 Qwen2-VL-7B-Instruct [38]: A large vision-language model developed by Alibaba, designed to handle multiple images as input. \u2022 Qwen2.5-VL-7B-Instruct [2]: An enhanced version of Qwen2-VL-7B-Instruct, offering improved performance in processing multiple images. \u2022 llava-v1.6-mistral-7b [22]: Also called LLaVA-NeXT, a vision-language model improved upon LLaVa-1.5, capable of interpreting and generating content from multiple images. \u2022 Phi-3.5-vision-instruct [1]: A model developed by Microsoft that integrates vision and language understanding, designed to process and generate responses based on multiple images. \u2022 llava-one-vision-7B [19]: A model trained on LLaVA-OneVision, based on Qwen2-7B language model with a context window of 32K tokens. \u2022 SmolVLM-Instruct [27]: A compact vision-language model developed by HuggingFace, optimized for handling image inputs efficiently. \u2022 ColBERTv2+Llama-3.1-8B-Instruct [12,32]: A textbased RAG pipeline that utilizes ColBERTv2 [32] for retrieving text segments and Llama-3.1-8B-Instruct as the LLM to generate responses. \u2022 M3DocRAG [5]: An image-based RAG pipeline that employs ColPali [9] for retrieving image segments and Qwen2-VL-7B-Instruct [38] as the LVLM for answer generation.",
            "score": 0.4702172968333761,
            "section_title": "A.1. Baseline Models",
            "char_start_offset": 25659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1231
                }
            ],
            "ref_mentions": [
                {
                    "start": 286,
                    "end": 290,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1133,
                    "end": 1136,
                    "matchedPaperCorpusId": "270870444"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1734619140625
        },
        {
            "corpus_id": "270380326",
            "title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs",
            "text": "We also include several prominent open-source models to provide a broader context for our evaluations. Specifically, we compare our VideoLLaMA 2 with VistaLLaMA (Ma et al., 2023), ChatUniVi (Jin et al., 2023b), LLaMA-VID (Li et al., 2023d), Video-LLaVA (Lin et al., 2023a), VideoChat2 (Li et al., 2023c), LLaVA-NeXT-Video Series13 (Liu et al., 2024b), VILA 1.5 (Lin et al., 2024), PLLaVA (Xu et al., 2024a), and LLaVA-OneVision (Li et al., 2024a). These open-source models are crucial for evaluating the performance of VideoLLaMA 2 within the context of accessible and reproducible research.",
            "score": 0.4687401692809069,
            "section_title": "Open-Source Models",
            "char_start_offset": 19609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 591
                }
            ],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 379,
                    "matchedPaperCorpusId": "266174746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1650390625
        },
        {
            "corpus_id": "275405970",
            "title": "Global Compression Commander: Plug-and-Play Inference Acceleration for High-Resolution Large Vision-Language Models",
            "text": "10.2. Results on LLaVA-OneVision-0.5B. \n\nWe adopt s sim i proposed in Section 8 as the measure for crop information richness and token informativeness, and evaluate our GlobalCom 2 on the LLaVA-OneVision model with more local crops across multiple benchmarks. \n\nOur experimental results show that GlobalCom 2 maintains satisfactory performance even under low retention ratios (R = 25%, 15%, 10%). Particularly, GlobalCom 2 achieves promising results on AI2D and MMStar benchmarks which involve high-resolution images. Interestingly, we observe that on the SQA benchmark, the model performance shows minimal degradation as token retention rate decreases. We attribute this to SQA's lower dependency on visual signals and higher reliance on LLM capabilities. Given that LLaVA-OneVision employs the powerful Qwen2 [44] as its LLM decoder, it consistently performs well on this benchmark regardless of retention ratio.",
            "score": 0.46631208553571424,
            "section_title": "Additional Experiments",
            "char_start_offset": 32621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 5
                },
                {
                    "start": 6,
                    "end": 38
                },
                {
                    "start": 41,
                    "end": 259
                },
                {
                    "start": 262,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 914
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.236328125
        },
        {
            "corpus_id": "277244459",
            "title": "When Less is Enough: Adaptive Token Reduction for Efficient Image Representation",
            "text": "The LLaVA-OneVision model includes a built-in compression mechanism, which reduces the size of the visual feature tensor through interpolation. In the LLaVA-OneVision implementation, interpolation compression is activated from a predefined reference size, and the compression ratio varies depending on the input image dimensions. For example, larger documents typically result in compression ratios of 1.7 to 2. \n\nSince our main goal is to demonstrate that our proposed selector effectively identifies informative visual features, we disabled the default compression mechanism in LLaVA-OneVision instead of combining two distinct compression methods. \n\nDuring preliminary experiments, we noticed that if the visual context exceeds the internally fixed context length of the model, the performance tends to degrade. Therefore, we limited our experiments to the range p \u2208 [0.1, 0.2, 0.3, 0.4, 0.5, 0.6 \u2248 1 1.7 ], ensuring that the length of the retained visual features does not exceed approximately 1  1.7 the original input length. Similar to the LLaVA-NeXT evaluations, the results for LLaVA-OneVision are divided into two categories, as depicted in Figs. 6 and 7. \n\nFor OCR-based benchmarks involving high-resolution images (Fig. 6), our trained selector consistently outperforms random selection, proving its ability to effectively preserve informative visual features. \n\nIn contrast, for non-OCR benchmarks (Fig. 7), the results are very similar to those observed previously with LLaVA-NeXT (Fig. 5), indicating that the performance is largely unaffected by the choice of selection method in these sce-",
            "score": 0.4662800880335053,
            "section_title": "LLaVA-OneVision: Our Selector vs. Random Selector",
            "char_start_offset": 16867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1606
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6474609375
        },
        {
            "corpus_id": "271719914",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "text": "We conduct standardized and reproducible evaluations for LLaVA-OneVision models on all benchmarks using LMMs-Eval [161]. For fair comparison with other leading LMMs, we primarily report results from original papers. When results are unavailable, we onboard the models in LMMs-Eval and evaluate them using consistent settings. All our results are reported with greedy decoding and 0-shot settings unless otherwise specified. \n\nTo reveal the generality and effectiveness of the designed paradigm, we comprehensively evaluate our LLaVA-OneVision models across different modalities in Three model sizes are provided (0.5B, 7B and 72B), to accomodate applications with different performance-throughput trade-off, ranging from edge device to cloud serving. The GPT-4V and GPT-4o results are presented as references. Our largest model LLaVA-OneVision-72B yields superior performance between GPT-4V and GPT-4o on most benchmarks. It suggests that the proposed recipe is effecitve, revealing a promising path for further scaling. However, a relatively larger gap remains in complex tasks such as visual chat scenarios, we leave it as future research in stronger LLMs, larger training data and better preference learning.",
            "score": 0.4655527286303148,
            "section_title": "Experimental Results",
            "char_start_offset": 20069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1211
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34375
        },
        {
            "corpus_id": "277993601",
            "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark",
            "text": "We evaluate a range of open-weight models on POPE and RePOPE covering different architectures and model sizes. Included are also some of the top models for POPE on OpenVLM Leaderboard [4]: InternVL2.5 [3] (8B/26B/38B/78B and 8B-MPO/26B-MPO), LLaVA-NeXT [9] (Vicuna [11]/Mistral [6]/Llama [5]), LLaVA-OneVision [8], Ovis2 [10] (1B/2B/4B/8B), PaliGemma-3B [2] and PaliGemma2 [13] (3B/10B).",
            "score": 0.4631473784514311,
            "section_title": "Models",
            "char_start_offset": 6065,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 387
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 187,
                    "matchedPaperCorpusId": "271218736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0182342529296875
        },
        {
            "corpus_id": "272986933",
            "title": "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering",
            "text": "We have added an analysis of in-context learning for two representative datasets in For the cost of our data construction pipeline, we use about 1/1.5 day on 32 A100s GPU for LLaVA-1.5 and about 2/3 days on 48 A100s GPU for LLaVA-NeXT. For the data obtained by W2C pipeline, we get 34K from LLaVA-1.5-7B, 33K from LLaVA-1.5-13B, 37K from LLaVA-NeXT-7B, and 29K from LLaVA-NeXT-13B. The reasons for the inconsistency in the amount of data are multifaceted. On the one hand, a minor portion of the data was discarded due to improper handling of anomalous data throughout the processing stage. On the other hand, a significant amount of data was eliminated during the consistency filtering stage owing to inconsistencies detected by the VLMs. Additionally, the generative capabilities of various VLMs vary, and the inherent randomness within VLMs themselves also contributes to these inconsistencies. \n\nTraining Details During the training of VLMs, we use different dataset combinations. We utilize the original paper's open-source dataset during both the PT and IT training stages for LLaVA-1.5. In contrast, for the training of LLaVA-NeXT, the lack of disclosure regarding the specific details of the IT stage, we trained using all training set from LLaVA 665k (Liu et al., 2023a), DocVQA (Tito et al., 2021), ChartQA (Masry et al., 2022) and ShareGPT4V (Chen et al., 2023a). Furthermore, by aligning our dataset with that of the original study, we achieved comparable experimental results. We use the CLIP-pretrained ViT-L/14 (Radford et al., 2021) as a vision encoder, which input resolution is 336\u00d7336. We freeze the vision encoder during training on the LLaVA-1.5 and only freeze the vision encoder on the PT stage during training on the LLaVA-NEXT following the original paper.",
            "score": 0.46056071155483974,
            "section_title": "B.2 Code Parsing Ability Evaluation",
            "char_start_offset": 23597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1781
                }
            ],
            "ref_mentions": [
                {
                    "start": 1260,
                    "end": 1279,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1288,
                    "end": 1307,
                    "matchedPaperCorpusId": "233444227"
                },
                {
                    "start": 1526,
                    "end": 1548,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10894775390625
        },
        {
            "corpus_id": "277313273",
            "title": "RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models",
            "text": "We evaluate a total of 19 recent VLMs that support multiimage inputs, including Chameleon-7B [47], InternVL2.5 series [13], SmolVLM [42], Llava-Next series [30,33,37], Pixtral-12B [6], Llava-Onevision-ov series [31], MiniCPM-V-2.6 [53], MiniCPM-o-2.6 [2], Qwen2-VL [52] and Qwen2.5-VL [48] series, and GPT4o [43]. To facilitate the evaluation process, we incorporate VLMEvalKit [16] as our evaluation toolkit.",
            "score": 0.458705088096911,
            "section_title": "Target Models",
            "char_start_offset": 19115,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 409
                }
            ],
            "ref_mentions": [
                {
                    "start": 378,
                    "end": 382,
                    "matchedPaperCorpusId": "271218736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03240966796875
        },
        {
            "corpus_id": "277781774",
            "title": "TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language Models",
            "text": "Calibration Datasets. Following established practices in model pruning (Sun et al., 2024b;Frantar and Alistarh, 2023;Sung et al., 2024), we use a random subset of 128 samples from the training datasets of the target models as calibration data. For LLaVA-NeXT, we use ShareGPT4V (Chen et al., 2024b) as the calibration dataset. For Vide-oLLaMA2, we choose MUSIC-QA (Li et al., 2022) as the calibration source as its samples consist of both video and audio modalities. For LLaVA-OneVision, we use NLVR2 (Suhr et al., 2019) as it constitutes the largest portion of its training dataset. \n\nEvaluation pipeline To ensure consistency and reproducibility, the benchmarks are assessed through LMMs-Eval framework (Li et al., 2024b) and evaluation pipelines of the models. We follow the LMMs-Eval prompt templates provided in the official GitHub repository of the LMMs-Eval to evaluate the LLaVA-NeXT and LLaVA-OneVision models. We implement the VideoLLaMA2 architecture on the LLMs-Eval framework and evaluate the model on audio and video benchmarks.",
            "score": 0.4578649374544995,
            "section_title": "A Details of Experimental Setups",
            "char_start_offset": 28194,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1042
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 90,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 117,
                    "end": 135,
                    "matchedPaperCorpusId": "263620656"
                },
                {
                    "start": 278,
                    "end": 298,
                    "matchedPaperCorpusId": "265308687"
                },
                {
                    "start": 364,
                    "end": 381,
                    "matchedPaperCorpusId": "247763132"
                },
                {
                    "start": 501,
                    "end": 520,
                    "matchedPaperCorpusId": "53178856"
                },
                {
                    "start": 705,
                    "end": 723,
                    "matchedPaperCorpusId": "265308687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28515625
        },
        {
            "corpus_id": "278000706",
            "title": "EEmo-Bench: A Benchmark for Multi-modal Large Language Models on Image Evoked Emotion Assessment",
            "text": "In this section, we evaluate the performance of 5 proprietary MLLMs and 14 open-source MLLMs, while the latter are divided by parameter scale into two categories: medium-scale (lower than 8b parameters) and large-scale (72b parameters). To be specific, MLLMs are classified into 1) Proprietary MLLMs: Gemini-1.5-pro [44], GPT-4o [40], Qwen-VL-Plus, Qwen-VL-Max [5] and Claude-3.7-Sonnet [4]. 2) Medium-scale Open-source MLLMs: Deepseek-VL-7B-Chat [33], Deepseek-VL2-tiny [61], InternVL2-8B [51], InternVL2.5-8B [12], Janus-Pro-7B [10], LLaVA-Onevision (Qwen2-7B) [27], LLaVA-NeXT (Llama3-8B) [28], mPLUG-Owl2 (LLaMA2-7B) [64], mPLUG-Owl3 (Qwen2-7B) [63], Qwen2-VL-7B-Instruct [52], and Qwen2.5-VL-7B-Instruct [6]. 3) Large-scale Open-source MLLMs: LLaVA-Onevision (Qwen2-72B) [27], Qwen2-VL-72B [52], and Qwen2.5-VL-72B [6].",
            "score": 0.4564490783137869,
            "section_title": "Experiment Setup",
            "char_start_offset": 21399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 824
                }
            ],
            "ref_mentions": [
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "265050943"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038238525390625
        },
        {
            "corpus_id": "274305727",
            "title": "NEMO: Can Multimodal LLMs Identify Attribute-Modified Objects?",
            "text": "Vision encoders fine-tuned on InternViT-300M-448 perform similarly, possibly because their small scale MLLM limits further improvement. In contrast, larger MLLMs like LLaVA-NeXT-72B and LLaVA-NeXT-110B, perform worse than LLaVA-NeXT-8B. A similar, al-beit less pronounced, trend is observed for InternVL2-26B, InternVL2-40B, and InternVL2-LLaMA3-76B, suggesting that fine-tuning with larger LLMs may weaken the vision encoder's performance. As noted in Question 2, LLaVA-NeXT-8B achieves comparable performance to LLaVA-NeXT-72B and LLaVA-NeXT-110B on attribute-modified objects, despite significantly larger scales of the latter two. \n\nAnswer 3: Stronger vision encoders improve MLLM performance, but larger LLMs often weaken vision encoders during fine-tuning. Preserving vision encoder strength is essential, especially when fine-tuning with larger LLMs.",
            "score": 0.4563894394674324,
            "section_title": "Question 2: How does MLLM size affect the gap?",
            "char_start_offset": 16818,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 857
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1209716796875
        },
        {
            "corpus_id": "274437586",
            "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
            "text": "LLaVA-1.5 [34]. LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection. The overall architecture remains consistent with LLaVA: the visual encoder encodes continuous video frames individually, and the representations are concatenated as inputs to the LLM. After joint training, Video-LLaVA is capable of understanding both image and video data. Qwen-VL [3]. Qwen-VL is another widely used opensource vision-language model. Similar to LLaVA, it includes a visual encoder (OpenCLIP) and a text decoder (Qwen LLM).",
            "score": 0.4559431707743645,
            "section_title": "Model architectures",
            "char_start_offset": 36898,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 16,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2003
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 14,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "277349695",
            "title": "Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model",
            "text": "We provide a detailed comparison of our models in terms of both speed and accuracy with previous methods in Table 1. \n\nFor efficiency, we compare total parameters, operating input resolution, number of input frames, and model throughput. \n\nThe throughput is measured as tokens per second with a batch size of 1 on a single GPU. Overall, our Mobile-VideoGPT-0.5B variant performs comparably to LLaVA-OneVision (Li et al., 2024a) while being twice as fast (45.9 tokens/sec vs. 22.7 tokens/sec for LLaVA-OneVision-0.5B). Similarly, our 1.5B variant achieves results close to the recently released LLaVA-Mini-8B (Zhang et al., 2025) while running up to 9\u00d7 faster (41.0 tokens/sec vs. 4.6 tokens/sec for LLaVA-Mini-8B). These results demonstrate the advantages of our architectural design and training approach, enabling strong performance while maintaining efficiency. \n\nOn ActivityNet-QA, our 0.5B model achieves 51.6% accuracy, outperforming LLaVA-OneVision-0.5B by 1.1%. Our 1.5B model achieves a score of 54.4%, surpassing LLaVA-Mini-8B by 2.1% and coming just 0.9% behind the closedsource Gemini-1.5-Flash. Since ActivityNet-QA focuses on activity understanding in videos, the presence of the video encoder enables our model to capture rich temporal dependencies, leading to strong performance on activity-centric benchmarks. Although our model is not explicitly designed for long videos, our models still achieve competitive results on the long-video MLVU benchmark, highlighting the effectiveness of our dual-encoder design. \n\nSimilarly, on MVBench, our 0.5B model achieves a score of 53.5, surpassing LLaVA-Mini-8B and LLaVA-OneVision-0.5B by 9% and 6.3%, respectively.",
            "score": 0.4519761650453837,
            "section_title": "Discussion on Quantitative Results",
            "char_start_offset": 16652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1527
                },
                {
                    "start": 1530,
                    "end": 1673
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2425537109375
        },
        {
            "corpus_id": "271719914",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "text": "The SoTA proprietary LMMs, such as GPT-4V [109], GPT-4o [110], Gemini [131] and Claude-3.5 [3], exhibit excellent performance in versertile vision scenarios, including single-image, multi-image and video settings. In the open research community, existing works typically develop models tailored to each individual scenario separately. Specifically, most focus on pushing the performance limits in single-image scenarios [26,83,173,73,164,35], only a few recent papers have begun to explore multi-image scenarios [70,47]. While video LMMs excel in video understanding, they often do so at the expense of image performance [72,76]. It is rare to have a single open model that reports excellent performance in all three scenarios. LLaVA-OneVision aims to fill this gap by demonstrating state-of-the-art performance across a broad range of tasks, and showcasing interesting emerging capabilities through cross-scenario task transfer and composition. \n\nTo the best of our knowledge, LLaVA-NeXT-Interleave [68] is the first attempt to report good performance in all three scenarios, LLaVA-OneVision inherits its training recipe and data for improved performance. Other versatial open LMMs with potentials to excel include VILA [77], InternLM-XComposer-2.5 [162]. Unfortunately, their results are not fully evaluated and reported; we compare with them in the experiments. In addition to building systems with versatial capabilities, LLaVA-OneVision is benefited from large-scale high-quality data training, including model-synthesized knowledge and the new collection of diverse instruction tuning data. For the former, we inherit all the knowledge learning data in [64]. For the latter, our are motivated by FLAN [136,88,145]. The data collection process is con-current with Idefics2 [63] and Cambrian-1 [133], but we focus on a smaller but more carefully curated collection of datasets. A similar conclusion is observed: a large amount of visual instruction tuning data can significantly improve performance.",
            "score": 0.4506260947016788,
            "section_title": "Related Work",
            "char_start_offset": 3961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2003
                }
            ],
            "ref_mentions": [
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "265466723"
                },
                {
                    "start": 1221,
                    "end": 1225,
                    "matchedPaperCorpusId": "266174746"
                },
                {
                    "start": 1712,
                    "end": 1715,
                    "matchedPaperCorpusId": "256415991"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.466552734375
        },
        {
            "corpus_id": "276775957",
            "title": "DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models",
            "text": "Baselines and Models: We consider five baselines, namely, FastV [4], PruMerge [35], VTW [21], FitPrune [46] and M3 [3]. Among these, we consider FastV, PruMerge, and VTW as our main competitors as they are plug-and-play and do not rely on any further costly finetuning or calibration process. However, for the sake of completeness, we also report performance comparison with respect to one finetuning-based (M 3 ) and one calibration-based (FitPrune) methods. Note that, VTW, by default, requires calibration to determine the best layer for a given task. However, doing that does not allow us to set a specific TFLOP ratio, complicating the comparison. Hence, whenever required we disable the calibration of VTW to select the layer that matches the FLOP requirement of a particular experiment. \n\nWe test DivPrune and the baselines with popular LMMs namely LLaVA 1.5-7B [22] 1 , LLaVA 1.5-13B [22] 2 LLaVA 1.6-7B 3 (also known as LLaVA-NeXT [23]), and LLaVA-NeXT-Video-7B [51] 4 to demonstrate the generality of Di-vPrune. For each tested model and task, we report only the relevant subset of baseline that is applicable to that specific model and task, alongside our results. \n\nAll the tested LMMs used CLIP vision encoder [33]. LLaVA 1.5 model uses 576 visual tokens to represent images. LLaVA 1.6 converts each image into a varying number of patches, resulting in 3-5 times more visual tokens compared to LLaVA 1.5. LLaVA-NeXT-Video uses 144 tokens to process each frame. For all the experiments with LLaVA-NeXT-Video we used a total of 8 frames resulting in 1152 tokens for the processed frames. \n\nDatasets, Tasks, and Metrics: We selected a comprehensive set of common tasks and datasets aimed at multimodal reasoning and understanding.",
            "score": 0.4487831104700567,
            "section_title": "Experimental Settings",
            "char_start_offset": 13173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1740
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 67,
                    "matchedPaperCorpusId": "268358224"
                },
                {
                    "start": 869,
                    "end": 873,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1223,
                    "end": 1227,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281494140625
        },
        {
            "corpus_id": "276107430",
            "title": "Rethinking Homogeneity of Vision and Text Tokens in Large Vision-and-Language Models",
            "text": "Emerging large vision-language models (LVLMs) have made significant progress in visual understanding, particularly in Visual Question Answering (VQA). The predominant architectures can be summarized as different combinations of a vision encoder, an adapter, and a large language model (LLM). To name a few, LLaVA (Liu et al., 2024b), LLaVA NeXT (Liu et al., 2024a), LLaVA-OneVision (Li et al., 2024a), Instruct BLIP (Dai et al., 2023), BLIP3 (Xue et al., 2024), VILA (Lin et al., 2024), QWen2-VL (Wang et al., 2024), CuMo (Li et al., 2024b), Intern-XC-2.5 (Zhang et al., 2024), miniGemini (Li et al., 2024c), Cambrian-1 (Tong et al., 2024a), Phi-3 VL (Abdin et al.,",
            "score": 0.448556378955852,
            "section_title": "RELATED WORKS",
            "char_start_offset": 20459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 665
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 332,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 416,
                    "end": 434,
                    "matchedPaperCorpusId": "258615266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.095703125
        },
        {
            "corpus_id": "273549699",
            "title": "Needle In A Video Haystack: A Scalable Synthetic Evaluator for Video MLLMs",
            "text": "We select 12 video understanding models, including 9 open-source models and 3 proprietary models. The proprietary models are Gemini 1.5 Pro (Reid et al., 2024) and the GPT-4 series (OpenAI, 2023), accessed via the official API. We evaluate 2 different GPT-4 version include gpt-4-turbo-2024-04-09 and gpt-4o-2024-05-13. The open-source video MLLMs include LLaVA-NeXT-Video (Zhang et al., 2024), ST-LLM (Liu et al., 2024d), LLaMA-VID (Li et al., 2023g), Video-LLaVA (Lin et al., 2023), VideoChatGPT (Maaz et al., 2023), VideoChat2 (Li et al., 2023f), Video-LLaMA2 (Zhang et al., 2023), Qwen2-VL (Wang et al., 2024b) and LLaVA-OneVision (Li et al., 2024). The detailed model setting is shown in Appendix C.1.",
            "score": 0.44830002133951163,
            "section_title": "MODELS",
            "char_start_offset": 16938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 706
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.097412109375
        },
        {
            "corpus_id": "273098670",
            "title": "AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity",
            "text": "As shown in Table 3, except for text-intensive VQA benchmarks that require very fine-grained visual information, such as TextVQA and ChartVQA, AVG-LLaVA significantly reduces the number of visual tokens and improves inference speed across other benchmarks. Particularly, on the AI2D benchmark, AVG-LLaVA achieves better performance than LLaVA-NeXT while using only 14.7% of the visual tokens, and the inference speed increased by 2.53 \u00d7.3 Notably, even with the addition of two extra modules, there is no significant slowdown in inference speed on the ChartVQA benchmark when using a comparable number of visual tokens. Moreover, AVG-LLaVA only increases the number of parameters by 1.66% compared to LLaVA-NeXT.",
            "score": 0.44256384302467827,
            "section_title": "Standard-resolution LMMs",
            "char_start_offset": 20611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 712
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40234375
        },
        {
            "corpus_id": "277321635",
            "title": "Beyond Intermediate States: Explaining Visual Redundancy through Language",
            "text": "Leading MLLMs [19,20,25,26] process high-resolution images and multiple video frames by incorporating numerous visual tokens. For instance, LLaVA-Next and LLaVA-OneVision represent an image using a maximum of 2,880 and 7,290 visual tokens, respectively. These visual tokens occupy a large portion of the context window of their LLM 1 , leading to increased computational overhead and potentially impairing MLLMs' text processing capabilities [45].",
            "score": 0.4411678082747236,
            "section_title": "Related Work",
            "char_start_offset": 5304,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 447
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 24,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08447265625
        },
        {
            "corpus_id": "268513094",
            "title": "PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset",
            "text": "An overall VHE as shown in Tab. 5 is useful for providing a big picture of which MLLM hallucinates the most (or the least). The leading open-source MLLMs are LLaVA-OneVision, followed by Molmo and InternVL-1.5. Since their vision encoders and LLMs vary, the results are insufficient to conclude which component is the most effective to mitigate hallucinations. That said, comparisons among the same model series remains meaningful. Consider the LLaVA series for instance. While one would normally expect that a larger LLM yields a better MLLM, as LLaVA-1.6-L vs LLaVA-1.6, the difference between LLaVA-1.5-L and LLaVA-1.5 is marginal (0.270 vs 0.265). In order to analyze and consequently understand such an counterintuitive result, PhD enables a zoom-in analysis in mode-oriented (Sec. 4.3) and task-oriented (Sec. 4.4) styles, unavailable in the previous benchmarks. One more advantage of PhD compared to its predecessors lies in its discrimination ability. The relatively small performance gap between GPT-4o and the top open-source models as measured by POPE and AMBER might lead to an overly optimistic interpretation that the open-source alternatives are catching up with the proprietary model. In fact, a substantial gap remains, as revealed by PhD. Fig. 3 further illustrate the qualitative results, where GPT-4o exhibits fewer hallucinations in its response. This is achieved either with stronger visual encoders, as the cases of LLaVA-OneVision, InternVL-1.5, and Cambrian-1 using SoViT-400m/14, InternViT-6B or hybrid vision structure, or supporting higher image resolutions, see Molmo and MiniGPT4-v2 that accept multiscale or larger input.",
            "score": 0.44107142964756074,
            "section_title": "Using PhD for Overall VHE",
            "char_start_offset": 22800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1652
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06512451171875
        },
        {
            "corpus_id": "275405970",
            "title": "Global Compression Commander: Plug-and-Play Inference Acceleration for High-Resolution Large Vision-Language Models",
            "text": "Specifically, LLaVA-NeXT employs CLIP-ViT-L-336px [36] and Vicuna-v1.5 for vision and language modeling respectively, while LLaVA-OneVision utilizes SigLIP-So400m-Patch14-384 [50] and Qwen2 [44]. To effectively process high-resolution visual inputs, both models adopt flexible grid configurations -LLaVA-NeXT supports {2 \u00d7 2, 1 \u00d7 {2, 3, 4}, {2, 3, 4} \u00d7 1} with maximum 5 \u00d7 576 grid tokens, while LLaVA-OneVision allows {1 \u00d7 1, ..., 6 \u00d7 6} grids up to 10 \u00d7 729 tokens. Comparison Methods. We compare our GlobalCom 2 with three dominant baseline methods: \n\n\u2022 FastV [8] performs one-time token pruning after the selected layer of LLM based on attention map analysis. \u2022 SparseVLM [53] ranks token importance by text-visual attention maps, pruning through pre-selected text prompts to reduce attention noise. \u2022 FasterVLM [52] reranks visual tokens by [CLS] attention scores from CLIP-ViT and preserves top-k tokens. \n\nAll experiments are conducted on NVIDIA A100-SXM4-80GB GPUs. Considering that both FasterVLM and GlobalCom 2 perform token compression at the vision encoding stage, Table 6 focuses on comparing these two methods. The results reveal several key findings: (1) Our GlobalCom 2 outperforms FasterVLM on most benchmarks, maintaining above 90% of uncompressed performance across different retention ratios on both LLaVA-NeXT-7B and 13B models. (2) On visual text understanding (VQA T ), GlobalCom 2  shows particular strength at low retention ratios, surpassing FasterVLM by 2.6% at R = 10%, thanks to its effective preservation of object and textual information. (3) While FasterVLM performs slightly better on general visual tasks (VQAv2, MMB, MMBCN) at high retention ratios due to its uniform token compression, GlobalCom superior degradation resistance at low ratios.",
            "score": 0.43843687197217207,
            "section_title": "Detailed Experimental Settings",
            "char_start_offset": 30563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1779
                }
            ],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 54,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 175,
                    "end": 179,
                    "matchedPaperCorpusId": "257767223"
                },
                {
                    "start": 563,
                    "end": 566,
                    "matchedPaperCorpusId": "268358224"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.316162109375
        },
        {
            "corpus_id": "270068206",
            "title": "Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment",
            "text": "Implementation Details In this paper, we verify our proposed CAL on two leading model structures: LLaVA-1.5/LLaVA-NeXT [6,10] and Mini-Gemini/Mini-Gemini-HD [11]. LLaVA-1.5 uses CLIPpretrained ViT-L as the visual encoder. For resolution scaling, LLaVA-NeXT employs a simple while adaptive image cropping strategy, encodes each image and concatenates them in one single sequence. Mini-Gemini (MGM) further introduces a LAION-pretrained ConvNeXt-L [22,41] for high-resolution refinement. For MGM/MGM-HD/LLaVA-1.5, we follow the same setting as the original paper as it is public available, where the learning rate for the PT stage is set to 1e \u22123 and the IT stage is set to 2e \u22125 for both Vicuna-7B and Vicuna-13B. For LLaVA-NeXT, where only the evaluation code is made public, we reproduce LLaVA-NeXT with the same learning rate as MGM, and set the learning rate of ViT to 1/10 of the base learning rate (our reproduction presents on-par performance with the original paper/blog. We present a comparison of our reproduction results with those of the original papers in Appendix A.1). We also set the lower bound \u03b1 and upper bound \u03b2 in Equation ( 3) to 1 and 5 respectively, and we set l in Equation ( 4) to 3 for all experiments. We use 16 A100 for experiments, except for 8 GPUs in LLavA-1.5/Gemma-2B and 32 GPUs in MGM-HD-13B. \n\nDatasets For experiments on LLaVA-NeXT [10], since the detailed composition of training datasets is not publicly available, we use a slightly different training dataset combination, where we include the mixture of LLaVA 665k [6], VQA Doc [26], VQA Chart [25] and the ShareGPT4V [16]. For experiments of LLaVA-1.5 [6] and MGM/MGM-HD [11], we use the same dataset combination with original paper.",
            "score": 0.43841756222141726,
            "section_title": "Experimental Setup",
            "char_start_offset": 11551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1724
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 122,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "245837420"
                },
                {
                    "start": 450,
                    "end": 453,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 1555,
                    "end": 1558,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1568,
                    "end": 1572,
                    "matchedPaperCorpusId": "233444227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29931640625
        },
        {
            "corpus_id": "273098670",
            "title": "AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity",
            "text": "As summarized in Table 2, AVG-LLaVA surpasses all other models, exhibiting superior overall performance and highlighting its adaptability and effectiveness across various disciplines. Specifically, AVG-LLaVA outperforms the second best model by 9.4 and 6.1 on MME and MME C , respectively, and by 1.9 and 1.2 on MMB and MMB CN , respectively. Moreover, AVG-LLaVA's performance on the POPE and MMMU benchmarks demonstrates its ability to reduce hallucinations and perform complex reasoning. (Bai et al., 2023) Qwen-7B 1487.6 360.7 60.6 ---InternVL-Chat (Chen et al., 2023) Vicuna-7B 1525.1 ---86.4 -mPLUG-Owl2 (Ye et al., 2023) LLaMA2-7B 1450.2 -64.5 ---MQT-LLAVA (Hu et al., 2024) Vicuna-7B 1434.5 353.6 64.3 -84.4 34.8 LLaVA-1.5 (Liu et al., 2023a) Vicuna-7B To validate the effectiveness of dynamic visual granularity selection, we compare AVG-LLaVA with LLaVA-NeXT in terms of visual token number and inference speed across multiple benchmarks. Specifically, for each type of benchmark, we select three benchmarks for comparison, and report the reduction in the number of visual tokens per grid and the actual speedup during inference. As shown in Table 3, except for text-intensive VQA benchmarks that require very fine-grained visual information, such as TextVQA and ChartVQA, AVG-LLaVA significantly reduces the number of visual tokens and improves inference speed across other benchmarks.",
            "score": 0.4383547217596071,
            "section_title": "Standard-resolution LMMs",
            "char_start_offset": 19472,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1395
                }
            ],
            "ref_mentions": [
                {
                    "start": 663,
                    "end": 680,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 730,
                    "end": 749,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2093505859375
        },
        {
            "corpus_id": "277452239",
            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
            "text": "For training the LLaVA model, we utilized software and resources available in the LLaVA-NeXT repository8 . Only minor modifications were applied to the original software to load our dataset and enable the loading of LoRA fine-tuned models using the Qwen LLM. C1 in the Appendix provides an example of user-assistant conversations for two sample radio images, comparing responses from the base and fine-tuned LLaVA-OneVision 7B models. \n\nThe fine-tuned models are publicly available in the Hugging Face repository: https://huggingface.co/inaf-oact-ai.",
            "score": 0.43793838947492303,
            "section_title": "Implementation details",
            "char_start_offset": 23973,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 550
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18505859375
        },
        {
            "corpus_id": "278170893",
            "title": "Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains",
            "text": "The experimental results are presented in Table 1. Both LLaVA-OneVision and Qwen2-VL demonstrate consistent performance improvements across seven benchmarks after fine-tuning with our synthesized dataset VISC-150K, indicating the effectiveness of our method across different VLM architectures. \n\nLLaVA-OneVision achieves benchmark-leading improvements of 6.20% on MMIU and 7.85% on MuirBench, demonstrating breakthrough capabilities. Significantly, our method elevates even the already superior Qwen2-VL model across multiple benchmarks, achieving an average gain of 2.24% over its strong baselines. When combined with LLaVA-OneVision's 3.16% average improvement, these consistent enhancements across both highperforming and emerging models conclusively validate the universality of our approach. \n\nOur method establishes new state-of-the-art results on four benchmarks: MMIU, MIRB, BLINK, and NLVR2. These improvements can be attributed to three key characteristics: (1) the richness of visual information, (2) the diversity of inter-image relationships, and (3) the complexity of task formulations. These characteristics align well with our method's enhanced capabilities in visual perception, comprehension, and reasoning. \n\nMoreover, our method demonstrates measurable performance gains on the video benchmark MVBench. Given that a video is essentially a collection of frames, it falls into a multi-image scenario with temporal correlations, where our method maintains efficacy. This result substantiates that our Focus-Centric Visual Chain reasoning paradigm operates in a domain-agnostic manner, exhibiting robust applicability as well as transferability.",
            "score": 0.436022626274237,
            "section_title": "Results",
            "char_start_offset": 16930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1661
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.386474609375
        },
        {
            "corpus_id": "276558424",
            "title": "Social Genome: Grounded Social Reasoning Abilities of Multimodal Models",
            "text": "The LLaVA-Video model LLaVA-Video-7B-Qwen2 (Zhang et al., 2024c) has an architecture with a SigLIP SO400M vision transformer and Qwen2 language model, has 7B parameters, can process up to 110 frames, and was trained on mixture of single image, multi-image, and video tasks from the LLaVA-Video-178K and LLaVA-OneVision datasets (Li et al., 2024a). \n\nLLaVA-Video-Only The LLaVA-Video-Only model LLaVA-Video-7B-Qwen2-Video-Only is identical to the LLaVA-Video model, with the exception of the training data (Zhang et al., 2024c). LLaVA-Video-Only was solely trained on the LLaVA-Video-178K dataset. \n\nLongVA The LongVA model LongVA-7B-DPO (Zhang et al., 2024a) aligns a unified multimodal transformer (UMT) with QFormer and aligns this visual encoder with a Qwen2 7B language model. LongVA was trained on visual instructionfollowing datasets and multimodal document data and has a context length of over 200,000 visual tokens; this longer context length was achieved by extending the context length of the language backbone to train on long text samples, before performing multimodal alignment and additional training to transfer this ability to the multimodal domain.",
            "score": 0.4339668345136177,
            "section_title": "LLaVA-Video",
            "char_start_offset": 37717,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 347
                },
                {
                    "start": 350,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 1166
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2166748046875
        },
        {
            "corpus_id": "270619607",
            "title": "GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs",
            "text": "For LLaVA-1.5 (7B and 13B) and LLaVA-1.6/LLaVA-NeXT models up to the 13B versions, we used one 40 GB NVIDIA A100. For LLaVA-Yi-34B, and Intern-VL-Chat-1.5 26B, we used one 80 GB NVIDIA A100. For LLaVA-NeXT-Qwen-1.5 110B, we used four 80 GB NVIDIA A100 GPUs. In most cases, except the Intern-VL model, we used the Huggingface [26] library setup and checkpoints.",
            "score": 0.4337938796156381,
            "section_title": "A.2 Computational Resources",
            "char_start_offset": 14142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 360
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0384521484375
        },
        {
            "corpus_id": "277435623",
            "title": "Learning to Instruct for Visual Instruction Tuning",
            "text": "Model Architectures. (1) TinyLLaVA (Zhou et al., 2024): We follow TinyLLaVA to utilize small-scale LLMs, including Qwen-2-0.5B (Yang et al., 2024) and Phi-2-3B (Gunasekar et al., 2023). The SigLIP-400M (Zhai et al., 2023) is adopted as the vision encoder architecture due to its effective performance in combination with small-scale LLMs. \n\nThe multimodal connector follows LLaVA 1.5 (Liu et al., 2024a) with a two-layer MLP and GELU activation. (2) LLaVA 1.5, LLaVA-NeXT (Liu et al., 2024b): For LLaVA 1.5, we use Vicuna-v1.5-7B (Peng et al., 2023) and Vicuna-v1.5-13B (Peng et al., 2023) as the base LLMs, while for LLaVA-NeXT, Vicuna-v1.5-7B serves as the base LLM. The base vision encoder is CLIP-ViT-L-14 (Radford et al., 2021), and the multimodal connector is a two-layer MLP. \n\nTraining Datasets. (1) Pretraining Stage: We adopt the LLaVA 1.5 framework, utilizing the LLaVA-pretrain-558k data (Liu et al., 2024a) for all pretraining phases across TinyLLaVA, LLaVA 1.5, and LLaVA-NeXT. (2) Finetuning Stage: For instruction tuning, we use the LLaVA-mix-665k data (Liu et al., 2024a) on TinyLLaVA and LLaVA 1.5. For LLaVA-NeXT, we use the LLaVA-NeXT-Data (Liu et al., 2024b), an expansion of LLaVA-mix-665k with diverse instruction data. More details can be referred to Appendix B.2. \n\nImplementation Details.",
            "score": 0.4316884592690231,
            "section_title": "Setup",
            "char_start_offset": 10810,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 782
                },
                {
                    "start": 785,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1314
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 220,
                    "matchedPaperCorpusId": "257767223"
                },
                {
                    "start": 384,
                    "end": 403,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 710,
                    "end": 732,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 900,
                    "end": 919,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1069,
                    "end": 1088,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.319091796875
        },
        {
            "corpus_id": "277787375",
            "title": "AgMMU: A Comprehensive Agricultural Multimodal Understanding and Reasoning Benchmark",
            "text": "Cambrian-1 is a recent state-of-theart model that excels at visual-centric tasks. This model explores combinations of vision encoders, text and image integration techniques, and instruction tuning strategies. We use the official implementation and checkpoint2 with a LLaMA3-8B-Instruct LLM backbone model in our evaluation. InternVL2 [43]. InternVL scales up the vision foundation model while aligning it with the backbone LLM, and is trained on web-scale image-text data to achieve strong performance across a variety of vision-centric tasks. We use the official implementation and checkpoint 3 with the InternViT-300M-448px vision backbone and Internlm2.5-7B-chat language backbone in our evaluation. LLaMA-3.2 [10]. LLaMA-3.2 is the first collection of multimodal large language model from the LLaMA family that was previously text-only. The integration of vision involves utilizing cross-attention layers and a pre-trained vision encoder that feeds directly into the text-processor. The model follows a commonly used training recipe that includes pretraining on noisy image-text pairs and then highquality knowledge enhanced pairs. Notably, the languagemodel parameters were frozen during the training of alignment of image and text to retain strong text-only capabilities. We use the official implementation and checkpoint 4that uses a LLaMA-3.1 text-only language backbone in our evaluation. When evaluating the model, we choose to use a custom prompt since no standard prompt is provided. LLaVA-NeXT [25]. LLaVA-NeXT expands on LLaVA by using the same instruction tuning method to give the model the ability to process and reason about multi-images, multi-grames, and multi-views. We use the official imple-mentation and checkpoint 5 with LLaMA-3-8B Instruct as the language backbone in our evaluation. [21]. LLaVA-OneVision builds on LLaVA-NeXT with the capability to analyze single images, multi-images, and video scenarios. Most impressively, it allows for video understanding through task transfer from images but this is not really explored in our evaluations.",
            "score": 0.4283877982843016,
            "section_title": "A. Evaluated Large Vision Language Models",
            "char_start_offset": 33182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2072
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.479248046875
        },
        {
            "corpus_id": "277112739",
            "title": "LLaVA-UHD v2: an MLLM Integrating High-Resolution Semantic Pyramid via Hierarchical Window Transformer",
            "text": "Table 1 showcases a comparative analysis of our proposed LLaVA-UHD v2 against state-of-the-art MLLMs across 15 widely recognized benchmarks. (1) LLaVA-UHD v2 outperforms current counterparts. Compared with general models (such as LLaVA-1.5, Dense Connector) and highresolution MLLMs (like PIIP-LLaVA, SliME-Llama3-8 and DeepseekVL-7B), LLaVA-UHD v2 demonstrates consistent improvements across various tasks, including general VQA (e.g., 77.1% on MMB and 49.4% on MMStar), ultra-high-resolution image perception (e.g., 59.9% on HR-Bench). Notably, LLaVA-UHD v2 surpasses OCR-centric models like TextMonkey on DocVQA (72.9% vs. 66.5%) and outperforms those with multiple experts (such as MG-LLaVA), achieving superior performance on general tasks like SEED (73.6%). These results underscore the value of rich semantics derived from multi-level multi-modal granularity, enhancing both the understanding and perception abilities of MLLMs. \n\n(2) LLaVA-UHD v2 indicates efficiency on data utilization and computation. \n\nCompared to LLaVA-Next and Mini-Gemini, both operating at a 672\u00d7672 resolution, LLaVA-UHD v2 supports 1.5 times the resolution (i.e., 672\u00d71008) and achieves superior performance with less than 40% of the computational cost. Furthermore, in contrast to Honey-bee and VILA, which utilize 52.5M and 51M data samples respectively, LLaVA-UHD v2 attains comparable or superior performance using only \u223c2.8% of the data, demonstrating the data efficiency of our model. As for the training duration, under the same model configuration and data volume, LLaVA-UHD v2 requires \u223c27 hours to train on 8\u00d7A100 GPUs, while LLaVA-Next needs \u223c42 hours, which is well-suited for low-cost exploratory research in the academic community.",
            "score": 0.42813755463366876,
            "section_title": "Main Performance",
            "char_start_offset": 19592,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 934
                },
                {
                    "start": 937,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1729
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1796875
        },
        {
            "corpus_id": "271329151",
            "title": "SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models",
            "text": "Open-Ended VideoQA results are shown in Table 1. SF-LLaVA obtains better performance than existing training-free methods on all benchmarks. Specifically, SF-LLaVA outperforms IG-VLM (Kim et al., 2024) by 2.1% and 5.0% on MSRVTT-QA, 5.7% and 1.5% on TGIF-QA, 1.2% and 0.8% on ActivityNet-QA, using 7B and 34B LLMs, respectively. When even compared to state-of-the-art SFT methods, SF-LLaVA achieves on-par results on most benchmarks (i.e., MSVD-QA, MSRVTT-QA, and TGIF-QA), and only the results of PLLaVA (Xu et al., 2024) and LLaVA-NeXT-Video-DPO (Zhang et al., 2024b) (Kim et al., 2024) on all benchmarks. Specifically, on the challenging EgoSchema dataset, which involves complex long-form temporal reasoning (Mangalam et al., 2024), SF-LLaVA outperforms IG-VLM by 11.4% and 2.2% when using 7B and 34B LLMs, respectively. This highlights the ability of SF-LLaVA on long-form video understanding. Note that VideoTree (Wang et al., 2024b) is leading the benchmark because it is built upon a proprietary LLM (i.e., GPT-4 (Achiam et al., 2023)) whose performance is much better than the open-sourced LLMs. When compared to SFT methods (Cheng et al., 2024), SF-LLaVA 34B model also achieves better results (+2.5%) on EgoSchema, which confirms the capability of our SlowFast design on long videos. \n\nText Generation benchmarks are shown in Table 3, where SF-LLaVA-34B outperforms all trainingfree baselines on average.",
            "score": 0.42689988200477635,
            "section_title": "MAIN RESULTS",
            "char_start_offset": 15997,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1414
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1343994140625
        },
        {
            "corpus_id": "269982528",
            "title": "G3: An Effective and Adaptive Framework for Worldwide Geolocalization Using Large Multi-Modality Models",
            "text": "To explore the impact of LMMs on G3, we conduct the experiments of G3 and Img2Loc with LLaVA (LLaVA-Next-LLaMA3-8b) on IM2GPS3K. From Table 4 we can find that: (1) After switching LMMs from GPT4V to LLaVA, the performance of G3 shows some decline across various metrics but remains competitive. (2) Additionally, compared to Img2Loc (LLaVA), G3 (LLaVA) significantly outperforms Img2Loc (LLaVA), demonstrating the effectiveness of the proposed modules. (3) Finally, by comparing the performance of G3 equipped with LLaVA and GPT4V to Img2Loc equipped with LLaVA and GPT4V, we can observe that G3 shows more stable performance across different LMMs.",
            "score": 0.4265845362278047,
            "section_title": "Impact of LMMs on G3",
            "char_start_offset": 27057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 648
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10089111328125
        },
        {
            "corpus_id": "273821149",
            "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model",
            "text": "Liu at el. introduced an end-to-end trained large vision-language assistant (LLaVA [51]) on instruction-following data for general purpose visual and language understanding, which gained widespread attention upon release. Subsequent research has further enhanced LLaVA's performance. For instance, LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios. LLaVA-Interactive [53] serves as a comprehensive demonstration platform, incorporating features such as image chatting, segmentation, and generation and editing capabilities, significantly expanding LLaVA's original functionalities. MoE-LLaVA [54], a sparse LVLM architecture based on Mixture of Experts (MoE), was developed to tackle performance degradation in multimodal sparse learning. MG-LLaVA [55] enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, allowing it to handle features at various resolutions and object centers. \n\nLLaVA has set new standards for efficiency and effectiveness in multimodal learning and has quickly been adapted across various domains. For example, LLaVA-based models, including LLaVA-Med [18], PathChat [19], QUILT-LLaVA [56], PA-LLaVA [57], have been designed for medical image understanding, where they outperform traditional methods. Zheng et al. [58] developed the first large-scale open-source dataset, MMTab, to address the multimodal table understanding problem and trained a multifunctional tableformat LLM called Table-LLaVA. In the power sector, Wang et al. [21] proposed Power-LLaVA, a large vision-language assistant designed for reliable inspection of power transmission lines, showcasing strong capabilities in this field. In the food domain, Fnu Mohbat et al. [20] introduced LLaVA-Chef, trained on a carefully selected recipe dataset, enabling it to recognize ingredients and generate detailed recipes. In this study, we aim to construct a unified multimodal Vision-Language Model for human-scene tasks.",
            "score": 0.4260461346019444,
            "section_title": "Various Vision-Language Applications",
            "char_start_offset": 8442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 1227,
                    "end": 1231,
                    "matchedPaperCorpusId": "258999820"
                },
                {
                    "start": 1242,
                    "end": 1246,
                    "matchedPaperCorpusId": "270437603"
                },
                {
                    "start": 1260,
                    "end": 1264,
                    "matchedPaperCorpusId": "266149936"
                },
                {
                    "start": 1389,
                    "end": 1393,
                    "matchedPaperCorpusId": "270391758"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8681640625
        },
        {
            "corpus_id": "271719914",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "text": "In addition to reporting the LLaVA-OneVision's capabilities across various benchmarks, we also observe the emerging behaviors of the proposed model with task transfer and composition, paving a promising way to generalize to tackle real-world computer vision tasks in the wild. We illustrate several emerging capabilities using examples as below. 9. Given an image and a target video, the model constructs a coherent and vivid narrative for the video, detailing elements such as characters, actions, background settings, and scene specifics. This task leverages both single-image analysis and video comprehension. It is hypothesized that this ability is generalized from the composition of single-image editing instruction task and video detailed description task. \n\nS5: Video-to-Video Difference (Transfer from multi-image and video). Understanding differences in images is a common ability in recent large multimodal models (LMMs), but our models extend this capability to videos. Table 10 showcases LLaVA-OneVision's ability to analyze differences between two video sequences with the same beginning frame but different endings. The model provides a detailed comparison, describing characters, actions, and scene changes. In Table 11, LLaVA-OneVision's describe the differences one by one between videos with a similar background but different main object in the foreground. This task leverages spot the difference in the multi-image analysis to generalize to video scenarios. \n\nS6: Multi-camera Video Understanding in Self-driving (Transfer from single-image and multiimage to video). Understanding videos in a normal aspect ratio is straightforward, what about the videos with multi-views? In Table 12, we observe that LLaVA-OneVision could analyze and interprets multi-camera video footage from self-driving cars. Given video showing four camera views, the model describes each view in detail and plans the ego car's next move. This task combines multi-panel comprehension, video detailed description, and spatial-temporal reasoning. \n\nS7: Composed Sub-video Understanding (Transfer from multi-image to video). Besides multiview video, we see our model generalize to vertical videos with two sub-scenes. Table 13 demonstrates LLaVA-OneVision's ability to understand and describe the content and layout of a composed subvideo.",
            "score": 0.42552379092656034,
            "section_title": "Emerging Capabilities with Task Transfer",
            "char_start_offset": 27257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2038
                },
                {
                    "start": 2041,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2330
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4951171875
        },
        {
            "corpus_id": "273098155",
            "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
            "text": "We further evaluate LLaVA-Critic's performance in providing reward signals for iterative DPO. LLaVA-OneVision's supervised fine-tuned checkpoint is used as the base policy model, and question-image pairs from LLaVA-RLHF [38] serve as the multimodal instructions. For each pair, K = 5 candidate responses are generated through random decod- ing (with a temperature of 0.7 and top-p of 0.9) to ensure response diversity. LLaVA-Critic is employed as described in Sec. 4.3 to construct the pairwise feedback data, which is then used for one epoch of DPO training. We perform iterative DPO for M = 3 rounds in total. \n\nTo assess the effectiveness of LLaVA-Critic's reward signals, we evaluate the final LMM checkpoint on 6 open-ended multimodal benchmarks: four imagebased tasks (LLaVA-in-the-Wild [24], LLaVA-Wilder [15], WildVision-Bench [28] and LiveBench [48]), one videobased task (Video Detailed Captioning [15]), and one hallucination benchmark (MMHal-Bench [38]). We compare LLaVA-Critic with two baselines: (1) reward model from LLaVA-RLHF [38], which is trained on human preferences, and (2) a naive baseline that replaces LLaVA-Critic with LLaVA-OneVision as a zero-shot reward model. \n\nAs shown in Table 6, preferences provided by LLaVA-Critic significantly improve LLaVA-OneVision's visual chat capacities and reduce hallucination across challenging tasks. LLaVA-Critic consistently surpasses other baseline reward models on 5 out of 6 benchmarks for the 7B base model and all 6 benchmarks for the 72B base model. Despite the preference alignment conducted solely with images, LLaVA-Critic also enhances LLaVA-OneVision's performance in Video Detailed Captioning (+0.12 on OV-7B and +0.26 on OV-7B), demonstrating its ability to generalize to both image and video contexts.",
            "score": 0.4253386687539299,
            "section_title": "Preference Learning",
            "char_start_offset": 26353,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1781
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27734375
        },
        {
            "corpus_id": "277999630",
            "title": "Unveiling the Lack of LVLM Robustness to Fundamental Visual Variations: Why and Path Forward",
            "text": "Phi-3.5-vision, the latest iteration, is a lightweight yet powerful model featuring a 128K token context length and support for both single and multi-image processing. \n\nInternVL The InternVL family includes: Mono-InternVL (Luo et al., 2024a), which established the foundation with its vision-language capabilities; InternVL-2 (Chen et al., 2024c), which expanded the model sizes and improved performance; and InternVL-2.5 (Chen et al., 2025b), the latest iteration that introduces significant architectural and training improvements. \n\nLLaVA LLaVA-1.5 (Liu et al., 2023) uses pretrained CLIP (Radford et al., 2021) and Vicuna language model as the backbone, establishing the foundation with impressive performance across 12 benchmark datasets. LLaVA-OneVision-Qwen2 (Li et al., 2024) pushes performance boundaries across single-image, multi-image, and video scenarios while enabling strong task transfer capabilities. LLaVA-1.6 (Liu et al., 2024c) GLM-4 GLM-4V-9B (GLM et al., 2024) supports high-resolution image processing at 1120*1120 pixels and enables dialogue capabilities in both Chinese and English. \n\nMiniCPM-V MiniCPM-V (Yao et al., 2024) is a series of multimodal large language models (MLLMs) designed specifically for deployment on end-side devices like mobile phones and personal computers.",
            "score": 0.4253149835104757,
            "section_title": "D.1 Evaluated Models",
            "char_start_offset": 35770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 170,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1305
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.312255859375
        },
        {
            "corpus_id": "273098427",
            "title": "Video Instruction Tuning With Synthetic Data",
            "text": "We conducted evaluations for the LLaVA-Video models across all benchmarks using LMMs-Eval (Zhang et al., 2024a) to ensure standardization and reproducibility. To fairly compare with other leading video LMMs, we primarily used results from original papers. When results were not available, we integrated the models into LMMs-Eval and assessed them under consistent settings. Following LLaVA-OneVision (Li et al., 2024c), we employed SigLIP (Zhai et al., 2023) as our vision encoder, and Qwen2 (Yang et al., 2024) as the LLM. The LLaVA-Video model builds on the single-image (SI) stage checkpoint from the LLaVA-OneVision model (Li et al., 2024c), which was trained using only image data. \n\nVideo Representations Following the classic SlowFast idea in video representations (Feichtenhofer et al., 2019;Xu et al., 2024b;Huang et al., 2024), we develop LLaVA-Video SlowFast to optimize the balance between the number of frames and the count of visual tokens, within the budget of the limited context window in LLM and GPU memory for video representation. Please refer to Appendix B for detailed information. Specifically, we represent each video as a sequence with maximum T frames. Each frame is represented in M tokens. we categorize the frames into two groups, based on the a strike rate s, where the every s frames are uniformly selected to form the slow frame group, and the rest of the frames are consdiered as the fast frame group. Note that a special case s = 1 leads to only one group, reducing the SlowFast representation to the original simple representation. For each group, we apply different pooling rate using Pytorch function pooling avg_pool2d(). p \u00d7 p pooling and 2p \u00d7 2p pooling for slow and fast frames, respectively. To summarize, we paramterize the video representation configuration as V = (T, M, s, p).",
            "score": 0.4244222545325383,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 17068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1822
                }
            ],
            "ref_mentions": [
                {
                    "start": 439,
                    "end": 458,
                    "matchedPaperCorpusId": "257767223"
                },
                {
                    "start": 772,
                    "end": 800,
                    "matchedPaperCorpusId": "54463801"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.372802734375
        },
        {
            "corpus_id": "271924066",
            "title": "RoVRM: A Robust Visual Reward Model Optimized via Auxiliary Textual Preference Data",
            "text": "Performance on Different LVLMs To further validate the generalizability of our RoVRM, we conduct additional experiments using the LLaVA-NeXT (as known as LLaVA-1.6) models which improve input image resolution and use an improved visual instruction tuning data (Liu et al. 2024b). \n\nThe experimental results, presented in",
            "score": 0.42322955464110107,
            "section_title": "Conclusion",
            "char_start_offset": 25720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 279
                },
                {
                    "start": 282,
                    "end": 320
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.189208984375
        },
        {
            "corpus_id": "277349201",
            "title": "BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding",
            "text": "Comparison with State-of-the-Art Methods on Standard Benchmarks. Table 3 presents the comparison of our approach against state-of-the-art methods. Recently, LLaVA-OneVision [13] has achieved strong performance on these datasets, surpassing prior methods such as Vide-oLLaMA2 [7] and thus establishing a robust baseline. \n\nOur training-free, query-guided inverse transform sampling (ITS) method further enhances the performance of LLaVA-OneVision. Specifically, on Video-MME, accuracy improves from 53.8% to 56.1% using 8 frames, and on MLVU, it increases from 58.9% to 63.4% with 8 frames. On Nex-tQA, our performance remains comparable to the baseline, likely due to the brevity of the videos. Notably, our method continues to yield improvements even with 32 frames, demonstrating significant gains on long-video benchmarks, such as a 3.2% increase on LongVideoBench and a 3.6% increase on MLVU. These results highlight the importance of selecting the right frames at inference time to fully unleash the potential of existing VLMs, and our proposed inverse transform sampling consistently surpasses uniform sampling by a substantial margin. Results on Off-the-Shelf VLMs. As shown in Table 4, we extend the proposed approach to additional video VLMs, including Video-LLaVA [17], LongVA [43], InternVL2 [6], and LLaVA-Video [45], and evaluate its effectiveness on the Video-MME benchmark. The results clearly demonstrate that our training-free approach, BOLT, consistently boosts the performance of off-the-shelf VLMs, achieving approximately 1% to 3% accuracy improvements. For instance, the performance of LLaVA-Video notably increases from 56.0% to 58.6% with only 8 frames, verifying the strong generalization capability of inverse transform sampling.",
            "score": 0.4222775635139272,
            "section_title": "Benchmark Results",
            "char_start_offset": 22351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 319
                },
                {
                    "start": 322,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1755
                }
            ],
            "ref_mentions": [
                {
                    "start": 1303,
                    "end": 1306,
                    "matchedPaperCorpusId": "266521410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.455810546875
        },
        {
            "corpus_id": "275118957",
            "title": "MVTamperBench: Evaluating Robustness of Vision-Language Models",
            "text": "Our benchmarking efforts encompass 45 models across diverse categories and tampering scenarios, including Drop, Mask, Repeat, Rotate, and Substitute (Table 3). The results reveal significant variability in performance, influenced by model size, architecture, and training data. Models such as VILA1.5-40B and InternVL2.5-8B emerged as top performers, achieving consistent resilience across all tampering types, with overall scores of 0.879 and 0.875, respectively. This highlights the importance of architectural innovations and advanced training techniques for tampering robustness. In contrast, early-generation models like LLaVA-OneVision underperformed across all categories, with overall scores as low as 0.001, reflecting limitations in temporal coherence and token alignment. \n\nSpecialized models for video tasks, such as Chat-UniVi and Video-LLaVA, demonstrated substantial improvements over base LLaVA models. Chat-UniVi-7B-v1.5 achieved an overall score of 0.658, significantly outperforming LLaVA-OneVision, showcasing its ability to handle complex temporal manipulations. Meanwhile, Video-LLaVA-7B-HF maintained robust performance across categories, further validating the effectiveness of unified tokenization and video-specific optimizations. However, LLaVA-Video, despite efforts to improve alignment and pretraining, continues to struggle with certain tampering types, reflecting the challenges of adapting image-centric architectures to video modalities. \n\nInterestingly, medium-sized models like Phi3.5-Vision demonstrated notable performance improvements compared to earlier iterations such as Phi3-Vision, indicating that scaling alone does not account for robustness gains. Specialized models like Ovis1.6-Gemma2-9B showcased strengths in spatial tampering scenarios (e.g., Mask and Rotate) but struggled with temporal disruptions like Repeat and Substitute. This trend underscores the importance of task-specific optimizations. \n\nFuture Benchmarking Plans While our analysis has covered an extensive set of models, several promising entries are yet to be evaluated.",
            "score": 0.42176869779323334,
            "section_title": "Benchmarking Efforts",
            "char_start_offset": 29464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 782
                },
                {
                    "start": 785,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1471
                },
                {
                    "start": 1474,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1949
                },
                {
                    "start": 1952,
                    "end": 2087
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333740234375
        },
        {
            "corpus_id": "275405970",
            "title": "Global Compression Commander: Plug-and-Play Inference Acceleration for High-Resolution Large Vision-Language Models",
            "text": "Benchmark Details. We evaluate GlobalCom 2 on various multimodal understanding benchmarks detailed as follows: \n\n\u2022 GQA [17]: Contains 113,018 real-world images with scene graphs for structured visual reasoning. \u2022 VizWiz [15]: Features over 39,000 questions from blind users with lower-quality images and conversational style. \u2022 SQA [33]: Contains 21,000 scientific questions covering 26 topics and 379 different reasoning skills. \u2022 VQA T [39]: Features 28,000 images focusing on comprehension of text embedded in natural scenes. \u2022 AI2D [19]: Contains 5,000 high-resolution scientific diagrams paired with visual-spatial reasoning questions. \u2022 MMStar [7]: Contains 12,000 high-resolution images for spatial, temporal, and commonsense reasoning. \u2022 POPE [23]: Features 14,000 images with binary questions for detecting object hallucination phenomena. \u2022 MME [12]: Contains 15,000 images across 14 perceptual and cognitive reasoning subtasks. \u2022 MMB [32]: Features 20,000 multiple-choice questions designed for robust visual reasoning. \u2022 MMBCN [32]: Chinese counterpart of MMB with 20,000 questions for cross-lingual evaluation. \n\n\u2022 MM-Vet [48]: Features 16,000 questions across recognition, OCR, knowledge, and spatial reasoning. \u2022 SEED-Bench [20]: Contains 19,000 human-annotated questions across 12 evaluation dimensions. Baseline Models. We select LLaVA-NeXT [29] and LLaVA-OneVision (SI) [21] as our baseline models, and follow the same inference setting as the original paper as it is publicly available1 . LLaVA-NeXT and LLaVA-OneVision share a common three-component architecture: a pre-trained vision encoder, a large language model (LLM) backbone, and a two-layer MLP projector bridging the two. Specifically, LLaVA-NeXT employs CLIP-ViT-L-336px [36] and Vicuna-v1.5 for vision and language modeling respectively, while LLaVA-OneVision utilizes SigLIP-So400m-Patch14-384 [50] and Qwen2 [44].",
            "score": 0.4214282169780882,
            "section_title": "Detailed Experimental Settings",
            "char_start_offset": 28863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 110
                },
                {
                    "start": 113,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1122
                },
                {
                    "start": 1125,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1895
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 123,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 220,
                    "end": 224,
                    "matchedPaperCorpusId": "3831582"
                },
                {
                    "start": 332,
                    "end": 336,
                    "matchedPaperCorpusId": "252383606"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "2682274"
                },
                {
                    "start": 650,
                    "end": 653,
                    "matchedPaperCorpusId": "268793433"
                },
                {
                    "start": 751,
                    "end": 755,
                    "matchedPaperCorpusId": "258740697"
                },
                {
                    "start": 944,
                    "end": 948,
                    "matchedPaperCorpusId": "259837088"
                },
                {
                    "start": 1038,
                    "end": 1042,
                    "matchedPaperCorpusId": "259837088"
                },
                {
                    "start": 1134,
                    "end": 1138,
                    "matchedPaperCorpusId": "260611572"
                },
                {
                    "start": 1238,
                    "end": 1242,
                    "matchedPaperCorpusId": "271963485"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1844482421875
        },
        {
            "corpus_id": "270688372",
            "title": "Holistic Evaluation for Interleaved Text-and-Image Generation",
            "text": "While there is still a significant gap between the performance of GPT-4o and LLaVA, probably due to the limited capability of LLaVA-NeXT-Interleave, we believe our InterleavedEval-LLaVA is a good alternative to InterleavedEval-GPT-4o in the scenarios where transparency and reproducibility are highly desired. We leave how to build a more powerful open-sourced evaluator for future work.",
            "score": 0.4207246081427012,
            "section_title": "Output: Output:",
            "char_start_offset": 26498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 387
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1484375
        },
        {
            "corpus_id": "274437704",
            "title": "DaDu-E: Rethinking the Role of Large Language Model in Robotic Computing Pipeline",
            "text": "At task levels 2 and 3, the VLM component contributes 69 % of the overall latency reduction relative to the baseline. \n\nThese findings underscore the effectiveness of our VLM processing and optimized task planning in reducing system latency. By integrating llava-onevision-7b into the data pipeline, we achieve substantial computational efficiency while maintaining performance on par with GPT-4o.",
            "score": 0.41977315400617843,
            "section_title": "System performance",
            "char_start_offset": 32226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 120,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 397
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1231689453125
        },
        {
            "corpus_id": "277244542",
            "title": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models",
            "text": "Due to limitation of computation resources, we compare the vLLMs on 0.5B parameters track. As of now, the only available 0.5B video large language model baselines are LLaVA-OV-SI and LLaVA-OneVision. For efficiency, we plug our model into the LLaVA-OneVision baseline for quick training and evaluation, minimizing GPU consumption. To ensure a fair comparison, we evaluate models separately for the 0.5B and 7B parameter configurations. While 7B models have 14 times the parameters and benefit from significantly more GPU resources, our 0.5B model-with only 0.14% of token numbers-outperforms some 7B models, demonstrating our method's efficiency and effectiveness.",
            "score": 0.41973434297456214,
            "section_title": "Video Large Language Model Baselines",
            "char_start_offset": 21740,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 664
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.293212890625
        },
        {
            "corpus_id": "278714870",
            "title": "Cross-Image Contrastive Decoding: Precise, Lossless Suppression of Language Priors in Large Vision-Language Models",
            "text": "To demonstrate the generalizability of the proposed CICD as a broadly applicable plug-and-play module, we evaluate its performance across six different LVLMs spanning three model families: Instruct-BLIP (Dai et al., 2023), LLaVA-1.5 (Liu et al., 2024c), LLaVA-Next (Liu et al., 2024d), Qwen-VL-Chat (Bai et al., 2023b), Qwen2-VL-Instruct (Wang et al., 2024b), and Qwen2.5-VL-Instruct (Bai et al., 2025). InstructBLIP and LLaVA-1.5 are both built upon Vicuna-7B (Chiang et al., 2023) as their language backbone, while LLaVA-Next is based on LLaMA3. The Qwen-VL series, including Qwen2-VL-Instruct and Qwen2.5-VL-Instruct, is developed using the QwenLM framework. As for vision encoders, InstructBLIP, LLaVA-1.5, LLaVA-Next, and Qwen-VL-Chat utilize the pretrained CLIP model (Radford et al., 2021), whereas Qwen2-VL-Instruct and Qwen2.5-VL-Instruct adopt a fine-tuned ViT (Dosovitskiy et al., 2021). All models are implemented at the 8B parameter scale.",
            "score": 0.4193455893442285,
            "section_title": "B.2 Evaluated LVLMs",
            "char_start_offset": 30851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 952
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 221,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 233,
                    "end": 252,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 774,
                    "end": 796,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 871,
                    "end": 897,
                    "matchedPaperCorpusId": "225039882"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.308349609375
        },
        {
            "corpus_id": "272986636",
            "title": "Q-Bench-Video: Benchmarking the Video Quality Understanding of LMMs",
            "text": "The annotation process of Q-Bench-Video is conducted in a well-controlled laboratory environment. A total of 8 experts are employed and trained to ensure the consistency of the annotations. The experts are required to watch the videos in their entirety before making annotations. Each annotated question-answer pair is then reviewed by at least three other experts to ensure its validity and accuracy. [30], LLaVA-v1.5 [28], and mPLUG-Owl2 [54]; b) 9 Open-source Video LMMs: mPLUG-Owl3 [52], LLaVA-OneVision [19], InternVL-Chat [4], VILA1.5 [16], PLLaVA [50], LLaVA-Next-Video [58], ST-LLM [31], Video-LLaVA [27], and VideoChat2 [22]; c) 5 Proprietary LMMs: Gemini 1.5 Flash, Gemini 1.5 Pro [39], GPT-4o mini, GPT-4o, and GPT-4 Turbo [2].",
            "score": 0.41921406709534315,
            "section_title": "Questions & Answers Annotation",
            "char_start_offset": 14861,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 738
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0211639404296875
        },
        {
            "corpus_id": "278170893",
            "title": "Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains",
            "text": "Additionally, some of these sub-tasks (e.g., geographic understanding) involve image types and task categories that are not present in VISC-150K, which emphasize the crossdomain generalization capabilities of our method. RQ3: How does the number of input images impact performance? \n\nTo investigate how the number of input images impacts our method, we conducted a detailed analysis of LLaVA-OneVision on the MMIU benchmark. The instances are grouped into different buckets by the number of input images. \n\nAs demonstrated in Figure 5, when handling 3-8 images, LLaVA-OneVision-VISC achieves remarkable improvements, suggesting successful identi- fication of cross-image relationships in mediumsized image sets. This capability persists even with larger inputs (11-14 images), where LLaVA-OneVision-VISC maintains superior performance compared to baseline models while avoiding performance degradation from information overload. However, when processing more than 15 images, the performance of LLaVA-OneVision-VISC exhibits slight degradation, which may be attributed to amplified noise levels or interference from irrelevant data patterns in prolonged image sequences. RQ4: Does VISC-150K affect general ability? \n\nAlthough our method demonstrates remarkable improvements in multi-image tasks, it is crucial to evaluate whether these gains come at the expense of general task performance. To investigate this issue, we adopt Qwen2-VL as the base model and conduct analysis on four benchmarks in other domains. These benchmarks are based on single-image inputs and evaluate the fine-tuned model's performance from various perspectives, including hallucination, single-image data language capability, domain-specific knowledge, and mathematical reasoning. Specifically: HallusionBench (Guan et al., 2023) is designed to assess VLMs' ability to comprehend and interpret visual data; MMStar (Chen et al., 2024a) requires advanced multi-modal capabilities for accurate interpretation; MMMU (Yue et al., 2024) focuses on evaluating models' ability to apply domain-specific knowledge; Math-Vista (Lu et al., 2023) integrates mathematical reasoning with visual tasks. The results are presented in Table 2.",
            "score": 0.41912014644296924,
            "section_title": "Discussions",
            "char_start_offset": 20808,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 281
                },
                {
                    "start": 284,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1213
                },
                {
                    "start": 1216,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 1986,
                    "end": 2004,
                    "matchedPaperCorpusId": "265466525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.392578125
        },
        {
            "corpus_id": "276725099",
            "title": "RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete",
            "text": "Evaluation on Planning Task We selected 6 powerful MLLMs as our baselines for comparison, including both open-source and closed-source models with different architectures. Specifically, these models include GPT-4V [2], Claude3 [1], LLaVA-1.5 [48], LLaVA-OneVision-7b [41], Qwen2-VL-7b [86] and RoboMamba [50]. Our specific experimental results are shown in Fig. 5. Our RoboBrain outperformed all baseline models across three robotic benchmarks. RoboBrain significantly outperformed all baseline models on OpenEQA and ShareRobot, which can be attributed to its robust capabilities in understanding robotic tasks and perceiving long videos. Additionally, this pattern was observed in other benchmarks as well, with Robo-Brain consistently demonstrating superior performance on RoboVQA, achieving a BLEU-4 score that exceeded that of the second-place model by 18.75. This result highlights its capability to decompose complex long-range task planning. \n\nEvaluation on Affordance Prediction Our results are summarized in Tab. 2. We compare the Qwen2-VL-7B and LLaVA-NeXT-7B models. Qwen2-VL [86] has a superior visual grounding ability and LLaVA-NeXT [44] owns a high-resolution and strong vision tower. We test them all on the AGD20K affordance test set. Our RoboBrain outperforms significantly the other models. It surpasses Qwen2- Table 2. The comparison of affordance prediction. We utilize AP as the metric, and test them on affordance test set.",
            "score": 0.4190085159257595,
            "section_title": "Evaluation on Robot Brain Task",
            "char_start_offset": 18673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1446
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 246,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.229248046875
        },
        {
            "corpus_id": "274192345",
            "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models",
            "text": "We implement the proposed Dy-Coke on the LLaVA-OneVision-0.5B, LLaVA-OneVision-7B, and LLaVA-OneVision-72B models using NVIDIA RTX 4090 (24GB), A6000 (48GB), and A100 (80GB) GPUs, respectively. And we use the PyTorch framework. To set the pruning ratios for all methods, we use total calculated FLOPs to ensure fair comparison. The attention computation layer for FastV is set to layer 5. For video input, we follow the official requirements of the LLaVA-OneVision model, with a default of 32 video input frames and N v = 196, except for experiments with specific instructions. In the comparison experiments, L is set to 3 and P to 0.7, with the first-stage pruning rate K serving as the primary experimental variable.",
            "score": 0.4184940981790407,
            "section_title": "Evaluation Setups and Implementation Details",
            "char_start_offset": 18442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 718
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11126708984375
        },
        {
            "corpus_id": "276929061",
            "title": "Multi-Cue Adaptive Visual Token Pruning for Large Vision-Language Models",
            "text": "For a fair comparison and efficient validation, we employ the LMMs-Eval [59] evaluation framework across all experiments. The models used include LLaVA-1.5 (7B and 13B variants) [29], LLaVA-NEXT-7B [10], and InternVL2 (2B and 8B variants) [10], with configurations based on the their official settings. Detailed model architectures and settings are provided in the supplementary material.",
            "score": 0.41762222290917256,
            "section_title": "Model Settings",
            "char_start_offset": 17615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 388
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 182,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34765625
        },
        {
            "corpus_id": "273549699",
            "title": "Needle In A Video Haystack: A Scalable Synthetic Evaluator for Video MLLMs",
            "text": "In Table 11, we report the whole result on 9 VNBench tasks and the overall score for each model on the main split of VNBench. We summarize the result as follows: \n\n1) Proprietary models perform better than open-source models on most VNBench tasks. In terms of overall accuracy, the highest performance among open-source models (58.7% for LLaVA-OneVision-72B) and the highest performance among proprietary models (66.7% for Gemini 1.5 Pro) differ by 8.0%. Additionally, the average accuracy of proprietary models is also significantly higher than that of open-source models. \n\n2) Performance on multiple-needle long-dependency tasks is lower than on single-needle shortdependency tasks. When comparing the accuracy across different tasks, we find that most models Table 2: Evaluation Results on VNBench. VNBench comprises three synthetic tasks constructed using the VideoNIAH method, with each task divided into three splits. \"E\" denotes intra-frame editing needles, while \"I\" represents inter-frame inserting needles. The numbers \"1\" and \"2\" refer to the difficulty levels of the sub-tasks, with \"1\" indicating simple and \"2\" indicating hard. In total, we evaluated 3 proprietary models and 9 open-source models across these tasks. \n\nProprietary Models \n\nGemini 1.5 Pro (Reid et al., 2024)  perform much better on retrieval tasks than on ordering and counting. For most proprietary models, they can retrieve almost all the inserted information (for example, 100% accuracy on Retrieval-E task). For some open-source models (such as ST-LLM, LLaVA-NeXT-Video, Qwen2-VL, LLaVA-OneVision), the retrieval accuracy is also significantly higher than on the other two tasks. \n\n3) The gap between open and proprietary models in the ordering task is enormous. The most advanced proprietary models are far ahead of other models in the ordering task (with Gemini 1.5 Pro at 72.9% accuracy on ordering task and GPT-4o at 73.4%), while most open-source models are nearly incapable of completing the ordering task with the exception of the LLaVA-OneVision series.",
            "score": 0.41648133808291865,
            "section_title": "MAIN RESULTS",
            "char_start_offset": 17661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 161
                },
                {
                    "start": 164,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1665
                },
                {
                    "start": 1668,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 2047
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07611083984375
        },
        {
            "corpus_id": "273186838",
            "title": "Intriguing Properties of Large Language and Vision Models",
            "text": "As shown in Figure 1, the group-wise visual tokens in the LLaVA-1.5-7B model demonstrate varying levels of performance on the MMStar (Chen et al., 2024a) and MME (Fu et al., 2023), suggesting that each token captures localized visual information rather than global concept understanding. Additionally, the central visual tokens contain more informative content compared to those at the edges. \n\nLLVMs are permutation invariant in terms of visual patch tokens. From our above results, we empirically verify that each visual patch token from the projector contains localized visual information. Here, we aim to understand how LLVMs systematically process and perceive images based on these visual patch tokens. Given that LLVMs generate answers in an autoregressive manner, we Original Image 2 X 2 4 X 4 8 X 8 14 X 14 investigate whether they exhibit order bias regarding visual patch tokens. To study this, we strongly hypothesize that if LLVMs have permutation variance, the performance drop (\u2206) will be significant when a random permutation is applied to the visual patch tokens X V . \n\nAs shown in Table 1, the overall performance across most benchmarks declines when the visual patch tokens are randomly shuffled. However, the performance gap between the original and the shuffled (Perm.) versions is not substantial, remaining within a 0-2% range, for LLaVA-1.5 and LLaVA-NeXT. Considering that LLaVA-1.5 uses 576 visual tokens, this is an intriguing observation. \n\nIt suggests that current LLVMs interpret images in a global manner, despite each visual patch token containing localized information (see Figure 1), and even though they process both images and text autoregressively. In the case of LLaVA-OneVision which has many visual tokens (729), the avg. performance drop (\u2206) is non-trivial. We hypothesize that this global interpretation may result from recent LLVMs being trained via backpropagation, with the loss signal primarily derived from the text output of the Assistant: side.",
            "score": 0.414937035431561,
            "section_title": "DO LLVMS PERCEIVE IMAGES GLOBALLY?",
            "char_start_offset": 11637,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1467
                },
                {
                    "start": 1470,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1994
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1229248046875
        },
        {
            "corpus_id": "271719914",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "text": "As the primary data source, our principle for single-image data has always been that quality outweighs quantity. Given limited resources, we strive to use high-quality data to maximize the performance. \n\nThe first version of the LLaVA-NeXT models (LLaVA-NeXT-Vicuna-7B/13B, Mistral-7B, Hermes-Yi-34B), comprising 760K data samples [82], includes 665K samples from LLaVA-1.5 [81], 3,247 samples from AI2D [53], 18,317 samples from ChartQA [101], 10,194 samples from DocVQA [103], 20,000 samples from DVQA [49], 40,093 samples from SynthDOG-EN [58], and 15,131 samples from user requests on LLaVA's demo, re-annotated with GPT-4V. In the subsequent iteration, we added 20,000 samples from COCO Caption [78], forming a new 790K version. This 790K dataset supported the second release of LLaVA-NeXT models (LLaVA-NeXT-LLaMA3-8B, LLaVA-NeXT-Qwen-72B, LLaVA-NeXT-Qwen-110B). \n\nIn subsequent collections, we accumulated open-sourced datasets from the Internet and referred to the dataset collection processes of other advanced LMMs, such as Qwen-VL [8], DeepSeek-VL [89], Intern-VL [22], Vision-Flan [146], UReader [150], Idefics-2 (Cauldron) [63], and Cambrian. During the data iteration process, we strictly adhered to the initial LLaVA-1.5 strategy. For each dataset, we manually inspected and ensured its quality and QA format. We also designed specific formatting prompts to make data from different sources compatible with each other, thus avoiding conflicts. \n\nSome data sources, such as AI2D and ChartQA, appear in different dataset collections and may be duplicated. Since Cauldron includes special formatting prompts, its data is not straightforward to re-format.",
            "score": 0.4144945057782491,
            "section_title": "E.1 Single-Image Data Curation",
            "char_start_offset": 46928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 201
                },
                {
                    "start": 204,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 868
                },
                {
                    "start": 871,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1666
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "2682274"
                },
                {
                    "start": 438,
                    "end": 443,
                    "matchedPaperCorpusId": "247593713"
                },
                {
                    "start": 472,
                    "end": 477,
                    "matchedPaperCorpusId": "220280200"
                },
                {
                    "start": 504,
                    "end": 508,
                    "matchedPaperCorpusId": "4445015"
                },
                {
                    "start": 542,
                    "end": 546,
                    "matchedPaperCorpusId": "250924870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1390380859375
        },
        {
            "corpus_id": "277322527",
            "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems",
            "text": "LLaVA-OneVision vs. Qwen2-VL: Which is Easier to Edit? Although both models use Qwen2 as Large Language Model, LLaVA-Onevision achieve better results across most metrics in terms of reliability and generality. Specifically, on Prompt and WISE methods, Qwen2-VL demonstrates stronger performance in retaining the original predicted answers. Qwen2-VL slightly outperforms LLaVA-OneVision in terms of reliability and generality by using AdaLora. However, locality is significantly compromised compared to LLaVA-OneVision. \n\nWhich Scenario Type Knowledges are Easier to Update? We choose the average generality metric, which is the mean of the text and multimodal generality of Qwen2-VL and LLaVA-OneVision, to assess the performance of the edits. (Since both WISE and GRACE achieve Reliability close to 100%, analyzing the Reliability metric becomes less meaningful. This observation applies to all subsequent analyses in this study.) According to the averaged metrics across three scenarios illustrated in Figure 5, for AdaLora, decision scenario is more challenging to learn compared to perception and understanding scenario, given a fixed number of training epochs. This can be attributed to the higher complexity of decision making data. In contrast, Prompt, GRACE, and WISE show relatively consistent performance metrics across all scenarios, with minimal variation in their generality. Overall, these knowledge editing methods are broadly applicable to various driving scenarios, effectively updating knowledge in LMMs. Which Data Type is Easier to Edit? We compute the baseline performance across different types of modality, including video, multi-view images and single image, as shown in Figure 6. WISE demonstrates the highest performance on video data, likely due to its memory mechanism to store and update knowledge in temporal changes in driving scenario. Both Prompt and AdaLoRA exhibit a gradual increase in generality as the length of the visual sequence decreases. Does Reducing Video Frames Impact the Effectiveness of Knowledge Editing? To assess the impact of video frames on editing performance, we sequentially test videos with 1 to 5 frames as input in single editing setting.",
            "score": 0.41317570304768847,
            "section_title": "Analysis",
            "char_start_offset": 14876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2198
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.229736328125
        },
        {
            "corpus_id": "270923718",
            "title": "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts",
            "text": "We evaluate: Moondream2-1.6b (Moondream, 2024) based on Phi-1.5 (Li et al., 2023c), LLaVA-1.5 (Liu et al., 2023a) with Vicuna-7b (Peng et al., 2023) as the LLM backbone, LLaVA-1.6 (LLaVA-Next) (Liu et al., 2024), an improvement over LLaVA-1.5 with higher image resolution and better visual reasoning, uses Mistral-7b (Jiang et al., 2023), PaliGemma-3b (Google, 2024), based on open components from the SigLip (Zhai et al., 2023) image encoder and the Gemma (Team et al., 2024) language model, Mantis-bakllava-7b (Jiang et al., 2024b) fine-tuned from BakLLaVA (SkunkworksAI, 2023) and derived from LLaVA but using Mistral-7b (Jiang et al., 2023), Mantis-Idefics2-8b (Jiang et al., 2024b), the current stateof-the-art Mantis variant, based on Idefics2-8b (Lauren\u00e7on et al., 2024).",
            "score": 0.41280585531431724,
            "section_title": "A Evaluated Models",
            "char_start_offset": 25793,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 778
                }
            ],
            "ref_mentions": [
                {
                    "start": 409,
                    "end": 428,
                    "matchedPaperCorpusId": "257767223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1378173828125
        },
        {
            "corpus_id": "273502227",
            "title": "EVA: An Embodied World Model for Future Video Anticipation",
            "text": "As shown in Table 3, for the quantitative result of the Next-Step task, EVA once again achieved the best performance. As a result, it provides a better text (semantic) condition to guide EVA-Generator for improved generation outcomes. In contrast, LLAVA-OneVision (Li et al., 2024a) and Qwen2-VL-7B performed worse in this task compared to the How-To scenario due to their inability to accurately predict the next-step description. This clearly demonstrates the importance of a VLM that is thoroughly trained in embodied scenes for the Embodied World Model. Such deficiency in EVAS-Language also affects the generation quality, leading to the lower performance of LLAVA-OneVision(51.75) and Qwen2-VL-7B(57.23) on EVAS-Vision. This comparison also shows the overall consistency and quality of EVA-Score.",
            "score": 0.41260045691021413,
            "section_title": "EMBODIED NEXT-STEP ANTICAPTION",
            "char_start_offset": 25265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 802
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11126708984375
        },
        {
            "corpus_id": "274281000",
            "title": "Filter, Correlate, Compress: Training-Free Token Reduction for MLLM Acceleration",
            "text": "As shown in Figure 4, we present the trends of throughput and TFLOPs changes after applying FiCoCo in the LLaVA-NeXT and LLaVA-1. 5  that after introducing FiCoCo into the LLaVA-NeXT architecture, when TFLOPs decrease by 93.2%, the throughput of FiCoCo-V and FiCoCo-L increases by 2.08\u00d7 and 1.71\u00d7, respectively. Meanwhile, when FiCoCo is introduced into the LLaVA architecture, with a TFLOPs reduction of 82.4%, the throughput of FiCoCo-V and FiCoCo-L increases by 1.43\u00d7 and 1.29\u00d7, respectively. These results demonstrate that FiCoCo can significantly reduce computational overhead while effectively improving throughput.",
            "score": 0.412353558553064,
            "section_title": "Efficiency Analysis",
            "char_start_offset": 27507,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 621
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303955078125
        },
        {
            "corpus_id": "278170893",
            "title": "Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains",
            "text": "For open-source VLMs, our baselines include Qwen2-VL (Wang et al., 2024b), Qwen-VL-Chat (Bai et al., 2023), LLaVA-OneVision (Li et al., 2024a), LLaVA-1.6 (Liu et al., 2024a), LLaVA-1.5 (Liu et al., 2023a), InternVL2.5 (Chen et al., 2024b), InternVL2 (Chen et al., 2024c), Mantis-Idefics2 (Jiang et al., 2024), Idefics2 (Lauren\u00e7on et al., 2024), VILA-1.5 (Lin et al., 2023) and OpenFlamingo-v2 (Awadalla et al., 2023). Among close-source VLMs, we select GPT-4V/GPT-4o (OpenAI, 2024a) as the baseline. Please refer to Appendix C for more details.",
            "score": 0.41203240196186763,
            "section_title": "Baselines",
            "char_start_offset": 14874,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 544
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 204,
                    "matchedPaperCorpusId": "266174746"
                },
                {
                    "start": 354,
                    "end": 372,
                    "matchedPaperCorpusId": "266174746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10015869140625
        },
        {
            "corpus_id": "273098670",
            "title": "AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity",
            "text": "InstructBLIP (Dai et al., 2023) Vicuna-7B 49.2 60.5 34.5 ----IDEFICS-9B (Team, 2023) LLaMA-7B 38.4 -35.5 25.9 ---Qwen-VL (Bai et al., 2023) Qwen  (Liu et al., 2024c) Qwen-VL-7B ---65.9 58.2 64.3 -Mini-Gemini-HD (Li et al., 2024a) Vicuna-7B ---68.4 ---LLaVA-NeXT (Liu et al., 2024b) Vicuna-7B General VQA Benchmarks The results in Table 1 show that AVG-LLaVA outperforms all standard-resolution LMMs on the General VQA benchmarks and other high-resolution LMMs on VizWiz. Although it does not achieve the best results on GQA and ScienceQA, it is important to note that AVG-LLaVA uses fewer visual tokens compared to other high-resolution models, and this comparison will be detailed in Section 4.3. \n\nText-oriented VQA Benchmarks In this category of benchmarks, as illustrated in Table 1, except for TextVQA, AVG-LLaVA outperforms all other comparison models. Back to TextVQA, AVG-LLaVA achieves the second-best result, only trailing behind Mini-Gemini-HD. Notably, Mini-Gemini-HD utilizes more than twice the amount of data during the pretraining and approximately 1.5 times the amount of data during the visual instruction fine-tuning compared to AVG-LLaVA. \n\nGeneral Multimodal Benchmarks Compared to traditional VQA datasets, this type of benchmark covers a broader range of evaluation aspects, requiring models to possess more complex perception and reasoning capabilities. As summarized in Table 2, AVG-LLaVA surpasses all other models, exhibiting superior overall performance and highlighting its adaptability and effectiveness across various disciplines.",
            "score": 0.40937839356221545,
            "section_title": "Standard-resolution LMMs",
            "char_start_offset": 18094,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1561
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 165,
                    "matchedPaperCorpusId": "252383606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0751953125
        },
        {
            "corpus_id": "277104362",
            "title": "Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided Visual Tokenizer and Manufacturing Process",
            "text": "The zero-shot anomaly detection results are presented in Tab. 1. Although state-of-the-art LMMs such as LLaVA-OneVision-ov-7B [16] and Qwen2-VL-72B [31] demonstrate strong zero-shot performance on MVTec-AD, WFDD, and PCB-Bank (see Sec. A.1), our Triad-ov-7B remains highly competitive. Notably, by integrating manufacturing processes, Triad-ov-7B surpasses LLaVA-OneVision-ov-7B by 1.6%, highlighting the advantage of manufacturing-aware reasoning in IAD. In contrast, most general-purpose LMMs fail to integrate manufacturing information, resulting in a significant performance drop. Interestingly, we also observe that Myriad benefits from manufacturing processes, likely due to its built-in visual enhancement mechanisms. \n\nTo evaluate the adaptability of our approach, we provide two versions of Triad based on different LMM backbones, LLaVA-1.6-7B and LLaVA-OneVision-ov-7B. When starting with the relatively weak LLaVA-1.6-7B model, Triad yields substantial gains of 8.1% (from 76.9% to 85.0% on MVTec-AD) and 3.5% (from 63.8% to 67.3% on WFDD). Moreover, equipping Triad-llava-1.6-7B with manufacturing processes adds a further 2.5% and 2.6% improvement, respectively. Similarly, Triad-ov-7B outperforms the original LLaVA-OneVision-ov-7B and gains an additional 1.4% and 0.9% boost from manufacturing-aware reasoning on MVTec-AD and WFDD, respectively. These results confirm that our approach generalizes effectively across different model architectures and continues to enhance performance even at higher baseline accuracies (e.g., above 90%). \n\nTable 2 shows 1-shot results. Providing a single reference image generally degrades the performance of most general-purpose LMMs due to their limited instructionfollowing capabilities.",
            "score": 0.4090745652138077,
            "section_title": "Quantity Results",
            "char_start_offset": 17857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1739
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326416015625
        },
        {
            "corpus_id": "276647578",
            "title": "Repurposing the scientific literature with vision-language models",
            "text": "LLaVA (Large Language and Vision Assistant) combines vision and language processing by aligning CLIP-derived image features with language embeddings, enabling interactive image understanding. We built on LLaVA-Next's improvements -including its multilayer projection, patch-based processing of large images, and enhanced pre-training (Fig. S6a). As a starting point, we used the 34B-parameter version of LLaVA-Next, built on Nous Hermes 2 Yi-34B, available on HuggingFace Transformers.",
            "score": 0.4076992264536705,
            "section_title": "Model Architecture and Training Vision-Language Model Backbone",
            "char_start_offset": 22355,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 485
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6435546875
        },
        {
            "corpus_id": "273482109",
            "title": "ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom",
            "text": "We first analyze the behaviors of several existing methods on the MMMU (Yue et al., 2023) dataset, a challenging multi-modal benchmark requiring comprehensive college-level knowledge and fine-grained reasoning abilities. \n\nAll experiments are conducted with three recent LVLMs for the robustness of the results: Llama3-LLaVA-NeXT-8B (Li et al., 2024a), LLaVA-OneVision-Qwen2-7B-OV (Li et al., 2024b) and Qwen-VL-Chat (Bai et al., 2023). Further details are provided in Sec. 4.1.",
            "score": 0.4072654731145537,
            "section_title": "Preliminary Observations",
            "char_start_offset": 5100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 478
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06610107421875
        },
        {
            "corpus_id": "277065890",
            "title": "Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection",
            "text": "Similarly, LLaVA-1.5-13B sees an average improvement of 2.6%, with a substantial 33.5% performance boost on V * , a detail-oriented understanding task. As discussed earlier, a larger backbone LLM benefits from a higher-dimensional latent space, which enhances modality projection. \u03c8 optimal helps overcome the vision encoder's resolution bottleneck, making the 13B model outperform its 7B counterpart across all tasks. However, its theoretical upper bound is still 2.3% lower than that of LLaVA-NeXT-7B. \n\nThe discrepancy of upper bounds highlights a key insight: while LLaVA-1.5-13B improves performance by increasing the language model size, LLaVA-NeXT-7B benefits from longer visual token sequences and more training data. From this, we hypothesize that enhancing visual understanding in VLMs is more efficiently achieved by training with more data and refining visual encoding strategies, rather than merely increasing language model size.",
            "score": 0.40714515411608476,
            "section_title": "Where is the upper bound of SEMCLIP?",
            "char_start_offset": 22217,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 943
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.470947265625
        },
        {
            "corpus_id": "278129780",
            "title": "HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?",
            "text": "We include a total of 28 models in our experiment. Phi-3.5 [2] is a lightweight model designed for efficient language understanding and generation. We include Phi 3.5 vision instruct [2] for experiments. DeepSeek Janus Pro 7B [9] is a model that integrates multi-modal reasoning capabilities. DeepSeek-VL2 [73] is a vision-language model, with deepseek vl2 27B included in our evaluation. In-terVL2 [10][11][12]22] is a family of multi-modal models ranging from small to large-scale by OpenGVLab. We include InterVL2 1B, InterVL2 2B, InterVL2 4B, In-terVL2 8B, InterVL2 26B, InterVL2 40B, and InterVL2 Llama3 76B for experiments. Qwen2-VL [63] is a visionlanguage model, and we consider both Qwen2 VL 7B Instruct and Qwen2 VL 72B Instruct. MolMo [15] is a series of models designed for molecular and scientific applications. We include Molmo 72B 0924 and its distilled variant, Molmo 7B D 0924. LLaVA-Onevision [46] is an open-source multimodal LLM, we selected llavaonevision-qwen2-72b-ov-hf model for our experiments. Llava-Next [78] is an evolution of LLaVA, and we include llama3-llava-next-8b-hf, llava-v1.6-vicuna-13b-hf, llava-v1.6-34b-hf, and llava-next-72b-hf in our experiments. Llama3.2 builds on the Llama architecture with enhanced scalability. We include Llama-3.2-11B-Vision-Instruct and Llama-3.2-90B-Vision-Instruct in our experiments.",
            "score": 0.40704208968175326,
            "section_title": "Model Details",
            "char_start_offset": 29469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1352
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 911,
                    "end": 915,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0987548828125
        },
        {
            "corpus_id": "276409039",
            "title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model",
            "text": "The main results on VideoMME, NExT-QA and the Riv-aBench are shown in Table 2. No subtitles are given to any of the models under test for VideoMME. As performance references, we include GPT-4o (checkpoint at 2024-08-06) and Gemini-1.5-pro, with their results on VideoMME as reported in Fu et al. (2024). When testing GPT-4o with videos, each video is split into images at a frame rate of 2 fps with a maximum of 30 frames due to token limitation, and the sequence of images is sent as the input. For open-source models, we compare video-SALMONN-o1 to LLaVA-OneVision (Li et al., 2024a) (same visual encoder and LLM backbone), together with video-SALMONN (Sun et al., 2024b) and Video-LLaMA 2 (Cheng et al., 2024) as the two most recent audio-visual LLMs. \n\nProprietary LLM performance on RivaBench: For the two proprietary LLMs, GPT-4o underperforms Gemini-1.5pro on StandUp and Academic test sets due to the lack of audio information. This indicates that RivaBench provides challenging questions that require more audio-visual joint understanding compared to VideoMME. On the Syn-thDec set, since only the visual part is synthesized, GPT-4o demonstrated a stronger ability. Moreover, by performing reasoning with GPT-4o and Gemini-1.5-pro, larger improvements are found on StandUp and Academic test sets than VideoMME and NExT-QA, indicating the necessity of reasoning on RivaBench. \n\nOpen-source LLM performance comparison: Audiovisual SFT on video-SALMONN-o1 already yields better performance than LLaVA-OneVision on VideoMME due to the ability to comprehend speech and audio information, whereas no obvious improvements are found on the other benchmarks. The main improvements on other benchmarks come from pDPO, which achieved 4.1%, 8.1% and 5.8% absolute accuracy improvements on NExT-QA, StandUp and Academic test sets respectively compared to the SFT model.",
            "score": 0.40696881268765295,
            "section_title": "Main Results",
            "char_start_offset": 20694,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1865
                }
            ],
            "ref_mentions": [
                {
                    "start": 654,
                    "end": 673,
                    "matchedPaperCorpusId": "270703250"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10723876953125
        },
        {
            "corpus_id": "268513094",
            "title": "PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset",
            "text": "Joint mode-task analytics per model is shown in Tab. 6. LLaVA-OneVision struggles with sentiment recognition and counting, especially when faced with textual or CCS distractions, underscoring the need for improvement in these areas. Similarly, Molmo also faces these challenges, but its counting performance under CCS distractions is notably better than LLaVA-OneVision's (0.737 vs 0.563). The above zoom-in analytics will be informative for MLLM developers to prioritize their efforts on model refinement.",
            "score": 0.406636370747566,
            "section_title": "Using PhD for Task-Oriented VHE",
            "char_start_offset": 28700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 506
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09332275390625
        },
        {
            "corpus_id": "277321522",
            "title": "BASKET: A Large-Scale Video Dataset for Fine-Grained Skill Estimation",
            "text": "In this section, we provide additional details of model implementation. Model Hyperparameters. In Table 7, we provide additional details about the hyperparameters used for all of our tested models. LLaVA-OneVision Implementation. \n\nTo fine-tune LLaVA-OneVision, we reformat our BASKET dataset into a text-based instruction-tuning format, commonly used by modern VLMs. Figure 7 provides a sample prompt used as input to the LLaVA-OneVision model. Specifically, we prompt the model to assign a numerical category to each of the twenty skills. We then evaluate the model's performance using top-1 accuracy based on the generated outputs.",
            "score": 0.406315193050112,
            "section_title": "Additional Implementation Details",
            "char_start_offset": 28436,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 634
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.266357421875
        },
        {
            "corpus_id": "276938164",
            "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
            "text": "Mainstream Vision-Language Models (VLMs) are typically based on the Transformer architecture of deep learning and achieve multimodal understanding by integrating visual and linguistic features. For example, the mPLUG [29], the Qwen2-VL [58], and the LLaVA-OneVision [28] are all based on the Transformer architecture and incorporate pretraining along with self-supervised learning. mPLUG introduces an effective and efficient vision-language architecture with novel cross-modal skip connections, enabling time-consuming self-attention on the visual side through the creation of inter-layer shortcuts [29]. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, enabling the model to dynamically process images of different resolutions into varying numbers of visual tokens. This allows the model to generate more efficient and accurate visual representations [58]. LLaVA-OneVision simultaneously pushes the performance boundaries of open-domain Large Multimodal Models (LMMs) across three key computer vision scenarios-single image, multiple images, and video-while enabling powerful transfer learning across different modalities and scenarios, resulting in the emergence of new capabilities [28]. InternLM-XComposer-2.5 [70] and MoE-LLaVA [34] both demonstrate strong performance in enhancing the model's multitask adaptability and computational efficiency. InternLM-XComposer-2.5 supports long-context inputs and outputs, maintaining strong text-image understanding capabilities while also expanding the use of additional LoRA parameters for text-image synthesis [70]. MoE-LLaVA employs a mechanism of a mixture of experts, dynamically selecting the most relevant \"expert\" sub-models during inference based on the input. This significantly reduces computational costs while maintaining high performance [34].",
            "score": 0.40590184160660747,
            "section_title": "Large Vision-Language Model",
            "char_start_offset": 4766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1815
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66064453125
        },
        {
            "corpus_id": "270391770",
            "title": "AIM: Let Any Multi-modal Large Language Models Embrace Efficient In-Context Learning",
            "text": "LLaVA-Next (Liu et al., 2024a) specialized in single-image inference, and it achieved outstanding performance on popular multimodal benchmarks and textual-only ICL, while its excellent performance failed to extrapolate to practical multimodal ICL settings, exhibiting poor ability of instruction following.Additionally, LLaVA-Next connected pre-trained ViT and LLM via an MLP that resulted in thousands of visual tokens for high-resolution pictures, causing more serious length disasters than perceivers based on cross-attention.",
            "score": 0.4051247169736717,
            "section_title": "In-Context Learning",
            "char_start_offset": 9009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 306
                },
                {
                    "start": 306,
                    "end": 529
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2039794921875
        },
        {
            "corpus_id": "277272119",
            "title": "V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts for Better Human-Model Interaction",
            "text": "Evaluation Models. We evaluate the performance of 12 open-source models that support multi-image or video input. The model list is shown in the third block of Table 2. We sample a fixed number of frames from the original videos at regular intervals to accommodate the context length of the models. Specifically, we sample 16 frames for LLaVA-NeXT (Liu et al., 2024a), PLLaVA (Xu et al., 2024), ShareGPT4Video (Chen et al., 2024c), MiniCPM-V 2.6 (Yao et al., 2024), In-ternVL2 (Chen et al., 2024e), InternVL2.5 (Chen et al., 2024d) and Qwen2-VL (Wang et al., 2024a), 32 frames for VideoLLaMA2 (Cheng et al., 2024), 64 frames for LLaVA-OneVision (Li et al., 2024a), mPLUG-Owl3 (Ye et al., 2024), LLaVA-Video (Zhang et al., 2024) and LLaVA-NeXT-INST-IT (Peng et al., 2024). All models are evaluated on 8 V100 GPUs. Additionally, we conduct extensive evaluations on 4 closed-source models: GPT-4o (Hurst et al., 2024), GPT-4o-mini (Hurst et al., 2024), Gemini-1.5-Pro (Team et al., 2024), and Gemini-1.5-Flash (Team et al., 2024). For GPT models, we average 64 frames from the original videos; for Gemini-1.5 models, the raw videos were uploaded directly. \n\nHuman and Blind Answering. For human evaluations, we divide all the questions equally and assign them to three human experts. To prevent any data leakage, we ensure that the human experts participating in the test have never been involved in the annotation process.",
            "score": 0.40489208290824913,
            "section_title": "Experiment Setup",
            "char_start_offset": 15329,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1419
                }
            ],
            "ref_mentions": [
                {
                    "start": 476,
                    "end": 496,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 644,
                    "end": 662,
                    "matchedPaperCorpusId": "265308687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06512451171875
        },
        {
            "corpus_id": "270521771",
            "title": "Long Story Short: Story-level Video Understanding from 20K Short Films",
            "text": "In this section, we evaluate recent open-source and commercial vision-language models on SF20K-Test. In particular, we benchmark eleven open-source models: Frozen-BiLM [6], mPLUG-Owl2 [80], TimeChat [52], Video-Llava [36], MovieChat [59], Llava-OneVision [32], Long-Llava [67], LongVA [84], Llava-Next-Video [85], Llama-3.2-Vision [43], and Pixtral [3]; and two commercial models: LloVi [83] (based on GPT-3.5 [45]), and GPT-4o-mini [46]. \n\nSee Appendix E for more details on prompts and metrics. The models are tested in a zero-shot video questionanswering setting, adapted for long-form video understanding by incorporating subtitles and sampling the maximum number of frames possible to fit in a single A100 GPU. For multiple-choice questions, we compute the accuracy score by extracting the option letter choice from the model's response. For open-ended questions, we rely on GPT-3.5 to compute the similarity between the predicted and correct answers, as detailed in [39]. All methods are evaluated across three modalities to better assess the contribution of each: Vision-Only (V) with video frames, Language-Only (L) with subtitles, and Vision-Language (VL) combining both. The results are summarized in Table 4. Multiple-Choice Question Answering (MCQA). Our results reveal that several models exhibit strong performance, with GPT-4o-mini achieving the highest accuracy of 79.4%. \n\nNotably, recent open-source models are closing the gap with commercial ones; for example, Llava-OneVision-7B lags be-  hind by just 1.2 percentage points. However, there remains significant room for improvement, as human performance reaches 89.8%. \n\nOpen-Ended Question Answering (OEQA). This task is more challenging, as evidenced by significantly lower accuracy across all models, ranging from 1.6% to 60.0%.",
            "score": 0.40384062317573455,
            "section_title": "Baselines",
            "char_start_offset": 18573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1387
                },
                {
                    "start": 1390,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1800
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "232478955"
                },
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "272397612"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12493896484375
        },
        {
            "corpus_id": "274281000",
            "title": "Filter, Correlate, Compress: Training-Free Token Reduction for MLLM Acceleration",
            "text": "Results on LLaVA-1.5-7B. Tab. 1 presents the performance of FiCoCo across 6 benchmarks based on LLaVA-1.5-7B, including ScienceQA (SQA) [30], TextVQA (VQA T ) [34], POPE [21], GQA [15], MMBench (MMB) [29] and VQAv2 [11]. Several highlights can be observed from the results: (1) FiCoCo-V, and FiCoCo-L generally outperform existing training-free methods. (2) FiCoCo-L demonstrates superior performance over FiCoCo-V. This indicates that supplying comprehensive visual information to LLMs and reducing visual tokens within LLMs can more effectively maintain task performance. When TFLOPs = 1.5, the average performance of FiCoCo-V and FiCoCo-L both exceed 92%, indicating that by applying our compression method, the performance of MLLM can be well preserved across different benchmarks.We also report more LLaVA-1.5-7B results in Tab. 6, and LLaVA-1.5-13B results in Tab. 7 to show superiority. \n\nResults on LLaVA-NeXT-7B. LLaVA-NeXT-7B [18] em-ploys the AnyRes strategy, increasing input resolution fourfold to better retain image details and mitigate hallucinations. However, this also substantially increases visual tokens, intensifying computational demands. To verify the effectiveness of FiCoCo in high-resolution scenarios, we conduct further token compression experiments based on the LLaVA-NeXT-7B architecture. \n\nIn the experimental setup, we apply two sets of computational constraints: For PDrop [38], we control the TFLOPs at 5.0 to match the reported results. And for SparseVLM [45] and FiCoCo, we constrain the TFLOPs to 2.9. Experimental results on four datasets show that, under TFLOPs = 2.9, FiCoCo-V and FiCoCo-L outperform Spar-seVLM by 3.6% and 4.2%, respectively.",
            "score": 0.40287135425577664,
            "section_title": "Comparisons with State-of-the-art Methods",
            "char_start_offset": 20051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1319
                },
                {
                    "start": 1322,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1684
                }
            ],
            "ref_mentions": [
                {
                    "start": 136,
                    "end": 140,
                    "matchedPaperCorpusId": "252383606"
                },
                {
                    "start": 159,
                    "end": 163,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "258740697"
                },
                {
                    "start": 180,
                    "end": 184,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 200,
                    "end": 204,
                    "matchedPaperCorpusId": "259837088"
                },
                {
                    "start": 215,
                    "end": 219,
                    "matchedPaperCorpusId": "8081284"
                },
                {
                    "start": 1407,
                    "end": 1411,
                    "matchedPaperCorpusId": "273507889"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1649169921875
        },
        {
            "corpus_id": "273403795",
            "title": "Exploring the Design Space of Visual Context Representation in Video MLLMs",
            "text": "Similar to the resampler, BLIP-2 (Li et al., 2023b) and InstructBLIP (Dai et al., 2023) utilize a cross-attention module called Q-Former to compress the image embeddings and directly input the resulting visual embeddings into the LLM. Another line of work, represented by LLaVA (Liu et al., 2024a), directly projects the image embeddings into the semantic space of the LLM with an MLP, achieving decent performance and converging quickly. Based on this, several works (Yao et al., 2024;Cai et al., 2024) propose adding a pooling module after the MLP to reduce the number of visual embeddings. However, when adapting MLLMs to video tasks, the design of the projector is less explored. LLaVA-NeXT-Video (Zhang et al., 2024c) and LLaVA-OneVision (Li et al., 2024a) use mean pooling or bilinear interpolation to aggregate visual embeddings, while neglecting the temporal dependency of video frames. To model the temporal dependency, VideoLLaMA2 (Cheng et al., 2024) introduces a downsampling module and a spatial-temporal convolution module.",
            "score": 0.40282175124433245,
            "section_title": "Video MLLM.",
            "char_start_offset": 26753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1037
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 51,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 278,
                    "end": 297,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.412353515625
        },
        {
            "corpus_id": "277043430",
            "title": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models",
            "text": "These benchmarks cover a wide range of video complexities and durations, ensuring a comprehensive evaluation. Notably, FastVID prunes 90% of video tokens while maintaining 98.0% of LLaVA-OneVision's original performance across all benchmarks. \n\nThe main contributions are summarized as follows:",
            "score": 0.4026189711934046,
            "section_title": "Introduction",
            "char_start_offset": 4402,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 242
                },
                {
                    "start": 245,
                    "end": 294
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5126953125
        },
        {
            "corpus_id": "272689453",
            "title": "Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models",
            "text": "To conduct the statistical analysis, we use 655 samples (0.1%) from the LLaVA-655k data (Liu et al. 2023b) to generate the pruning strategy. To validate FitPrune, we apply it to LLaVA (Liu et al. 2023c), LLaVA-HR (Luo et al. 2024), and LLaVA-NEXT (Liu et al. 2024). The base language models used in these MLLMs are all Vicuna (Chiang et al. 2023). Under the default setting, LLaVA-1.5 7B has 576 visual tokens, while LLaVA-HR 7B stands out with 1024 visual tokens, resulting in better performance and higher computational complexity. For a higher resolution, LLaVA-NEXT 7B dynamically introduces visual tokens up to 2880, improving the performance by 6.0% with 3.5\u00d7 computation overhead compared to LLaVA. In our experiments, we randomly select 655 samples (0.1%) from LLaVA-655k dataset (Liu et al. 2023b) to generate the pruning strategy. In our statistical process, the binary search tolerance parameter \u03f5 is set to 0.01. Our experiments are conducted on a single A100 40G GPU. More details can refers to our projects.",
            "score": 0.40245707083150717,
            "section_title": "Implementation Details",
            "char_start_offset": 15879,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1021
                }
            ],
            "ref_mentions": [
                {
                    "start": 247,
                    "end": 264,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2171630859375
        },
        {
            "corpus_id": "275458756",
            "title": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?",
            "text": "[44], alongside six recent open-source MLLMs like Qwen2-VL [46] and LLaVA-OneVision [22]. Despite their strong offline performance, these models struggle with online-style queries (e.g., What is happening now?), showing a significant gap from human performance. Further experiments on recent streaming models, such as Flash-VStream [57], reveal an even wider performance gap compared to offline counterparts, highlighting a substantial research space for further exploration and improvement.",
            "score": 0.40226769741375146,
            "section_title": "Introduction",
            "char_start_offset": 5093,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 491
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07598876953125
        },
        {
            "corpus_id": "270559956",
            "title": "On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning",
            "text": "To test the impact of reducing the number of tokens in LLaVA-NeXT models, we constraine the grid size, resulting in a maximum token count of 1728 (either 336x672 or 672x336 pixels). As shown in Table 6, reducing the vision token count leads to significant performance drops across all evaluated tasks. For example, the performance of the 13B model on DocVQA decreases from 69.8 to 53.9 when the token count is restricted, with similar trends observed in other variants. This analysis highlights the trade-off between token count and model performance: while reducing tokens can enhance computational efficiency, it may lead to a compromise in accuracy. \n\nIn contrast, ELVA models demonstrate strong performance along with improved efficiency in both speed and memory usage, underlining their robustness in handling high-resolution text-centric tasks efficiently. The ELVA models effectively balance performance and efficiency, outperforming the LLaVA-NeXT variants with reduced token counts. times that of LLaVA-NeXT, we find this a fair tradeoff for the gains in efficiency and performance. Our approach remains competitive and more resourceefficient compared to several contemporary models. For detailed analysis, please refer to Appendix A. \n\nRegarding memory usage, practical deployment often uses quantization (Dettmers et al., 2023), significantly reducing memory costs. For instance, the LLaVA-NeXT-13B model originally requires two V100 GPUs but can run on a single V100 with quantization, albeit with more latency and some performance loss. Despite these drawbacks, quantization shows promise and will likely improve. Our ELVA models, built for efficiency, complement these advancements, promising even greater value when combined with quantization techniques.",
            "score": 0.4019295391731873,
            "section_title": "Ablations with LLaVA-NeXT Variants",
            "char_start_offset": 22947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1769
                }
            ],
            "ref_mentions": [
                {
                    "start": 1315,
                    "end": 1338,
                    "matchedPaperCorpusId": "258841328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1981201171875
        },
        {
            "corpus_id": "278129780",
            "title": "HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?",
            "text": "We include a total of 28 VLMs in our experiment. The details are in supplementary materials. This includes one Phi-3.5 [2], two DeepSeek [9], seven InterVL2 [10][11][12]22], two Qwen2-VL [63], two MolMo [15], one LLaVA-Onevision [46],one LLaVA-HR [51], four Llava-Next [78], two Llama-3.2 [19], and two GPT-4o [32], two Gemini [60], and two Claude [4]. \n\nFor WhiteBackground, we follow the accuracy in VQAv2 [23]. For ComplexGrid dataset, we prompt the model to generate the column and row of the needle image and compare with the gold column and row using an exact match. For real-world datasets, since they are MCQ-based, we directly use exact math as metrics. Supplementary materials show more details.",
            "score": 0.40179570736688974,
            "section_title": "Implementation Details",
            "char_start_offset": 15654,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 705
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 229,
                    "end": 233,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 247,
                    "end": 251,
                    "matchedPaperCorpusId": "268248921"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "8081284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0280609130859375
        },
        {
            "corpus_id": "271719914",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "text": "\u2022 The Ablation blog [64] summarizes our empirical exploration except the visual instruction data itself, including the choice of architectures (scaling of LLM & vision encoder), visual representations (resolution & #tokens), as well as training strategies (trainable modules & high-quality data) in the pursuit of data scaling success. \u2022 The Interleave blog [68] describes the strategies to extend and improve the capability in new scenarios including multi-image, multi-frame (video) and multi-view (3D), while maintaining the single-image performance. \n\nThese explorations, conducted within a fixed compute budget, aimed to offer useful insights along the way as we navigate the project, rather than push performance limits. During the process, we have also been accumulating and curating a large collection of the high-quality datasets from January to June. By consolidating these insights and execute the experiments with \"yolo run\" on newly accumulated larger datasets, we introduce LLaVA-OneVision. We implement the new model with the available compute, without extensively de-risking individual components. This leaves room for further improvements in capabilities through additional data and model scaling following our recipe, Please see the detailed development timeline in Section A. In particular, our paper makes the following contributions: \n\n\u2022 Large multimodal models. We develop LLaVA-OneVision, a family of open large multimodal models (LMMs) that improves the performance boundaries of open LMMs in three important vision settings, including single-image, multi-image, and video scenarios. \u2022 Emerging Capabilities with Task Transfer. Our design in modeling and data representations allow task transfer across different scenarios, suggesting a simple approach to yield new emgerging capabilities. In particular, LLaVA-OneVision demonstrate strong video understanding through task transfer from images. \u2022 Open-source. To pave the way towards building a general-purpose visual assistant, we release the following assets to the public: the generated multimodal instruction data, the codebase, the model checkpoints, and a visual chat demo.",
            "score": 0.4017377753501298,
            "section_title": "Introduction",
            "char_start_offset": 1791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2153
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62890625
        },
        {
            "corpus_id": "274131643",
            "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models",
            "text": "Image LLMs aim to bridge the representation space between the vision encoder [33] and LLM [1,6,12,37] using image-text data. The BLIP family, including BLIP-2 [14] and InstructBLIP [7], employs a Querying Transformer (QFormer) to connect vision and language modalities via learnable queries in cross-attention modules. QFormer is also adopted by follow-up works, such as QWen-VL [2] and mPLUG-Owl [46]. QWen-VL incorporates interleaved image-text data in a three-stage training pipeline, while mPLUG-Owl adopts a modularized learning approach. The LLaVA family, in contrast, uses an MLP for connecting the vision encoder and LLM. LLaVA [21] leverages GPT-4 [31] generated visual instruction data for fine-tuning. Subsequent versions, LLaVA-v1.5 [22] and LLaVA-v1.6(NeXT) [23] further improve LLaVA's performance through better data, higher resolution and stronger LLM. As a concurrent work, MiniGPT-4 [54] aligns a frozen LLM with frozen ViT [8] and QFormer using a trainable linear layer to enable multimodal instruction following. \n\nTraining-based video LLMs are further trained on massive video data to endow image LLMs or LLMs with video understanding capabilities. Video-ChatGPT [26] uses LLaVA as its backbone, applying separate temporal and spatial pooling to visual features. The model further utilizes 100k video instruction tuning data. VideoChat [16] leverages QFormer for video token compression, and VideoChat2 [17] refines this approach with improved vision-language alignment and instruction tuning. It also introduces the multitask video understanding benchmark, MVBench. Video-LLaVA [20] learns a shared projector for image and video encoders. Video-LLaMA [49] and Video-LLaMA2 [5] incorporate video, audio and language modalities to support tasks oriented toward video and audio.",
            "score": 0.40107972514250273,
            "section_title": "Related Works",
            "char_start_offset": 3113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1797
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 81,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 159,
                    "end": 163,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 745,
                    "end": 749,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 942,
                    "end": 945,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 1184,
                    "end": 1188,
                    "matchedPaperCorpusId": "259108333"
                },
                {
                    "start": 1424,
                    "end": 1428,
                    "matchedPaperCorpusId": "265466214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64013671875
        },
        {
            "corpus_id": "274437635",
            "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification",
            "text": "Vision understanding. As demonstrated in Tab. 1, our proposed Dynamic-LLaVA framework achieves superior performance compared to SoTA vision context sparsification methods in most benchmarks while reducing image tokens by approximately 80%. Additionally, the Dynamic-LLaVA framework reduces FLOPs by more than 70% for official LLaVA-1.5 base-  lines in the prefill stage, with only minimal performance degradation across most benchmarks. \n\nEspecially, for the LLaVA-1.5 with 7B and 13B parameters, the Dynamic-LLaVA even shows performance improvements on the SciQA, POPE, MME, and MMBench benchmarks. For instance, on the SciQA benchmark, the 7B and 13B Dynamic-LLaVA models achieve performance gains of +2.3% and +0.8%, respectively, compared to the original LLaVA-1.5. More comparison of SoTA vision context sparsification methods are presented in Appendix A.5.1. As shown in Tab. 2, for efficient vision projector methods for MLLMs, Dynamic-LLaVA exclusively utilizes the original MLP of LLaVA-1.5 as the vision projector. This approach surpasses those that modify the vision projector across most benchmarks, achieving an average performance that exceeds the best alternative methods by 0.9%. Furthermore, our Dynamic-LLaVA framework can be integrated with other efficient vision projector methods. For instance, our combination with the TokenPacker projector, Dynamic-LLaVA-TokenPacker, achieves significant reductions in vision tokens by an additional 60% on top of the already reduced count by efficient vision projector methods, with only a minimal loss in performance. Compared to the official LLaVA-TokenPacker method, our Dynamic-LLaVA-TokenPacker incurs only a minimal performance loss of 0.3% and 0.1% (for 7B and 13B models) while using fewer vision tokens (57 vs. 144). Moreover, we observe that the sparsification of the output text tokens does not impede the comprehension ability of MLLMs.",
            "score": 0.4003311066031888,
            "section_title": "MAIN RESULTS",
            "char_start_offset": 25218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 436
                },
                {
                    "start": 439,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1906
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56298828125
        },
        {
            "corpus_id": "277321635",
            "title": "Beyond Intermediate States: Explaining Visual Redundancy through Language",
            "text": "Results on multi-image and video comprehension tasks are presented in Table 2. On Mantis-test and MuirBench, the performance of LLaVA-OneVision improves by 5% after randomly removing 80% of visual tokens, while our method achieves a higher enhancement of 10%. This suggests that an excessive number of visual tokens may impede the model's ability to comprehend image-text-interleaved contexts. In the MVBench video understanding benchmark, our approach maintains 94% performance even with an extreme visual token removal rate of 92%, significantly surpassing the random baseline. These results demonstrate that our method can effectively transfer from single-image to multiimage and video comprehension tasks. Table 2. LLaVA-OneVision-7B results on multi-image and video comprehension benchmarks. Our proposed method maintains over 90% of peak performance and achieves a 10% performance gain by pruning 80% to 90% of input visual tokens.",
            "score": 0.40032017295004524,
            "section_title": "Multi-Image and Video Comprehension",
            "char_start_offset": 24627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 937
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.288818359375
        },
        {
            "corpus_id": "272986933",
            "title": "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering",
            "text": "In this paper, we employ two types of leading methods: LLaVA-1.5 (Liu et al., 2023a) uses a CLIP-pretrained ViT-L/14 (Radford et al., 2021) as a vision encoder, a projector and an LLM, and LLaVA-NeXT (Liu et al., 2024a) increases the input image resolution by applying an adaptive image cropping strategy to concatenate all vision tokens. To ensure a fair and comprehensive comparison Table 1 and Table 2 present results both excluding and including the ShareGPT4V dataset, as well as results from the incorporation of our dataset. Table 3 We have reproduced LLaVA-NeXT with a learning rate of ViT to 1/10 of the base learning rate for the reason that LLaVA-NeXT only publishes their evaluation code. The learning rate for the PT stage is set to 1e Data Processing Details During the data construction pipeline, we employ NLTK (Bird, 2006) tool to extract noun phrases from the captions, and the resulting set of phrases is then post-processed using WordNet (Miller, 1995) to remove duplicates and filter out inaccurately named entities. The total amount of final data after consistency filtering will not be completely consistent for different VLMs and we show the details in Appendix C.1. The checkpoints of the VLM we used in our data processing are the original checkpoints of the official release. For LLaVA-1.5, which is not trained with the ShareGPT4V dataset, LLaVA-NEXT is trained with part of the ShareGPT4V dataset. The detailed GPU hours can be found in Appendix C.2 and we show the visualization of our W2C samples in Appendix C.3.",
            "score": 0.39985497820888105,
            "section_title": "Implementation Details",
            "char_start_offset": 16037,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1544
                }
            ],
            "ref_mentions": [
                {
                    "start": 65,
                    "end": 83,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 117,
                    "end": 139,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 827,
                    "end": 839,
                    "matchedPaperCorpusId": "1438450"
                },
                {
                    "start": 958,
                    "end": 972,
                    "matchedPaperCorpusId": "1671874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2802734375
        },
        {
            "corpus_id": "270063538",
            "title": "Matryoshka Multimodal Models",
            "text": "LLaVA-1.5-M 3 We evaluate LLaVA-1.5-M 3 on the common multimodal understanding and reasoning benchmarks. Results are shown in Table 1. LLaVA-1.5-M 3 with full tokens maintains the performance of LLaVA-1.5 across diverse benchmarks. More importantly, our approach shows strong performance even with 1 or 9 tokens. Specifically, in MMBench, a comprehensive multimodal understanding benchmark, LLaVA-1.5-M 3 with 9 tokens surpasses Qwen-VL-Chat with 256 tokens, and achieves similar performance as Qwen-VL-Chat with even 1 token. Compared with Instruct-BLIP [59], LLaVA-1.5M 3 with 9 tokens surpasses InstructBLIP-7B and InstructBLIP-13B across all benchmarks. This demonstrates that our model has both flexibility and strong empirical performance under diverse number of visual tokens. \n\nLLaVA-NeXT-M 3 We use the proposed Matryoshka Multimodal Models to finetune LLaVA-NeXT, and compare LLaVA-NeXT-M 3 with SS, which denotes the setting where the LLaVA-NeXT is trained under a Specific Scale of visual tokens also for 1 epoch. We also include the oracle upperbound performance. Specifically, 'Oracle' denotes the case where the best tradeoff between visual tokens and performance is picked for each test instance. Specifically, for each test instance, we select the the scale with the fewest amount of tokens but can answer the question correctly. Results are shown in Table 2. Our approach, M 3 , is at least as good as SS, while performing better on tasks such as document understanding (TextVQA and ChartQA) and common benchmarks such as MMBench [23]. \n\nOur results also show that dataset level biases towards the visual token scales do exist. For example, ScienceQA maintains consistent performance across all visual token scales. AI2D and MMBench only encounter a small performance drop for even as few as 9 to 1 tokens. On the other hand, dense visual perception tasks such as TextVQA and DocVQA show a significant performance drop with fewer tokens.",
            "score": 0.39973910930907186,
            "section_title": "Image Understanding",
            "char_start_offset": 16919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1955
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23193359375
        },
        {
            "corpus_id": "277349217",
            "title": "InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression",
            "text": "Experiments configuration. We initialize InternVL-X models with the same pre-trained components as the In-ternVL2 series. For the 2B model, we employ InternViT-300M-448px [8] as the visual encoder and InternLM2-Chat-1.8B [3] as the LLM, while the 8B variant uses the same visual encoder but upgrades to InternLM2.5-Chat-7B [43] for LLM. Our training strategy follows a two-stage approach similar to LLaVA-1.5. In the pretraining stage, we finetune only the projector module and freeze other parameters, followed by a instruction tuning stage where all model parameters are finetuned. All models are trained for 1 epoch using the AdamW optimizer with a cosine learning rate scheduler. The initial learning rate is set to 4e-5. The training is conducted on 8 NVIDIA A800 GPUs. In normal resolution experiments, we use PVTC and LVTC, and in high resolution, we adopt PVTC, LVTC, and RVTC. \n\nTraining datasets. We follow the training datasets as LLaVA-1.5 and LLaVA-NeXT. In the first stage, we utilize LCS-558K [34] dataset to finetune the projector. In the second stage, for normal resolution model, we use LLaVA-665K [34] dataset for instruction tuning. For high resolution model, as the dataset used in LLaVA-NeXT is not publicly available, we assemble an instruction tuning dataset with 770K samples following the guidance of LLaVA-NeXT. \n\nEvaluation benchmarks. For fair evaluation, we assess our model on 12 diverse benchmarks, which span textoriented VQA (TextVQA [44], DocVQA [39], ChartQA [38], InfoVQA [40]), general VQA (GQA [21], VQAv2 [15], VizWiz [17]), and comprehensive evaluation (MMB [36], MMVet [50], MMMU [51], POPE [27], SEED [22]).",
            "score": 0.39963327426301687,
            "section_title": "Implement details and datasets",
            "char_start_offset": 18312,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 885
                },
                {
                    "start": 888,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1338
                },
                {
                    "start": 1341,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1650
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 174,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 1008,
                    "end": 1012,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1468,
                    "end": 1472,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 1481,
                    "end": 1485,
                    "matchedPaperCorpusId": "220280200"
                },
                {
                    "start": 1533,
                    "end": 1537,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 1545,
                    "end": 1549,
                    "matchedPaperCorpusId": "8081284"
                },
                {
                    "start": 1558,
                    "end": 1562,
                    "matchedPaperCorpusId": "3831582"
                },
                {
                    "start": 1599,
                    "end": 1603,
                    "matchedPaperCorpusId": "259837088"
                },
                {
                    "start": 1622,
                    "end": 1626,
                    "matchedPaperCorpusId": "265466525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1866455078125
        },
        {
            "corpus_id": "276929450",
            "title": "QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long Video Comprehension",
            "text": "Table 2. The overall performance on MVBench [15], VNBench [49] and NeXT-QA [39] at 7B-scale LVLMs with the setting of the original frame rates. By applying QuoTA to LLaVA-Video-7B [48], we observed an average performance improvement of 4.8% across three benchmarks, especially a 10.3% improvement on the Needle-In-A-Haystack benchmark VNBench [49], which set a new state-of-the-art performing better then LongVILA [40], demonstrating QuoTA assist query-oriented keyframes focusing. \n\nFix-len. Dy-len. [7] and VNBench [49] when using Long-LLaVA-7B [43] as the based LVLM on QuoTA. Fix-len. and Dy-len. represent fix sampled 96-frame and dynamic sampled 96\u223c160 frames with the same token budget Nt as the baseline, respectively. Wei. and CoT-Dec. denote the LVLM-based frame scoring and CoT-Driven Query Decouple in Section 3.3 and 3.4, respectively. \n\nto 25% token budget (T base = 64, \u03b1 = 32), it maintains competitive performance with only a 1.1% accuracy drop while achieving 2.1\u00d7 inference speedup. Effect of different visual token assignment strategy. In Section 3.4, we proposed three types of dynamic visual token assigners. To evaluate their comparative efficacy, we conducted an ablation study implementing QuoTA within LLaVA-Video-7B [48] on the Video-MME [7] and VN-Bench [49]. As detailed in  4. Results on different frame-sampling strategies and token budgets in Video-MME [7] and VNBench [49]. Gray rows represent baseline LLaVA-Video-7B [48] with fixed frame sampling while others represent dynamic sampling with varying T base and \u03b1 in different budget Nt when extending the baseline with QuoTA. #Time denotes the average time during inference per sample. \n\n[1], we contend that such a methodology disrupts spatial coherence in video representations, consequently impeding effective cross-modal interaction during early fusion stages.",
            "score": 0.39802802740943155,
            "section_title": "Ablation Studies",
            "char_start_offset": 22302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 848
                },
                {
                    "start": 851,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1670
                },
                {
                    "start": 1673,
                    "end": 1849
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 48,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 75,
                    "end": 79,
                    "matchedPaperCorpusId": "234763093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2445068359375
        },
        {
            "corpus_id": "271719914",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "text": "We further evaluate LLaVA-OneVision in multi-image interleaved settings, where users may ask questions between multiples images. In particular, we perform comprehensive assessment on the diverse subtasks of LLaVA-Interleave Bench [68], such as Spot the Difference [45], Image Edit Instruction (IEI) [68], Visual Storytelling (VST) [40], Text-rich VQA (TR-VQA) [85], Multi-image VQA (MI-VQA) [117], Raven Puzzle [24], Q-Bench (QB) [139], and NLVR2 [125]). We also utilize several multi-view benchmarks for evaluation, which depict 3D environments with multiple viewpoints, including 3D Dialogue (3D-Chat) and Task Decomposition (3D-TD) from 3D-LLM [38], ScanQA [5], ALFRED [122], and nuScenes VQA [9]. We refer to these datasets as in-domain evaluations, since our training data includes the training split of them. \n\nMoreover, we conduct evaluations on different out-domain tasks, which reveals the generalization capability of our approach. They include the multi-image split of math QA benchmark MathVerse [165] and science QA benchmark SciVerse [34], multi-image perception benchmark BLINK [31], MMMU-(multi-image) [157] that contains all multi-image QA in MMMU, and MuirBench [135] spanning 12 diverse multi-image tasks. \n\nAs shown in Table 4, LLaVA-OneVision (SI) consistently outperforms existing multi-image LMMs in all benchmarks. After additional tuning on multi-image and video data, LLaVA-OneVision shows a marked improvement over GPT-4V in specific areas, with significant margins. This highlights its strong performance in complex tasks such as multi-image reasoning, identifying differences, and understanding 3D environments. In addition, we observe a consistent performance enhancement on after the one-vision training stage, which is more evident on multi-view benchmarks that are absent  in single-image data. This demonstrates the significance of our one-vision paradigm for empowering LMMs with comprehensive visual capbalities.",
            "score": 0.3979294964716682,
            "section_title": "Multi-Image Benchmarks",
            "char_start_offset": 23302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1948
                }
            ],
            "ref_mentions": [
                {
                    "start": 447,
                    "end": 452,
                    "matchedPaperCorpusId": "19435386"
                },
                {
                    "start": 647,
                    "end": 651,
                    "matchedPaperCorpusId": "260356619"
                },
                {
                    "start": 660,
                    "end": 663,
                    "matchedPaperCorpusId": "245334889"
                },
                {
                    "start": 672,
                    "end": 677,
                    "matchedPaperCorpusId": "208617407"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "221340919"
                },
                {
                    "start": 1118,
                    "end": 1123,
                    "matchedPaperCorpusId": "265466525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.274658203125
        },
        {
            "corpus_id": "274762940",
            "title": "ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation",
            "text": "We report ablations in Table 3  Input Frames. In rows 1-3, we train our model using T s = {1, 2, 4} frames instead of T s = 8 in the final setting. We see that using fewer frames strongly reduces performance, especially for captioning, showing that our dataset/benchmark is video-centric and requires temporal context to effectively tackle. \n\nTask Synergy. For rows 4 and 5, we train the model for either one of our benchmark tasks. We see that the model trained only for LG-VIS achieves 18.5 mAP which is much worse than the 20.5 achieved by the multi-task model. Meanwhile, the Caption Accuracy remains unchanged at 3.0. This indicates that pixel-level segmentation benefits greatly from holistic video understanding. \n\nLLaVA Baseline. Row 6 shows results for a LLaVA-NeXT baseline trained with our recipe which uses a CLIP vision encoder and no Slow-Fast Pooling. This achieves 19.9 mAP and 2.9 CA, both of which are worse than our final setting.  4 compares Video-LLaVA-Seg to other methods on the validation set. For video captioning, we evaluate off-the-shelf LLaVA-OneVision [54] and MiniCPM-o [123] models which achieve 2.9 and 3.0 CA scores, respectively. This is the same as Video-LLaVA-Seg, but these models are trained on significantly more data and cannot tackle LG-VIS. For LG-VIS, we evaluate multiple existing Referral-VOS approaches. LMPM is an earlier transformer-based approach which only achieves 8.4 mAP. DsHmp [41] performs better with 13.3 mAP since the architecture decouples static and motion cues to improve video segmentation performance. Finally, VideoLISA [10] is a recent LLMbased approach which only achieves 10.7 mAP when finetuned on ViCaS. By contrast, Video-LLaVA-Seg achieves 20.5 mAP while tackling both captioning and LG-VIS. Furthermore, unlike existing Referral-VOS approaches, it can predict multiple segmentation masks for a single prompt.",
            "score": 0.3973869465594668,
            "section_title": "Ablation Studies",
            "char_start_offset": 26629,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1881
                }
            ],
            "ref_mentions": [
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "271719914"
                },
                {
                    "start": 1101,
                    "end": 1106,
                    "matchedPaperCorpusId": "271709626"
                },
                {
                    "start": 1432,
                    "end": 1436,
                    "matchedPaperCorpusId": "268889456"
                },
                {
                    "start": 1585,
                    "end": 1589,
                    "matchedPaperCorpusId": "272986987"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22509765625
        },
        {
            "corpus_id": "277066415",
            "title": "Grounded Chain-of-Thought for Multimodal Large Language Models",
            "text": "We conduct a comprehensive evaluation of leading MLLMs through our GCoT dataset, selecting both pioneering architectures and state-of-the-art performers. The evaluated models include LLaVA (7B, 13B) [28] and LLaVA-OneVision (0.5B, 7B, 72B) [22], along with top-performing Qwen2.5-VL (3B, 7B, 72B) [41] and InternVL2.5 (4B, 8B, 38B, 78B). To enable GCoT adaptation, we replace the original instruction-tuning samples with an equivalent number of examples from our GCoT dataset. Then we train LLaVA GCoT from the scratch using this modified training set following the default settings of LLaVA.",
            "score": 0.3973264029607133,
            "section_title": "Implement Details",
            "char_start_offset": 14391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 592
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1534423828125
        },
        {
            "corpus_id": "277621021",
            "title": "InstructionBench: An Instructional Video Understanding Benchmark",
            "text": "As illustrated in Increasing the number of input frames significantly enhances the performance of closed-source models. GPT-4o shows a 4.76% improvement when we input 16 frames instead of 8 frames to it. GPT-4V and Gemini Pro Vision also get better results when the input video frames increase. \n\nHowever, for open-source models, increasing frame input offers little to no benefit and can even decrease performance, especially for those lacking temporal and spatial modeling. For example, LLaVA-NeXT-Video sees a 1.5% overall performance drop in our InstructionBench when the input video frames increase from 8 to 16. Despite processing more frames at 1 frame per second, LLaMA-VID underperforms compared to 8-frame Video-LLaVA, which uses Language-Bind's (Zhu et al. 2023) video encoder for better temporal and spatial attention integration. Furthermore, LLaMA-VID compresses one video frame to just 1-2 tokens, resulting in significant information loss of video frames. This approach leads to LLaMA-VID achieving the lowest results in Fine-Grained tasks on our InstructionBench. These findings suggest that existing Video-LLMs must improve their ability to process multiple frames to fully capture video information.",
            "score": 0.3965954972885876,
            "section_title": "Overall Results",
            "char_start_offset": 17257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1218
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1370849609375
        },
        {
            "corpus_id": "273186838",
            "title": "Intriguing Properties of Large Language and Vision Models",
            "text": "We hypothesize that this global interpretation may result from recent LLVMs being trained via backpropagation, with the loss signal primarily derived from the text output of the Assistant: side. Based on these experiments, we argue that while LLVMs are trained with an autoregressive objective, they internally handle images globally. This observation offers a possible explanation for the success of pixel shuffling (Chen et al., 2024c) in achieving both strong performance and efficiency. LLVMs are sensitive to spatial structures. Instead of treating visual patch tokens as permutation invariants, we explore how LLVMs behave when the sequence of image patches is permuted. To examine the sensitivity to spatial structure, we randomly shuffle image patches at varying grid sizes (2, 4, 8, 14), as shown in Figure 2. In our experiments, we observe that LLaVA-OneVision is sensitive to spatial structures on the MathVista (Lu et al., 2023) and AI2D (Kembhavi et al., 2016) tasks, despite the ViT learning all interactions between image patches. This result contrasts with previous study (Naseer et al., 2021) suggesting that ViT-based vision encoders exhibit high permutation invariance to patch positions than CNN counterparts. We posit that on the MMVP Tong et al. ( 2024) dataset, which involves perception task, LLaVA-OneVision would also show permutation invariance to randomly shuffled patches, similar to existing work (Naseer et al., 2021) analyzing the ImageNet (Deng et al., 2009) val. dataset. However, unlike ImageNet, the MathVista and AI2D datasets contain more structurally complex images (e.g., charts, code screenshots) that are highly sensitive to spatial structure, as the original numerical understanding is significantly disrupted. Interestingly, both LLaVA-1.5 and LLaVA-NeXT exhibit insensitivity to spatial structure, particularly in the MathVista dataset, where performance drops were minimal. These results suggest the need for further investigation, which we address in the following sections.",
            "score": 0.39652987568364556,
            "section_title": "DO LLVMS PERCEIVE IMAGES GLOBALLY?",
            "char_start_offset": 13437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 950,
                    "end": 973,
                    "matchedPaperCorpusId": "2682274"
                },
                {
                    "start": 1088,
                    "end": 1109,
                    "matchedPaperCorpusId": "235125781"
                },
                {
                    "start": 1427,
                    "end": 1448,
                    "matchedPaperCorpusId": "235125781"
                },
                {
                    "start": 1472,
                    "end": 1491,
                    "matchedPaperCorpusId": "57246310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1077880859375
        },
        {
            "corpus_id": "273186947",
            "title": "DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination",
            "text": "Considering that CHAIR can more precisely assess the generative capabilities of the model, and given that LLaVA-1.5 and LLaVA-NeXT have similar model structures, we choose to test the parameter sensitivity of DAMRO on LLaVA-1.5 and Instruct-BLIP using CHAIR. The following two parameter ablation experiments are based on this setup. As for how many visual tokens are enough, we conduct ablation experiments on LLaVA-1.5 using POPE, CHAIR and MME benchmarks.",
            "score": 0.3962770948087505,
            "section_title": "B Ablation Study",
            "char_start_offset": 19992,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 457
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0894775390625
        },
        {
            "corpus_id": "274822212",
            "title": "FastVLM: Efficient Vision Encoding for Vision Language Models",
            "text": "1b and 4). In particular, FastVLM outperforms several prior works while being smaller, faster, and trained with less data (Table 6). Compared to LLaVA-OneVision [45] operating at the highest possible resolution (1152\u00d71152), FastVLM obtains comparable performance with the same 0.5B LLM, but with 85\u00d7 faster TTFT and a 3.4\u00d7 smaller vision encoder. \n\nThe following is a summary of our contributions:",
            "score": 0.39580674508811753,
            "section_title": "Introduction",
            "char_start_offset": 3806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 397
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.324462890625
        },
        {
            "corpus_id": "270285696",
            "title": "DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs",
            "text": "We mainly follow the training recipe of Llava [51], of which the training pipeline consists of two stages, i.e. pre-training (PT) stage and supervised-finetuning (SFT) stage.We utilize pre-trained CLIP-large-336 [61] as our default image encoder.To obtain high-resolution feature maps, we split the high-resolution image into patches to comply with the resolution requirement and mosaic the image feature together as whole-image features.\n\nPre-training dataset.We utilize LCS-558k [51] as pre-training data for both experiments based on LLaVA-1.5 and LLaVA-Next, which contain 558k samples from LAION [66], CC [9] and SBU [84], captioned by BLIP [45].Fine-tuning datasets.We utilize LLaVA-mixed-665k [51] as instruction-following data for both experiments based on LLaVA-1.5.However, the SFT dataset used in Llava-Next is not publicly available, we thus combine an SFT dataset of 748K samples following the guidance [50].In contrast, we do not involve the user images uploaded to their website.\n\nTraining configuration.We train our model with only the projection model tuned in the PT stage.\n\nIn SFT stage, we unfreeze LLM.For Experiments on DeepStack-V and DeepStack-HD, we tune the image encoder with a learning rate of 1e-6 following [50].Otherwise, we freeze our vision encoder for a fair comparison.We use 16\u00d7 V100 for experiments with Phi-3 [1] and 8\u00d7 H100 for experiments with Vicuna [15].Please refer to our supplementary material for more detailed training hyper-parameters.",
            "score": 0.39502158361593637,
            "section_title": "Implementation Details",
            "char_start_offset": 18801,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 174,
                    "end": 246
                },
                {
                    "start": 246,
                    "end": 438
                },
                {
                    "start": 440,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 672
                },
                {
                    "start": 672,
                    "end": 775
                },
                {
                    "start": 775,
                    "end": 921
                },
                {
                    "start": 921,
                    "end": 994
                },
                {
                    "start": 996,
                    "end": 1019
                },
                {
                    "start": 1019,
                    "end": 1091
                },
                {
                    "start": 1093,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1242
                },
                {
                    "start": 1242,
                    "end": 1304
                },
                {
                    "start": 1304,
                    "end": 1396
                },
                {
                    "start": 1396,
                    "end": 1483
                }
            ],
            "ref_mentions": [
                {
                    "start": 601,
                    "end": 605,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 610,
                    "end": 613,
                    "matchedPaperCorpusId": "231951742"
                },
                {
                    "start": 622,
                    "end": 626,
                    "matchedPaperCorpusId": "12851637"
                },
                {
                    "start": 646,
                    "end": 650,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11181640625
        },
        {
            "corpus_id": "274131643",
            "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models",
            "text": "Video-LLaMA [49] and Video-LLaMA2 [5] incorporate video, audio and language modalities to support tasks oriented toward video and audio. LLaVA-NeXT-Video [52] fine-tunes LLaVA-NeXT on video data, with a variant that applies DPO [34] for improved performance. LITA [11] employs a slow-fast design [9,41] to capture spatial and temporal information more effectively. \n\nTraining-free video LLMs extend image LLMs for video understanding without requiring additional fine-tuning on video data. As a pioneering approach, IG-VLM [13] constructs a grid-view image from video frames, which is then fed directly into a frozen image LLM with a specially designed prompt. While promising, the image grid approach has limitations, such as reduced resolution and the limited number of frames it can include, which we further discuss in the next section. FreeVA [40] explores various temporal aggregation methods, but similarly uses a limited number of frames. The current state-of-the-art SF-LLaVA [45] adopts the slow-fast design, which is proven to be effective in action recognition [9,41], and in LITA, as mentioned earlier. SF-LLaVA designs a slow pathway compressing fewer frames, and a fast pathway heavily compressing more frames. While both SF-LLaVA and our method use a twostream design, our Thumbnail-and-Sampling strategy leads to significantly better performance on various video understanding benchmarks, while maintaining a better token efficiency. We extend the discussion in Sec. 4.",
            "score": 0.3948997097959168,
            "section_title": "Related Works",
            "char_start_offset": 4774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1486
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 232,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 296,
                    "end": 299,
                    "matchedPaperCorpusId": "54463801"
                },
                {
                    "start": 1073,
                    "end": 1076,
                    "matchedPaperCorpusId": "54463801"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.625
        },
        {
            "corpus_id": "277781774",
            "title": "TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language Models",
            "text": "TAMP outperforms baselines on LLaVA-NeXT. Table 2 reports performance of LLaVA-NeXT at a 50% sparsity ratio. Across 6 of 7 benchmarks, including MME, AI2D, MMMU, Mathvista, and MMBench, TAMP ranks either first or second. On average, TAMP surpasses the strongest baseline by 1.9 percent points (pp) in relative performance, demonstrating its strength in preserving key parameters essential for versatile visual comprehension. \n\nFurthermore, Figure 5 presents the performance of LLaVA-NeXT across a range of sparsity levels. TAMP exhibits the best performance-sparsity tradeoff. In contrast, pruning baselines experience steep accuracy declines beyond 50% sparsity, whereas our adaptive approach shows superior retention of model ability in high sparsity regimes (e.g., 60% and 70%), highlighting its robustness. \n\nTAMP effectively preserves diverse multimodal understanding. To further examine our approach, we evaluate VideoLLaMA2 at a 60% sparsity ratio, with results presented in Table 3. TAMP ranks the top position in nearly all audio and video tasks and a close second in the audiovisual benchmark, outperforming the second-best baseline by Relative Performance (%) \n\nFigure 5: Average relative performances of all pruning techniques at different sparsity ratios for the LLaVA-NeXT. \n\n1.2 pp in average relative performance. These results demonstrate that our approach effectively captures modality-specific contributions, validating its universality across multiple modalities and tasks. Additional experiments on LLaVA-OneVision (Li et al., 2024c), which handles interleaved image and video modalities, are provided in Appendix B. As shown in Figure 6, TAMP consistently maintains strong performance across different sparsity levels in VideoLLaMA2. This further shows TAMP's robustness in maintaining diverse multimodal comprehension even under aggressive sparsity constraints. Moreover, in both Figure 5 and Figure 6, OWL suffers from severe per- formance drops at high sparsity ratios, unlike ECoFLaP and TAMP. OWL assigns layer-wise sparsity ratios proportional to the prevalence of outlier values within input activations computed across all input tokens.",
            "score": 0.3947476222144724,
            "section_title": "Results and Discussion",
            "char_start_offset": 18649,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 424
                },
                {
                    "start": 427,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2166
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37548828125
        },
        {
            "corpus_id": "272423598",
            "title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations",
            "text": "In this section, we compare our TC-LLaVA with recent advanced works, including Video-LLaMA [47], LLaMA-Adapter [48], Video-ChatGPT [30], Chat-UniVi [13], MovieChat [34], VideoChat [18], VideoChat2 [20], Vista-LLaMA [29], LLaMA-VID [22], IG-VLM LLaVA [15], ST-LLM [26], PLLaVA [44], and GPT-4V [1], across various video understanding benchmarks. The best performance is indicated in bold, and the second-best results are indicated with underlining. As shown in Table 1, our TC-LLaVA achieves a new state-of-the-art performance across MSVD-QA, TGIF-QA, and Video-ChatGPT, surpassing GPT-4V by 2.5%, 7.9%, and 0.02%, respectively. Additionally, our TC-LLaVA achieves the best performance across video question-answering benchmarks on the Score metric. Compared to the latest work PLLaVA, which is also initialized from LLaVA-Next and continues using original causal attention mask and RoPE, our TC-LLaVA outperforms it across all five evaluation benchmarks, demonstrating the effectiveness of our proposed methods. \n\nFurthermore, we evaluate TC-LLaVA on MVbench, a multiple-choice video question answering benchmark, focusing on questions that require comprehensive understanding of the entire video. As shown in Table 2, TC-LLaVA achieves state-of-the-art performance in the average MVbench score. Specifically, for time-related tasks such as Action Sequence (AS), Object Existence (OE), Moving Count (MC), Moving Attribute (MA), State Change (SC), Character Order (CO), and Counterfactual Inference (CI), TC-LLaVA demonstrates a significant performance margin of at least 0.5% over other open-source models. Even when compared to GPT-4V, we maintain an edge in average performance across all 20 tasks by 13.1%.",
            "score": 0.39459011769524877,
            "section_title": "Comparison with SOTA",
            "char_start_offset": 17584,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1709
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 152,
                    "matchedPaperCorpusId": "265157455"
                },
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "226262299"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17626953125
        },
        {
            "corpus_id": "276903707",
            "title": "Does Acceleration Cause Hidden Instability in Vision Language Models? Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study",
            "text": "Table 4. LoFi results on LLaVA-NEXT [21]. We evaluate LoFi using \u03b2 = 4 as a representative configuration. Compared to its application on LLaVA-1.5, LoFi applied to LLaVA-NEXT achieves significantly greater computational cost reductions while maintaining high accuracy and consistency. \n\nacross GQA, VQAv2, and TextVQA at comparable or lower computational costs (TFLOPs). For instance, on GQA, dynamic pruning attains 87.3% consistency and 60.8% accuracy, compared to fixed ratios that reach at most 85.9% consistency and 60.1% accuracy. Similar trends are observed in VQAv2 and TextVQA. These results confirm that dynamically adjusting the pruning ratio effectively preserves both accuracy and consistency while maintaining computational efficiency.",
            "score": 0.39428772154976666,
            "section_title": "GQA VQAv2 TextVQA",
            "char_start_offset": 25807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 749
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.242431640625
        },
        {
            "corpus_id": "271088459",
            "title": "LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models",
            "text": "Multi-image Results. As reported in after our M4-Instruct tuning. After adding DPO, our 7B model attains SoTA performance on VDD and VideoChat-GPT benchmarks, surpassing the previous LLaVA-NeXT-Video (34B). This demonstrates the effective temporal understanding and reasoning capabilities of our model across sequential frames. Note that we calculate the average scores by multiplying a weight of 10 times by the score of Video Detailed Description and VideoChat-GPT. \n\nMulti-view (3D) Results. For 3D perception in Table 3, our model also obtains leading results for both indoor and outdoor scenarios on five in-domain benchmarks. Compared to 3D-LLM and Point-LLM with additional point clouds as input, LLaVA-NeXT-Interleave only accepts multi-view images to interpret the 3D world, attaining significantly higher scores in challenging 3D scenarios. \n\nMulti-patch (single-image) Results. We also add 307k (40%) of original LLaVA-NeXT single-image data, which makes our model capable of doing single-image tasks. We use the anyres training for single-image data, which divides an image into multiple patches, forming another multiimage setting. As shown in Table 4, we maintain the singleimage performance of LLaVA-NeXT-Image. As singleimage data is of high quality and diversity, adding singleimage data also improves the instruction-following ability and enables task transfer from single-image to multi-image, which is demonstrated in Section 6.",
            "score": 0.39353268075374226,
            "section_title": "Main Results",
            "char_start_offset": 14338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1448
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.329345703125
        },
        {
            "corpus_id": "268248187",
            "title": "Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation",
            "text": "For the experiments, we tried to measure SOTA MLLM models: LLaVA 1.5 and LLaVA-NeXT, ShareGPT4V, and ChatGPT4. We were unable to measure the newly released Gemini Ultra, as it outright refused to work with images of people. We've also made improvements to the state-of-the-art specialized model MiVOLO [14] to ensure fair competition among cutting-edge models. Figure 1 demonstrates an example of work of evaluated models and figure 2 provides a graphical representation.",
            "score": 0.39324740574848577,
            "section_title": "Introduction",
            "char_start_offset": 2119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 471
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09979248046875
        },
        {
            "corpus_id": "276903685",
            "title": "From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models",
            "text": "We evaluate 25 LVLMs, including both commercial and open-source models. For commercial models, we include prominent options such as GPT-4o/4o-mini (OpenAI et al., 2024a), Claude-3.5-Sonnet (Anthropic, 2023), Gemini-1.5-Flash/Pro (Team et al., 2024), Gemini-Pro (Team et al., 2023), and GPT-4V (OpenAI et al., 2024b). For opensource models, we include Phi-3.5-Vision (Abdin et al., 2024), InternVL2 (Chen et al., 2023), LLaMA-3.2 (Grattafiori et al., 2024), Molmo (Deitke et al., 2024), Pixtral (Agrawal et al., 2024), Qwen2-VL (Wang et al., 2024b), NVLM-D (Dai et al., 2024), IXC-2.5-Reward (Zang et al., 2025), LLaVA-OneVision (Li et al., 2024a), LLaVA-Critic (Xiong et al., 2024), CogVLM (Wang et al., 2023a), and LLaVA-1.5/LLaVA-1.6 (Liu et al., 2023).",
            "score": 0.3917836859343137,
            "section_title": "Baselines",
            "char_start_offset": 13497,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 755
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0966796875
        },
        {
            "corpus_id": "276741475",
            "title": "Streaming Video Question-Answering with In-context Video KV-Cache Retrieval",
            "text": "To further assess the generalizability of our approach, we tested it on additional Video-LLMs: Video-LLaVA-7B (Lin et al., 2024), LongVA-7B (Zhang et al., 2024b), and LLaVA-OV-72B (Li et al., 2024a). We used model sharding for LLaVA-OV-72B, significantly slowing inference. To mitigate this, we reduced the FPS to 0.1 and the number of retrieved frames to 32, ensuring efficient evaluation. As shown in Table 6, ReKV consistently improved performance across various models and benchmarks, highlighting its robustness and adaptability. 4 and 5 compared LLaVA-OneVision+ReKV with Flash-VStream. However, these comparisons may be unfair due to different architecture and training data. Thus, here we conduct fair comparisons using the same Video-LLM backbone, including the identical visual encoder (CLIP-ViT-L/14), projector (2-layer MLP), LLM (Vicuna-7B-v1.5), training data, and train/eval pipelines. \n\nDue to the inaccessibility of WebVid videos3 used in Flash-VStream's original training, we use 232K randomly sampled InternVid videos4 as a substitute. This ensures comparable experimental settings. We train a baseline Video-LLM model (Base) and a Flash-VStream-enhanced version (Base+Flash). Similarly, we integrate ReKV into the same baseline (Base+ReKV) for fair comparisons. To maintain parity, the baseline uniformly samples 16 frames per video, resized to 224 \u00d7 224. Visual features of shape (T, 16, 16, D) are average-pooled to (T, 8, 8, D) before being passed through the MLP projector and into the LLM. Both Flash-VStream and ReKV process video at 0.5 FPS, with ReKV retrieving 16 frames. As shown in Table 7, Base+ReKV consistently outperforms the base Video-LLM Base and surpasses Base+Flash in most cases, highlighting its superiority under fair comparative conditions.",
            "score": 0.3914817525588594,
            "section_title": "B ADDITIONAL EXPERIMENTS B.1 EXPERIMENTS WITH MORE VIDEO-LLMS AND BENCHMARK",
            "char_start_offset": 30675,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1784
                }
            ],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 128,
                    "matchedPaperCorpusId": "265281544"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1273193359375
        },
        {
            "corpus_id": "276928069",
            "title": "VAGUE: Visual Contexts Clarify Ambiguous Expressions",
            "text": "Models We use the following multimodal models in our experiments. The detailed characteristics and descriptions of each model are provided in Appendix C. \u2022 Phi3.5-Vision-Instruct (4B) [27] \u2022 LLaVA Onevision (7B) [22] \u2022 Qwen2.5-VL-Instruct (7B) [40] \u2022 InternVL-2.5-MPO (8B, 26B) [6] \u2022 Idefics2 (8B) [21] \u2022 LLaVA NeXT Vicuna (13B) [23] \u2022 Ovis2 (16B) [26] \u2022 GPT-4o [29] \u2022 Gemini 1.5 Pro [11]",
            "score": 0.3914757939795037,
            "section_title": "Experiments",
            "char_start_offset": 22839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 388
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 302,
                    "matchedPaperCorpusId": "269587869"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0154876708984375
        },
        {
            "corpus_id": "274859644",
            "title": "Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception",
            "text": "Model and Training Set. We conduct experiments on LLaVA-v1.5 [31] and LLaVA-NeXT [32] to demonstrate the effectiveness of DCE. Specifically, we using CLIP-L [42] as the visual encoder and Vicuna-v1.5 [12] as the large language model. We adopt a two-stage training strategy: (1) Pre-Training Stage. We train only the projector for initial alignment. Then, following SharGPT4V [8], we set the last 12 layers of the visual encoder in LLaVA-v1.5 as trainable and make the entire LLaVA-NeXT model trainable, following [23], to further enhance perceptual capabilities. (2) Instruction Tuning Stage. We use the open-source LLaVA-mix-665K and LLaVA-NeXT-data to respectively train the LLaVA-v1.5 and LLaVA-NeXT models. The detailed training procedure is provided in the supplementary material. Evaluation Benchmarks. We evaluate on seven visual question answering (VQA) tasks across domains such as document understanding, general knowledge, and scientific reasoning, including VQAv2 [15], DocVQA [37], OKVQA [44], GQA [17], TextVQA [47], ScienceQA [35], and Ai2d [19]. Additionally, we evaluate performance on five widely used LMM benchmarks designed to test multimodal models on visual grounding, scene understanding, and generalization: MMBench [34], MM-Vet [56], SEED [21], MMMU [57], and POPE [27]. Comparison Method. We compare LLaVA-v1.5 and LLaVA-NeXT models trained on DCE-1M against current SOTA MLLMs. Additionally, we evaluate the performance of various MLLMs in generating image captions, using the advanced models InternVL2-26B [11] and LLaVA-NeXT-34B [24] to generate competitive captions on the DCE-118K images.",
            "score": 0.39112318513457534,
            "section_title": "implementation details",
            "char_start_offset": 21601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1619
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 976,
                    "end": 980,
                    "matchedPaperCorpusId": "8081284"
                },
                {
                    "start": 989,
                    "end": 993,
                    "matchedPaperCorpusId": "220280200"
                },
                {
                    "start": 1001,
                    "end": 1005,
                    "matchedPaperCorpusId": "249375629"
                },
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 1025,
                    "end": 1029,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 1041,
                    "end": 1045,
                    "matchedPaperCorpusId": "252383606"
                },
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "2682274"
                },
                {
                    "start": 1275,
                    "end": 1279,
                    "matchedPaperCorpusId": "265466525"
                },
                {
                    "start": 1290,
                    "end": 1294,
                    "matchedPaperCorpusId": "258740697"
                },
                {
                    "start": 1534,
                    "end": 1538,
                    "matchedPaperCorpusId": "266521410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38623046875
        },
        {
            "corpus_id": "274422305",
            "title": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks",
            "text": "Model Variability and Capabilities: We selected both generic and geospatial-specific VLMs, prioritizing recently developed models with advanced capabilities across various tasks. Generic VLMs include LLaVA-1.5 [30], LLaVA-NeXT [29], LLaVA-OneVision [23], Sphinx [27], Ferret [62], InternVL2 [9], and Qwen2-VL [54]. Also, we added GPT-4o, a closed-source, commercially available model, to compare its adaptability to remote sensing tasks with open-source models. These VLMs demonstrate robust performance in tasks such as scene understanding, and finegrained visual classification. For geospatial-specific VLMs, we selected GeoChat [19], RS-LLaVA [5], SkySenseGPT [34], EarthDial [47], and LHRS-Bot-Nova [36], models tailored for satellite and aerial image interpretation. Domain Specific Model Suitability: We consider VLMs based on their relevance to geospatial tasks. Despite being trained on satellite and aerial data, domain-specific models [5,19,34,36,47] may struggle with counting and spatial relationships due to dataset or architecture constraints. Meanwhile, we evaluate whether generic VLMs can effectively handle scene understanding, object detection, and visual reasoning in remote sensing. This selection ensures a focused study of VLM performance across diverse geospatial applications. Open vs Closed Models: Open-source models like LLaVA [23,29,30], Qwen2-VL [54], GeoChat [19], and others provide transparency, aiding in understanding their strengths and limitations. Closed models like GPT-4o, despite their lack of transparency, demonstrate strong generalization and performance on complex tasks, benefiting from proprietary datasets and advanced architectures. Evaluating both ensures a comprehensive assessment of VLM capabilities.",
            "score": 0.39065471190233814,
            "section_title": "Selection of VLMs",
            "char_start_offset": 11741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1753
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "265456719"
                },
                {
                    "start": 646,
                    "end": 649,
                    "matchedPaperCorpusId": "269354784"
                },
                {
                    "start": 703,
                    "end": 707,
                    "matchedPaperCorpusId": "267412588"
                },
                {
                    "start": 945,
                    "end": 948,
                    "matchedPaperCorpusId": "269354784"
                },
                {
                    "start": 948,
                    "end": 951,
                    "matchedPaperCorpusId": "265456719"
                },
                {
                    "start": 954,
                    "end": 957,
                    "matchedPaperCorpusId": "267412588"
                },
                {
                    "start": 1362,
                    "end": 1365,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1390,
                    "end": 1394,
                    "matchedPaperCorpusId": "265456719"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.324951171875
        },
        {
            "corpus_id": "277321635",
            "title": "Beyond Intermediate States: Explaining Visual Redundancy through Language",
            "text": "During inference, visual tokens that exhibit higher similarity to these prototypes are deemed more likely to be redundant and are removed before sending to the LLM. \n\nWe evaluate the effectiveness of our approach on five single-image Visual Question Answering (VQA) benchmarks [10,18,23,28,40] and two image captioning benchmarks [1,49]. On average, our method preserves 90% of the performance of LLaVA-Next [19,26] and LLaVA-1.5 [24,25] while pruning 90% of visual tokens, outperforming representative methods [4,31] that rely on MLLMs' intermediate states. Furthermore, our approach is adaptable to multiimage and video comprehension tasks [15,21,39], achieving up to a 10% performance improvement for LLaVA-OneVision [20] while pruning 80% of visual tokens. These results validate the effectiveness of our approach",
            "score": 0.3903976402105427,
            "section_title": "Semantic interpretation of individual visual tokens from the MLLM's own language decoder",
            "char_start_offset": 4470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 167,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 817
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 287,
                    "matchedPaperCorpusId": "258740697"
                },
                {
                    "start": 287,
                    "end": 290,
                    "matchedPaperCorpusId": "259837088"
                },
                {
                    "start": 330,
                    "end": 333,
                    "matchedPaperCorpusId": "56517630"
                },
                {
                    "start": 430,
                    "end": 434,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 434,
                    "end": 437,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 511,
                    "end": 514,
                    "matchedPaperCorpusId": "268358224"
                },
                {
                    "start": 646,
                    "end": 649,
                    "matchedPaperCorpusId": "265466214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3564453125
        },
        {
            "corpus_id": "268248921",
            "title": "Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models",
            "text": "Similarly, \"ViT+Resampler\" and \"ViT+Pooling+MLP\" also sacrifice performance for efficiency. Overall, these comparisons further confirm the designs of MRA. \n\nDespite effectiveness, the expenditure of LLaVA-HR is also cost-effective. In particular, increasing resolution from 384 However, these costs are reduced to only 17.6% and 20.8% in LLaVA-HR. Despite better performance, the training and inference speeds of LLaVA-HR are three times faster than LLaVA-1.5. Besides, the costs of GPU memory also remain cheap for LLaVA-HR. For example, adapting the resolution of 1,536 \u00d7 1,536 for LLaVA-HR only consumes 52G GPU memory, but the same settings for LLaVA-1.5 will cause GPU memory overflow. These results greatly confirm the efficiency of our MRA and LLaVA-HR. \n\nAblation studies. In Tab. 3, we conduct comprehensive ablation studies for MRA on four VL benchmarks. Firstly, we validate the different designs of the dual visual pathways. \n\nFrom these results, we find that removing one pathway will lead to significant performance drops, e.g., -1.5% on VQAv2. Besides, scaling up the high-resolution encoder brings more gains than that of the low-resolution one, e.g., +2.1% vs. +0.9% on TextVQA. We assume that the stronger high-resolution image encoder can better capture the finegrained visual information. Then, we ablate different fusion directions and strategies in MRA. Specifically, changing the fusion direction obviously degenerates the performance, e.g., Comparison with existing MLLMs. In Tab. 4 -5, we compare LLaVA-HR with existing MLLMs on 11 VL tasks. \n\nOn the four MLLM benchmarks, we observe comprehensive advantages of LLaVA-HR against existing MLLMs. \n\nIn particular, LLaVA-HR achieves 1554.9 scores in MME benchmark, outperforming LLaVA-1.5 by +23.6.",
            "score": 0.39029293973020945,
            "section_title": "QUANTITATIVE ANALYSIS",
            "char_start_offset": 18709,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 154
                },
                {
                    "start": 157,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1566
                },
                {
                    "start": 1569,
                    "end": 1669
                },
                {
                    "start": 1672,
                    "end": 1770
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.152587890625
        },
        {
            "corpus_id": "277452180",
            "title": "When 'YES' Meets 'BUT': Can Large Models Comprehend Contradictory Humor Through Comparative Reasoning?",
            "text": "The underlying philosophy selection and title matching tasks require sophisticated reasoning based on comic narratives. Our results show that for the underlying symbolism selection task, GPT-4o achieves the highest accuracy (80.38%), while for the title matching task, Qwen2-VL-72B demonstrates the highest accuracy of 81.25%. \n\nA key finding is that larger models generally demonstrate better comic comprehension capabilities, aligning with previous research indicating enhanced reasoning abilities in models with higher parameter counts. Comparing across the LLaVA model series with equivalent parameter counts, LLaVA-Next models consistently outperform LLaVA-1.5 models, while LLaVA-OneVision models generally surpass LLaVA-Next models. This performance hierarchy can be attributed to the progressive improvements in each model series: LLaVA-Next enhances reasoning abilities and world knowledge [59], while LLaVA-OneVision further incorporates supervised fine-tuning on multi-image and video inputs, strengthening its capacity to understand relationships and changes across multiple images. These findings suggest a promising direction for improving VLMs' understanding of juxtaposed humorous comics: enhancing models' reasoning and multi-image relationship comprehension through targeted fine-tuning. \n\nAdditionally, we observe that LLMs consistently underperform compared to VLMs of equivalent scale. This performance gap can be attributed to the LLMs receiving literal descriptions generated by LLaVA-Next-13B as input, thus inheriting any errors, information loss, or hallucinations present in the VLM-generated descriptions. We provide a more detailed analysis of description quality's impact on LLM performance in Section 6.1.2. \n\nAnother notable observation is that most models perform worse on title matching than on underlying symbolism selection, with Qwen2-VL-72B being the sole exception. This discrepancy likely stems from titles being shorter and more abstract representations of narratives that do not explicitly convey the underlying comic concepts. Consequently, distinguishing between correct titles and distractors requires deeper, more rigorous understanding and reasoning capabilities, presenting a more significant challenge for most models.",
            "score": 0.3898420527871496,
            "section_title": "Deep Reasoning Tasks",
            "char_start_offset": 32283,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 326
                },
                {
                    "start": 329,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2267
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35986328125
        },
        {
            "corpus_id": "274437775",
            "title": "PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos",
            "text": "Evaluation Settings. We benchmark PhysGame on 8 proprietary multi-modal LLMs, i.e., Claude3.5-Sonnet [4], Claude3.5-SonnetV2 [4], Gemini-1.5-pro [114], Gemini-1.5-pro-flash [114], GPT-4V [1], GPT-4o-0806 [92], GPT-4o-mini-0718 [92] and Qwen-VL-max [6], as well as 8 open-source models including LLaVA-Next-Video [72], Video-LLaVA [68], LLaVA-OneVision [58], InternVL2 [29], VideoChat2 [64], ST-LLM [77], Chat-UniVi [54] and PPLLaVA [78]. We follow Video-MME [38] to utilize the official frame configurations provided for each video LLM. We employ accuracy as the evaluation metric for our curated multi-choice questions. The evaluation prompt is   [83]. Methods marked by * use DPO or PPO [104]. CI, DO, CU, TU, and CO respectively denote correctness of information, detail orientation, contextual understanding, temporal understanding, and consistency. AVG is the average result. proprietary model GPT-4o by an absolute margin of 3.4% in the metric of average accuracy.",
            "score": 0.38887011911853686,
            "section_title": "Evaluations on PhysGame",
            "char_start_offset": 20955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 970
                }
            ],
            "ref_mentions": [
                {
                    "start": 385,
                    "end": 389,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 415,
                    "end": 419,
                    "matchedPaperCorpusId": "265157455"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.012481689453125
        },
        {
            "corpus_id": "276928973",
            "title": "Painting with Words: Elevating Detailed Image Captioning with Benchmark and Alignment Learning",
            "text": "Figure 4: Qualitative results of FEEDQUILL-7B compared with LLaVA-Onevision-7B (Li et al., 2024a) in terms of image captioning. \n\nWe provide qualitative results of LLaVA-Onevision-7B and FEEDQUILL-7B in Figure 4 to illustrate the effectiveness of our proposed method. In the example above, LLaVA-Onevision-7B incorrectly identifies the red wine in the glasses as a vibrant screen. In contrast, our model correctly identifies it as a red liquid with fewer instances of hallucination. Additionally, while LLaVA-Onevision-7B generically names both phone as \"cell phone\", FEEDQUILL-7B specifically identifies them as a Blackberry device and a flip phone, showcasing its strong fine-grained captioning capabilities. We refer readers to the Appendix for more qualitative results.",
            "score": 0.38883332462014386,
            "section_title": "Wrong Elements (Hallucinations) Correct Elements Additional Correct Elements",
            "char_start_offset": 30942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 130,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 773
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06646728515625
        },
        {
            "corpus_id": "273507658",
            "title": "VipAct: Visual-Perception Enhancement via Specialized VLM Agent Collaboration and Tool-use",
            "text": "In addition to the GPT-4o model used in our main experiments, we also evaluate other VLMs on our tasks. Specifically, we explore LLaVA-OneVision-7B (Li et al., 2024a), the latest opensource model in the LLaVA series, which supports multiple images as inputs and demonstrates superior performance compared to other open-source models of similar size. However, when applying VIPACT's prompt (described in Section H) to LLaVA-OneVision-7B, we found that it fails to follow key instructions, such as adhering to the required format, making it unable to extract the tool-use indicators necessary for integrating external tools. Additionally, we empirically observe that LLaVA-OneVision-7B frequently generates image captions, even when no such instruction is provided in the prompt. This suggests that current open-source SOTA VLMs struggle with complex instruction-following and are biased towards image captioning or description tasks, potentially due to imbalanced pretraining data formats. \n\nWe also evaluate LLaVA-OneVision-7B using the prompting baselines, with results on the Blink benchmark shown in Table 7. We observe that across all prompts, the model only achieves aboverandom accuracy on tasks such as object counting, where it counts the number of objects in an image, and spatial relations, where it determines spatial relationships between objects in a scene. These tasks are more typical of standard VQA problems frequently found in previous VQA datasets (Li et al., 2024b;Bansal et al., 2020). In contrast, for other tasks, the model's performance is near or below random, and we observe significant positional biases (Zhang et al., 2024f;Shi et al., 2024), with the model consistently predicting the first option for nearly all data points in a given task.",
            "score": 0.38855424267895006,
            "section_title": "C EXPLORATION OF DIFFERENT VLMS",
            "char_start_offset": 39067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1770
                }
            ],
            "ref_mentions": [
                {
                    "start": 1485,
                    "end": 1505,
                    "matchedPaperCorpusId": "221340919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.289306640625
        },
        {
            "corpus_id": "277452282",
            "title": "XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?",
            "text": "The MLLMs evaluated on XLRS-Bench are grouped into three categories: (a) open-source VLMs, including Qwen2-VL [63], LLava-Onevision [27], LLava-Next [32], LLaVA-1.5 [58], CogVLM2 [18], InternLM-XComposer-2.5 [73] and InternVL-2 [64]; (b) closed-source VLMs, such as GPT-4o [42] and GPT-4o-mini [43]; and (c) the specialized RS model Geochat [23]. For fair comparison, we used a zero-shot setting with uniform prompts for all VLMs. The appendix details the architecture and parameter sizes of each open-source VLM and includes additional results across various settings. Except for GeoChat which was evaluated using its native framework, all other models were evaluated using LMMs-Eval [26,71]. More results are shown in the Appendix.",
            "score": 0.38801877269560925,
            "section_title": "Experimental Setup",
            "char_start_offset": 23828,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 733
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 345,
                    "matchedPaperCorpusId": "265456719"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06805419921875
        },
        {
            "corpus_id": "274822932",
            "title": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence",
            "text": "CHAIR. Table 1 presents the performance of VHR in comparison to all baseline approaches on the CHAIR benchmark. The results for OPERA on LLaVA-NeXT are absent due to its excessive memory requirements. VHR demonstrates robust performance across all three LVLMs, achieving reductions of up to 16.36 in CHAIR S and 4.61 in CHAIR I on LLaVA-1.5. Notably, with increased image resolution and enhanced model capabilities, LLaVA-NeXT already exhibits a significant reduction in hallucinations compared to other base LVLMs, but VHR continues to exhibit notable effectiveness in mitigating its hallucinations. Moreover, VHR consistently outperforms baseline methods with greater stability, requiring only minor trade-offs in the length or richness of the generated description. POPE. As shown in Table 2, VHR outperforms all decoding methods considering all LVLMs. Additionally, VHR continues to yield further improvements with the most advanced models. This suggests that VHR enhances models across various performance levels, positioning it as an efficient and effective training-free decoding strategy. \n\nLLaVA-Bench. The GPT-4V evaluation results on LLaVA-Bench are presented in Table 3. These results highlight that VHR can also improve the model's accuracy on highly diverse and challenging tasks, while preserving a consistent level of detailedness.",
            "score": 0.38689645627723424,
            "section_title": "Results",
            "char_start_offset": 17681,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 7,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1347
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259765625
        },
        {
            "corpus_id": "276903707",
            "title": "Does Acceleration Cause Hidden Instability in Vision Language Models? Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study",
            "text": "In order to evaluate the generalization capability of LoFi, we test whether it addresses the consistency issue across different model architectures. We apply LoFi to the LLaVA-Next model using a representative setting of \u03b2 = 4. As shown in Table 4, LoFi achieves approximately 85% consistency on GQA and 86% on VQAv2 while preserving over 96% accuracy. Notably, compared to LLaVA-1.5 under the same hyper-parameter setting, LoFi reduces computational cost by about 70% in FLOPs. This result indicates that vision tokens in LLaVA-Next are more redundant than those in LLaVA-1.5, and LoFi effectively adapts by dynamically adjusting the number of vision tokens.",
            "score": 0.38652324022220313,
            "section_title": "Applying LoFi to LLaVA-NEXT",
            "char_start_offset": 26588,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 659
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1842041015625
        },
        {
            "corpus_id": "276776495",
            "title": "MedHEval: Benchmarking Hallucinations and Mitigation Strategies in Medical Large Vision-Language Models",
            "text": "We apply mitigation methods to LLaVA-NeXT 7B and 13B, with results presented in Table 10. Similar to findings on LLaVA-Med and LLaVA-Med-1.5, PAI, which performs well in close-ended evaluations, fails to improve performance in open-ended tasks. Certain methods, such as DAMRO on LLaVA-NeXT 7B, reduce CHAIR but at the expense of recall, highlighting the trade-off between hallucination mitigation and report completeness.",
            "score": 0.3864836416607705,
            "section_title": "C.5 Mitigation Results on LLaVA-NeXT",
            "char_start_offset": 38078,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 421
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08343505859375
        },
        {
            "corpus_id": "274306239",
            "title": "Cross-modal Information Flow in Multimodal Large Language Models",
            "text": "We investigate the current state-of-the-art and open-source multimodal large language models from the LLaVA series: LLaVA-1.5-7b, LLaVA-1.5-13b [27], \n\nLLaVA-v1.6-Vicuna-7b [28] and Llama3-LLaVA-NEXT-8b [2], which achieve state-of-the-art performance across a diverse range of 11 tasks including GQA. These models are trained on similar publicly available data but with different architectures and model sizes, which allows us to explore cross-modal interaction and processing over different architectures and minimize interference of unknown factors from training data. All these models have the same image encoder (CLIP-ViT-L-336px [35]) but with different LLM: Vicuna-v1.5-7b [47] with 32 layers (transformer blocks) in LLaVA-1.5-7b and LLaVA-v1.6-Vicuna-7b, Vicuna-v1.5-13b [47] with 40 layers in LLaVA-1.5-13b and Llama3-8b [14] with 32 layers in Llama3-LLaVA-NEXT-8b, where Vicuna-v1.5 is the standard and dense transformer architecture [41] and Llama3 adopts grouped query attention [4]. \n\nIn terms of image processing, LLaVA-1.5-7b and LLaVA-1.5-13b directly feed the original fixed-length image patch features from the image encoder into the LLM as input tokens. In contrast, LLaVA-v1.6-Vicuna-7b and Llama3-LLaVA-NEXT-8b employ a dynamic high-resolution technique, which dynamically adjusts image resolution, resulting in variable-length image patch features with higher resolution. Due to space limitations, we will primarily present the results for the model LLaVA-1.5-13b in the subsequent sections of this paper, while similar findings for other models are presented in Appendix E.",
            "score": 0.3859717803597237,
            "section_title": "Models",
            "char_start_offset": 16408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 149
                },
                {
                    "start": 152,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 994
                },
                {
                    "start": 997,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1595
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 634,
                    "end": 638,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 943,
                    "end": 947,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3896484375
        },
        {
            "corpus_id": "277781774",
            "title": "TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language Models",
            "text": "Experimental Setups We conduct additional model pruning experiments on LLaVA-OneVision (Li et al., 2024c) with 7B parameters, which processes both interleaved images and video modalities. After pruning, we evaluate its zero-shot performance in the two modalities, following the evaluation protocols in LLaVA-OneVision: 1) interleaved images: Muirbench (Wang et al., 2024b) for diverse multi-image tasks, Mantis (Jiang et al., 2024) for reasoning over multiple images, BLINK (Fu et al., 2024) for multi-image visual perception tasks, and Text-rich VQA (Liu et al., 2023) for multi-image text recognition; 2) video: VideoMME and NeXTQA-MC for diverse video domains and durations, EgoSchema for long video understanding, and MVBench for spatio-temporal understanding. \n\nExperimental Results Table 6 summarizes performance of LLaVA-OneVision at a 50% sparsity ratio. Across 6 out of 8 interleaved images understanding and video benchmarks, TAMP ranks either first or second. On average, TAMP surpasses the Wanda and the strongest baseline by 5.3 pp and 0.3 pp, respectively, in relative performance. These results demonstrate that the effectiveness of our method can be transferred to the pruning of other recent MLLMs with different multimodal settings, further supporting the universality of our approach. \n\nC In-Depth Analysis on Layer-wise Sparsity Ratios \n\nC.1 Sparsity of Projection Layer Type \n\nIn Figure 3, we observe significant variations in intra-and inter-modality diversities across different projection layer types and leverage these variations to estimate layer importance. In this ablation study, we examine the sparsity results of different projection layer types in MLLMs. Figure 7 presents the average sparsity ratios across blocks for each projection layer type in VideoLLaMA2 pruned at 70% sparsity using TAMP. Our analysis reveals that in the MHA module, comprising query, key, value, and output projection layers, the value projection layer consistently exhibits the lowest sparsity ratio.",
            "score": 0.38591585438225906,
            "section_title": "B Experiments on LLaVA-OneVision",
            "char_start_offset": 29273,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1303
                },
                {
                    "start": 1306,
                    "end": 1355
                },
                {
                    "start": 1358,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 411,
                    "end": 431,
                    "matchedPaperCorpusId": "265466525"
                },
                {
                    "start": 474,
                    "end": 491,
                    "matchedPaperCorpusId": "269214091"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3388671875
        },
        {
            "corpus_id": "276903457",
            "title": "CASP: Compression of Large Multimodal Models Based on Attention Sparsity",
            "text": "Models. For the experiments on image-language benchmarks, we use three different LMMs namely LLaVA1.5-7B2 [27], LLaVA1.5-13B 3 , and LLaVA-Next-7B 4 (i.e., LLaVA1.6-7B) [26]. We also use LLaVA-Next-Video-7B5 [56] for the video-language experiments. All the abovementioned models use Llama2-7B [41] or 13B as their underlying LLM (depending on the model size) and CLIP [39] as their vision encoder. LLaVA1.5 and LLaVA-Next encode the input image to 576 and a dynamic number of visual tokens, respectively, while LLaVA-Next-Video uses 144 visual tokens for each frame in the video. Metrics and Benchmarks. Our main evaluation metric is perplexity (PPL), which is the metric commonly used for quantization methods in the literature [9,24,42]. PPL is defined as the exponentiation of the average negative log-likelihood of a sequence of tokens [20]. We further evaluate CASP on different downstream tasks that use their specific metrics. Except for the PPL results, all other results on downstream datasets are obtained using the LMMs-eval framework6 [55]. In order to measure PPL, we use LiveBench (LiveB) [47] and LLaVA-Bench-Wilder (LWilder) [20] as open-ended QA datasets and LLaVA-Bench-COCO (LCOCO) [20] as an imagecaptioning dataset. For downstream task performance analysis, multi-choice QA benchmarks such as SEED-Bench [21], MMU [54], ScienceQA (SQA) [30], MME [10], and MMBench [28] benchmarks are used.",
            "score": 0.3856085244500042,
            "section_title": "Experimental Setting",
            "char_start_offset": 15291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 8,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1410
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 368,
                    "end": 372,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 732,
                    "end": 735,
                    "matchedPaperCorpusId": "271271084"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "121680873"
                },
                {
                    "start": 1141,
                    "end": 1145,
                    "matchedPaperCorpusId": "121680873"
                },
                {
                    "start": 1201,
                    "end": 1205,
                    "matchedPaperCorpusId": "121680873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.218505859375
        },
        {
            "corpus_id": "270521771",
            "title": "Long Story Short: Story-level Video Understanding from 20K Short Films",
            "text": "In this experiment, we fine-tune Llava-OneVision-0.5B [32] on SF20K-Train (Section 3.2), using LoRA for efficiency. \n\nThe input includes video frames, subtitles, and the question. We use next-token prediction to generate responses. In the multiple-choice case, we add multiple-choice options to the query and predict the correct option index in addition to the answer. Fine-tuning is performed over 191,007 samples",
            "score": 0.3853090551405677,
            "section_title": "Instruction tuning",
            "char_start_offset": 24393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 118,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 414
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.075439453125
        },
        {
            "corpus_id": "273403785",
            "title": "debiaSAE: Benchmarking and Mitigating Vision-Language Model Bias",
            "text": "We evaluate the performance and biases of various VLMs across different sizes and architectures, including both open and closed-source models. For the open-source models, we assess the LLaVa v1.6 series and the PaliGemma-2 series, specifically: llava-v1. 6-34b-hf, llava-v1.6-vicuna-7b, llava-v1.6-mistral-7b-hf Liu et al. (2023), google/paligemma2-10b-pt-224, and google/paligemma2-10b-pt-448 Steiner et al. (2024). Additionally, we evaluate two CLIP models, CLIP L-224 and CLIP L-336, with the latter serving as the Vision Transformer (ViT) component for all the LLaVa models mentioned. For the closed-source models, we include Google's May 2024 release: gemini-1.5-flash-001 Radford et al. (2021); Goldin et al. (2024). For reproducibility, we set the temperature to 0 in all cases. More details about hardware and API versioning can be found in Appendix A.3",
            "score": 0.3852936693308652,
            "section_title": "Models",
            "char_start_offset": 7664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 861
                }
            ],
            "ref_mentions": [
                {
                    "start": 678,
                    "end": 699,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0567626953125
        },
        {
            "corpus_id": "277313523",
            "title": "PAVE: Patching and Adapting Video Large Language Models",
            "text": "Video large language models. Recent advances in instruction tuning with visual and text data [38][39][40] have led to a surge of interest in developing Video LLMs. Many of these models share a common design, where visual features are extracted using a pre-trained visual encoder, projected into the text latent space of an LLM, and subsequently processed by the pre-trained LLM to generate responses. Video-ChatGPT [45] introduces instruction tuning into the video domain. Video-LLaVA [36] improves model performance with better text-aligned video features [83], while VideoChat2 [33] resorts to increasing the quality and quantity of the video instruction tuning set. Recent vision-LLM models like LLaVA-NeXT [39], LLaVA-OneVision [28], LLaVA-Video [79], Qwen2-VL [60], and mPlug-Owl3 [71] consider multi-stage training with both video and image, which substantially improves the model performance. Recent works in VideoLLM focus on long video understanding. [12,25,56,64,65,76] propose to use Q-former [32] or text-query-based cross-attention to compress vision tokens, while others [47,63] resort to state-space models [18]. \n\nResearchers also extend the instructional tuning into different video sub-domains. For instance, CAT [72] focuses on audio-visual understanding, while Scene-LLM [15] and LLaVA-3D [84] address 3D QA tasks. Built on these developments, our work specifically focuses on adapting pre-trained Video LLMs to downstream tasks with side-channel signals, aiming to significantly extend the capabilities of these models. \n\nAdaptation of vision foundation models. Adapting vision foundation models to downstream tasks has received significant attention. Prior works have studied learning lightweight adapters [4,6,8,35,52,58], prepending learnable input tokens (e.g. prompts) [24,82], or in-context learning [3,62,68,78]. Recently, adapter-and promptbased methods have been explored for Video LLMs.",
            "score": 0.38508348408171966,
            "section_title": "Related Works",
            "char_start_offset": 5040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1540
                },
                {
                    "start": 1543,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1917
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 97,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 101,
                    "end": 105,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 415,
                    "end": 419,
                    "matchedPaperCorpusId": "259108333"
                },
                {
                    "start": 485,
                    "end": 489,
                    "matchedPaperCorpusId": "265281544"
                },
                {
                    "start": 557,
                    "end": 561,
                    "matchedPaperCorpusId": "263608698"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 964,
                    "end": 967,
                    "matchedPaperCorpusId": "266362624"
                },
                {
                    "start": 973,
                    "end": 976,
                    "matchedPaperCorpusId": "268889590"
                },
                {
                    "start": 1004,
                    "end": 1008,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1085,
                    "end": 1089,
                    "matchedPaperCorpusId": "270123263"
                },
                {
                    "start": 1231,
                    "end": 1235,
                    "matchedPaperCorpusId": "268264651"
                },
                {
                    "start": 1728,
                    "end": 1731,
                    "matchedPaperCorpusId": "261100811"
                },
                {
                    "start": 1731,
                    "end": 1733,
                    "matchedPaperCorpusId": "249097890"
                },
                {
                    "start": 1733,
                    "end": 1735,
                    "matchedPaperCorpusId": "248834106"
                },
                {
                    "start": 1735,
                    "end": 1738,
                    "matchedPaperCorpusId": "252918870"
                },
                {
                    "start": 1741,
                    "end": 1744,
                    "matchedPaperCorpusId": "245123737"
                },
                {
                    "start": 1795,
                    "end": 1799,
                    "matchedPaperCorpusId": "247618727"
                },
                {
                    "start": 1799,
                    "end": 1802,
                    "matchedPaperCorpusId": "237386023"
                },
                {
                    "start": 1830,
                    "end": 1833,
                    "matchedPaperCorpusId": "254246343"
                },
                {
                    "start": 1833,
                    "end": 1836,
                    "matchedPaperCorpusId": "270045544"
                },
                {
                    "start": 1836,
                    "end": 1839,
                    "matchedPaperCorpusId": "256416477"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57421875
        },
        {
            "corpus_id": "270559556",
            "title": "VideoVista: A Versatile Benchmark for Video Understanding and Reasoning",
            "text": "In this section, we evaluate latest open-source Video-LLMs, including Video-LLaMA [54], Video-ChatGPT [31], IVA [19], Video-LLaVA [22], LLaMA-VID [17], LLaVA-NeXT-Video [23], VideoChat2 [16], as well as commercial models GPT-4o [34] and Gemini-1.5-Flash[40].Detailed introductions of these models are shown in the Appendix \u00a7A.4.",
            "score": 0.3845213408689891,
            "section_title": "Experiments",
            "char_start_offset": 11530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 258
                },
                {
                    "start": 258,
                    "end": 328
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02069091796875
        },
        {
            "corpus_id": "273346082",
            "title": "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment",
            "text": "The response is highly faithful, accurately reflecting the image content. \n\nTable 8: Visual faithfulness assessment annotation guideline for GPT-4V model. \n\nLLaVA models. Specifically, we adopt LLaVA-Next (Liu et al., 2024a) with two models. Following the original implementation, the DPO is performed on our VLFeedback dataset with a learning rate of 1e-6 for one epoch. As shown in Table 12, the performance is boosted on 5 out of 6 benchmarks, showcasing the effectiveness and generalizability of our VLFeedback dataset. The minor performance drop of LLaVA-Next on MMMU (37.0 to 36.9 with LLaVA-Next-Mistral-7B and 34.0 to 33.1 for LLaVA-Next-Vicuna-7B) can be attributed to the unique nature of this benchmark,  where LLM knowledge plays a crucial role on this benchmark (Tong et al., 2024). Differences in base LLMs could be the cause of this slight degradation.",
            "score": 0.38429079868405436,
            "section_title": "Highly Faithful:",
            "char_start_offset": 29680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 76,
                    "end": 154
                },
                {
                    "start": 157,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 867
                }
            ],
            "ref_mentions": [
                {
                    "start": 775,
                    "end": 794,
                    "matchedPaperCorpusId": "270703300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1251220703125
        },
        {
            "corpus_id": "270258348",
            "title": "A-Bench: Are LMMs Masters at Evaluating AI-generated Images?",
            "text": "To ensure the results are comprehensive and up-to-date, we select the latest and widely used LMMs for benchmarking.The Proprietary LMMs (closed-source) include Gemini 1.5 Pro [Reid et al., 2024], GPT-4v [OpenAI, 2023], GPT-4o [OpenAI, 2024], and Qwen-VL-Max [Bai et al., 2023].The Open-source LMMs include CogVLM2-19B (Llama3-8B) [Wang et al., 2023], IDEFICS-2 (Mistral-7B-Instruct-v0.2) [Huggingface, 2023], DeepSeek-VL-7B [Lu et al., 2024], InternLM-XComposer2-VL [Dong et al., 2024], LLaVA-NeXT (Llama3-8B), LLaVA-NeXT (Qwen-72B), LLaVA-NeXT (Qwen-110B) [Liu et al., 2024], mPLUG-Owl2 (LLaMA-7B) [Ye et al., 2023], LLaVA-v1.5 (Vicuna-v1.5-7B),LLaVA-v1.5 (Vicuna-v1.5-13B)[Liu et al., 2023b], CogVLM-17B (Vicuna-v1.5-7B)[Wang et al., 2023], Qwen-VL (Qwen-7B) [Bai et al., 2023], BakLLava (Mistral-7B) [Liu et al., 2023a], and Fuyu-8B (Persimmon-8B) [Adept, 2023].All LMMs are tested with zero-shot setting.\n\nFor human performance on A-Bench, we conduct a user-study experiment with five ordinary people in a controlled laboratory setting.Initially, participants familiarize themselves with the tasks through exposure to similar cases.",
            "score": 0.3839535971526351,
            "section_title": "Benchmark Candidates",
            "char_start_offset": 15491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 115,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 865
                },
                {
                    "start": 865,
                    "end": 908
                },
                {
                    "start": 910,
                    "end": 1040
                },
                {
                    "start": 1040,
                    "end": 1136
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02276611328125
        },
        {
            "corpus_id": "276725304",
            "title": "MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts",
            "text": "The progress in visual-language alignment and rapid evolution of LLMs have endowed these models with visual capabilities, with notable achievements including CLIP [35] and BLIP [19]. CLIP aligns image-text pairs via contrastive learning, while BLIP combines contrastive and generative learning to align and generate image-text pairs. Inspired by these, MiniGPT-4 [53], LLaMA-Adapter [49], and LLaVA [24] extend text-only models into multi-modal ones through vision-language alignment and instruction tuning. Subsequently, large multimodal models emerged, such as the closed-source GPT-4V [30], Gemini [37], Claude [2], GPT-4o [32] and open-source DeepSeek-VL [27], Qwen-VL [3], InternLMXComposer-VL [10], LLaVA-NeXT [23], and LLaVA-OneVision [17], which perform well on benchmarks like MMMU [46] and MathVista [28]. In mathematics, G-LLaVA [13] and Math-LLaVA [36] enhance performance on mathematical tasks using large instruction datasets Geo170K [13] and MathV360K [36]. MAVIS [51] improves visual perception with three-stage fine-tuning, while MultiMath [33] refines visual-language alignment through a four-stage training process.",
            "score": 0.3833045945250191,
            "section_title": "Multimodal Foundation Models",
            "char_start_offset": 7856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1134
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 167,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 177,
                    "end": 181,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 363,
                    "end": 367,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 383,
                    "end": 387,
                    "matchedPaperCorpusId": "271745829"
                },
                {
                    "start": 791,
                    "end": 795,
                    "matchedPaperCorpusId": "265466525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.312255859375
        },
        {
            "corpus_id": "275119020",
            "title": "MBQ: Modality-Balanced Quantization for Large Vision-Language Models",
            "text": "The main idea of the proposed MBQ is to consider the sensitivity across different modalities during quantization, aiming to enhance performance in both vision-language and language-only tasks. Accordingly, we evaluated the performance of the quantized LLaVA-onevision-7B VLM on the MMLU benchmark with different quantization methods. \n\nAs shown in Tab. 7, with W3A16 and W4A8 quantization, MBQ achieves a performance improvement of 0.9% and 2%, compared to AWQ and SmoothQuant. This result demonstrates that for quantized VLMs, considering the sensitivity differences across modalities not only significantly reduces the performance loss on vision-language tasks, but also helps maintain performance on language-only tasks. with different weight matrix shapes in LLaVA-onevision-7B. Specifically, as shown in Tab. 8, we evaluate the kernel on 4 different shapes of weight matrices, including 3584 \u00d7 3584 (the out proj layers), 3584\u00d710752 (the qkv proj layers), 3584\u00d718944 (the up proj and gate proj layers), and 18944\u00d73584 (the down proj layers). For each shape, we compare the proposed fused W3A16 GEMV kernel with the original FP16 GEMV kernel.",
            "score": 0.3823583774987594,
            "section_title": "The Performance on Language-only Benchmark",
            "char_start_offset": 20589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 333
                },
                {
                    "start": 336,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1146
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.366455078125
        },
        {
            "corpus_id": "277781296",
            "title": "Mavors: Multi-granularity Video Representation for Multimodal Large Language Model",
            "text": "We conduct a series of ablation studies to validate our model design. Given the extensive training time required for the full training paradigm, these ablations utilize standard compositive datasets and train various versions up to the completion of Stage 2. Specifically, Stage 1 employs LLaVA-Pretrain-558K [53] and LLaVA-Hound-Pretrain [122]; Stage 1.5 uses M4-Instruct [44] and ShareGPT4o [16]; and Stage 2 utilizes LLaVA-OneVision and LLaVA-Video. This approach reduces the duration of a full training cycle to under 24 hours with 64 A800 GPUs. Performance is subsequently monitored using MMMU, MathVista, and CapsBench for image understanding capa- To ensure exposure to richer visual information, we finalize the F = 16 setting. We further evaluate these four model variants on six benchmark datasets in Figure 7. \n\nOn image-based tasks, we observe a marginal improvement in performance metrics with increasing F . We hypothesize that this improvement stems from the model's increased exposure to individual frames during video processing when F is larger, thereby enhancing its image understanding capabilities. Conversely, for video understanding tasks, performance degrades significantly for F = 4 due to insufficient temporal information and for F = 32, likely due to excessive information compression. \n\nEffect of the IFA Module. We establish two baseline models for comparison in Table 3. The first baseline completely removes the inter-chunk feature aggregator (L Inter =0), where the output from the IVE module is passed directly through a projector and then concatenated with the LLM's input sequence. In this setup, the integration of temporal and spatial information relies solely on the LLM. The second baseline utilizes only a single Transformer layer (L Inter =1) for the aggregator, thereby reducing its computational complexity. In",
            "score": 0.3817051337832382,
            "section_title": "Ablation Studies",
            "char_start_offset": 21958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1854
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281005859375
        },
        {
            "corpus_id": "277103933",
            "title": "Growing a Twig to Accelerate Large Vision-Language Models",
            "text": "Results on LLaVA-1.5 and LLaVA-NeXT. As shown in Tables 1 and 2, we take LLaVA-1.5-7B and LLaVA-NeXT-7B as the base VLMs and compare TwigVLM with the state-of-the-art VLM acceleration methods on six commonly-used VLM benchmarks. We can see that TwigVLM consistently and significantly outperforms all its counterparts under different pruning ratios. Notably, TwigVLM achieves near-perfect performance preservation (more than 99.0%) when pruning 66.7% visual tokens on LLaVA-1.5-7B and 77.8% tokens on LLaVA-NeXT-7B, showing the superiority of our TTP strategy over existing pruning strategies. Comparisons on more benchmarks and base VLMs are provided in the supplementary. \n\nTo better understand the effectiveness of our approach, we compare TwigVLM with two representative methods [8,52] by visualizing the attention map used for token selection. The examples in Figure 5 suggest that TwigVLM can better understand the fine-grained semantics in both the prompt and image, thus identifying more informative visual tokens for token pruning. \n\nResults on Video-LLaVA for video understanding. Our TwigVLM is general and can be extended to the challenging       Generation speed comparisons. As mentioned above, TwigVLM can effectively accelerate the generation. To validate this, we conduct intensive experiments on two typical benchmarks TextVQA [42] and MM-Vet [57] to compare the speedup of different VLM acceleration methods based on LLaVA-1.5-7B. The results in Figure 6 show that: (i) TwigVLM achieves superior or competitive speedup in all configurations, suggesting dual advantages in speed and accuracy. (ii) All the methods attain the similar Rel-Spd (120%\u223c130%) on TextVQA with short responses. In contrast, TwigVLM delivers significantly higher speedup (\u223c150%) than FastV (\u223c104%) and VisionZip (\u223c106%) on MM-Vet with long responses. This reveals the superiority of our SSD strategy in long response generation.",
            "score": 0.3810474195645923,
            "section_title": "Main Results",
            "char_start_offset": 18597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1919
                }
            ],
            "ref_mentions": [
                {
                    "start": 782,
                    "end": 785,
                    "matchedPaperCorpusId": "268358224"
                },
                {
                    "start": 1344,
                    "end": 1348,
                    "matchedPaperCorpusId": "85553602"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2958984375
        },
        {
            "corpus_id": "276724839",
            "title": "Fine-Grained Retrieval-Augmented Generation for Visual Question Answering",
            "text": "Our experiments were conducted on RTX 4090 GPUs. GPT-4o and Qwen 2.5-VL-32b use the base version of the API interface, while LlaVa conducts experiments using the Hugging Face transformers library 1 . The LLaVa NEXT-7b model used the weight file 'llava-v1.6-mistral-7b-hf'. In our method's settings, OVEN, INFO SEEK, and OK-VQA all use entities as the knowledge unit, while E-VQA uses events as the knowledge unit. For the recall of knowledge units and chunks, the top-k is set to 3. For the experiment evaluation, we used accuracy as the metric.",
            "score": 0.38078807142771554,
            "section_title": "Experimental Setting",
            "char_start_offset": 22813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 545
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07208251953125
        },
        {
            "corpus_id": "277113272",
            "title": "VisNumBench: Evaluating Number Sense of Multimodal Large Language Models",
            "text": "We evaluate 17 well-known MLLMs from 8 model families, including 13 open-source models: Phi-3.5vision [1], LLaVA-v1.5 (7B, 13B), and LLaVA-v1.6-34B [27], LLaVA-Onevision (7B, 72B) [23], Qwen2.5-VL (3B, 7B, 72B) [41], InternVL2.5 (8B, 38B, 78B) [5], and Janus-Pro-7B [4]. Additionally, we assess 4 proprietary models: GPT-4o [19], Gemini 1.5 Flash, Gemini 2.0 Flash, and Gemini 1.5 Pro [36]. \n\nWe randomly selected 600 samples (50 QA pairs from each category), with 300 sourced from VisNumBench-Synthetic and 300 from VisNumBench-Real. Human evaluators independently answered each question and provided assessments. Accuracy (%) is reported for all experimental results, and all the results are provided in Tables 2 and 3.",
            "score": 0.3805475683299088,
            "section_title": "Experiments",
            "char_start_offset": 11876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 721
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 184,
                    "matchedPaperCorpusId": "258999820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04559326171875
        },
        {
            "corpus_id": "277043430",
            "title": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models",
            "text": "Benchmarks. We evaluate our method on several widely used video understanding benchmarks: MVBench [16,25], LongVideoBench [36], MLVU [44], and VideoMME (wo sub.) [10] Table 1. Comparison of state-of-the-art methods across various benchmarks. We implement the baseline methods based on their official implementations. The performance of the vanilla versions of LLaVA-OneVision and LLaVA-Video differs slightly from their reported results but remains within the margin of error. The best performance at the same retention ratio R is highlighted in bold. \n\ninto short, medium, and long subsets. These benchmarks contain videos of varying durations and complex scenarios, providing a comprehensive evaluation of our method's effectiveness and generalization. \n\nImplementation Details. We apply our method to two representative Video LLMs: LLaVA-OneVision-7B [14] is a general Video LLM, while LLaVA-Video-7B [43] employs newline tokens based on positional information of video tokens. These models encompass different architectural characteristics, ensuring a robust evaluation of our approach. \n\nFor LLaVA-OneVision, 32 sampled frames generate a 32 \u00d7 196 token input to the LLM, while for LLaVA-Video, 64 sampled frames generate a 64 \u00d7 169 token input. Our method focuses on token compression at the LLM input stage without modifying the prefilling or decoding stages. Consequently, the retention ratio R% of input tokens di-rectly impacts inference speed and model complexity. Unless otherwise specified, we adopt the hyperparameter setting c = 8, \u03c4 = 0.9, d = 0.4, p = 4, \u03b2 = 0.6 for all experiments. We conduct all evaluations using LMMs-Eval [40]. \n\nCompared Baselines. We compare FastVID with strong baselines from both image and video compression methods. \n\nFor image compression, we adopt both the widely used classic method FastV [4] and the current state-of-the-art approach VisionZip [38]. FastV prunes tokens during the prefilling stage based on attention scores from query tokens. In contrast, VisionZip prunes tokens at the input stage.",
            "score": 0.380390687437558,
            "section_title": "Experimental Settings",
            "char_start_offset": 17344,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 551
                },
                {
                    "start": 554,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1090
                },
                {
                    "start": 1093,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1648
                },
                {
                    "start": 1651,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 102,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "15928602"
                },
                {
                    "start": 122,
                    "end": 126,
                    "matchedPaperCorpusId": "271329356"
                },
                {
                    "start": 1835,
                    "end": 1838,
                    "matchedPaperCorpusId": "268358224"
                },
                {
                    "start": 1891,
                    "end": 1895,
                    "matchedPaperCorpusId": "274514545"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28369140625
        },
        {
            "corpus_id": "275405970",
            "title": "Global Compression Commander: Plug-and-Play Inference Acceleration for High-Resolution Large Vision-Language Models",
            "text": "We conduct a comprehensive analysis of both theoretical and practical efficiency of our GlobalCom 2 with LLaVA-NeXT-7B/13B on a single NVIDIA A100-SXM4-80GB GPU, as shown in Table 8 and Table 9. We measure computational efficiency using TFLOPs and peak memory consumption directly, while other metrics are estimated using LLM-Viewer [49]. Given that the sequence length of visual tokens substantially exceeds that of textual and system tokens, we exclude the latter two from our theoretical anal- Table 9. Efficiency analysis of our GlobalCom 2 with LLaVA-NeXT-13B on one NVIDIA A100-SXM4-80GB GPU. \n\nysis. For practical efficiency, \"Throughout\" and the corresponding \"Performance\" measurements are conducted on the MME benchmark [12], which includes 2374 examples. \n\nAs shown in Table 8 and Table 9, GlobalCom 2 substantially improves the computational efficiency of LLaVA-NeXT models across both theoretical and practical metrics. Specifically, while preserving model performance, our method achieves significant reductions in GPU memory consumption and notable acceleration in inference speed compared to the original LLaVA-NeXT. Most importantly, these efficiency improvements are achieved through a training-free compression scheme, demonstrating the immediate practical value of GlobalCom 2 .",
            "score": 0.37971823913729924,
            "section_title": "Detailed Efficiency Analysis.",
            "char_start_offset": 34858,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1298
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255615234375
        },
        {
            "corpus_id": "276724729",
            "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models",
            "text": "MLLMs generally process multi-modal input information and generate language output. Kosmos (Huang et al., 2023) introduced an end-to-end framework that integrated visual inputs with LLM from cohesive training. Flamingo (Alayrac et al., 2022) and InstructBLIP (Dai et al., 2023) merged visual and linguistic features through crossattention and a Q-Former module, respectively. MiniGPT-4 (Zhu et al., 2024) and LLaVA (Liu et al., 2023) simplified the integration by linearly projecting visual features directly into the LLM embedding space. \n\nRecent studies focused on different aspects to enhance the above early attempts in MLLMs. Cosmos-2 (Peng et al., 2024) and NeXT-GPT (Wu et al., 2024) have expanded MLLM applications to broader multi-modal tasks. LLaVA-1.5 (Liu et al., 2024) explored adding high-quality multitask training data, and scaling up the resolution and LLM size to boost MLLM performance. LLaVA-OneVision (Li et al., 2024a) explored to unify dynamic image resolution, multi-image, and video into a unified input format.",
            "score": 0.3792031331441435,
            "section_title": "Multi-modal Large Language Models",
            "char_start_offset": 7702,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1036
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 241,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 673,
                    "end": 690,
                    "matchedPaperCorpusId": "261696650"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.591796875
        },
        {
            "corpus_id": "266053531",
            "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
            "text": "General VLM benchmarks. In Tab. 1, we evaluate our method on common comprehensive Vision-Language benchmarks, MME [27] and MMBench [26]. Prompt Highlighter on LLaVA-v1.5 demonstrates a consistent performance improvement compared to well-trained models by designating the entire image as the highlighted part in the input context. Notably, though these benchmarks primarily assess overall performance with single-token generation and are not designed for user interactions, we still get a competitive place in both MMBench and MME perception. Additionally, further benchmarking with different hyper-parameter selection validates our performance enhancements with LLaVA-v1.5 13B, as evidenced by gains on POPE (85.9 -87.8, +1.9), MMB CN (63.4 -64.0, +0.6), and SQA I (71.6 -72.4, +0.8). With LLaVA-NeXT-34B [45], we achieve a SOTA performance on MMB test with 81.3.",
            "score": 0.3791182941777508,
            "section_title": "Quantitative Evaluation",
            "char_start_offset": 24730,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 863
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.185302734375
        },
        {
            "corpus_id": "275405668",
            "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
            "text": "Currently, multimodal large language models can be categorized into community models and proprietary models. Proprietary models [3,[52][53][54]64] often achieve better performance but are not open-sourced. Meanwhile, community models [17,23,27,29,30,35,38,39,80,81,85,91], which have seen rapid performance improvements, are garnering increasing attention due to their open-source nature, including model architecture, weights, and even training data. LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously. \n\nBased on the ideas of LLaVA, several variant series have emerged, such as the mPLUG-owl series. mPLUG-owl [80] introduces a new paradigm for training large language models through modularity, and the latest version, mPLUG-owl3 [78], can even understand 2-hour movie videos. BLIP-2 [31] uses Q-Former [88] to connect the visual and linguistic modalities. In BLIP-3 [76], Q-Former is replaced by more scalable visual token samplers, such as perceptual resamplers. We observe that numerous methods have explored various visual projectors. However, to the best of our knowledge, we are the first to classify these projectors and analyze their complementarity.",
            "score": 0.37910050337997114,
            "section_title": "Multimodal Large Language Model",
            "char_start_offset": 6442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1724
                }
            ],
            "ref_mentions": [
                {
                    "start": 1350,
                    "end": 1354,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66552734375
        },
        {
            "corpus_id": "272827598",
            "title": "Phantom of Latent for Large Language and Vision Models",
            "text": "Table 3: Detailed comparison for challenging evaluation benchmarks. Sub-benchmark category names in (c), (d), and (g) are represented in Appendix B. For (f), LLaVA-Wilder (Zhang et al., 2024a) is a more advanced challenging evaluation benchmark over LLaVA W (Liu et al., 2023c). (a) Comparison with LLVMs using additional modules and projector: OmniFusion Goncharova et al. (2024), DeepSeek-VL (Lu et al., 2024), MoVA (Kar et al., 2024), Eagle (Shi et al., 2024), CoLLaVO (Lee et al., 2024c), MoAI (Lee et al., 2024d) Validation and Ablation Studies. We present an overview of Phantom's vision-language performance in Figure 1-2, and evaluate it on generally used standard evaluation benchmarks as shown in Table 1-2. In the table, LLaVA-OneVision-8B (Li et al., 2024a) uses significant number of image tokens up to 7290 with three training steps on 558K+4M+3.2M datasets. To highlight the benefits of Phantom, Table 3 reports performance on more challenging evaluation benchmarks. These results demonstrate that Phantom offers a significant advantage on tasks requiring reasoning abilities and densely learned knowledge. Descriptions of the evaluation benchmarks can be found in Appendix B, and Phantom's text generation quality is illuminated in Appendix C. In conclusion, Phantom achieves outstanding performance across numerous vision-language tasks, with a large margin over competing LLVMs, despite having a smaller model size and fewer instruction tuning samples. To better understand the source of this effectiveness, Table 4 presents an ablation study focusing on three key factors: (a) Weighted-Average (WA), (b) Phantom Dimension (PD), and (c) Phantom Optimization (PO).",
            "score": 0.37884226903146334,
            "section_title": "Preprint.",
            "char_start_offset": 19434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1681
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 277,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09124755859375
        },
        {
            "corpus_id": "270620510",
            "title": "Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs",
            "text": "To investigate whether the reasoning ability of ChatGPT constrains state-of-the-art VLMs, we implemented a Prism pipeline that decouples GPT-4o by using it as both the perception and reasoning module.The result reveals that this Prism pipeline, with post-processing, achieves an overall accuracy of 61%, nearly identical to the end-to-end GPT-4o performance of 61.6%.\n\nHow does Language Model Size Affect Perception Ability?During evaluation, we observe that the LLaVA-v1.5series shows no significant improvement when using a larger language model (Vicuna-13B instead of Vicuna-7B, etc.).This suggests that perception performance may be independent of the language model size when using a relatively low-resolution vision backbone.However, the situation appears to differ with LLaVA-NeXT.Quantitative results for the LLaVA-NeXT series tell that scaling up the language model slightly enhances model perception, particularly when using query-specify instructions.Through a detailed qualitative analysis, we identified the primary factors contributing to the superior performance of larger LLaVA-NeXT models over smaller ones as follows:\n\n(1) More Elaborate Expression: Models equipped with a larger language encoder exhibit enhanced ability to articulate visual information.More detailed and organized narratives make it easier for the reasoning module to answer the question; (2) More Adaptive to Instruction: Larger language backbones entitle the model with a better understanding of instructions, yielding more suitable textual visual information for reasoning, particularly in response to query-specific instructions.In Fig. 3, we provide some qualitative results about the two typical modes.",
            "score": 0.3786368556600913,
            "section_title": "Detailed Analysis",
            "char_start_offset": 17426,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 200,
                    "end": 367
                },
                {
                    "start": 369,
                    "end": 424
                },
                {
                    "start": 424,
                    "end": 473
                },
                {
                    "start": 473,
                    "end": 588
                },
                {
                    "start": 588,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 962
                },
                {
                    "start": 962,
                    "end": 1135
                },
                {
                    "start": 1137,
                    "end": 1273
                },
                {
                    "start": 1273,
                    "end": 1620
                },
                {
                    "start": 1620,
                    "end": 1695
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.271240234375
        },
        {
            "corpus_id": "276558319",
            "title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback",
            "text": "4.1 EXPERIMENT SETUP Evaluation Models. We evaluate the performance of foundation models served as the feedback receiver M r across 10 representative LMMs: LLaVA-1.5-7B (Liu et al., 2024a), LLaVA-1.6-7B (Liu et al., 2024b) (Mistral-7B), LLaVa-OneVision-7B (Li et al., 2024a) (Qwen2-7B (Yang et al., 2024)), Qwen2-VL-7B (Wang et al., 2024), GLM-4V-9B (Wang et al., 2023), InternVL2 (OpenGVLab, 2024), Molmo (Deitke et al., 2024), MiniCPM-V (Yao et al., 2024), Phi-3.5-Vision (Abdin et al., 2024), and Fuyu-8B (Bavishi et al., 2023). The feedback provider M p includes the three best available models from three model families: OpenAI (gpt-4o-2024-08-06), Gemini (Gemini-1.5-Pro), and Claude (Claude-3.5-Sonnet-2024-10-22). \n\nEvaluation Metrics. In addition to the Accuracy metric, we leverage the Correction Rate, defined as the percentage of corrected answers of all erroneous samples. Let N denote the total number of samples, N e the number of erroneous samples, and N c the number of samples that have been corrected. The Accuracy and Correction Rate metrics can be formulated as follows: \n\nImplementation Details. We set the temperature to 0 for all tested models and API models. The image resolution of the Qwen2-VL model we restrict to 512 \u00d7 512 to avoid the memory exceeded error. All evaluations were conducted on two NVIDIA RTX A6000 GPUs. To ensure the reliability of results, we obtain the intersection set for both the feedback receiver and provider models that are able to output the correct answer format. Based on our preliminary experiments, we limited the interactive benchmarking to a single round.",
            "score": 0.37828022074786927,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 16510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1091
                },
                {
                    "start": 1094,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1616
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 188,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 406,
                    "end": 427,
                    "matchedPaperCorpusId": "274514412"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06781005859375
        },
        {
            "corpus_id": "270559956",
            "title": "On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning",
            "text": "To identify the most effective model architecture, we test various LLMs ranging from 160M to 13B as follows: Llama-160M, Tiny-Vicuna-1B, Phi3-3.8B, Vicuna-7B, and Vicuna-13B 1 . \n\nFor the vision encoder, we replace the OpenAI CLIP-Large-336-14 module (used in LLaVA-1.5) with OpenAI CLIP-Base-224-32 and utilize the AnyRes technique (Liu et al., 2024c) to optimize the balance between resolution and token count. OpenAI CLIP-Large-336-14 processes a 336x336 area into 576 tokens, while OpenAI CLIP-Base-224-32 processes a 224x224 area into 49 tokens. \n\nBy applying AnyRes, we increase the resolution to 896x676px, with the maximum token count capped at 637, in contrast to LLaVA-NeXT's 2880 tokens for 672x672px. It is important to note that patch size alone does not determine performance; rather, a balanced consideration with resolution ensures optimal results. Through this modification, we achieve a higher input resolution with a slight increase in maximum token count and a marginal decrease in performance (from LLaVA-1.5-7B's 40.1 to 39.8; see \u00a74.1.1 in Figure 1). \n\nNext, we expand the dataset using Idefics2 (Lauren\u00e7on et al., 2024b), LLaVAR (Zhang et al., 2023), and several open-source datasets to enhance performance. This includes 1.1M images for alignment tasks and 1M for instruction tuning. Further dataset details are in Appendix B.2. As shown in Figure 1, this approach improves performance (39.8 to 50.7) but does not reach LLaVA-NeXT-7B's levels at 53.2 (see \u00a74.1.2 in Figure 1).",
            "score": 0.377625009587754,
            "section_title": "Preliminary: Base Architecture Modification and Initial Data Curation",
            "char_start_offset": 9237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 177
                },
                {
                    "start": 180,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 550
                },
                {
                    "start": 553,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1501
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26318359375
        },
        {
            "corpus_id": "273098155",
            "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
            "text": "As illustrated in Table 5, with only 9.4k input prompts, the reward signal provided by LLaVA-Critic substantially improve the base model's performance across various openended visual chat benchmarks. It achieves the best improvements of +10.1 on LLaVA-W, +3.0 on LLaVA-Wilder, +8.8 on WildVision-Bench, along with the second-highest gains of + 4.4 on LiveBench and +0.13 on MMHal-Bench, respectively. At the same time, the overall capacities of LLaVA-v1. 5 other comprehensive benchmarks. This is superior to other competing methods, which either result in smaller performance gains or achieve improvements by compromising the overall capabilities on other benchmarks. \n\nInference Time Search Applying LLaVA-Critic for bestof-n sampling [37] further enhances LMM performance during inference. For the LLaVA-OV-7B checkpoint after iterative DPO training, we generate n = 5 responses for each question with a temperature of 0.7 and top-p of 0.9, then use LLaVA-Critic-7B to select the best responses. As shown in Table 7, this results in additional gains of +1.7 on LLaVA-W and +3.2 on LLaVA-Wilder.",
            "score": 0.37683992104094766,
            "section_title": "Preference Learning",
            "char_start_offset": 29442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1097
                }
            ],
            "ref_mentions": [
                {
                    "start": 737,
                    "end": 741,
                    "matchedPaperCorpusId": "221665105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.371826171875
        },
        {
            "corpus_id": "277043956",
            "title": "Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers",
            "text": "Implementation Details Based on our ablation study (Table 2), we adopt Model D as our final design and initialize VAMBA from Qwen2-VL-7B. We source \u223c3M image caption data from CC12M and PixelProse for pretraining and use LLaVA-Video-178K [75], the training set for NExT-QA [67], ActivityNet [70] and PerceptionTest [50] and 2M image-text data from LLaVA-OneVision [33] for instruction-tuning. The full implementation details can be found in Appendix 7. \n\nHour-Long Video Understanding We evaluate our model's ability to handle extremely long videos using two public benchmarks: LVBench [63] and HourVideo [8] (development set). We further compose a new benchmark called HourEval by selecting all questions related to videos longer than 30 minutes from Video-MME [21], MLVU [76] development set, and LongVideoBench [66] validation set. The average video lengths for LVBench, HourVideo, and HourEval are 68.4, 47.2, and 54.7 minutes, respectively. We compare our model against four efficient video LMMs: LLaVA-Mini [74], LongLLaVA [64], Video-XL [55] and LongVU [54]. We also include the results from Qwen2-VL-7B (our baseline transformer-based LMM) as a reference. \n\nExperimental results are shown in Table 4. Our VAMBA consistently outperforms all efficient video LMMs across the three hour-long video benchmarks, highlighting its exceptional ability to understand and reason over hour-scale videos. Notably, our model surpasses the baseline Qwen2-VL-7B on the LVBench benchmark, and its performance on HourVideo is also very close to Qwen2-VL-7B. These results underscore that our VAMBA is competitive with the best open-sourced transformer-based LMMs, while being significantly more efficient during training and inference.",
            "score": 0.37671810868993755,
            "section_title": "Main Evaluation Results",
            "char_start_offset": 14494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1725
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "234763093"
                },
                {
                    "start": 291,
                    "end": 295,
                    "matchedPaperCorpusId": "69645185"
                },
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "258840892"
                },
                {
                    "start": 605,
                    "end": 608,
                    "matchedPaperCorpusId": "273877964"
                },
                {
                    "start": 814,
                    "end": 818,
                    "matchedPaperCorpusId": "271329356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1451416015625
        },
        {
            "corpus_id": "275758457",
            "title": "Can Multimodal LLMs do Visual Temporal Understanding and Reasoning? The answer is No!",
            "text": "In our experiments, we evaluated a series of MLLMs, including strong closed-source models such as GPT-4o (Hurst et al., 2024) and Gemini-1.5-Pro (Team, 2023), as well as state-of-theart open-source MLLMs like LLaVA-NeXT (Liu et al., 2023a), InternVL (Chen et al., 2024), Qwen-VL (Bai et al., 2023), Llama-3-Vision (Dubey et al., 2024), and LLaVA-CoT (Xu et al., 2024).",
            "score": 0.37530224987476446,
            "section_title": "Evaluated MLLMs",
            "char_start_offset": 11100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 368
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06646728515625
        },
        {
            "corpus_id": "274437586",
            "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
            "text": "Some works [35,39,58] attempt to improve VLM performance on visual question answering tasks by increasing the input image resolution. However, this resolution increase introduces much more visual tokens, further intensifying the computational load on the VLM. In this section, we apply VisPruner to LLaVA-NeXT-7B, which can handle up to 2880 visual tokens. The results are presented in Tab. 2. Compared to LLaVA-1.5, LLaVA-NeXT involves a greater number of visual tokens, implying a higher degree of redundancy. At a 94.4% reduction ratio, retaining only 160 visual tokens, VisPruner preserves 86.7% of the original performance, significantly outperforming FastV (74.7%), Sparse-VLM (74.5%) as well as VisionZip (82.3%), demonstrating the value of VisPruner for high-resolution visual inputs.",
            "score": 0.37526905841375147,
            "section_title": "VisPruner with higher resolution",
            "char_start_offset": 23674,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 792
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.325927734375
        },
        {
            "corpus_id": "277150595",
            "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning",
            "text": "Architectural Components. The LLaVA-MORE family follows the standard LLaVA architecture, employing CLIP ViT-L/14@336 as the visual backbone while varying the underlying language model. We categorize the selected LLMs into two groups based on scale: smallscale models, including Gemma-2-2B and Phi-4-3.8B, and medium-scale models, such as LLaMA-3.1-8B, DeepSeek-R1-Distill-LLaMA-8B, and Gemma-2-9B. To further investigate the impact of the visual backbone, we conduct experiments on the best-performing models, replacing CLIP [49] with alternative vision encoders, including DINOv2 [46], SigLIP [69], and SigLIP2 [64]. Additionally, we examine the effects of applying Scaling on Scales (S 2 ) [53] to both the CLIP and SigLIP2 architectures. Finally, motivated by the LLaVA-1.5 framework and insights from [11,12], which highlight the advantages of using multilayer perceptrons over linear projections in self-supervised learning, we adopt a two-layer MLP as the vision-language adapter to enhance multimodal fusion. Training Details. Following the LLaVA framework, we adopt a two-stage training strategy. In the first stage, only the weights of the adapter are updated using image-caption pairs as training data. Specifically, the caption style follows the alt-text structure as in web-scale multimodal datasets. The models are trained for one full epoch, covering a total of 558k samples from a combination of different sources (i.e., LAION [52], CC3M [10], and SBU [47]). In the second step, we fine-tune the model on high-quality visual instruction-following data to improve its multimodal reasoning capabilities. This sequential training approach has been shown to significantly improve performance on downstream tasks [40]. Notably, the next token prediction is used as the loss function in both training phases. \n\nLLaVA-MORE models are trained with the same set of hyperparameters as LLaVA-1.5 to ensure consistency and comparability.",
            "score": 0.374042277576902,
            "section_title": "Implementation Details",
            "char_start_offset": 11259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1817
                },
                {
                    "start": 1820,
                    "end": 1940
                }
            ],
            "ref_mentions": [
                {
                    "start": 525,
                    "end": 529,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 594,
                    "end": 598,
                    "matchedPaperCorpusId": "257767223"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "276482166"
                },
                {
                    "start": 692,
                    "end": 696,
                    "matchedPaperCorpusId": "268536728"
                },
                {
                    "start": 805,
                    "end": 809,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1442,
                    "end": 1446,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 1453,
                    "end": 1457,
                    "matchedPaperCorpusId": "231951742"
                },
                {
                    "start": 1467,
                    "end": 1471,
                    "matchedPaperCorpusId": "14579301"
                },
                {
                    "start": 1723,
                    "end": 1727,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34521484375
        },
        {
            "corpus_id": "273098835",
            "title": "Contrastive Localized Language-Image Pre-Training",
            "text": "We further show that the CLOC encoder is also competitive against CLIP on general VQA tasks without regression and can even provide performance improvements. We use the Vicuna 7B LLM decoder for two experiments based on LLaVA-1.5 (frozen encoder) and Open-LLaVA-NeXT (unfrozen encoder with AnyRes (Liu et al., 2024) inputs). Since general VQA does not provide spatial referring inputs, we simply replace the ViT in LLaVA. Table 5 summarizes the results. Encouragingly, with our CLOC designs, the improved region-level alignment is also beneficial to some general multimodal benchmarks, as they may also require fine-grained image understanding.",
            "score": 0.37291174870368526,
            "section_title": "GENERAL VQA WITH LLAVA-1.5 AND LLAVA-NEXT",
            "char_start_offset": 28481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 644
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.253173828125
        },
        {
            "corpus_id": "277112934",
            "title": "HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding",
            "text": "Next, we expanded the input resolution and used CLIP-ViT-L-14 (Radford et al., 2021) with an input resolution of 336 as the vision teacher for pre-training. When training with the 'mix-v2' data, the increased resolution resulted in a 1.5% improvement in average performance, with notable gains on GQA (Hudson & Manning, 2019), MMBench (Liu et al., 2024d), Alpaca (51 K) (Taori et al., 2023), and ShareGPT (143 K) (ShareGPT, 2023). The auxiliary samples involve 558 K image caption data (Liu et al., 2024c) and 1 M pure text data (Computations, 2023). The auxiliary samples are randomly combined with the main samples during training. Assume a main sample is S m and a auxiliary sample is S a , the combined samples can be one of < S a , S m > and < S m , S a >. In this way, we obtain interleaved samples. \n\nSingle-Image instruction data. 665 K instruction data is from LLaVA-1.5 (Liu et al., 2024a). 4 M instruction data is partly from LLaVA-OneVision (Li et al., 2024a), where we filter some error samples. \n\nMulti-Image instruction data. We use the interleaved data collected by (Li et al., 2024b) to endow the model (HaploVL-8B-MI) with the ability to process multiple images.",
            "score": 0.3727399585005557,
            "section_title": "B. More Results",
            "char_start_offset": 32954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1180
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 84,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 301,
                    "end": 325,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 335,
                    "end": 354,
                    "matchedPaperCorpusId": "259837088"
                },
                {
                    "start": 880,
                    "end": 899,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12249755859375
        },
        {
            "corpus_id": "274422737",
            "title": "Evaluating Vision-Language Models as Evaluators in Path Planning",
            "text": "In order for VLMs to perform successfully in our benchmark, they need to perform successfully at three different levels, i.e, recognizing the critical descriptors required by each scenario (Attribute Abstraction), exhibiting sharp low-level perception to precisely assess each path's properties (Low-level Perception), and integrating the prior information to make a rational decision on the better path (Information Integration). Among them, the first two levels reflect parallel properties that serve as a foundation for the third level. In this section, we evaluated a set of 9 VLMs on PATHEVAL in a zero-shot manner and analyzed their capabilities at these three levels. These VLMs include (1) two closed-source VLMs, i.e., GPT-4o and GPT-4o-mini [47], and ( 2) seven open-source VLMs with various sizes, including LLaVA-NeXT-7b and LLaVA-NeXT-13b [38,39], \n\nLLaVA-OneVision-7b [32], Qwen2-VL-7b [69], LLaMA-3.2-11b [17], and Intern-VL2-8b and Intern-VL2-40b [12]. We include all prompt scripts used in this Section in Appendix C.1.",
            "score": 0.3725373200118825,
            "section_title": "Can SOTA VLMs Evaluate Planned Paths?",
            "char_start_offset": 16865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1036
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1175537109375
        },
        {
            "corpus_id": "276961504",
            "title": "SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems",
            "text": "Recent advances in multi-modal AI have been marked by significant developments in LMMs, which combine the capabilities of LLMs and vision models to process diverse visual inputs. While proprietary models like GPT-4V (OpenAI, 2023c), Claude (Anthropic, 2024), Gemini (Gemini Team, 2023), and GPT-4o (OpenAI, 2024a) have shown remarkable visual reasoning abilities, their closed nature has spurred the development of open-source alternatives. Early opensource LMMs like LLaVA (Liu et al., 2023b) and MiniGPT-4 (Zhu et al., 2023) paired CLIP-based image encoders (Radford et al., 2021) with LLMs for multi-modal instruction tuning. Later models such as LLaVA-NeXT (Li et al., 2024a), LLaVA-OneVision (Li et al., 2024b), ShareGPT4V (Chen et al., 2023b), InternVL (Chen et al., 2024b), SPHINX (Lin et al., 2023), and Qwen-VL (Qwen Team, 2024) expanded these capabilities through broader training datasets and advanced training strategies. In this paper, we aim to comprehensively evaluate their fine-grained capabilities in scientific domains, guiding the future developments of LMMs.",
            "score": 0.3721490776415851,
            "section_title": "Large Multi-modal Models (LMMs)",
            "char_start_offset": 17461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1079
                }
            ],
            "ref_mentions": [
                {
                    "start": 560,
                    "end": 582,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 759,
                    "end": 779,
                    "matchedPaperCorpusId": "266521410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5654296875
        },
        {
            "corpus_id": "273532279",
            "title": "LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding",
            "text": "Quantitative Results. Table 1 presents our experimental results on multiple video understanding benchmarks. \n\nOur results compares favorably to all the baselines across various video understanding benchmarks. For example, on VideoMME (Fu et al., 2024), our LongVU outperforms VideoChat2 (Li et al., 2024b), LLaVA-OneVision (Li et al., 2024a) by 6.0% and 2.4% respectively. Notably, on VideoMME Long subset (Fu et al., 2024), our model surpasses LLaVA-OneVision (Li et al., 2024a) by 12.8%. These results indicate the strong video understanding capabilities of our model. Note that our model achieves significant improved performance with a much smaller training dataset, comparing to LLaVA-OneVision (Li et al., 2024a) trained on OneVision-1.6M (multi-image, video) that has not yet been made publicly available1 . With the same video training dataset from VideoChat2-IT (Li et al., 2024b), our LongVU shows much higher performance than VideoChat2 (Li et al., 2024b), \u223c10% accuracy improvement in average. Interestingly, we also find that our model can even beat proprietary model GPT4-o (OpenAI, 2024) on MVBench (Li et al., 2024b) with densely sampled video input and reduce the accuracy gap comparing to proprietary models on other video benchmarks. \n\nWe also scale our LongVU with a lightweight LLM, Llama3.2-3B (Llama, 2024), to further demonstrate the strong video understanding capabilities. We observe the consistent improvement of our light-weight LongVU over baselines in Table 2. Our method outperforms Phi-3.5-vision-instruct (Abdin et al., 2024) on VideoMME (Long) by margin of 3.4% accuracy. This set of experiments validate the effectiveness of our method even scaling to a smaller size.",
            "score": 0.3715970988004346,
            "section_title": "Video Understanding",
            "char_start_offset": 18672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 107
                },
                {
                    "start": 110,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1702
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 305,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 871,
                    "end": 889,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 948,
                    "end": 966,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 1114,
                    "end": 1132,
                    "matchedPaperCorpusId": "265466214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.162353515625
        },
        {
            "corpus_id": "275133761",
            "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video Understanding",
            "text": "Comparison with SoTAs. We conducted experiments on VideoMME [5], MLVU [51], LongVideoBench [38], and LVBench [35], as shown in Table 1. For base VideoLLMs with different architectures, RETAKE consistently improves performance on long video understanding, achieving an average gain of 3.1% and 2.9% for QWen2VL-7B [34] and LLaVA-Video-7B [48], respectively. Notably, for LVBench, which features the longest average video length, the performance gain reaches a remarkable 5.3%. Additionally, RETAKE outperforms existing models of similar size, including LongVA [46], LongVILA-7B [41], and LLaVA-OneVision-7B [14], with improvements ranging from 1.6% to 4.9%. It even surpasses much larger models, such as VideoLLaMA2-72B [3] on VideoMME, LLaVA-OneVision-72B [14] on MLVU, and Oryx-1.5-34B [22] on LVBench. Furthermore, RETAKE significantly outperforms GPT-4o [25] on both MLVU and LVBench. These benchmarks cover diverse video durations and question types, demonstrating the robustness and generality of RETAKE. \n\nEfficiency Analysis. To evaluate the efficiency of RETAKE, we measured its FLOPs and TTFT for prefilling and TPOT for decoding with 256 max frames and 0.5 compression ratio, shown in  video sequence length, lowering both prefilling computational cost (FLOPs) and decoding latency (TPOT). However, without optimization, RETAKE incurs notable overhead (+28% TTFTs for QWen2VL and +62% for LLaVA-Video), which the optimized version (RETAKE-OPT) significantly reduces to just 8% and 11%, demonstrating the efficiency of our implementation.",
            "score": 0.37135920160021296,
            "section_title": "Main Results",
            "char_start_offset": 9804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1547
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1422119140625
        },
        {
            "corpus_id": "278130017",
            "title": "VEU-Bench: Towards Comprehensive Understanding of Video Editing",
            "text": "We conduct extensive benchmarking across mainstream Vid-LLMs, including open-source models LLaVA-Video [52], InternLM-X [51], InternVL [6], LLaVA-OneVision [19], Qwen2-VL [40], Video-CCAM [9], VideoLLaMA2 [7], Oryx [25], ViLA [22] and proprietary models include Gemini-1.5-Pro [37] and GPT-4o [30]. \n\nFor model training, we use Qwen2-VL-7B [40] as the base model and apply LoRA [13] fine-tuning with r = 16 and \u03b1 = 32. The learning rate is set to 1e \u2212 4, weight decay to 0.01, and warmup ratio to 0.05. The model is optimized using AdamW [26]. For each video, frames are sampled at 1 fps with a maximum limit of 64 frames. All experiments are conducted on 4 A100 GPUs.",
            "score": 0.37122680998759944,
            "section_title": "Implementation Details",
            "char_start_offset": 16232,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 298
                },
                {
                    "start": 301,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 668
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "266174746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0416259765625
        },
        {
            "corpus_id": "272986933",
            "title": "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering",
            "text": "We freeze the vision encoder during training on the LLaVA-1.5 and only freeze the vision encoder on the PT stage during training on the LLaVA-NEXT following the original paper. The experiments of VLM training are all conducted on 16 A100 GPUs.",
            "score": 0.3711789510995164,
            "section_title": "B.2 Code Parsing Ability Evaluation",
            "char_start_offset": 25202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 243
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2188720703125
        },
        {
            "corpus_id": "278481332",
            "title": "MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks",
            "text": "To ensure a fair comparison, all evaluated models, with the exception of doubao-1-5-vision-pro and Gemini Flash2.0, are based on 7B-LLMs. Specifically, LLaVA-v1.6 is based on Vicuna-7B-v1.5 [55], LLaVA-Med-v1.5 utilizes Mistral-7B-Instruct-v0.2 [19], and Hu-atuoGPT integrates the LLaVA-1.5 model architecture with LLaMA-3-8B [50]. The configurations of all models were set according to their respective open-source codes. The results of these models are summarized in Table 3, with the first five rows presenting the performance of general LVLMs, the next two rows representing medical LVLMs, and the final three rows showing the results of our fine-tuned version of LLaVA-Med based on different training sets. \n\nMost general LVLMs perform poorly on MM-Skin. For instance, doubao-1-5-vision-pro-32k achieves only a Bleu-4 score of 2.16 in the pathology modality, reflecting its inability to generate relevant responses. LLaVA-v1.6-7B shows better recall but still scores low in Bleu-4 (2.14), indicating limited accuracy. Specialized models like LLaVA-Med-7B and HuatuoGPT-7B perform better, with LLaVA-Med-7B achieving 7.70 in Bleu-4 for pathology, and HuatuoGPT-7B scoring 4.42. The higher recall of HuatuoGPT-7B suggests a stronger understanding of medical images, highlighting the benefits of specialized training. The fine-tuned SkinVL-MM model, trained on MM-Skin, shows substantial improvement, achieving a Bleu-4 of 22.04 in pathology. It improves by nearly 10% in Bleu-4  compared to InternVL 2.5-8B, the best general model for dermoscopic images, demonstrating the effectiveness of domain-specific fine-tuning.",
            "score": 0.37097814791802486,
            "section_title": "Evaluation on Visual Question Answering",
            "char_start_offset": 19277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1621
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 194,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1627197265625
        },
        {
            "corpus_id": "271962730",
            "title": "A New Era in Computational Pathology: A Survey on Foundation and Vision-Language Models",
            "text": "There are four research works/models that fall in this category (PA-LLaVA [163], PathInsight [164], CLOVER [165], PathGen-LLaVA [51] and Quilt-LLaVA [56]). \n\nIn PA-LLaVA, a domain-specific large language-vision assistant is proposed which is a LLaVA-based model. In terms of architecture, PLIP is used as the vision encoder and LLama3 is used as the LLM (in conjunction with LoRA [124]). A connector module that consists of self-attention and crossattention blocks is leveraged to project image embeddings to match the dimension of language tokens. PathInsight leverages three different multi-modal models LLaVA, Qwen-VL [166] and InternLM [167] (in conjunction with LoRA or full-parameter tuning). As the vision encoder either ViT or CLIP is used. CLOVER [165] proposes a framework for cost-effective instruction learning in CPath. It utilizes Quilt-1M as the vision-language dataset, BLIP-2 [138] for visionlanguage pre-training and a frozen visual encoder (EVA-ViT-G/14 [168]) and a frozen LLM (FlanT5 [116] or Vicuna [54]). The next work PathGen-LLaVA replaces the vision encoder for the CLIP model in LLaVA-v1.5 with PathGen-CLIP and as the frozen LLM Vicuna is used. The last work in this category Quilt-LLaVA is initialized with the general-domain LLaVA. Then it is tuned with QUILT for histopathological domain-alignment and finally, Quilt-instruct is used for visual instruction tuning.",
            "score": 0.3708201444321674,
            "section_title": "C. Visual Instruction Tuning in CPath",
            "char_start_offset": 44906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1395
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 153,
                    "matchedPaperCorpusId": "266149936"
                },
                {
                    "start": 893,
                    "end": 898,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 973,
                    "end": 978,
                    "matchedPaperCorpusId": "253510587"
                },
                {
                    "start": 1005,
                    "end": 1010,
                    "matchedPaperCorpusId": "253018554"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.217529296875
        },
        {
            "corpus_id": "269304163",
            "title": "Temporal Grounding of Activities using Multimodal Large Language Models",
            "text": "Instruction-tuning For the baseline, we ran inference on a non-modified LLaVA 7B (LLaVA-1.5-7B)model using the general prompt strategy introduced in Section 3.1.We then instruction-tuned the model using the dataset described in Section 4.1 using the finetune_task_lora.shscript1 provided by LLaVA over 1 epoch, with a learning rate of 2e \u22124 , for a total of 1.4k steps.Total time for instruction-tuning took around 2 hours on an NVIDIA H100 GPU.Due to the longer context generated by describing each frame in the video, we chose Qwen 7B (Bai et al., 2023) for the text-based LLM as it has a context length of 8192 tokens.Using LLaMA 2 (Touvron et al., 2023), we ran into issues with context length, as the model only has half the context length of that of Qwen models.Qwen 7B 20.1 7.9 2.5 14.9 15.6 7.0 0.8 12.7 GPT-4 Vision GPT-4 28.9 10.9 6.3 20.7\n\nTable 1: R@IoU for base LLaVA, instruction-tuned LLaVA using the general prompting strategy.\n\nFor comparison, we also show video-based LLM (Huang et al., 2023), GPT-4 Vision metrics.\n\nTwo-stage LLM runs Next, we compare metrics of different combinations of multimodal LLM and text-based LLMs as listed in Table 2.We use the activity prompting strategy in our multimodal LLMs to optimize for performance.For comparison, we run models on the state-of-the-art task-specific model (ViGA) by Cui et al. (2022) and a state-of-the-art video-based LLM (Reid et al., 2024), Gemini.\n\nFor inference in all experiments, LLaVA 7B (LLaVA-1.5-7B2) and 16B (LLaVA-1.6-Vicuna-13B3",
            "score": 0.37062165159130767,
            "section_title": "Experimental details",
            "char_start_offset": 12761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 621
                },
                {
                    "start": 621,
                    "end": 768
                },
                {
                    "start": 768,
                    "end": 849
                },
                {
                    "start": 851,
                    "end": 943
                },
                {
                    "start": 945,
                    "end": 1033
                },
                {
                    "start": 1035,
                    "end": 1164
                },
                {
                    "start": 1164,
                    "end": 1254
                },
                {
                    "start": 1254,
                    "end": 1423
                },
                {
                    "start": 1425,
                    "end": 1482
                },
                {
                    "start": 1482,
                    "end": 1514
                }
            ],
            "ref_mentions": [
                {
                    "start": 1338,
                    "end": 1355,
                    "matchedPaperCorpusId": "248266558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09857177734375
        },
        {
            "corpus_id": "277112989",
            "title": "Visual Position Prompt for MLLM based Visual Grounding",
            "text": "Transferability studies hold two implications for our work: First, whether the proposed VPP can be transferred to models beyond LLaVA-v1.5 and similarly enhance their visual grounding capabilities; second, whether the proposed VPP-LLaVA can be transferred to other datasets in a zero-shot manner. To address these concerns, we apply the proposed VPP to LLaVA-NeXT-7B using the main experiment's training strategy, with results in Table III. Additionally, in Table IV, we present the zero-shot visual grounding test of VPP-LLaVA-7B on the ReferIt dataset. \n\nFrom Table III, we observe that the proposed VPP method can be transferred to other models, thereby enhancing their visual grounding performance. \n\nFrom Table IV, it can be observed that compared to LLaVA-7B, VPP-LLaVA-7B demonstrates a significant improvement in zero-shot visual grounding performance on the ReferIt dataset. Specifically, VPP-LLaVA-7B achieves an accuracy boost of absolutely 8.6% on the ReferIt val split and 9.11% on the ReferIt test split.",
            "score": 0.3700860597787916,
            "section_title": "E. Transferability Studies for VPP",
            "char_start_offset": 25340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1018
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265625
        },
        {
            "corpus_id": "271218708",
            "title": "FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models",
            "text": "Considering that fine-tuning VLMs may encounter the catastrophic forgetting problem, we evaluate the instruction-following ability of FIRE-LLaVA, using 10 commonly used multimodal benchmarks, as shown in Tab. 1. Our model achieves comparable performance to the original LLaVA-NeXT-8B model, showing that we do not compromise the instruction-following ability when learning the feedback-refining ability.",
            "score": 0.37006271315523065,
            "section_title": "Evaluation in Instruction Following",
            "char_start_offset": 17587,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 403
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2039794921875
        },
        {
            "corpus_id": "277272119",
            "title": "V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts for Better Human-Model Interaction",
            "text": "(Hurst et al., 2024) -20.2 19.4 17.4 14.0 17.9 32.1 21.6 23.5 15.9 19.3 16.0 17.9 20.7 Gemini-1.5-pro (Team et al., 2024) -10.1 14.6 2.2 3.5 24.1 13.9 29.7 16.0 20.5 12.3 14.0 5.3 13.7 Qwen2-VL (Wang et al., 2024a) 7B 26.4 31.8 31.5 28.1 33.9 32.9 28.4 33.6 39.8 24.6 25.0 27.4 30.8 InternVL2.5 (Chen et al., 2024d) 8B 30.2 26.9 21.7 21.1 30.4 32.9 27.0 29.4 27.2 22.8 24.0 27.4 27.9 Closed-source Models GPT-4o (Hurst et al.,  3 Gemini-1.5-pro (Team et al., 2024) -65.3 82.3 64.1 67.9 Gemini-1.5-Flash (Team et al.,  Below we summarize our key findings as follows: \n\nExpert models demonstrate mediocre performance. LLaVA-NeXT-INST-IT fine-tunes on a visual prompt dataset derived from LLaVA-NeXT. However, it achieves only a marginal improvement of 0.3% over LLaVA-NeXT's 46.0%, suggesting that the fine-tuning process is nearly ineffective. We attribute this to the model's inadequate robustness and comprehensiveness in video sources, as well as its reliance on a single type of visual prompt (SoM only), which constrains the model's generalization capabilities. \n\nEven the most powerful closed-source models perform poorly on our benchmark. In the evaluation results, closed-source models GPT-4o (Hurst et al., 2024) and Gemini-1.5-Pro",
            "score": 0.3699693082549161,
            "section_title": "Results on V2P-Bench",
            "char_start_offset": 18379,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1239
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03173828125
        },
        {
            "corpus_id": "274598288",
            "title": "Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events",
            "text": "Default settings for resolution are used, and max number of frames are 32. 5. VILA [16] VILA 1.5 is the latest in the VILA series of models, one of the first models to support multi-image understanding. We use the Llama-3-VILA1.5-8b-Fix model. We uniformly sample 5 frames for each part of the video. \n\n6. LLaVA-Video [43] LLaVA was first introduced as a multimodal model with performance rivaling GPT-4. Following the release of LLaVA 1.5 and LLaVA-NeXT, LLaVA-Video is the most advanced version of this model, specifically trained on a new 178K video dataset. Specifically, we use the LLaVA-Video-7B-Qwen2 model specification. When asking questions, we directly upload the entire video clip. In the case of Detective , the clip has the V main blacked out. 32 frames are used. We also test the 72B LLaVA-Video (LLaVA-Video-72B-Qwen2) model; however, due to compute limitations and setup issues, we were only able to run that model on a subset of the data. The results are detailed in Appendix F.3. \n\nOur implementation of all open-source models follows their respective instructions on GitHub and Huggingface, and we tried our best to recreate the same environment as the original developers for each model. We do not do batch inference on any of these models, and instead query them iteratively in chat mode only (each chat conversations for every question is independent). For the generative variant, in order to obtain different results each time, we turn sampling on. \n\nWe use the following prompts for each task: \n\nForecaster -Gen Describe what could happen next, by explaining the sequence of actions leading to the outcome.",
            "score": 0.36973103994240675,
            "section_title": "E. Baselines",
            "char_start_offset": 35823,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1631
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.191162109375
        },
        {
            "corpus_id": "272986978",
            "title": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning",
            "text": "This section covers the methodology used to report results for Phi-3-Vision [3], LLaVA-OneVision [74], InternVL2 [21] and MiniCPM-V2 [169]. When available, we reported the results published by the original authors, either in their technical reports or on public leaderboards 8 . When not available, we implemented inference runners using publicly released checkpoints. Commonly, we followed [72]'s implementations that we adapted on our own internal fork of lm-eval-harness [36,118]. To verify the validity of our inference implementations, we ensured we could reproduce previously published results within standard deviation. Below, we share details for each model implementation:",
            "score": 0.3697196149397131,
            "section_title": "A.7 Methodology for Running Competitor Models",
            "char_start_offset": 61668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 681
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16845703125
        },
        {
            "corpus_id": "277103970",
            "title": "Improving LLM Video Understanding with 16 Frames Per Second",
            "text": "Most image LLMs adopt the approach of connecting an image encoder to an LLM via modality adapters, achieving remarkable success. LLaVA (Liu et al., 2024b;a) applies instruction tuning (Wei et al., 2022), enabling zero-shot image understanding. BLIP-2 (Li et al., 2023) integrates a frozen image encoder with an LLM using Q-Former to bridge the modalities. InternVL (Chen et al., 2023) further enhances accuracy by scaling up the visual encoder for more precise image representations. InternVL 2.5 (Chen et al., 2024) delves into model scaling and achieves better test-time performance. \n\nRegarding video understanding, recent studies typically sam-ple at a low frame rate or select a fixed number of frames, treating them as separate images with orders. In many approaches, each frame is first encoded using a pre-trained image encoder, then mapped to the text space via a modality aligner, and finally fed into an LLM backbone for response generation. Video-LLaVA (Lin et al., 2024a) uniformly samples 8 frames from a video, processes each frame independently through an image encoder, and generates video tokens that are then passed to the LLM. LLaVA-OneVision (Li et al., 2024) is designed to handle single images, multiple images, and videos. For videos, it samples frames at 1 FPS and reduces the number of video tokens using bilinear interpolation. Building on a similar architecture, Zhang et al. develops a strong video-understanding LLM with synthetic data, but still limits sampling to 1 FPS. Qwen2-VL (Wang et al., 2024) increases the frame rate to 2 FPS and employs a rotary position embedding to enhance temporal modelling. Meanwhile, VideoLLaMA 2 (Cheng et al., 2024) and video-SALMONN 2 (Tang et al., 2024) incorporate full audio information to support video understanding. However, VideoLLaMA 2 only processes 16 frames from each video, while video-SALMONN 2 uses no more than 30 frames.",
            "score": 0.3696209734828546,
            "section_title": "From Image LLM to Video LLM",
            "char_start_offset": 4331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1903
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 154,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 184,
                    "end": 202,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 251,
                    "end": 268,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 497,
                    "end": 516,
                    "matchedPaperCorpusId": "274581884"
                },
                {
                    "start": 965,
                    "end": 984,
                    "matchedPaperCorpusId": "265281544"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.364501953125
        },
        {
            "corpus_id": "277313371",
            "title": "Exploring Hallucination of Large Multimodal Models in Video Understanding: Benchmark, Analysis and Mitigation",
            "text": "We conducted our thinking-based training on LLaVA-NeXT-Video-DPO-7B with about 5K synthetic data for SRFT and 3K preference data for TDPO. As shown in Table 5, the base model trained using our method-LLaVA-NeXT-Video-7B-Thinking-outperformed the original LLaVA-NeXT-Video-7B by 7.65% and achieved a 3.34% improvement over the original model with CoT. Notably, it has even surpassed the performance of the 34B-parameter LLaVA-NeXT-Video-DPO. Table 6 shows the consistency evaluation of LLaVA-NeXT-Video-7B-Thinking alongside other LMMs. It demonstrates that LLaVA-NeXT-Video-7B-Thinking exhibits a higher consistency compared to the original LLaVA-NeXT-Video-DPO-7B-both with and without CoT-achieving bias score reductions of 4.5% and 4.13% respectively. We present some cases of response from LLaVA-NeXT-Video-7B-Thinking in Appendix C.2.",
            "score": 0.3695576859707735,
            "section_title": "Experiment Result",
            "char_start_offset": 22726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 839
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07794189453125
        },
        {
            "corpus_id": "276408016",
            "title": "VAQUUM: Are Vague Quantifiers Grounded in Visual Data?",
            "text": "Figure 7 shows density plots displaying the distributions of human ratings in VAQUUM, as well as scores generated by VLMs as a response to prompt 5 in Appendix D.1, discussed in \u00a75. Note that for LLaVA-NeXT, LLaVA-OneVision and Molmo, the scores tend towards the extremes. However, in the human distribution, this is only the case for the unquantified control statement (as expected).",
            "score": 0.3695198175488567,
            "section_title": "D.2 Distribution of Generated Scores",
            "char_start_offset": 32805,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 384
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03192138671875
        },
        {
            "corpus_id": "270063538",
            "title": "Matryoshka Multimodal Models",
            "text": "Model We use LLaVA-1.5 [5] and LLaVA-NeXT [4] as the base LMMs, both with Vicuna 7B as the language model backbone. We finetune the whole model using the exact visual instruction data from LLaVA-1.5 and LLaVA-NeXT, respectively. The learning rate of LLM is 2 \u00d7 10 \u22125 and 1 \u00d7 10 \u22125 , respectively for LLaVA-1.5 and LLaVA-NeXT. The learning rate for the visual encoder is 2 \u00d7 10 \u22125 for both models. We train both models for 1 epoch using 8 NVIDIA H100 GPUs. \n\nInstead of training the language model from scratch, we initialize the language model weights from pre-trained LLaVA-1.5 and LLaVA-NeXT, which we empirically works better. We name our Matryoshka Multimodal Models LLaVA-1.5-M 3 and LLaVA-NeXT-M 3 . \n\nVisual Token Scales We design 5 scales for the visual tokens. LLaVA-1.5 [5] and LLaVA-NeXT [4] both leverage CLIP-ViT-L-336 [29] as the visual encoder, where an image is embedded into 24 \u00d7 24 visual tokens. We gradually apply 2 \u00d7 2 pooling with stride 2, resulting in 12 \u00d7 12, 6 \u00d7 6, and 3 \u00d7 3 visual tokens, where we finally apply a 3 \u00d7 3 pooling to get the final single visual token. Therefore, the size of Matryoshka visual token sets are S \u2208 {1, 9, 36, 144, 576}, following a nested manner. The efficiency anlaysis on the system level is shown in Appendix B, where M 3 boosts the speed of the LMM prefill process through diminished floating-point operations (FLOPs) and lessens computational memory requirements. \n\nEvaluations.",
            "score": 0.36951101703197564,
            "section_title": "Experiment Settings",
            "char_start_offset": 14859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 455
                },
                {
                    "start": 458,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1439
                }
            ],
            "ref_mentions": [
                {
                    "start": 832,
                    "end": 836,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.241455078125
        },
        {
            "corpus_id": "277955366",
            "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs",
            "text": "3.1. Evaluation Setup Benchmark Models. We evaluate a broad spectrum of MLLMs spanning diverse model families, parameter scales, and training paradigms. On the closed-source side, we include three of the most prominent model families -Gemini-2.0 [45], Claude-3.7 [1], and GPT-4o [40]. For open-source models, we examine recent breakthroughs from Deepseek-VL2 [52], Qwen2.5-VL [4], InternVL2.5 [8], Cambrian [46], LLaVA-OneVision [28], LLaVA-NeXT-Video [63], and OVIS [36]. In all experiments, we follow standard protocols and set the temperature to zero unless otherwise specified. \n\nHuman Evaluation. We randomly select a subset of 250 questions from our All-Angles Bench -encompassing all six task categories for evaluation by human annotators, each of whom independently answers every question. For fair comparison, we also report performance of Gemini-2.0-Flash, Claude-3.7-Sonnet, GPT-4o, Qwen2.5-VL-72B, and InternVL2.5-38B on this subset.",
            "score": 0.3689604781742545,
            "section_title": "MLLMs Have Multi-View Understanding?",
            "char_start_offset": 10290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 4
                },
                {
                    "start": 5,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 945
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0987548828125
        },
        {
            "corpus_id": "274981713",
            "title": "Is Your World Simulator a Good Story Presenter? A Consecutive Events-Based Benchmark for Future Long Video Generation",
            "text": "Apart from GPT-4o, we also conduct the experiments using open-source advanced VLM LLaVA-OneVision-Chat-72B as illustrated in Sec. 2.2. The results are shown in Table 3. We can observe that the result of using LLaVA-OV-Chat-72B has very close ranking correlations with that of  3. StoryEval evaluation results on 11 video generative models with LLaVA-OV-Chat-72B [22] verifier. The \u03c1 and \u03c4 are evaluated between the completion rates of GPT-4o and that of LLaVA-OV-Chat-72B model. More details are in Appendix. \n\nusing GPT-4o, showing the robustness StoryEval in terms of VLM verifiers. What's more, we also note that compared to GPT-4o, LLaVA-OV-Chat-72B will further over-estimate the results, resulting higher average completion rates.",
            "score": 0.3685505804153809,
            "section_title": "Using LLaVA-OV-Chat-72B",
            "char_start_offset": 25283,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 736
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053192138671875
        },
        {
            "corpus_id": "273638179",
            "title": "TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning",
            "text": "By applying Token Shuffle, we further reduced the computational cost of VideoChat-T, giving it a significant computational advantage over high-performance models like LLaVA-OneVision (Li et al., 2024a) and Qwen2-VL (Wang et al., 2024a). Under the same settings, VideoChat-T uses only 3 tokens per frame, with flops consumption at just 5.1% of LLaVA-OneVision. Its inference time on single A100 is only 0.63 seconds, reaching real-time response levels, making it highly suitable for applications requiring rapid response, such as online video understanding.",
            "score": 0.36809888130166923,
            "section_title": "C COMPUTATIONAL EFFICIENCY",
            "char_start_offset": 39875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 556
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1671142578125
        },
        {
            "corpus_id": "276937833",
            "title": "Teaching LMMs for Image Quality Scoring and Interpreting",
            "text": "In our experiments, we set the batch size to 64 and the learning rate to 1e-5. We use LLaVA-Onevision-0.5B and LLaVA-Onevision-7B as the small and primary LMM models for Q-SIT, respectively. For training the small LMM, we stop once the validation loss plateaus, indicating no further improvement. For the primary model, we train for a default of 3 epochs. Training is performed on 8 NVIDIA A800 80G GPUs, and inference latency is reported on a single NVIDIA A800 80G GPU. The employed datasets are described as follows: \n\n\u2022 For the D1 data, we select KonIQ [50] and SPAQ [49]. \n\n\u2022 For the D2 data, we use the constructed image quality interpreting data discussed in Sec.III-B. \u2022 For the D3 data, we use the LLaVA-150K [8] dataset. \n\n\u2022 For Validation, we use 10% of the training data from KonIQ and SPAQ for the image quality scoring task, and for the image quality interpreting task, we select LLVisionQA-dev [7] as the validation set. \u2022 For Testing, we choose KonIQ (testing sets), SPAQ (testing sets), LIVEC [44], AGIQA-3K [52], and KADID-10k [60] as scoring test sets. We choose LLVisionQA-test and LLDescribe [7] as interpreting test sets. \n\nIt is important to note that all datasets are cleaned to ensure no overlap between the training/validation and testing sets. This gives us the Coarse-Grained Optimal Data Mix Ratio and the details are presented in Fig. 4. It is worth noting that, to reduce training time and computational costs, these experiments are conducted on LLaVA-OneVision-0.5B. Additionally, we record the validation loss ratio between image quality scoring and interpreting tasks, which is approximately 1:4.66. Fine-Grained Optimal Data Mix Ratio. Based on the coarsegrained ratio, we determine an approximate mix of D1:D2:D3 = 1.00: 2.50: 1.04.",
            "score": 0.3675279594018983,
            "section_title": "A. Experiment Settings",
            "char_start_offset": 30786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1768
                }
            ],
            "ref_mentions": [
                {
                    "start": 557,
                    "end": 561,
                    "matchedPaperCorpusId": "204509303"
                },
                {
                    "start": 571,
                    "end": 575,
                    "matchedPaperCorpusId": "219617103"
                },
                {
                    "start": 909,
                    "end": 912,
                    "matchedPaperCorpusId": "262824606"
                },
                {
                    "start": 1010,
                    "end": 1014,
                    "matchedPaperCorpusId": "8531217"
                },
                {
                    "start": 1113,
                    "end": 1116,
                    "matchedPaperCorpusId": "262824606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0894775390625
        },
        {
            "corpus_id": "273346267",
            "title": "Text4Seg: Reimagining Image Segmentation as Text Generation",
            "text": "Multimodal Large Language Models. MLLMs are typically developed by enhancing large language models (LLMs) with visual perception modules, which can generate coherent textual conversations grounded in multimodal inputs. For instance, Flamingo (Alayrac et al., 2022) introduces the Perceiver Resampler, which connects a pre-trained vision encoder with LLMs for effective fewshot learning. OpenFlamingo (Awadalla et al., 2023) and Otter (Li et al., 2023a) build upon this architecture with a focus on multi-modal in-context instruction tuning. BLIP-2 (Li et al., 2023b) and InstructBLIP (Dai et al., 2023) bridge the modality gap using a lightweight Querying Transformer (Q-Former), demonstrating enhanced performance on zero-shot vision-to-language tasks. The LLaVA seires (Liu et al., 2024c;a) employs a linear layer or MLP as a modality connector, trained on multimodal language-image instruction-following data generated with GPT-4, showcasing notable capabilities in multimodal chat interactions. They demonstrate impressive capabilities in multimodal chat interactions. In contrast, Qwen-VL (Bai et al., 2023) and mPLUG-Owl2 (Ye et al., 2024) explore feature compression to a fixed length through cross-attention mechanisms with learnable queries, optimizing computational efficiency. Recent advancements (Liu et al., 2024b;Xu et al., 2024;Li et al., 2024a;b;c;Lin et al., 2023) have focused on enhancing visual encoding through highresolution inputs. For example, LLaVA-UHD (Xu et al., 2024) implements an image modularization strategy, segmenting native-resolution images into smaller, variable-sized slices to improve scalability and encoding efficiency. Similarly, LLaVA-NEXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024a) utilize the AnyRes scheme to accommodate high-resolution image inputs.",
            "score": 0.3672405267729987,
            "section_title": "RELATED WORK",
            "char_start_offset": 6153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1812
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 264,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 548,
                    "end": 566,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 584,
                    "end": 602,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 771,
                    "end": 790,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1128,
                    "end": 1145,
                    "matchedPaperCorpusId": "265050943"
                },
                {
                    "start": 1362,
                    "end": 1364,
                    "matchedPaperCorpusId": "257952310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.314697265625
        },
        {
            "corpus_id": "270045537",
            "title": "ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models",
            "text": "We perform a comprehensive comparison with state-of-the-art models on 7 different benchmarks (Tab.5).Our model achieves consistent improvements compared to LLaVA-1.5.Our 7B model even exhibits comparable performance with LLaVA-1.5 13B and LLaVA-NExT 7B [31].\n\nOn OCR benchmarks like TextVQA and DocVQA, our model outperforms the LLaVA-1.For grounding benchmarks, our model and LLaVA are trained with the same set of grounding data.The comparison between them is fair.On RefCOCO, RefCOCO+, and RefCOCOg, ConvLLaVA exhibits consistent improvement when increasing resolution (Tab.6).ConvLLaVA outperforms LLaVA-7B and 13B model on all 8 test splits.This demonstrates the benefits of higher resolution for grounding tasks.Our 7B model also surpasses 13B LLaVA model on all 8 benchmarks.",
            "score": 0.3672048864669093,
            "section_title": "Quantitative Results",
            "char_start_offset": 22388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 101
                },
                {
                    "start": 101,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 258
                },
                {
                    "start": 260,
                    "end": 337
                },
                {
                    "start": 337,
                    "end": 431
                },
                {
                    "start": 431,
                    "end": 467
                },
                {
                    "start": 467,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 580
                },
                {
                    "start": 580,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 782
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25341796875
        },
        {
            "corpus_id": "268384843",
            "title": "Can We Talk Models Into Seeing the World Differently?",
            "text": "Here we provide an overview of all used models from the main paper. \n\nQwen-VL-Chat (Bai et al., 2023b) Adds vision capabilities to Qwen-7B (Bai et al., 2023a). We set a repetition penalty of 1.2 for this model. \n\nQwen-VL Plus/Max (Qwen Team, 2024) \n\nAliBaba's proprietary larger variants of Qwen-VL-Chat. Access only via API. \n\nCogAgent (Hong et al., 2023) A special model for interaction with graphical user interfaces (GUIs) at high-resolution. \n\nCogVLM (Wang et al., 2023) Adds \"trainable visual expert module\" in LLM layers to combine vision and language. \n\nEmu2 (Sun et al., 2023a) The 37B model claims \"strong multi-modal in-context learning abilities\". \n\nInstructBLIP (Dai et al., 2023) Connects frozen vision encoders and LLMs through a trainable Q-Former. Uses Vicuna or FLAN-T5 as LLMs. \n\nLLaVA v1.5 (Liu et al., 2023a) Improvements of LLaVA with modifications on the image encoder, the projector, and task-specific data. Uses Vicuna-7/13B as LLM. \n\nLLaVA-NeXT (Liu et al., 2024) Successor of LLaVA v1.5 supporting higher resolutions through patching, and using better SFT training data for training, claiming \"improved reasoning, OCR, and world knowledge\" (Liu et al., 2024). The 34B version switches from Vicuna-7/13B to Nous Hermes 2 Yi 34B. \n\nMoE-LLaVA v1.5 (Lin et al., 2024) Variants of LLaVA v1.5 employing 4 sparsely activated Mixture-of-Experts (MoE), and smaller LLMs (Qwen, Phi-2, StableLM). \n\nLLaVA-RLHF (Sun et al., 2023c) Variants of LLaVA v1.5 aligned with Factually Augmented RLHF (Fact-RLHF) (Sun et al., 2023c).",
            "score": 0.36710656443447554,
            "section_title": "A OVERVIEW OF VLMS",
            "char_start_offset": 36999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 70,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 210
                },
                {
                    "start": 213,
                    "end": 247
                },
                {
                    "start": 250,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 446
                },
                {
                    "start": 449,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1539
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26611328125
        },
        {
            "corpus_id": "277065606",
            "title": "GuideDog: A Real-World Egocentric Multimodal Dataset for Blind and Low-Vision Accessibility-Aware Guidance",
            "text": "Models We evaluate a diverse set of vision-language systems, including open-source, proprietary, and Socratic models [55]. For open-source models, we consider LLaVA 1.6 [31], LLaVA-OneVision [25], Qwen-2.5-VL [2], Cambrian-1 [47], and Molmo [8]. LLaVA 1.6, LLaVA-OneVision, and Qwen-2.5-VL represent high-performing open-source approaches, Cambrian-1 features a distinctive multi-encoder architecture, and Molmo is notable for its transparency. Additionally, we fine-tune Qwen-2.5-VL using LoRA [16] on our silver labels to investigate the impact of fine-tuning on performance. Among proprietary models, we include GPT-4o [1] and Gemini 2.0 Flash [12], both recognized for their exceptional performance on general vision-language tasks. Finally, we implement a Socratic Model (SM) [55], employing a two-stage framework: first, converting visual inputs into intermediate textual descriptions, followed by textonly reasoning with the descriptions along with associated object information A i generated from off-the-shelf models as additional input (used in Sec. 2.2). For image caption generation, we use LLaVA-1.6, while reasoning is performed using Gemini 2.0 Flash or GPT-4. This approach enables us to assess the role of direct visual perception.",
            "score": 0.36695312458807233,
            "section_title": "Experiments",
            "char_start_offset": 16603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1248
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 121,
                    "matchedPaperCorpusId": "247922520"
                },
                {
                    "start": 169,
                    "end": 173,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "270703300"
                },
                {
                    "start": 781,
                    "end": 785,
                    "matchedPaperCorpusId": "247922520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1798095703125
        },
        {
            "corpus_id": "273098835",
            "title": "Contrastive Localized Language-Image Pre-Training",
            "text": "Ferret can consume a point, a bounding box, or a free-form referring. It designs a complicated visual sampler module that involves point sampling and kNN grouping. We suggest the readers refer to Figure 3  More specifically, we simply convert all types of spatial referring as boxes. As evidenced by Table 3 and Table 4, our Prompter can indeed be a much simpler alternative and may perform even better as it is more consistent with CLOC pre-training. \n\nSecond, we evaluate on general VQA tasks that do not consider extra spatial referring inputs. The pre-trained ViT of CLOC is a drop-in-replacement of CLIP ViT in two sets of experiments of LLaVA-1.5 (Liu et al., 2023) and LLaVA-NeXT (Liu et al., 2024). The main difference includes different supervised fine-tuning (SFT) datasets. Also, LLaVA-NeXT uses the AnyRes technique that decomposes an image into several subimages that are encoded independently with the ViT and concatenated together as the input for the decoder. LLaVA-1.5 by default freezes the ViT, while LLaVA-NeXT fine-tunes all parameters during SFT. Since the official LLaVA-NeXT is trained on some proprietary datasets that are not reproducible, we use the Open-LLaVA-NeXT repository. 8 Our experiments in Table 5 demonstrate CLOC not only improves general VQA tasks besides referring and grounding tasks, but also generalizes well for both LLaVA-1.5 and LLaVA-NeXT settings.",
            "score": 0.36672472297078723,
            "section_title": "CONCLUSION",
            "char_start_offset": 31173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1395
                }
            ],
            "ref_mentions": [
                {
                    "start": 653,
                    "end": 671,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34619140625
        },
        {
            "corpus_id": "269430328",
            "title": "PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning",
            "text": "We draw the performance of a recent work IG-VLM [16] and our attempts in Figure 5. IG-VLM achieves almost no difference when applying 7B, 13B, and 34B models of LLaVA-Next [28].In our attempts of with pooling features (the first column of Figure 5), the performance of LLaVA-Next 34B is even worse than its 13B LLaVA-Next model.For IG-VLM, the input video frames are combined to a grid view image, confined by the resolu-tion, leading to the unsatisfactory scaling ability.As for our attempts, we found a tendency of shorter generations with larger MLLMs, thus we owe the degradation to the quality of video-text data pairs, which undermines the generation ability of LLMs in MLLM models.",
            "score": 0.366550200247081,
            "section_title": "Failure Cases Analysis for Applying Image MLLMs",
            "char_start_offset": 16877,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 177,
                    "end": 328
                },
                {
                    "start": 328,
                    "end": 473
                },
                {
                    "start": 473,
                    "end": 688
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06805419921875
        },
        {
            "corpus_id": "277501771",
            "title": "Slow-Fast Architecture for Video Multi-Modal Large Language Models",
            "text": "We construct the training data mixture using open-source datasets. In the first pre-training stage, we combine 537k video caption samples from LLaVA-Video-178k (Zhang et al., 2024d) with 558k image captions from LLaVA-1.5 (Liu et al., 2024a). In the second stage, we mix pure text, image, and video instruction tuning datasets. Table 1 provides a detailed breakdown of the data sources and sample sizes. The primary components include 1.4M video data from LLaVA-Video-178k (Zhang et al., 2024d) and 1.2M image data from LLaVA-OneVision (Li et al., 2024a). Following VideoChat2 (Li et al., 2024b), we also incorporate their conversation data alongside with TGIF (Li et al., 2016), SthSthv2 (Goyal et al., 2017), Kinetics-710 (Li et al., 2023c), and Ego4D (Grauman et al., 2022) into the training  (Goyal et al., 2017) 40k Kinetics-710 (Li et al., 2023c) 40k CLEVR (Johnson et al., 2017) 20k VideoChat2 (Li et al., 2024b) 10k Ego4D (Grauman et al., 2022) 8k \n\nImage & Pure Text LLaVA-1.5 (Liu et al., 2024a) 665k LLaVA-OneVision (Li et al., 2024a) 1,231k Sum 3,467k mixture. For the second stage of all ablation studies, due to resource limitations, we use a reduced dataset comprising 665k instruction tuning samples from LLaVA-1.5 (Liu et al., 2024a) and 1.4M video instruction tuning samples from LLaVA-Video-178k.",
            "score": 0.3661973781208497,
            "section_title": "Implementation Details",
            "char_start_offset": 19381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1315
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 241,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 577,
                    "end": 595,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 661,
                    "end": 678,
                    "matchedPaperCorpusId": "6262415"
                },
                {
                    "start": 689,
                    "end": 709,
                    "matchedPaperCorpusId": "834612"
                },
                {
                    "start": 724,
                    "end": 742,
                    "matchedPaperCorpusId": "253581816"
                },
                {
                    "start": 796,
                    "end": 816,
                    "matchedPaperCorpusId": "834612"
                },
                {
                    "start": 834,
                    "end": 852,
                    "matchedPaperCorpusId": "253581816"
                },
                {
                    "start": 863,
                    "end": 885,
                    "matchedPaperCorpusId": "15458100"
                },
                {
                    "start": 901,
                    "end": 919,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 986,
                    "end": 1004,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1231,
                    "end": 1249,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11029052734375
        },
        {
            "corpus_id": "276903457",
            "title": "CASP: Compression of Large Multimodal Models Based on Attention Sparsity",
            "text": "LLaVA-Next-7B generally outperforms LLaVA1.5-7b and LLaVA1.5-13b by increasing the input image resolution, which provides approximately 3-4X more visual tokens compared to the LLaVA1.5 models [26]. As also discussed in the proposed method section, this potentially suggests higher sparsity and lower compression error. Thus, compared to the other models, a higher relative improvement of 141% and 78% is achieved by CASP GPTQ and CASP AQLM on LLaVA-Next-7B. Moreover, CASP QuIP# obtains an average relative improvement of 2.3%, while the theoretical upper bound with QuIP# is only 16.9%.",
            "score": 0.3658628763697048,
            "section_title": "Image-Language Understanding",
            "char_start_offset": 19980,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 587
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1856689453125
        },
        {
            "corpus_id": "273098584",
            "title": "Dog-IQA: Standard-guided Zero-shot MLLM for Mix-grained Image Quality Assessment",
            "text": "(Liu et al., 2024c), LLaVA-v1.5-13b (Liu et al., 2024c), LLaVA-Next (Liu et al., 2024b), mPLUG-Owl (Ye et al., 2023), mPLUG-Owl2 (Ye et al., 2024b), and mPLUG-Owl3 (Ye et al., 2024a). All models are tested using 5 words with the whole image as input, reflecting their fundamental zero-shot IQA capabilities. The results are shown in Table 6, from which we can draw the following conclusions. First, from the version's perspective, the trend shows that the higher the version is, the better the model's performance is. Second, when we consider different models, mPLUG-Owl3 demonstrates a clear performance advantage, and LLaVA-Next gains sub-optimal performance. Therefore, we choose mPLUG-Owl3 as our scoring model.",
            "score": 0.36579869664086484,
            "section_title": "ABLATION STUDY",
            "char_start_offset": 30636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 715
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 116,
                    "matchedPaperCorpusId": "258352455"
                },
                {
                    "start": 129,
                    "end": 147,
                    "matchedPaperCorpusId": "265050943"
                },
                {
                    "start": 164,
                    "end": 182,
                    "matchedPaperCorpusId": "271843557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057281494140625
        },
        {
            "corpus_id": "271329151",
            "title": "SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models",
            "text": "Text Generation benchmarks are shown in Table 3, where SF-LLaVA-34B outperforms all trainingfree baselines on average. First, we observe that SF-LLaVA consistently performs worse than LLaVA-NeXT-Image (Zhang et al., 2024b) on Detail Orientation (DO). This is because LLaVA-NeXT-Image takes more \"high-resolution\" input frames than ours (i.e., 32 frames with 12 \u00d7 12 tokens v.s. 10 frames with 12 \u00d7 24 tokens), thus is able to capture more spatial information. Second, SF-LLaVA takes advantage of the SlowFast design to cover a longer temporal context by using even fewer visual tokens (i.e., 4608 tokens v.s. 3680 tokens), thus excels in all other tasks, especially in Temporal Understanding (TU). Third, we observe that SF-LLaVA-34B is superior to most SFT methods (e.g, outperforming Video-LLaMA2 (Cheng et al., 2024) +0.1 score on TU and +0.31 score on CO), but only needs to catch up with LLaVA-NeXT-Video-DPO (Zhang et al., 2024b).",
            "score": 0.3657848276799457,
            "section_title": "MAIN RESULTS",
            "char_start_offset": 17293,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 936
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.177978515625
        },
        {
            "corpus_id": "277112739",
            "title": "LLaVA-UHD v2: an MLLM Integrating High-Resolution Semantic Pyramid via Hierarchical Window Transformer",
            "text": "We add more cases to analyze the behavior of our LLaVA-UHD v2. The capabilities are summarized as three aspects as follows. \n\nDense Perception. In Fig. 9, we visualize the performance of well-known MLLMs on dense perception tasks. Dense perception tasks require models to possess highly robust fine-grained perception capabilities to distinguish object boundaries within a large number of densely packed similar objects, thereby accurately locating the target and its boundaries to identify the target precisely. \n\nIt is evident that LLaVA-UHD v2 and GPT-4V accurately identify the beginning time of the television program 'H20 X5 Mop' (case A), the performance date of the show 'The 3 Mile' (case B), the duration of whole workouts (case C), and the prize of Stoli (case D), indicating highly robust fine-grained perception capabilities provided by our visual pyramid representation. In comparison, other models either fail to precisely locate the target (LLaVA-Next) or cannot distinguish the target from similar adjacent objects, limited in accurately completing dense OCR tasks (Mini-Gemini). \n\nFine-grained Perception. In Fig. 10, we visualized the performance of well-known MLLM on fine-grained perception tasks. These tasks require models to have robust fine-grained perception capabilities to detect the textures of small or blurry targets, thereby accurately locating and identifying small targets. \n\nCase C indicates that LLaVA-UHD v2 accurately identified the small green light, and the tiny number of duration time associated with green light, demonstrating that the introduction of high-frequency information in hierarchical features can handle small, blurry targets effectively. In contrast, other models can not find the small green light, or fail to accurately perform OCR tasks due to the text being too small or blurry (e.g., GPT-4V, LLaVA-Next, Mini-Gemini). \n\nThis capability is further demonstrated in cases A, B and D, where both LLaVA-UHD v2 and GPT-4V accurately identified the tiny number on the basketball jersey (case A), the blurry number on the very end of the bus (case B), and the time on the phone (case D), while LLaVA-Next and Mini-Gemini exhibited limitations.",
            "score": 0.36576231113947977,
            "section_title": "B.2.2 Case studies.",
            "char_start_offset": 36393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 123
                },
                {
                    "start": 126,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 512
                },
                {
                    "start": 515,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1407
                },
                {
                    "start": 1410,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1877
                },
                {
                    "start": 1880,
                    "end": 2195
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1287841796875
        },
        {
            "corpus_id": "276741696",
            "title": "Zero-Shot Defense Against Toxic Images via Inherent Multimodal Alignment in LVLMs",
            "text": "First, analyze the image for any harmful or unsafe content. If such content is detected, please respond with a safe response; if not, proceed to fulfill the request. In this experiment, we evaluated our method on three common benchmarks MM-Vet (Yu et al., 2023), ScienceQA (Lu et al., 2022) and POPE (Li et al., 2023). Table 6 indicates that employing SafeCLIP during inference produces comparable results across all three benchmarks for both Llava-1.5 and Llava-Next. Similarly, finetuning Llava-1.5 with Safe-CLIP does not yield any significant performance change. In contrast, finetuning Llava-Next with SafeCLIP results in a modest performance decline, which is anticipated given that Llava-Next was predominantly finetuned on an alternative dataset optimized for enhanced performance. \n\n.",
            "score": 0.3656241752452811,
            "section_title": "Template 3",
            "char_start_offset": 25682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 793
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10357666015625
        },
        {
            "corpus_id": "274982597",
            "title": "Personalized Large Vision-Language Models",
            "text": "We implement a personalized LVLM tailored for models that accept multiple images, using LLaVA-OneVision [21] as the baseline. Specifically, we adopt the 7B model from LLaVA-OneVision and build Aligner on top of it. The architecture of Aligner aligns closely with LLaVA [23]. \n\nDuring training, in addition to text-only QA and recognition QA tasks similar to those in LLaVA, we incorporate a multiple-image QA task. In this task, given a reference image and multiple query images, the goal is to determine which query image corresponds to the reference image. For LLaVA-OneVision [21], we guide the model using the template: \"\u27e8IMAGE TOKEN\u27e9 This is the photo of \u27e8sks\u27e9 + instruction.\" Evaluation results, as shown in Table 1, are based on our test dataset. Our approach significantly outperforms LLaVA-OneVision, leveraging the entire reference image as the context embedding while requiring far fewer context tokens (16 compared to 730). We also provide an example with a multi-image personalization setting, which is shown in Figure 6.",
            "score": 0.3656197149259319,
            "section_title": "A. Multi-image personalization",
            "char_start_offset": 31964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 274
                },
                {
                    "start": 277,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1034
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494384765625
        },
        {
            "corpus_id": "277452239",
            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
            "text": "In Fig. 3 we report the classification F1-score of radio-llava fine-tuned models obtained on radio benchmarks for each class and overall (labelled as 'AVG'), compared to the base LLaVA-OneVision 7B model (solid red histograms). Blue histograms represent models fine-tuned on the Q&A dataset, using either deep/shallow full fine-tuning or LoRA fine-tuning. Orange histograms correspond to models fine-tuned on the combined Q&A and caption datasets. With the exception of B6, we observe a general improvement in performance when fully fine-tuning the base model. (f ) B6 Figure 2. Classification F1-scores obtained with VLMs of different sizes (0.5B, 2B, 3.1B, 7B, 8B, 72B) in zero-shot mode over B1-B6 evaluation benchmarks. We report the F1-score for individual classes, as well as the class-averaged F1-score (labelled as 'AVG'). LLaVA, TinyLLaVA, Qwen2VL, and InternVL models are respectively shown with blue, green, red, and orange histograms. \n\nThe performance boost is particularly notable for B1 (extended/diffuse source detection) and B3 (radio galaxy classification), where average classification scores improve by more than 20-30%. For the remaining tasks, the improvement is more moderate (\u223c10%). In contrast, LoRA fine-tuning leads to a clear improvement only in B3 and B5 tasks, with limited gains elsewhere. Deeper fine-tuning results in a modest improvement of only a few percentage points across all tasks, for both full and LoRA fine-tuning strategies. Fine-tuning on caption data (orange histograms) is observed to slightly decrease performance on radio benchmarks. This is somewhat expected, as all radio benchmarks are based on Q&A tasks rather than descriptive tasks. Caption data, on the other hand, have a positive impact on non-radio benchmarks, as discussed in the next section. Overall, the achieved metrics remain well below those obtained using a pure vision encoder model specialized for each task.",
            "score": 0.3653542553614899,
            "section_title": "Fine-tuning performance 4.3.1 Radio benchmarks",
            "char_start_offset": 31573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 946
                },
                {
                    "start": 949,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1926
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.169677734375
        },
        {
            "corpus_id": "276929180",
            "title": "EAZY: Eliminating Hallucinations in LVLMs by Zeroing out Hallucinatory Image Tokens",
            "text": "B.1 Model Details. \n\nLLaVA-1.5. The LLaVA-1.5 Model [Liu et al., 2024c] leverages the linear projector layer to align the vision and text modalities, with 576 image tokens. It adopted the pre-trained vision transformer from CLIP [Radford et al., 2021] and the pre-trained language model as Vicuna [Chiang et al., 2023]. \n\nShikra. The Shikra Model [Chen et al., 2023] introduces referential dialogue capabilities in multimodal large language models (MLLMs) by handling spatial coordinate inputs and outputs in natural language. It utilizes a vision encoder, an alignment layer, and a Vicuna-based language model without requiring extra vocabularies, position encoders, pre-/post-detection modules, or external plug-ins. \n\nThe model enables interaction with images through natural pointing and location-aware responses, supporting tasks such as Referring Expression Comprehension (REC), PointQA, Image Captioning, and Visual Question Answering (VQA) with promising performance. \n\nLLaVA-Next. The LLaVA-Next Model [Liu et al., 2024b] enhances multimodal capabilities by increasing the input image resolution up to 672 \u00d7 672 pixels, supporting three aspect ratios. It utilizes an improved visual instruction tuning data mixture to bolster visual reasoning and OCR capabilities. The model employs a pre-trained vision transformer from CLIP and integrates with advanced language models like Vicuna and Mistral.",
            "score": 0.3651970156306763,
            "section_title": "B Extended Experiment",
            "char_start_offset": 33972,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 21,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 319
                },
                {
                    "start": 322,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1404
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 71,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 229,
                    "end": 251,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.433349609375
        },
        {
            "corpus_id": "265466206",
            "title": "ChartLlama: A Multimodal LLM for Chart Understanding and Generation",
            "text": "To elucidate our training strategies, we provide some clarification about the modifications in LLaVA-1.5 [17], and introduce its essential model architectures. \n\nVision encoder: LLaVA-1.5 incorporates CLIP's vision encoder [25]. The primary distinction is that LLaVA-1.5 employs ViT-L/14@336px, while LLaVA uses ViT-L/14@224px. Another notable alteration concerns the image processor. Eschewing traditional center cropping, LLaVA-1.5 adopts padding as an image pre-processing technique, ensuring that all information in the provided image can be apprehended. Projection layer: In LLaVA-1.5, the initial single linear layer is substituted with a two-layer MLP, resulting in improved performance. \n\nLora Layer: Based on experiments in [17,19], implementing Lora [6] layers is sufficient to achieve performance comparable to full fine-tuning strategies. For the original LLaVA [18], Lora layers with a Lora rank of 64 suffice, whereas for LLaVA-1.5 [17], the Lora rank needs to exceed 128.",
            "score": 0.36517913129969826,
            "section_title": "A. Model architecture",
            "char_start_offset": 28238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 162,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 986
                }
            ],
            "ref_mentions": [
                {
                    "start": 760,
                    "end": 763,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51611328125
        },
        {
            "corpus_id": "276408016",
            "title": "VAQUUM: Are Vague Quantifiers Grounded in Visual Data?",
            "text": "In Figure 6 we show the patterns of the VLMs across all predictors for the prompt that does not target the amount. The question presented to the models is \"How would you describe the image?\", and we extract log probabilities for expressions of the form \"There are [QUANT] [OBJECT] in the image\" (unchanged from those used in \u00a74). \n\nFor LLaVA-NeXT and LLaVA-OneVision, the two models observed in \u00a74 to have the highest correlation with human ratings, we now find that patterns are the same across all quantifiers. We now find a \"layered\" or \"stacked\" pattern that is indicative of a bias towards a specific quantifier: while LLaVA-NeXT and LLaVA-OneVision tend towards always responding with a lot of, Instruct-BLIP and Molmo favor the unquantified statement.",
            "score": 0.3651625076563621,
            "section_title": "C.1 Targeting amounts",
            "char_start_offset": 31690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 758
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0948486328125
        },
        {
            "corpus_id": "277113272",
            "title": "VisNumBench: Evaluating Number Sense of Multimodal Large Language Models",
            "text": "Table 2 presents the results for various MLLMs on the VisNumBench-Synthetic dataset. Among the open-source models, Qwen2.5-VL-72B achieves the best performance, with an average accuracy of 58.46%. InternVL2.5-38B, InternVL2.5-78B, and LLaVA-v1.6-34B also demonstrate strong performance, each achieving either the best or the second-best accuracy in at least two tasks. LLaVA-v1.6-34B attains the highest accuracy in angle and depth estimation; however, its overall average accuracy is only 44.31%. \n\nLLaVA-Onevision-72B also performs well, achieving the highest accuracy in length estimation at 61.33%. In general, models with larger parameter sizes tend to exhibit superior performance, aligning with the intuition that larger models can better capture complex numerical relationships and fine-grained visual patterns. \n\nIn the API-based models, Gemini 2.0 Flash demonstrates the best performance, achieving an average accuracy of 57.57%. In contrast, GPT-4o and Gemini 1.5 Pro exhibit comparable performance, albeit with lower average accuracies. Gemini 1.5 Flash yields the weakest performance, with an average accuracy of 33.33%. Notably, certain opensource models perform on par with or even surpass proprietary models, suggesting that the disparity in numerical reasoning capabilities between open-source and closed-source models is minimal.",
            "score": 0.36513701222194167,
            "section_title": "Performance on VisNumBench-Synthetic",
            "char_start_offset": 13507,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 819
                },
                {
                    "start": 822,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1347
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06683349609375
        },
        {
            "corpus_id": "273098155",
            "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
            "text": "We also provide Kendall's Tau results in Appendix C.2 to assess model-level ranking, which reveal similar patterns and conclusions. \n\nFigure 2 presents a qualitative comparison between LLaVA-Critic and other LMM evaluators. While LLaVA-OneVision often assigns fixed scores (e.g., \"Tie\" on WildVision-Bench or \"6\" on MMHal-Bench), LLaVA-Critic produces more diverse and balanced scores that closely align with GPT-4o, leading to consistent rankings of response models. Notably, even without training on critic data, LLaVA-OneVision-72B demonstrates modellevel rankings that partially align with GPT-4o across four multimodal benchmarks.",
            "score": 0.3649553091565662,
            "section_title": "In-domain Pointwise Scoring",
            "char_start_offset": 22435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 134,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 635
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18701171875
        },
        {
            "corpus_id": "271854959",
            "title": "GlyphPattern: An Abstract Pattern Recognition for Vision-Language Models",
            "text": "We evaluate six models,2 including both open and closedsource state-of-the-art VLMs: \u2022 LLaVA-NeXT (Liu et al. 2024) is an open-source multimodal model in the LLaVA framework that uses Mistral-7B-Instruct-v0.2 as its language component. \u2022 InstructBLIP (Dai et al. 2024) is an open-source visionlanguage model. It uses BLIP-2 for visual instructiontuning with Vicuna-7b as its language component.",
            "score": 0.36462135339502627,
            "section_title": "Models",
            "char_start_offset": 11295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 394
                }
            ],
            "ref_mentions": [
                {
                    "start": 251,
                    "end": 267,
                    "matchedPaperCorpusId": "258615266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10211181640625
        },
        {
            "corpus_id": "276903670",
            "title": "PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training",
            "text": "Implementation Details To ensure the reliability and credibility of the results, we conducted experiments on the open-source and widely-used LLaVA1.5 dataset. By utilizing the LLaVA1.5 dataset, we fully replicated LLaVA1.5 training process and results, and use it as the baseline for our experiments. To ensure fairness, we apply perturbations directly to the LLaVA1.5 dataset rather than incorporating additional data. We conduct experiments by generating perturbed text using GPT-4o on the 160k data related to the VQA task in the dataset. \n\nComparative Methods We selected three representative methods, each addressing hallucination from different perspectives and demonstrating strong performance: the training-based RLAIF-V (Yu et al., 2024), the decoding strategy OPERA (Huang et al., 2024) and VCD (Leng et al., 2024). For RLAIF-V, we use the open-source model weights of RLAIF-7B, which were fine-tuned on LLaVA1.5 (Liu et al., 2024b). Notably, the reward model used in this setup is LLaVA-Next 34B (Liu et al., 2024a), which may transfer some of LLaVA-Next 34B's capabilities into RLAIF-7B, potentially making the comparison with other experimental setups less fair. The hyperparameters for OPERA and VCD are provided in Section A.5 to support the reproducibility of our results. We use beam search as the default decoding strategy, with N beams set to 5. \n\nWe additionally evaluate six best multimodal models available today, Ovis1.6 (Lu et al., 2024b), Qwen2-VL (Wang et al., 2024), LLaVA-OneVision (Li et al., 2024), InternVL2 (OpenGVLab, 2024), Idefics3 (Laurenc \u00b8on et al., 2024), and MiniCPM-2.6 (Hu et al., 2024), to further verify the effectiveness of our proposed metric.",
            "score": 0.36364228202606497,
            "section_title": "EXPERIMENTS SETTINGS",
            "char_start_offset": 22056,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1364
                },
                {
                    "start": 1367,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1689
                }
            ],
            "ref_mentions": [
                {
                    "start": 776,
                    "end": 796,
                    "matchedPaperCorpusId": "265498818"
                },
                {
                    "start": 805,
                    "end": 824,
                    "matchedPaperCorpusId": "265466833"
                },
                {
                    "start": 923,
                    "end": 942,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18115234375
        },
        {
            "corpus_id": "274822932",
            "title": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence",
            "text": "We conduct experiments on three of the most representative LVLMs, i.e. InstructBLIP ( Select the heads as H l \u25b7 Equation 76: \n\nReinforce the heads in H l \u25b7 Equation 7 8: end for 2023), LLaVA-1.5-7b (Liu et al., 2024b), and LLaVA-NeXT-7b (Liu et al., 2024c). LVLMs are typically composed of an image encoder, a connector, and an LLM. Specifically, LLaVA-1.5-7b and LLaVA-Next-7b leverage MLP to align the visual and textual embedding space and feed all the image tokens from the image encoder to the LLM. In contrast, InstructBLIP uses Q-Former to reduce the number of image tokens before passing them to the LLM. LLaVA-NeXT differs from LLaVA-1.5 by offering a higher image resolution, allowing it to capture more visual details.",
            "score": 0.3634972460594693,
            "section_title": "LVLMs",
            "char_start_offset": 14358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 124
                },
                {
                    "start": 127,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 729
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 217,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2059326171875
        },
        {
            "corpus_id": "271855335",
            "title": "Revisiting Multi-Modal LLM Evaluation",
            "text": "LLaVA-NeXT [24] is an improved version of LLaVA 1.5, with a focus on enhanced visual reasoning, optical character recognition (OCR), and multi-modal document understanding. LLaVA-NeXT scales the input image resolution of input images by 4\u00d7, up to 1344 \u00d7 336 compared to 336 \u00d7 336 in LLaVA 1.5 to enhance its ability to grasp finer-grained visual cues. LLaVA-NeXT is also trained on a more diverse and realistic visual instruction-tuning dataset (ShareGPT-4V and LAION-GPT-V), as well as a range of OCR, document, and chart datasets. We evaluate the 7B parameter version of LLaVA-NeXT. \n\nGPT-4o/GPT-4V [25,26] are closed-weight MLLMs created by OpenAI that enable users to leverage the capability of GPT-4 scale LLMs to analyze visual inputs. GPT-4V is a powerful generalist multi-modal model and can process arbitrarily interleaved image-text data. GPT-4V can perform many visual-language tasks well, including spatial understanding, object localization, and object counting [26]. GPT-4o is reportedly an end-to-end text, vision, and audio multi-modal model, where multi-modal tokens are processed within the same network. GPT-4o has also been reported to improve linguistic and multi-modal understanding. Given that these are closed-source MLLMs, we use the API provided by OpenAI for our evaluations.",
            "score": 0.36326457561378367,
            "section_title": "Introduction",
            "char_start_offset": 3542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1302
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.724609375
        },
        {
            "corpus_id": "275324083",
            "title": "MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders",
            "text": "Additionally, we incorporate Mixture-of-LoRA-Experts (MoLE) to prevent knowledge confusion. \u2022 Our framework has been applied to LLaVA and LLaVA-NeXT, achieving state-of-the-art performance across multiple benchmarks.",
            "score": 0.36297008198438374,
            "section_title": "Introduction",
            "char_start_offset": 4401,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 216
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2037353515625
        },
        {
            "corpus_id": "271244415",
            "title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
            "text": "Table 4: Comparison on the impact of additional visual input. All experiments are performed with LLaVa-NeXT as the underlying model, either providing or excluding a plot of the best previous function in the prompts (respectively ICSR-V and ICSR columns). We report the averages with their errors.",
            "score": 0.36295224654613467,
            "section_title": "A.3 Comparison of Text-Only and Vision-Language Models",
            "char_start_offset": 37020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 296
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2421875
        },
        {
            "corpus_id": "276928791",
            "title": "Seeing and Reasoning with Confidence: Supercharging Multimodal LLMs with an Uncertainty-Aware Agentic Framework",
            "text": "Baselines. We select the following strong baseline for comparison. \n\n\u2022 Base model: LLaVA-OneVision-Qwen2-7b (LLaVa-OV) [20]. We selected this base model to build the SRICE pipeline due to its excellent performance and support for multiple image inputs, which is essential for our implementation. We refer to this baseline as LLaVA-ov-7B. \u2022 LLaVA-1.5-13B [25]. LLaVA-1.5-13B is a popular baseline for VQA tasks with an increased parameter count. \u2022 Visual CoT (VisCoT) [31]. VisCoT is a state-of-the-art (SOTA) approach that fine-tunes an MLLM on a manually curated dataset, enabling it to automatically identify key areas in an image and facilitate CoT reasoning. We fine-tune the LLaVa-OV model on the VisCoT dataset following the procedure in [31] and denote the resulting model as LLaVA-ov-7B-VisCoT. Additionally, we compare our method with the vicuna-7B-VisCoT model released in [31]. \u2022 SPHINX [23] Tool selection. We choose popular lightweight tools: SEEM [50] for segmentation and Yolov11 [15] for object detection. We also try other tools in the ablation study. SRICE setting We set \u03b1 = 0.1 for both CP seg and CP det , following prior work [19,33]. Empirically, we set the parameter p to 0.9 for the prediction set-based uncertainty estimation. To compute the nonconformity scores for both tools, we use the COCO-2017 validation dataset [22], which contains 5,000 images with annotations for both segmentation and object detection. The time consumption for calculating the nonconformity score on the calibration set is 35 minutes and 8 minutes for the segmentation and object detection tools, respectively. For each test image, the average time taken for calibration is about 34ms and 0.2ms, respectively. The hardware and software platforms involved in the experiment are given in Appendix B.",
            "score": 0.3629228567882411,
            "section_title": "Experimental setting",
            "char_start_offset": 19452,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 66
                },
                {
                    "start": 69,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1801
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "258108410"
                },
                {
                    "start": 1152,
                    "end": 1155,
                    "matchedPaperCorpusId": "267627532"
                },
                {
                    "start": 1345,
                    "end": 1349,
                    "matchedPaperCorpusId": "14113767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.250732421875
        },
        {
            "corpus_id": "277112989",
            "title": "Visual Position Prompt for MLLM based Visual Grounding",
            "text": "Inputs. For the visual encoder, we follow the same preprocessing steps as LLaVA: padding each image based on its longer side and then applying the CLIP/L-336 processor, which resizes the padded image to 336 \u00d7 336 pixels while preserving the original aspect ratio. For the input to the local VPP generator (DETR-ResNet101 in our model), we resize and pad the images to 640 \u00d7 640 pixels without any additional data augmentations. Training details. We initialize our model with pre-trained LLaVA-v1.5-7B parameters and fine-tune it using the AdamW optimizer with a cosine annealing scheduler. The learning rates are set as follows: 2e-5 for the LLM (Vicuna-v1.5-7B), 2e-4 for the global VPP, 2e-5 for the local VPP generator, and 2e-4 for the projector that maps the local VPP to the LLM feature space. Unless otherwise specified, our default configuration refers to the 7B model (VPP-LLaVA-7B). Following recent works such as Cambrian-1 [21], Eagle [67], and LLaVA-NeXT [4], which demonstrate that unfreezing the visual encoder is greatly beneficial to vision-centric tasks, we adopt the same strategy, setting the learning rate of the visual encoder to 2e-6. Moreover, following Ferret, we fine-tune our model on the collected VPP-SFT dataset for 3 epochs, and the global batch size is set to 64, which takes roughly 30 hours on 8\u00d7 NVIDIA A800 GPUs (80GB) for the whole training process. Additionally, the width of the binary mask w and the tradeoff parameter \u03b1 in Eq. 2 are set to 30 pixels and 0.95 by default, respectively, with more details available in the Section IV-G7.",
            "score": 0.36243595526554556,
            "section_title": "B. Implementation Details",
            "char_start_offset": 19563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 8,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1575
                }
            ],
            "ref_mentions": [
                {
                    "start": 935,
                    "end": 939,
                    "matchedPaperCorpusId": "270703300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17333984375
        },
        {
            "corpus_id": "273346680",
            "title": "MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection",
            "text": "We compare the performance of over a dozen models, including commercial APIs, interleaved MLLMs, industrial MLLMs, and vision-centric MLLMs, as shown in Table 2. All models outperform the random baseline. The open-source models perform the best, with the average accuracy of the GPT-4o and Gemini-1.5-pro models reaching 74.9% and 73%, respectively. However, their cost-efficient counterparts, GPT-4o-mini and Gemini-1.5-flash, only achieved 66.3% and 68.9%, respectively, falling short of the best open-source model, InternVL2-76B, which achieved 70.8%. AnomalyGPT performs poorly overall, primarily due to its training on the IAD task in a fixed question-and-answer format, leading to severe overfitting issues. It demonstrates decent performance in anomaly discrimination because we specifically adapted the question format to suit its training. Similarly, the vision-centric MLLMs, Cambrian-1 and SPHINX, do not exhibit superior performance on the fine-grained visual tasks of MMAD, likely due to their foundational language models not being advanced enough. Among the general open-source MLLMs, earlier models like Qwen-VL-Chat and LLaVA-1.5 underperform compared to newer models like LLaVA-OneVision and MiniCPM-V2.6, indicating that advancements in general capabilities benefit performance on IAD tasks. MMAD uses a default 1-shot format, providing a normal image for comparison with the test image. Thus, multi-image understanding, especially image comparison, is crucial, while LLaVA-NEXT-Interleave, trained for this, does not perform outstandingly. LLaVA-NeXT-34B and InternVL2-76B, due to their larger scales, achieve the top two performances among open-source models, highlighting the importance of model size. \n\nHuman evaluation. We conduct a preliminary human evaluation using 177 examples randomly sampled from the entire benchmark. Eight evaluators were divided into two groups: 3 industrial anomaly detection researchers as experts and 5 ordinary participants. As shown in",
            "score": 0.3624291294898154,
            "section_title": "EXPERIMENTAL RESULTS",
            "char_start_offset": 19035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1723
                },
                {
                    "start": 1726,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 1990
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14013671875
        },
        {
            "corpus_id": "271329151",
            "title": "SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models",
            "text": "Experimental Settings. We perform all experiments on a system with 8 Nvidia A100 80G graphics cards. SF-LLaVA is built upon LLaVA-NeXT (Liu et al., 2024) 7B and 34B models. We use their pre-trained weights available on HuggingFace4 . To deal with long sequences, we follow LLaVA-NeXT-Video (Zhang et al., 2024b) to apply the rotary position embedding (RoPE) (Su et al., 2024), and use the scaling factor of 2, which doubles the context length to 8192 tokens. \n\nInput and Model Settings. SF-LLaVA takes as inputs a video with arbitrary size and length, and uniformly samples N = 50 frames as key frames. The key frames are resized to 336 \u00d7 336, and the visual encoder (i.e., OpenAI's CLIP-L-14) will output 24\u00d724 tokens for each of them. For the Slow pathway, we uniformly select N slow = 10 frame features from F v and pool their extracted features to 10 \u00d7 12 \u00d7 24; for the Fast pathway, we use features of all frames (i.e., N fast = N = 50) and pool their extracted features to 50 \u00d7 4 \u00d7 4. Thus, SF-LLaVA uses 10 \u00d7 12 \u00d7 24 + 50 \u00d7 4 \u00d7 4 = 3680 visual tokens in total, and we choose this as the maximum number since the inference on the SF-LLaVA-34B model already reaches 80G GPU memory. The SlowFast video tokens are then concatenated with the text tokens as inputs to the LLM.",
            "score": 0.36109325906984463,
            "section_title": "IMPLEMENTATION DETAILS",
            "char_start_offset": 14703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1277
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 375,
                    "matchedPaperCorpusId": "233307138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2137451171875
        },
        {
            "corpus_id": "271404168",
            "title": "How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?",
            "text": "ChatGPT, Copilot, and Gemini are developed by proprietary companies, whereas LLaVA-NeXT is an open-source multimodal LLM. Notably, at the time of our experiments, LLaVA-NeXT has achieved state-of-the-art results across various multimodal LLM benchmarks among open-source LLMs. Due to hardware constraints, our experiments were conducted with the 7B parameter variant of LLaVA-NeXT, despite the more advanced state-of-the-art 34B parameter model. Nevertheless, including LLaVA-NeXt in our evaluation covers an important landscape of LLM research. The LLaVA-NeXt model was run on a server with an Nvidia Titan RTX 24GB graphical processing unit. On the other hand, the proprietary models were evaluated via their Application Programming Interfaces (APIs). We have made the program code of the experiments publicly available on the OSF 1 . \n\nDataset: Our study compiled an evaluation dataset from previous research on misleading charts circulated on the internet [24]. The dataset was collected through search engines and social media. The chart images are annotated with the issues identified from the original web page or social media post. A total of 74 unique chart issues were identified in the study, providing a diverse sample of misleading charts that the general public may encounter. \n\nIn the initial phase of our exploratory study, we focused on the five most frequently identified issues: (1) Truncated Axis, (2) 3D Chart, (3) Missing Title, (4) Dual Axis, and (5) Misrepresentation. We subsequently expanded to include the ten most common issues in Experiment Two by including (6) Missing Axis Title, (7) Missing Legend, (8) Inconsistent Tick Intervals, (9) Not Data, and (10) Selective Data. In the third experiment, the study was further broadened to cover up to 21 issues due to a tie for the 20th spot, with both the 20th and 21st issues appearing an equal number of 25 times in the dataset. These additional issues included (11) 21) Inconsistent Binning Size.",
            "score": 0.36104113760224255,
            "section_title": "METHODOLOGY",
            "char_start_offset": 18407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 960,
                    "end": 964,
                    "matchedPaperCorpusId": "248266819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1070556640625
        },
        {
            "corpus_id": "273482195",
            "title": "Zero-shot Action Localization via the Confidence of Large Vision-Language Models",
            "text": "If video-level labels are available (if the set of action classes for a given video is constrained), our method enjoys strong performance gains. We also note that using LLaVA-OneVision as the LVLM in our setup performs marginally better than using Pixtral as LVLM.",
            "score": 0.3610114184810955,
            "section_title": "Zero-shot Action Localization",
            "char_start_offset": 22996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 264
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.314453125
        },
        {
            "corpus_id": "277043430",
            "title": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models",
            "text": "Video LLMs. With the rapid advancement of LMMs [1,6,32] and MLLMs [2,15,[19][20][21]33], there has been growing interest in Video LLMs. Existing Video LLMs can be broadly categorized based on how they process video tokens: general Video LLMs and Video LLMs with trainingtime compression. General Video LLMs [5,14,17,18,35,43] directly process raw video tokens or apply pooling. Video-LLaVA [17] leverages shared projection layers to obtain unified visual representations. LLaVA-OneVision [14] demonstrates strong video understanding through task transfer from images. LLaVA-Video [43] creates a high-quality synthetic dataset for video instruction-following. To better capture the spatiotemporal structure of video, some models [35,43] introduce additional designs for video positional information. Qwen2-VL [35] propose M-RoPE to decompose rotary embedding into temporal, height, and width components. LLaVA-Video 1 introduces newline tokens to distinguish spatial and temporal positions effectively. \n\nVideo LLMs with training-time compression [13,22,28,29,37,45] aim to significantly reduce the number of video tokens, enabling longer video sequences. PLLaVA [37] introduces an adaptive average structure pooling to extend image LLMs. Video-ChatGPT [22] extracts both spatial and temporal features through temporal and spatial pooling respectively. Chat-UniVi [13] progressively clusters visual tokens and provides multi-scale features. MovieChat [29] introduces a memory management mechanism to enhance long video understanding. LongVU [28] employs crossmodal query and inter-frame dependencies to adaptively reduce video redundancy. Apollo [45] explores scaling consistency and uses the Perceiver Resampler [12]. \n\nHowever, general Video LLMs remain the dominant paradigm, with LLaVA-OneVision being widely adopted due to its adaptability and superior performance. Moreover, Video LLMs with training-time compression often exhibit lower performance compared to their general counterparts.",
            "score": 0.3608709337475924,
            "section_title": "Related Work",
            "char_start_offset": 4713,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1717
                },
                {
                    "start": 1720,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 69,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 69,
                    "end": 72,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 72,
                    "end": 76,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1046,
                    "end": 1050,
                    "matchedPaperCorpusId": "265157455"
                },
                {
                    "start": 1050,
                    "end": 1053,
                    "matchedPaperCorpusId": "259108333"
                },
                {
                    "start": 1056,
                    "end": 1059,
                    "matchedPaperCorpusId": "260333927"
                },
                {
                    "start": 1252,
                    "end": 1256,
                    "matchedPaperCorpusId": "259108333"
                },
                {
                    "start": 1363,
                    "end": 1367,
                    "matchedPaperCorpusId": "265157455"
                },
                {
                    "start": 1450,
                    "end": 1454,
                    "matchedPaperCorpusId": "260333927"
                },
                {
                    "start": 1712,
                    "end": 1716,
                    "matchedPaperCorpusId": "232110866"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.451171875
        },
        {
            "corpus_id": "277313863",
            "title": "ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models",
            "text": "We present the ablations on STKit introduced in Sec. 3.2 and 3.3 to discuss Q1. In Tab. 6, we compare the baseline performance of LLaVA-OneVision [19] against models fine-tuned with different data configurations. Incorporating data made with labeled videos significantly improves the average performance from 27.9% to 51.1%. Furthermore, using pseudo-labeled data generated from unlabeled videos further enhances performance by 8.7% improvement, highlighting the effectiveness of our 4D reconstruction-based pseudo-labeling pipeline.",
            "score": 0.3608679371278809,
            "section_title": "Data composition analysis of STKit",
            "char_start_offset": 23464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 533
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.252685546875
        },
        {
            "corpus_id": "273482814",
            "title": "Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers",
            "text": "We extensively evaluate the effectiveness of our method with different language towers. As shown in Figure 7, replacing the original Vicuna-7B-v1.5 language model with Vicuna-13B-v1.5 (Zheng et al., 2024), Meta-Llama-3-8B-Instruct (Dubey et al., 2024), andMistral-7B-Instruct-v0.2 (Jiang et al., 2023), Victor remains highly effective and significantly outperforms the two baseline methods. For both Meta-Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2, Victor demonstrates minimal performance drop and a slow decay in performance as the number of visual tokens decreases. Notably, for these two models, when the number of visual tokens is reduced by half, the method shows no performance degradation at all. \n\nWe further demonstrate the performance of our method on a different vision-language model design: LLaVA-NeXT (LLaVA-v1.6) (Liu et al., 2024a). LLaVA-NeXT follows a similar architecture to LLaVA-v1.5 but increases the number of visual tokens from 576 to 2,880 by incorporating different aspect ratios, enhancing the model's capabilities. Additionally, LLaVA-NeXT utilizes Qwen2-7B-Instruct (Yang et al., 2024) as its language tower, benefiting from its extended context length. In our experiments, we reduce the number of visual tokens to 512, 256, 128, 64, 32, and 16. As indicated in Figure 7d, our method remains highly effective in the LLaVA-NeXT setting, consistently outperforming both FastV and the Perceiver Resampler. We show the results of the ablation study on which layer to drop the visual tokens (hyperparameter k) in Figure 8. In terms of throughput improvement, it is clear that the earlier we drop visual tokens, the more efficient the model becomes. For lower-layer numbers, such as k = 1 or k = 2, the model's efficiency significantly increases, with throughput reaching nearly a 4\u00d7 improvement.",
            "score": 0.36080638970113654,
            "section_title": "DIFFERENT LANGUAGE TOWERS",
            "char_start_offset": 23269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1825
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 204,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 834,
                    "end": 853,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343505859375
        },
        {
            "corpus_id": "267335231",
            "title": "From Training-Free to Adaptive: Empirical Insights into MLLMs' Understanding of Detection Information",
            "text": "D.1 How LLaVA-1.5 Represents Other MLLMs? \n\nIn the main body of our paper, we exclusively select LLaVA-1.5 1 for experimentation, considering it representative of the majority of state-of-the-art models. In this section, we will illustrate this choice from the following two aspects: \n\n(1) The representativeness of LLaVA-1.5. We choose LLaVA-1.5 as we are in a highly dynamic field and it is representative enough of most state-of-the-art MLLMs. The advanced MLLMs typically consist of three main modules: an image encoder, an input projector, and a LLM Backbone. LLaVA-1.5 adheres to this structure. \n\nImages are first processed through the image encoder and the input projector. Most advanced MLLMs typically include a dedicated branch like this for processing image features into analogous image token sequences. Next, the image tokens are concatenated with text tokens that represent input sentences and fed into the LLM. Specifically, the text tokens representing our detection information can be directly concatenated with the extracted image tokens and used during MLLM's training and inference. In other words, as long as the MLLM conforms to this structure, the additional detection information can be processed similarly before being inputted into the LLM, specifically being concatenated with image features at the embedding level. The way they are infused into the backbone LLM is similar. Therefore, the results of experiments conducted on LLaVA-1.5 can be applied to other MLLMs with similar structures. \n\nFurthermore, LLaVA-1.5 has proven to be highly successful, spawning numerous outstanding works. We conduct our study based on LLaVA-1.5, enabling the application of our experimental findings to the subsequent works of LLaVA-1.5. Consequently, we are positioned to contribute to the open-source community. \n\n(2) The empirical support on Qwen-VL. To better illustrate the versatility of our work, we also conduct experiments on another MLLM, Qwen-VL.",
            "score": 0.3605834149316388,
            "section_title": "D Model Architecture Rationale",
            "char_start_offset": 39759,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 44,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 1969
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53515625
        },
        {
            "corpus_id": "277955804",
            "title": "VideoPASTA: 7K Preference Pairs That Matter for Video-LLM Alignment",
            "text": "We compare VideoPASTA with (1) the original foundation models listed above, (2) other stateof-the-art models, and (3) off-the-shelf models enhanced via preference optimization. Table 1 presents the evaluation results. Figure 3 shows qualitative examples of how VideoPASTA improves spatial, temporal, and cross-frame reasoning. \n\nVideoPASTA enhances all foundation models. VideoPASTA performs well across various foundation models, showing strong generalizability and consistent performance improvements. For instance, VideoPASTA combined with Qwen2.5-VL achieves top scores on TempCompass, Per-ceptionTest, MLVU, and LongVideoBench. Similarly, VideoPASTA with LLaVA-OneVision attains the highest score on NeXTQA. In addition, VideoPASTA with InternVL2.5 achieves the best result on MVBench and improves the VideoMME score by 4.1%. These results show that VideoPASTA's targeted alignment helps a wide range of Video-LLMs. In contrast, simple supervised fine-tuning (SFT) using only aligned responses leads to only small improvements. This highlights the importance of training with adversarial preferences through DPO. \n\nComparison with State-of-the-Art. Compared to other state-of-the-art models listed in Table 1, VideoPASTA combined with Qwen2.5VL outperforms all models on all benchmarks, surpassing strong baselines like Qwen2-VL and LLaVA-Video. Key improvements include a +5.9 gain in temporal reasoning on TempCompass and a +1.5 increase on PerceptionTest over LLaVA-Video, which is instruction-tuned on 1.3M SFT pairs. VideoPASTA also shows strong performance on long-form video tasks, achieving +3.3 on LongVideoBench, +2.7 on MLVU, and +1.7 on VideoMME compared to LLaVA-Video. Similarly, when paired with the other three foundation models, VideoPASTA outperforms SOTA models on several, though not all, benchmarks. These results highlight VideoPASTA's ability to elevate smaller models to SOTA performance through targeted and dataefficient training.",
            "score": 0.36046574447487445,
            "section_title": "Results",
            "char_start_offset": 15013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 326
                },
                {
                    "start": 329,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1961
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.234375
        },
        {
            "corpus_id": "276903685",
            "title": "From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models",
            "text": "Table 1 summarizes the performance of our approach on the VL-RewardBench, evaluated across five metrics: General, Hallucination, Reasoning, Overall Accuracy, and Macro Average Accuracy. Our method is applied to three backbone models (LLaVA-OneVision-7B, Qwen2-VL-7B, and Llama-3.2-11B-Vision), and the results demonstrate substantial improvements over all the baseline versions and other competitive models. \n\nFor the LLaVA-OneVision backbone, the baseline model (LLaVA-OneVision-7B) achieves an Overall Accuracy of 29.6 and a Macro Average Accuracy of 36.5. In contrast, our enhanced version of LLaVA-OneVision-7B yields an Overall Accuracy of 68.7 and a Macro Average Accuracy of 67.4, accompanied by a significant boost in the General (72.8 vs. 32.2) and Hallucination (67.9 vs. 20.1) metrics. Compared with LLaVA-Critic-7b, which is also trained as a reward model based on LLaVA-OneVision-7B, CAREVL achieves higher performance across all metrics of VL-RewardBench while using less training data (75k vs. 113k), indicating the effectiveness of our method. \n\nSimilarly, for Qwen2-VL-7B, CAREVL elevates the Overall Accuracy from 28.3 to 67.8 and Macro Average Accuracy from 33.9 to 65.6. For Llama-3.2-11B-Vision, CAREVL gets the highest Overall Accuracy. Compared to IXC-2.5-Reward, one of the top-performing LVLM reward models on VL-RewardBench, CAREVL exhibits distinct strengths, with both models excelling in different metrics and showcasing complementary advantages. Notably, while IXC-2.5-Reward leverages both open-source and in-house data, CAREVL is trained exclusively on open-source data. Furthermore, CAREVL provides interpretable justifications for preference judgments, whereas IXC-2.5-Reward outputs only numerical scores. These results indicate that our method robustly aligns LVLM outputs with human judgments.",
            "score": 0.36021106881941967,
            "section_title": "Evaluation on VL-RewardBench",
            "char_start_offset": 16231,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 407
                },
                {
                    "start": 410,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1830
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.513671875
        },
        {
            "corpus_id": "273507889",
            "title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction",
            "text": "Effective for diverse settings. We first study the Pyra-midDrop on both LLaVA-1.5 and LLaVA-Next. To further validate the effectiveness of our method, we conduct comparisons using the identical training recipe as LLaVA-1.5-7B [29] with three other baselines: Q-Former [25], FastV [9], and LLaVolta [5]. As shown in Table 3, Pyra-midDrop reduces the training time (including both pretraining and fine-tuning stages) of the LLaVA-Next from 366 to 218 GPU hours, resulting in an impressive 40% reduction in overall time. Besides the promising efficiency Figure 3. LVLMs trained by PyramidDrop can condense key visual information into fewer vision tokens. We compare the performance of the vanilla and PyramidDrop-trained LLaVA-1.5 models, where we preserve different ratios of image tokens at layer 2, 8, 16, and 24, respectively. The horizontal axis represents the proportion of retained image tokens according to attention score. Table 7. Ablation study results about \u03bb. \u03bb balances the performance and efficiency of PyramidDrop, a larger \u03bb preserves more image information but slows down the training, and a smaller \u03bb has higher speedup while may influence the model performance. We adjust \u03bb form 0.4 to 0.6 for investigating the influence on performance and training time.",
            "score": 0.3601901046544513,
            "section_title": "Efficiency of PyramidDrop in Training",
            "char_start_offset": 20524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1272
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40673828125
        },
        {
            "corpus_id": "271533948",
            "title": "Diffusion Feedback Helps CLIP See Better",
            "text": "Next, with the help of our DIVA, we further evaluate the performance gains brought by the enhanced CLIP backbones for multimodal understanding and visual perception tasks. Preprint. \n\nEnhanced Vision Backbone for MLLMs. Firstly, we adopt LLaVA-1.5 (Liu et al., 2024a) as the baseline framework to explore the potential of improved visual encoders in MLLM. LLaVA employs a pre-trained CLIP vision encoder and trains a projector to semantically align visual tokens with textual tokens from large language model (LLM). To ensure fair comparisons, we train our model with the same setting in LLaVA and evaluate model performance on various multimodal understanding benchmarks (i.e., MMVP (Tong et al., 2024b), POPE (Li et al., 2023), MME-Perception (Fu et al., 2023), MMBench (Liu et al., 2023), MMBench-CN (Liu et al., 2023), LLaVA-Bench-in-the-Wild (Liu et al., 2024b)). It can be clearly seen from Table 2 that LLaVA's performance is greatly boosted by replacing the original CLIP vision encoder to ours. The big accuracy gains on these benchmarks (except MME) are all thanks to the significant enhancement in CLIP's visual perception capabilities brought by our DIVA paradigm utilizing generative feedback. Enhanced Vision Backbone for Fine-Grained Visual Perception. We also include segmentation task to evaluate the benefits brought by our enhanced CLIP backbones for visual dense prediction task. We adopt the recent state-of-the-art model in the open vocabulary semantic segmentation field, SAN (Xu et al., 2023c) with CLIP (Radford et al., 2021b) at both 224 and 336 image resolutions, as baselines. Four commonly used benchmarks (i.e., ADE20K-847/150 (Zhou et al., 2017) and Pascal Context-459/59 (Mottaghi et al., 2014) for are employed for performance evaluation.",
            "score": 0.3599120456020486,
            "section_title": "BACKBONE ENHANCEMENT PERFORMANCE EVALUATION",
            "char_start_offset": 15951,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 181
                },
                {
                    "start": 184,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1771
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 267,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 684,
                    "end": 704,
                    "matchedPaperCorpusId": "266976992"
                },
                {
                    "start": 847,
                    "end": 866,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1499,
                    "end": 1517,
                    "matchedPaperCorpusId": "257102616"
                },
                {
                    "start": 1528,
                    "end": 1551,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.318359375
        },
        {
            "corpus_id": "271710333",
            "title": "Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models",
            "text": "Models Details. To validate the effectiveness of our SID, we conduct experiments on four representative LVLMs: InstructBLIP (Dai & et al, 2023), Shikra (Chen et al., 2023a), LLaVA-1.5 (Liu et al., 2024b), and LLaVA-NeXT (Li et al., 2024). InstructBLIP employs Q-former (Li et al., 2023c) to condense image tokens to 32, as a result, we are unable to visualize the dynamic token pruning process of InstructBLIP like Fig. 3 and 4. Shikra, LLaVA-1.5, and LLaVA-NeXT directly leverage linear projection layers as vision-language connectors to align multimodal features. Shikra and LLaVA-1.5 encode 256 and 576 image tokens to LVLMs. LLaVA-NeXT increases the input vision resolution by 4\u00d7 to capture more visual details, resulting in 4\u00d7 more encoded vision tokens than LLaVA-1.5. All LVLMs utilize pre-trained vision encoders like CLIP (Radford et al., 2021) vision encoder, as well as pre-trained LLMs as language decoders, such as Vicuna v1.1 (Chiang & Li, 2023), LLaMA 2 (Touvron et al., 2023b), and recently released LLaMA 3 (Meta, 2024). We provide results at the 7 Billion (B) scale, and larger-scale results are in the Appendix A.7. \n\nImplementation Details. For sampling and greedy decoding, we adopt the default hyperparameter settings. As for Dola (Chuang et al., 2024), it is designed to alleviate hallucinations (i.e., improve factuality) of LLM by contrasting the differences in logits obtained from projecting the later layers versus premature layers.",
            "score": 0.35976608494559525,
            "section_title": "A.2 DETAILED EXPERIMENTAL SETTINGS",
            "char_start_offset": 32841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 16,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1460
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 203,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 269,
                    "end": 287,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 831,
                    "end": 853,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.283203125
        },
        {
            "corpus_id": "268248921",
            "title": "Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models",
            "text": "Comparison with baselines. In Tab. 1, we compare the performance and efficiency of LLaVA-HR with LLaVA-1.5 (Liu et al., 2023a) with different image resolutions. From this table, we observe that increasing image resolution obviously improves the performance of two models on four tasks, e.g., +4.8% of LLaVA-1.5 on TextVQA. However, the performance of LLaVA-1.5 drops significantly at the resolution of 1,024\u00d71,024. To explain, the number of visual tokens greatly exceeds the pre-trained context length of the LLM, which easily causes the instability during training. \n\nIn contrast, the performance of LLaVA-HR is consistently improved from 384 \u00d7 384 resolution to 1,024 \u00d7 1,024 resolution. Besides, the total gain of LLaVA-HR is more obvious than that of LLaVA-1.5 (Liu et al., 2023a), e.g., +8.33% of LLaVA-HR vs. +4.82% of LLaVA-1.5, greatly confirming the effectiveness of MRA. \n\nIn Tab. 2, we further compare four common baselines with the similar resolution, i.e., \u223c760\u00d7760. \"ViT+MLP\" is the default setting of LLaVA-1.5 as the reference. \"Conv+MLP\" replaces the visual backbone with ConvNeXt (Liu et al., 2022), which uses a larger downsampling rate to reduce the number of visual tokens. \"ViT+Resampler\" and \"ViT+Pooling+MLP\" refer to the two pooling strategies for reducing the number of visual tokens. As can be seen, all compared methods are inferior to LLaVA-HR. In particular, using a convolutional network as the visual backbone greatly improves efficiency, but its performance still lags behind LLaVA-HR by a large margin, e.g., -108.9 on MME (Fu et al., 2023). Similarly, \"ViT+Resampler\" and \"ViT+Pooling+MLP\" also sacrifice performance for efficiency.",
            "score": 0.35976069428195456,
            "section_title": "QUANTITATIVE ANALYSIS",
            "char_start_offset": 17133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1667
                }
            ],
            "ref_mentions": [
                {
                    "start": 1098,
                    "end": 1116,
                    "matchedPaperCorpusId": "245837420"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.277099609375
        },
        {
            "corpus_id": "274150456",
            "title": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation",
            "text": "In our experiments, we evaluate 11 SOTA LMMs: 1. gpt-4o-2024-05-13, 2. gpt-4o-mini-2024-07-18, 3. gemini-1.5-pro, 4. gemini-1.5-flash, 5. rhymes-ai/Aria, 6. Qwen/Qwen2-VL-72B-Instruct, 7. Qwen/Qwen2-VL-7B-Instruct 8. lmms-lab/LLaVA-Video-72B-Qwen2, 9. lmms-lab/LLaVA-Video-7B-Qwen2, 10. lmms-lab/llava-onevision-qwen2-72b-ov, 11. lmms-lab/llava-onevision-qwen2-7b-ov. We selected the top 10 LMMs that support long video analysis, along with their smaller versions, based on their performance on VideoMME as of October 15, 2024. Additional open-source LMMs were excluded for two reasons: first, some were released concurrently with our work, leaving insufficient time for evaluation; second, others exhibited weak performance on VideoMME or lacked support for long video analysis. Consequently, our experiments are limited to the 11 most popular and SOTA LMMs. \n\nFor user simulation, fault-driven evolution, automatic judging, and difficulty level evaluation, we use gpt-4o-2024-08-06, ensuring that the examiner and judge remain distinct from the LMMs throughout the entire evaluation process.",
            "score": 0.3597298211734543,
            "section_title": "B.2. LMMs Selection",
            "char_start_offset": 29005,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 1093
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.029144287109375
        },
        {
            "corpus_id": "278032999",
            "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs",
            "text": "In order to evaluate efficacy of our approach, we compare against several existing methods that focus on reducing the number of tokens for VLMs. Table 4. DYMU shows consistent effectiveness on an AnyRes VLM backbone, LLaVA-OneVision [19]. We additionally show performance on two comprehensive video understanding benchmarks, where DYMU-ov-low achieves \u223c96.5% of the baseline's performance with only \u223c14% tokens.",
            "score": 0.35951394323666375,
            "section_title": "Quantitative Evaluation Comparing Visual Token Merging Methods for VLMs",
            "char_start_offset": 21040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 411
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1905517578125
        },
        {
            "corpus_id": "277313523",
            "title": "PAVE: Patching and Adapting Video Large Language Models",
            "text": "Our experiments consider ScanQA [2] and SQA3D [44] datasets, using their corresponding train / test splits. Following previous work [84], we report the CIDEr (C), BLEU-4 (B-4), METEOR (M), ROUGE(R), and top-1 Exact Match (EM@1) metrics on ScanQA and report EM@1 on SQA3D. Similarly, we also report model inference FLOPs and parameters for PAVE. \n\nImplementation details. To encode 3D cues, we use the 3D encoder from LLaVA-3D [84]. It encodes camera pose, RGB images, and depth information into multi-view features defined on the RGB image plane. We again build PAVE with LLaVA-OneVision [28]. Specifically, we evenly sample 32 RGB-D frames with their camera poses from the scanning. The 3D encoder creates a sequence of 2D feature maps (i.e., multi-view features), leading to around 18K 3D tokens per scan. The visual encoder in LLaVA-OneVision separately embeds the 32 key RGB frames into video tokens. PAVE further integrates video tokens and 3D tokens. We train the PAVE 1 epoch with ScanQA / 2 epochs with SQA3D training set, and then evaluating PAVE performance on their test sets, respectively. \n\nBaselines. We again compare our method with two types of baselines: (1) Video LLMs for general video understanding, e.g., LLaVA-OneVision [28] and VideoChat2 [33], with zero-shot inference; and (2) Task-specific models fine-tuned on the target dataset, e.g., LEO [22], 3D-LLM [20], Scene-LLM [15] and LLaVA-3D [84]. We also add a baseline that directly fine-tunes LLaVA-OneVision with LoRA without using 3D cues, denoted as LLaVA-OV-7B-FT.",
            "score": 0.3591419761824822,
            "section_title": "Experiment protocol.",
            "char_start_offset": 17857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1543
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 35,
                    "matchedPaperCorpusId": "245334889"
                },
                {
                    "start": 46,
                    "end": 50,
                    "matchedPaperCorpusId": "252907411"
                },
                {
                    "start": 1262,
                    "end": 1266,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 1380,
                    "end": 1384,
                    "matchedPaperCorpusId": "260356619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26513671875
        },
        {
            "corpus_id": "273638057",
            "title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing",
            "text": "Pre-training VLMs is typically computationally intensive and timeconsuming. Consequently, fine-tuning presents an effective alternative that preserves most of the model's parameters while enhancing performance on downstream tasks. Fine-tuned models can often outperform the original general models, utilizing fewer computing resources and requiring less training time [25]. This advantage motivates the use of Parameter-Efficient Fine-Tuning (PEFT) methods for tasks involving geographical change detection. \n\nIn our work, we focus on fine-tuning two distinct models that have demonstrated a robust understanding of temporal data through video processing within the VLM framework for question-answering and captioning. The first model, LLaVA-NeXT [20], was introduced in early 2024, offering improved reasoning and world knowledge compared to other large models. It exhibits data efficiency comparable to SOTA models such as LLaVA-1.5 [19], while delivering higher image resolution and enhanced visual conversation capabilities. Shortly after the release of LLaVA-NeXT, a video variant was introduced, named LLaVA-NeXT-Video, which has demonstrated strong performance in zero-shot video tasks. \n\nThe second model utilized for comparison is Video-LLaVA [15], which excels in understanding visual language for downstream tasks and surpasses many existing video language models across various  benchmarks. Both projects have multiple variations based on the number of parameters for the models. For simplicity, we have chosen to use the 7B parameter variation from both models. The 7B variations can be fine-tuned with PEFT techniques on a single GPU, making them particularly well-suited for our dataset.",
            "score": 0.3588745246307341,
            "section_title": "Model Fine-tuning",
            "char_start_offset": 11891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1702
                }
            ],
            "ref_mentions": [
                {
                    "start": 368,
                    "end": 372,
                    "matchedPaperCorpusId": "268157336"
                },
                {
                    "start": 935,
                    "end": 939,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6494140625
        },
        {
            "corpus_id": "276580647",
            "title": "Detecting Offensive Memes with Social Biases in Singapore Context Using Multimodal Large Language Models",
            "text": "We used LLaVA-NeXT (a.k.a. LLaVA-v1.6) Mistral 7B (\"LLaVA\") and performed standard LoRA fine-tuning on our dataset. This establishes a baseline for our dataset and evaluation metrics. We then performed the same on Qwen2-VL-7B-Instruct (\"Qwen2-VL\"). This explores the importance of the base model's performance and pretrained knowledge on this task since the language model of LLaVA, the Mistral 7B v0.2, is older than Qwen2-VL's Qwen2-7B, and that Qwen2-VL is an overall stronger VLM than LLaVA. \n\nWe extend our baseline performance by experimenting with various LoRA-related techniques, including LoRA+ [16], DoRA [29], Rank-Stabalized LoRA [23], and PiSSA [31], on the Qwen2-VL.",
            "score": 0.3582749884132098,
            "section_title": "Vision-Language Model",
            "char_start_offset": 30316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 680
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1361083984375
        },
        {
            "corpus_id": "272689854",
            "title": "TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings",
            "text": "In \u2022 The proposed text-guided strategy demonstrates substantial improvements over the baseline. Compared with the original LLaVA-1.5, TG-LLaVA achieve much better performance. As shown in the first four rows. our method leads on the majority of evaluation datasets. \n\nIt is noteworthy that TG-LLaVA demonstrates an average improvement of 1.5% over the original LLaVA-1.5 across ten datasets when using Vicuna-7B, highlighting the method's significant value. When juxtaposed with the baseline LLaVA-1.5 Vicuna-7B model, we enhance performance metrics by +2.2% on MM-Bench, +2.4% on both MMStar and MMMU, and +3.2% on LLaVABenchs, respectively. For LLaVA-1.5 with Vicuna-13B, we also achieve an average performance improvement of 1%. Specifically, we see a +1.6% gain on MMStar, a +2.0% gain on MMMU, and a +3.2% gain on MME. These impressive results further validate the contribution of the proposed TG-LLaVA architecture to visual feature optimization, highlighting the favorable impact of our method. \n\n\u2022 The proposed TG-FOM and TG-DP modules can be universally applied as a modular plugnin to mainstream VLM frameworks. As shown in the rest part of Table 1. we further validate the versatility of our proposed method under various settings. We replace CLIP with SigLIP and substitute Vicuna with Llama3 and Qwen2 on top of the original LLaVA-1.5 framework. We compare these settings with our method as the baseline. The results in Table 1 confirm that our method continues to maintain a leading advantage across most datasets, demonstrating that the proposed TG-LLaVA exhibits excellent generalizability and possesses strong potential for adaptation to a wide range of VLM architectures.",
            "score": 0.35768078677408155,
            "section_title": "Genuine Improvement Over the Baseline",
            "char_start_offset": 19349,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 265
                },
                {
                    "start": 268,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1689
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3779296875
        },
        {
            "corpus_id": "276259332",
            "title": "Space-Aware Instruction Tuning: Dataset and Benchmark for Guide Dog Robots Assisting the Visually Impaired",
            "text": "For our SA-VLM, we selected LLaVA-OneVision [31] as our base model, a VL multimodal model to improve comprehension in complex visual scenarios. We fine-tuned the 7B model across all components: the vision encoder, MLP adapter, and the large language model on our SAIT dataset with a batch size of 64 for one epoch. The learning rate was set to 1e-5 with a cosine scheduler using 8 NVIDIA  RTX 6000 Ada GPUs for around 4 hours. For our automatic generation pipeline, we used LLaVA-v1.6-34b [32] without additional fine-tuning. Including object detection, depth estimation, masking, and generating descriptions, the process took about 62 seconds per image, using 4 NVIDIA RTX 6000 Ada and 4 Quadro RTX 6000 GPUs.",
            "score": 0.3574579457693373,
            "section_title": "A. Experiment Details",
            "char_start_offset": 17616,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 710
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09332275390625
        },
        {
            "corpus_id": "277244459",
            "title": "When Less is Enough: Adaptive Token Reduction for Efficient Image Representation",
            "text": "To evaluate the performance of the proposed feature selector, we integrated it with vision encoders used as backbones in two multimodal models: LLaVA-NeXT (visual encoder of CLIP [26]) and LLaVA-OneVision (visual encoder of SigLIP [36]). Our primary goal was to investigate whether the selector described in Sec. 4.1.1 can be applied directly (i.e., without further fine-tuning) to existing multimodal architectures, and to determine whether selecting informative features positively impacts model inference. To this end, we evaluated both models augmented with our trained feature selector under different pruning factors, comparing their performance to a baseline using random feature selection with the same pruning factors across multiple multimodal benchmarks.",
            "score": 0.35691653343547025,
            "section_title": "Experiments",
            "char_start_offset": 14040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 765
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48291015625
        },
        {
            "corpus_id": "268876083",
            "title": "Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns",
            "text": "Interestingly, our evaluation revealed that larger models did not consistently perform better in some tasks.The LLaVA-v1.5-13Bmodel performed worse than its 7B model in GEN, ERR, and VQA.Also, this trend is observed in LLaVA-v1.6models: 7B model outperforming in DDx and VQA.This result suggests that adding more parameters does not directly translate to performance improvements.",
            "score": 0.3566553348164588,
            "section_title": "Larger models do not perform well",
            "char_start_offset": 12772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 108,
                    "end": 126
                },
                {
                    "start": 126,
                    "end": 187
                },
                {
                    "start": 187,
                    "end": 229
                },
                {
                    "start": 229,
                    "end": 275
                },
                {
                    "start": 275,
                    "end": 380
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10174560546875
        },
        {
            "corpus_id": "274422305",
            "title": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks",
            "text": "Models such as Qwen2-VL [54] and RS-LLaVA [5] appear in the mid-range, while LLaVA-NeXT [29], EarthDial [47] and SkySenseGPT [34] generally show lower variance, indicating stability under varied prompt structures. Single vs. Multi-Temporal Data: For the single vs. multi-temporal data analysis (Fig. 9), we evaluate GPT-4o, LLaVA-OneVision [23], EarthDial [47], and Qwen2-VL [54] across three tasks: crop, disaster, and land use classifications. In crop classification, multi-temporal data result in slightly lower performance across all models, suggesting that the temporal information introduces variability that the models struggle to effectively capture. In disaster classification, GPT-4o, EarthDial, and Qwen2-VL benefit from multi-temporal data, whereas LLaVA-OneVision exhibits a slight decline. In land use classification, all models show improved performance with multi-temporal data, likely due to the stability of land use patterns over time, enabling better generalization for this task. Impact of Object Size on Detection Performance: In referring expression detection across different object sizes (Fig. 6), we evaluate multiple models on small, medium, and large objects. Sphinx [27] and Qwen2-VL [54] demonstrate relatively high performance, particularly excelling in detecting large objects. GPT-4o, however, performs the worst, struggling significantly with all sized objects, suggesting limitations in fine-grained localization. Ferret [62], EarthDial [47], LHRS-Bot-Nova [36] and GeoChat [19] exhibit moderate performance on large objects. For medium objects, EarthDial [47] and Sphinx [27] perform overall better. These results highlight the challenges in detection, especially in medium and small objects.",
            "score": 0.3565611288415466,
            "section_title": "Analysis",
            "char_start_offset": 20824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1728
                }
            ],
            "ref_mentions": [
                {
                    "start": 42,
                    "end": 45,
                    "matchedPaperCorpusId": "269354784"
                },
                {
                    "start": 1492,
                    "end": 1496,
                    "matchedPaperCorpusId": "267412588"
                },
                {
                    "start": 1509,
                    "end": 1513,
                    "matchedPaperCorpusId": "265456719"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1485595703125
        },
        {
            "corpus_id": "276937786",
            "title": "Oasis: One Image is All You Need for Multimodal Instruction Data Synthesis",
            "text": "We validate Oasis on medical benchmarks in Tab. 7. We sample 15k images from the MedTrinity-25M dataset and create 2k medical training data with Oasis. We SFT the LLaVA-NeXT baseline with 4k sampled LLaVA data and 2k LLaVA data + 2k synthesized medical data, respectively. The table below shows great performance improvements across 3 medical benchmarks with our data.",
            "score": 0.3564706339441621,
            "section_title": "A.5. Application on medical area",
            "char_start_offset": 30184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 368
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1597900390625
        },
        {
            "corpus_id": "276580076",
            "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference",
            "text": "(Liu et al., 2023), LLaVA-NeXT-7B (Liu et al., 2024c), and InternVL-v1.5-7B (Chen et al., 2023) for multi-images tasks, as well as LLaVA-Video-7B/32B (Zhang et al., 2024d), LongVA-7B (Zhang et al., 2024b), and LongVILA-8B (Xue et al., 2024) for long-video tasks. We also evaluate MEDA on diverse mutlimodal long-context datasets including MileBench (Song et al., 2024), Video-ChatGPT (Maaz et al., 2023), DREAM-1K (Wang et al., 2024a), and WorldQA (Zhang et al., 2024e). Our results show that MEDA outperforms both state-of-the-art text-based and multimodal KV cache methods including H2O (Zhang et al., 2024g), SnapKV (Li et al., 2024a), PyramidKV (Zhang et al., 2024c), LOOK-M (Wan et al., 2024c), and is able to achieve up to 2.82 times faster inference speed and reduce KV cache memory footprint by up to 72%, while maintaining or improving performance on the target tasks.",
            "score": 0.3561538518701386,
            "section_title": "Introduction",
            "char_start_offset": 3400,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 877
                }
            ],
            "ref_mentions": [
                {
                    "start": 589,
                    "end": 610,
                    "matchedPaperCorpusId": "259263947"
                },
                {
                    "start": 679,
                    "end": 698,
                    "matchedPaperCorpusId": "270738079"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.116943359375
        },
        {
            "corpus_id": "277244542",
            "title": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models",
            "text": "Our model is trained on the LLaVA-OneVision dataset and evaluated in a zero-shot setting on multiple benchmarks. To ensure practical efficiency, we adopt the 0.5B-parameter LLaVA-OneVision [12], based on QWen2 [27], as our baseline. We integrate our proposed Token Dynamics framework into this baseline and fine-tune it for evaluation. Training is conducted on 4 NVIDIA A100 GPUs for 85,000 iterations, using the AdamW optimizer. The learning rate follows a cosine decay schedule, starting at 1 \u00d7 10 \u22125 for the cross-dynamics attention module and 2 \u00d7 10 \u22126 for the ViT module. We employ Zero2 [19] optimization with a batch size of 8 and gradient accumulation steps of 2.",
            "score": 0.3559058717762533,
            "section_title": "Implementation Details",
            "char_start_offset": 21028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 671
                }
            ],
            "ref_mentions": [
                {
                    "start": 593,
                    "end": 597,
                    "matchedPaperCorpusId": "269617042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1961669921875
        },
        {
            "corpus_id": "277103933",
            "title": "Growing a Twig to Accelerate Large Vision-Language Models",
            "text": "We evaluate the performance of TwigVLM on three popular VLMs, namely LLaVA-1.5-7B [30], LLaVA-NeXT-7B [31] and Video-LLaVA-7B [26], to compare with the state-ofthe-art VLM acceleration methods. After that, we conduct comprehensive ablation studies to analyze the effectiveness of the key elements of TwigVLM.",
            "score": 0.3558187821361294,
            "section_title": "Experiments",
            "char_start_offset": 16976,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 308
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 86,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1510009765625
        },
        {
            "corpus_id": "272986991",
            "title": "VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs",
            "text": "Spatial Captioners We provide an ablation study over captioners comparing CogAgent vs. LLaVA-1.5 (Liu et al., 2023) on NExT-QA, indicating that",
            "score": 0.35573862425573277,
            "section_title": "Ablation on Spatial Stage",
            "char_start_offset": 23167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 143
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0299835205078125
        }
    ],
    "quotes": {
        "cost": 0.10936800000000002,
        "quotes": [
            {
                "idx": 0,
                "key": "[271719914 | Li et al. | 2024 | Citations: 867]",
                "snippets": "The first LLaVA model [83] demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors similar to GPT-4V on previously unseen images and instructions for the first time. LLaVA-1.5 (Liu et al., 2023) significantly expands and improves the capabilities by incorporating more academicrelated instruction data, achieving SoTA performance on a dozens of benchmarks with a data-efficient recipe. LLaVA-NeXT [82] inherits this property, further pushing performance boundaries through three key techniques: AnyRes for handling high-resolution images, expanding high-quality instruction data, and utilizing the best open LLM available at the time.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[263672058 | Liu et al. | 2023 | Citations: 2824]": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 495,
                        "end": 1145,
                        "sentence_offsets": [
                            {
                                "start": 495,
                                "end": 690
                            },
                            {
                                "start": 691,
                                "end": 896
                            },
                            {
                                "start": 897,
                                "end": 1145
                            }
                        ],
                        "ref_mentions": [
                            "263672058"
                        ],
                        "quote": "The first LLaVA model [83] demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors similar to GPT-4V on previously unseen images and instructions for the first time. LLaVA-1.5 (Liu et al., 2023) significantly expands and improves the capabilities by incorporating more academicrelated instruction data, achieving SoTA performance on a dozens of benchmarks with a data-efficient recipe. LLaVA-NeXT [82] inherits this property, further pushing performance boundaries through three key techniques: AnyRes for handling high-resolution images, expanding high-quality instruction data, and utilizing the best open LLM available at the time."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[271855335 | Lu et al. | 2024 | Citations: 3]",
                "snippets": "LLaVA-NeXT [24] is an improved version of LLaVA 1.5, with a focus on enhanced visual reasoning, optical character recognition (OCR), and multi-modal document understanding. LLaVA-NeXT scales the input image resolution of input images by 4\u00d7, up to 1344 \u00d7 336 compared to 336 \u00d7 336 in LLaVA 1.5 to enhance its ability to grasp finer-grained visual cues. LLaVA-NeXT is also trained on a more diverse and realistic visual instruction-tuning dataset (ShareGPT-4V and LAION-GPT-V), as well as a range of OCR, document, and chart datasets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 532,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 172
                            },
                            {
                                "start": 173,
                                "end": 351
                            },
                            {
                                "start": 352,
                                "end": 532
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "LLaVA-NeXT [24] is an improved version of LLaVA 1.5, with a focus on enhanced visual reasoning, optical character recognition (OCR), and multi-modal document understanding. LLaVA-NeXT scales the input image resolution of input images by 4\u00d7, up to 1344 \u00d7 336 compared to 336 \u00d7 336 in LLaVA 1.5 to enhance its ability to grasp finer-grained visual cues. LLaVA-NeXT is also trained on a more diverse and realistic visual instruction-tuning dataset (ShareGPT-4V and LAION-GPT-V), as well as a range of OCR, document, and chart datasets."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[273186838 | Lee et al. | 2024 | Citations: 0]",
                "snippets": "\u2022 LLaVA-1.5 (Liu et al., 2024a) incorporates academic task-oriented datasets to enhance performance in VQA tasks and features an MLP vision-language connector, which improves upon the original linear layer utilized in LLaVA (Liu et al., 2024c). It uses CLIP ViT-L/14 (Radford et al., 2021) with a 336px resolution as its vision encoder, resulting in a total of (336/14) 2 = 576 visual tokens.\n\n\u2022 LLaVA-NeXT (Liu et al., 2024b) (also known as LLaVA-1.6) enhances visual reasoning, OCR, and world knowledge, offering four times higher image resolution (up to 1344x336) and improved performance in visual conversations. Its architecture includes a CLIP ViT-L/14 as a vision encoder, paired with Vicuna models ranging from 7B to 34B as a backbone language model.\n\n\u2022 LLaVA-OneVision (Li et al., 2024b) is a LVLM designed for task transfer across singleimage, multi-image, and video scenarios, with strong capabilities in video understanding through image-to-video task transfer. Its architecture consists of a Qwen2 language model (Yang et al., 2024) with 8B to 72B parameters, and the SigLIP vision encoder (Zhai et al., 2023), which processes images at a base resolution of 384x384, producing 729 visual tokens. The model employs a 2-layer MLP projector.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "\u2022 LLaVA-1.5 (Liu et al., 2024a) incorporates academic task-oriented datasets to enhance performance in VQA tasks and features an MLP vision-language connector, which improves upon the original linear layer utilized in LLaVA (Liu et al., 2024c). It uses CLIP ViT-L/14 (Radford et al., 2021) with a 336px resolution as its vision encoder, resulting in a total of (336/14) 2 = 576 visual tokens.\n\n\u2022 LLaVA-NeXT (Liu et al., 2024b) (also known as LLaVA-1.6) enhances visual reasoning, OCR, and world knowledge, offering four times higher image resolution (up to 1344x336) and improved performance in visual conversations. Its architecture includes a CLIP ViT-L/14 as a vision encoder, paired with Vicuna models ranging from 7B to 34B as a backbone language model.\n\n\u2022 LLaVA-OneVision (Li et al., 2024b) is a LVLM designed for task transfer across singleimage, multi-image, and video scenarios, with strong capabilities in video understanding through image-to-video task transfer. Its architecture consists of a Qwen2 language model (Yang et al., 2024) with 8B to 72B parameters, and the SigLIP vision encoder (Zhai et al., 2023), which processes images at a base resolution of 384x384, producing 729 visual tokens. The model employs a 2-layer MLP projector.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[273821149 | Dai et al. | 2024 | Citations: 2]",
                "snippets": "LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Various Vision-Language Applications",
                        "pdf_hash": "",
                        "start": 298,
                        "end": 455,
                        "sentence_offsets": [
                            {
                                "start": 284,
                                "end": 455
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[274131643 | Qu et al. | 2024 | Citations: 2]",
                "snippets": "LLaVA-v1.5 (Liu et al., 2023) and LLaVA-v1.6(NeXT) [23] further improve LLaVA's performance through better data, higher resolution and stronger LLM.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[263672058 | Liu et al. | 2023 | Citations: 2824]": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."
                },
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 734,
                        "end": 868,
                        "sentence_offsets": [
                            {
                                "start": 713,
                                "end": 770
                            },
                            {
                                "start": 771,
                                "end": 868
                            }
                        ],
                        "ref_mentions": [
                            "263672058"
                        ],
                        "quote": "LLaVA-v1.5 (Liu et al., 2023) and LLaVA-v1.6(NeXT) [23] further improve LLaVA's performance through better data, higher resolution and stronger LLM."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[274437586 | Zhang et al. | 2024 | Citations: 10]",
                "snippets": "LLaVA-1.5 [34]. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement.\n\nLLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "LLaVA-1.5 [34]. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement.\n\nLLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[275405668 | Zhao et al. | 2025 | Citations: 5]",
                "snippets": "LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Multimodal Large Language Model",
                        "pdf_hash": "",
                        "start": 725,
                        "end": 1066,
                        "sentence_offsets": [
                            {
                                "start": 725,
                                "end": 852
                            },
                            {
                                "start": 853,
                                "end": 1066
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[276647578 | Alyakin et al. | 2025 | Citations: 0]",
                "snippets": "We built on LLaVA-Next's improvements -including its multilayer projection, patch-based processing of large images, and enhanced pre-training (Fig. S6a).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Model Architecture and Training Vision-Language Model Backbone",
                        "pdf_hash": "",
                        "start": 192,
                        "end": 345,
                        "sentence_offsets": [
                            {
                                "start": 192,
                                "end": 345
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We built on LLaVA-Next's improvements -including its multilayer projection, patch-based processing of large images, and enhanced pre-training (Fig. S6a)."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[276724729 | Wang et al. | 2025 | Citations: 0]",
                "snippets": "LLaVA-1.5 (Liu et al., 2024) explored adding high-quality multitask training data, and scaling up the resolution and LLM size to boost MLLM performance. LLaVA-OneVision (Li et al., 2024a) explored to unify dynamic image resolution, multi-image, and video into a unified input format.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Multi-modal Large Language Models",
                        "pdf_hash": "",
                        "start": 753,
                        "end": 1036,
                        "sentence_offsets": [
                            {
                                "start": 753,
                                "end": 905
                            },
                            {
                                "start": 906,
                                "end": 1036
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "LLaVA-1.5 (Liu et al., 2024) explored adding high-quality multitask training data, and scaling up the resolution and LLM size to boost MLLM performance. LLaVA-OneVision (Li et al., 2024a) explored to unify dynamic image resolution, multi-image, and video into a unified input format."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[276938164 | Yang et al. | 2025 | Citations: 0]",
                "snippets": "LLaVA-OneVision simultaneously pushes the performance boundaries of open-domain Large Multimodal Models (LMMs) across three key computer vision scenarios-single image, multiple images, and video-while enabling powerful transfer learning across different modalities and scenarios, resulting in the emergence of new capabilities [28].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Large Vision-Language Model",
                        "pdf_hash": "",
                        "start": 870,
                        "end": 1202,
                        "sentence_offsets": [
                            {
                                "start": 870,
                                "end": 1202
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "LLaVA-OneVision simultaneously pushes the performance boundaries of open-domain Large Multimodal Models (LMMs) across three key computer vision scenarios-single image, multiple images, and video-while enabling powerful transfer learning across different modalities and scenarios, resulting in the emergence of new capabilities [28]."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[276961504 | Guo et al. | 2025 | Citations: 7]",
                "snippets": "Early opensource LMMs like LLaVA (Liu et al., 2023b) and MiniGPT-4 (Zhu et al., 2023) paired CLIP-based image encoders (Radford et al., 2021) with LLMs for multi-modal instruction tuning. Later models such as LLaVA-NeXT (Li et al., 2024a), LLaVA-OneVision (Li et al., 2024b), ShareGPT4V (Chen et al., 2023b), InternVL (Chen et al., 2023), SPHINX (Lin et al., 2023), and Qwen-VL (Qwen Team, 2024) expanded these capabilities through broader training datasets and advanced training strategies.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231591445 | Radford et al. | 2021 | Citations: 29867]": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
                    "[266521410 | Chen et al. | 2023 | Citations: 1215]": "The exponential growth of large language models (LLMs) has opened up numerous possibilities for multi-modal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foun-dation model (Intern VL), which scales up the vision foun-dation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models."
                },
                "metadata": [
                    {
                        "section_title": "Large Multi-modal Models (LMMs)",
                        "pdf_hash": "",
                        "start": 441,
                        "end": 933,
                        "sentence_offsets": [
                            {
                                "start": 441,
                                "end": 628
                            },
                            {
                                "start": 629,
                                "end": 933
                            }
                        ],
                        "ref_mentions": [
                            "231591445",
                            "266521410"
                        ],
                        "quote": "Early opensource LMMs like LLaVA (Liu et al., 2023b) and MiniGPT-4 (Zhu et al., 2023) paired CLIP-based image encoders (Radford et al., 2021) with LLMs for multi-modal instruction tuning. Later models such as LLaVA-NeXT (Li et al., 2024a), LLaVA-OneVision (Li et al., 2024b), ShareGPT4V (Chen et al., 2023b), InternVL (Chen et al., 2023), SPHINX (Lin et al., 2023), and Qwen-VL (Qwen Team, 2024) expanded these capabilities through broader training datasets and advanced training strategies."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[277313523 | Liu et al. | 2025 | Citations: 1]",
                "snippets": "Recent vision-LLM models like LLaVA-NeXT [39], LLaVA-OneVision [28], LLaVA-Video [79], Qwen2-VL [60], and mPlug-Owl3 [71] consider multi-stage training with both video and image, which substantially improves the model performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 669,
                        "end": 899,
                        "sentence_offsets": [
                            {
                                "start": 669,
                                "end": 899
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent vision-LLM models like LLaVA-NeXT [39], LLaVA-OneVision [28], LLaVA-Video [79], Qwen2-VL [60], and mPlug-Owl3 [71] consider multi-stage training with both video and image, which substantially improves the model performance."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[277452239 | Riggi et al. | 2025 | Citations: 1]",
                "snippets": "Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Model overview",
                        "pdf_hash": "",
                        "start": 705,
                        "end": 1550,
                        "sentence_offsets": [
                            {
                                "start": 705,
                                "end": 964
                            },
                            {
                                "start": 965,
                                "end": 1241
                            },
                            {
                                "start": 1242,
                                "end": 1550
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.019686,
        "cot": "For a query about the technical innovations in LLaVA variants, I'll organize the information into dimensions that clearly explain each model's key innovations and improvements.\n\n1. \"Introduction to LLaVA Models\" - This dimension should be a synthesis paragraph providing context about what LLaVA models are generally and their evolution. This gives the user necessary background before diving into specific variants.\n\n2. \"LLaVA-1.5 Technical Innovations\" - This dimension should be a synthesis paragraph detailing the specific technical improvements in LLaVA-1.5 compared to the original LLaVA. The quotes suggest improvements in resolution, data quality, architectural changes (MLP connector), and training approaches.\n\n3. \"LLaVA-NeXT Technical Innovations\" - This dimension should be a synthesis paragraph explaining the key technical advancements in LLaVA-NeXT (also known as LLaVA-1.6), which appears to focus on resolution handling (AnyRes), OCR capabilities, and expanded datasets.\n\n4. \"LLaVA-OneVision Technical Innovations\" - This dimension should be a synthesis paragraph describing the technical innovations in LLaVA-OneVision, which seems to focus on unified processing across different visual scenarios.\n\n5. \"Comparative Performance Improvements\" - This dimension should be a synthesis paragraph that directly compares how these technical innovations translated into performance improvements across the variants.\n\nFor each of these dimensions, I'm choosing the synthesis format rather than list because the query asks about technical innovations that \"drove performance improvements,\" which requires explaining the connections between technical changes and their impacts, not just listing features. A cohesive narrative will better explain the evolution and relationships between innovations across these model variants.",
        "plan": {
            "Introduction to LLaVA Models (synthesis)": [
                0,
                10
            ],
            "LLaVA-1.5 Technical Innovations (synthesis)": [
                0,
                2,
                4,
                5,
                8,
                12
            ],
            "LLaVA-NeXT Technical Innovations (synthesis)": [
                0,
                1,
                2,
                4,
                5,
                7,
                10,
                11,
                12
            ],
            "LLaVA-OneVision Technical Innovations (synthesis)": [
                2,
                3,
                6,
                8,
                9,
                11,
                12
            ],
            "Comparative Performance Improvements (synthesis)": [
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                8,
                9
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to LLaVA Models",
                "tldr": "LLaVA models represent a significant advancement in large multimodal models (LMMs) that combine vision capabilities with language understanding. The evolution from the original LLaVA to LLaVA-1.5, LLaVA-NeXT, and LLaVA-OneVision demonstrates progressive improvements in multimodal capabilities through innovative techniques and training approaches. (5 sources)",
                "text": "\nThe LLaVA (Large Language and Vision Assistant) family of models has emerged as a leading framework for multimodal AI systems that can process both visual and textual information. The original LLaVA model demonstrated impressive multimodal chat abilities, exhibiting behaviors similar to GPT-4V when processing previously unseen images and instructions <Paper corpusId=\"271719914\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This breakthrough was achieved by combining CLIP-based image encoders with large language models (LLMs) for multi-modal instruction tuning <Paper corpusId=\"276961504\" paperTitle=\"(Guo et al., 2025)\" isShortName></Paper> <Paper corpusId=\"231591445\" paperTitle=\"(Radford et al., 2021)\" isShortName></Paper>.\n\nThe LLaVA architecture evolved rapidly through several major versions. LLaVA-1.5 significantly expanded the model's capabilities by incorporating more academic-related instruction data, achieving state-of-the-art performance across numerous benchmarks through a data-efficient recipe <Paper corpusId=\"271719914\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. A key insight from this version was that the fully-connected vision-language connector in LLaVA proved surprisingly powerful and data-efficient <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\nBuilding upon these foundations, subsequent models including LLaVA-NeXT and LLaVA-OneVision further pushed performance boundaries through broader training datasets and advanced training strategies <Paper corpusId=\"276961504\" paperTitle=\"(Guo et al., 2025)\" isShortName></Paper>. These improvements have positioned the LLaVA family alongside other notable vision-language foundation models such as ShareGPT4V, InternVL, SPHINX, and Qwen-VL <Paper corpusId=\"276961504\" paperTitle=\"(Guo et al., 2025)\" isShortName></Paper> <Paper corpusId=\"266521410\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. The evolution of these models demonstrates the rapid advancement in multimodal AI systems that can process and reason about visual and textual information in increasingly sophisticated ways.",
                "citations": [
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "The first LLaVA model [83] demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors similar to GPT-4V on previously unseen images and instructions for the first time. LLaVA-1.5 (Liu et al., 2023) significantly expands and improves the capabilities by incorporating more academicrelated instruction data, achieving SoTA performance on a dozens of benchmarks with a data-efficient recipe. LLaVA-NeXT [82] inherits this property, further pushing performance boundaries through three key techniques: AnyRes for handling high-resolution images, expanding high-quality instruction data, and utilizing the best open LLM available at the time."
                        ],
                        "paper": {
                            "corpus_id": 271719914,
                            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
                            "authors": [
                                {
                                    "authorId": "2310709478",
                                    "name": "Bo Li"
                                },
                                {
                                    "authorId": "2145784327",
                                    "name": "Yuanhan Zhang"
                                },
                                {
                                    "authorId": "2325209062",
                                    "name": "Dong Guo"
                                },
                                {
                                    "authorId": "2310650738",
                                    "name": "Renrui Zhang"
                                },
                                {
                                    "authorId": "2310758205",
                                    "name": "Feng Li"
                                },
                                {
                                    "authorId": "2267467406",
                                    "name": "Hao Zhang"
                                },
                                {
                                    "authorId": "2300086932",
                                    "name": "Kaichen Zhang"
                                },
                                {
                                    "authorId": "2315071527",
                                    "name": "Yanwei Li"
                                },
                                {
                                    "authorId": "2315193840",
                                    "name": "Ziwei Liu"
                                },
                                {
                                    "authorId": "2264692022",
                                    "name": "Chunyuan Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 867
                        },
                        "score": 0.84716796875
                    },
                    {
                        "id": "(Guo et al., 2025)",
                        "snippets": [
                            "Early opensource LMMs like LLaVA (Liu et al., 2023b) and MiniGPT-4 (Zhu et al., 2023) paired CLIP-based image encoders (Radford et al., 2021) with LLMs for multi-modal instruction tuning. Later models such as LLaVA-NeXT (Li et al., 2024a), LLaVA-OneVision (Li et al., 2024b), ShareGPT4V (Chen et al., 2023b), InternVL (Chen et al., 2023), SPHINX (Lin et al., 2023), and Qwen-VL (Qwen Team, 2024) expanded these capabilities through broader training datasets and advanced training strategies."
                        ],
                        "paper": {
                            "corpus_id": 276961504,
                            "title": "SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems",
                            "authors": [
                                {
                                    "authorId": "2237599228",
                                    "name": "Ziyu Guo"
                                },
                                {
                                    "authorId": "2350073299",
                                    "name": "Ray Zhang"
                                },
                                {
                                    "authorId": "2280286298",
                                    "name": "Hao Chen"
                                },
                                {
                                    "authorId": "2350025512",
                                    "name": "Jialin Gao"
                                },
                                {
                                    "authorId": "2293242031",
                                    "name": "Dongzhi Jiang"
                                },
                                {
                                    "authorId": "2254323687",
                                    "name": "Jiaze Wang"
                                },
                                {
                                    "authorId": "2274486861",
                                    "name": "Pheng-Ann Heng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.5654296875
                    },
                    {
                        "id": "(Radford et al., 2021)",
                        "snippets": [
                            "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."
                        ],
                        "paper": {
                            "corpus_id": 231591445,
                            "title": "Learning Transferable Visual Models From Natural Language Supervision",
                            "authors": [
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "2110935237",
                                    "name": "Jong Wook Kim"
                                },
                                {
                                    "authorId": "2004021329",
                                    "name": "Chris Hallacy"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "40087786",
                                    "name": "Gabriel Goh"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": "2115193883",
                                    "name": "Jack Clark"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 29867
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."
                        ],
                        "paper": {
                            "corpus_id": 263672058,
                            "title": "Improved Baselines with Visual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2143856368",
                                    "name": "Haotian Liu"
                                },
                                {
                                    "authorId": "2243126534",
                                    "name": "Chunyuan Li"
                                },
                                {
                                    "authorId": "1527091339",
                                    "name": "Yuheng Li"
                                },
                                {
                                    "authorId": "2256122200",
                                    "name": "Yong Jae Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 2824
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "The exponential growth of large language models (LLMs) has opened up numerous possibilities for multi-modal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foun-dation model (Intern VL), which scales up the vision foun-dation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models."
                        ],
                        "paper": {
                            "corpus_id": 266521410,
                            "title": "Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks",
                            "authors": [
                                {
                                    "authorId": "66350249",
                                    "name": "Zhe Chen"
                                },
                                {
                                    "authorId": "2109215916",
                                    "name": "Jiannan Wu"
                                },
                                {
                                    "authorId": "2257133501",
                                    "name": "Wenhai Wang"
                                },
                                {
                                    "authorId": "2276207009",
                                    "name": "Weijie Su"
                                },
                                {
                                    "authorId": "2155229619",
                                    "name": "Guo Chen"
                                },
                                {
                                    "authorId": "2191075284",
                                    "name": "Sen Xing"
                                },
                                {
                                    "authorId": "2276203785",
                                    "name": "Zhong Muyan"
                                },
                                {
                                    "authorId": "2276279994",
                                    "name": "Qinglong Zhang"
                                },
                                {
                                    "authorId": "2578924",
                                    "name": "Xizhou Zhu"
                                },
                                {
                                    "authorId": "152309485",
                                    "name": "Lewei Lu"
                                },
                                {
                                    "authorId": "2218579598",
                                    "name": "Bin Li"
                                },
                                {
                                    "authorId": "2253674868",
                                    "name": "Ping Luo"
                                },
                                {
                                    "authorId": "2276323159",
                                    "name": "Tong Lu"
                                },
                                {
                                    "authorId": "2258755556",
                                    "name": "Yu Qiao"
                                },
                                {
                                    "authorId": "3304536",
                                    "name": "Jifeng Dai"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 1215
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "LLaVA-1.5 Technical Innovations",
                "tldr": "LLaVA-1.5 introduced several key technical innovations including an upgraded MLP vision-language connector, increased image resolution from 224px to 336px, and the incorporation of academic task-oriented datasets to enhance performance across multiple benchmarks. (5 sources)",
                "text": "\nLLaVA-1.5 represented a significant advancement over the original LLaVA model through several technical innovations. One of the most important changes was replacing the original linear layer with a more sophisticated MLP (Multi-Layer Perceptron) vision-language connector, which proved to be \"surprisingly powerful and data-efficient\" <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This architectural improvement enabled better integration between visual and textual information.\n\nThe model also increased the input image resolution from 224px to 336px, using CLIP ViT-L/14 as its vision encoder, which resulted in 576 visual tokens (calculated as (336/14)\u00b2) <Paper corpusId=\"273186838\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This higher resolution allowed the model to capture more detailed visual information from images.\n\nPerhaps most significantly, LLaVA-1.5 incorporated more academic task-oriented visual question answering (VQA) datasets with response formatting prompts <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"274131643\" paperTitle=\"(Qu et al., 2024)\" isShortName></Paper>. This data enhancement strategy was remarkably efficient - the final 13B checkpoint used only 1.2M publicly available data points and could be fully trained in approximately one day on a single 8-A100 node <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\nThese improvements collectively enabled LLaVA-1.5 to achieve state-of-the-art performance across 11 benchmarks <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>. The model also laid important groundwork for addressing open challenges in large multimodal models, including \"scaling to higher resolution inputs, compositional capabilities, and model hallucination\" <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."
                        ],
                        "paper": {
                            "corpus_id": 263672058,
                            "title": "Improved Baselines with Visual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2143856368",
                                    "name": "Haotian Liu"
                                },
                                {
                                    "authorId": "2243126534",
                                    "name": "Chunyuan Li"
                                },
                                {
                                    "authorId": "1527091339",
                                    "name": "Yuheng Li"
                                },
                                {
                                    "authorId": "2256122200",
                                    "name": "Yong Jae Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 2824
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2024)",
                        "snippets": [
                            "\u2022 LLaVA-1.5 (Liu et al., 2024a) incorporates academic task-oriented datasets to enhance performance in VQA tasks and features an MLP vision-language connector, which improves upon the original linear layer utilized in LLaVA (Liu et al., 2024c). It uses CLIP ViT-L/14 (Radford et al., 2021) with a 336px resolution as its vision encoder, resulting in a total of (336/14) 2 = 576 visual tokens.\n\n\u2022 LLaVA-NeXT (Liu et al., 2024b) (also known as LLaVA-1.6) enhances visual reasoning, OCR, and world knowledge, offering four times higher image resolution (up to 1344x336) and improved performance in visual conversations. Its architecture includes a CLIP ViT-L/14 as a vision encoder, paired with Vicuna models ranging from 7B to 34B as a backbone language model.\n\n\u2022 LLaVA-OneVision (Li et al., 2024b) is a LVLM designed for task transfer across singleimage, multi-image, and video scenarios, with strong capabilities in video understanding through image-to-video task transfer. Its architecture consists of a Qwen2 language model (Yang et al., 2024) with 8B to 72B parameters, and the SigLIP vision encoder (Zhai et al., 2023), which processes images at a base resolution of 384x384, producing 729 visual tokens. The model employs a 2-layer MLP projector."
                        ],
                        "paper": {
                            "corpus_id": 273186838,
                            "title": "Intriguing Properties of Large Language and Vision Models",
                            "authors": [
                                {
                                    "authorId": "2119566888",
                                    "name": "Young-Jun Lee"
                                },
                                {
                                    "authorId": "9726578",
                                    "name": "ByungSoo Ko"
                                },
                                {
                                    "authorId": "2118626092",
                                    "name": "Han-Gyu Kim"
                                },
                                {
                                    "authorId": "2296345851",
                                    "name": "Yechan Hwang"
                                },
                                {
                                    "authorId": "2260282476",
                                    "name": "Ho-Jin Choi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.6640625
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "LLaVA-1.5 [34]. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement.\n\nLLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM."
                        ],
                        "paper": {
                            "corpus_id": 274437586,
                            "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
                            "authors": [
                                {
                                    "authorId": "2118051520",
                                    "name": "Qizhe Zhang"
                                },
                                {
                                    "authorId": "2292408664",
                                    "name": "Aosong Cheng"
                                },
                                {
                                    "authorId": "2331417542",
                                    "name": "Ming Lu"
                                },
                                {
                                    "authorId": "2275104296",
                                    "name": "Renrui Zhang"
                                },
                                {
                                    "authorId": "2333364107",
                                    "name": "Zhiyong Zhuo"
                                },
                                {
                                    "authorId": "2268711797",
                                    "name": "Jiajun Cao"
                                },
                                {
                                    "authorId": "2333442704",
                                    "name": "Shaobo Guo"
                                },
                                {
                                    "authorId": "2331326229",
                                    "name": "Qi She"
                                },
                                {
                                    "authorId": "2332857566",
                                    "name": "Shanghang Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 10
                        },
                        "score": 0.88232421875
                    },
                    {
                        "id": "(Qu et al., 2024)",
                        "snippets": [
                            "LLaVA-v1.5 (Liu et al., 2023) and LLaVA-v1.6(NeXT) [23] further improve LLaVA's performance through better data, higher resolution and stronger LLM."
                        ],
                        "paper": {
                            "corpus_id": 274131643,
                            "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2173939633",
                                    "name": "Tingyu Qu"
                                },
                                {
                                    "authorId": "2112132080",
                                    "name": "Mingxiao Li"
                                },
                                {
                                    "authorId": "1704728",
                                    "name": "T. Tuytelaars"
                                },
                                {
                                    "authorId": "1802161",
                                    "name": "M. Moens"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.64013671875
                    },
                    {
                        "id": "(Riggi et al., 2025)",
                        "snippets": [
                            "Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks."
                        ],
                        "paper": {
                            "corpus_id": 277452239,
                            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
                            "authors": [
                                {
                                    "authorId": "2292400830",
                                    "name": "S. Riggi"
                                },
                                {
                                    "authorId": "2042077694",
                                    "name": "T. Cecconello"
                                },
                                {
                                    "authorId": "2352941747",
                                    "name": "A. Pilzer"
                                },
                                {
                                    "authorId": "2352939581",
                                    "name": "S. Palazzo"
                                },
                                {
                                    "authorId": "2299008238",
                                    "name": "N. Gupta"
                                },
                                {
                                    "authorId": "2298907506",
                                    "name": "A. Hopkins"
                                },
                                {
                                    "authorId": "2258840598",
                                    "name": "C. Trigilio"
                                },
                                {
                                    "authorId": "2349648144",
                                    "name": "G. Umana"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.908203125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "LLaVA-NeXT Technical Innovations",
                "tldr": "LLaVA-NeXT (also known as LLaVA-1.6) introduced three key innovations: AnyRes for dynamic high-resolution image processing, expanded high-quality instruction datasets, and integration with advanced open-source LLMs, which collectively enhanced its visual reasoning, OCR, and multimodal document understanding capabilities. (8 sources)",
                "text": "\nLLaVA-NeXT represented a significant evolution from LLaVA-1.5, with several technical innovations that substantially improved performance across multiple domains. One of the most important advancements was the introduction of the AnyRes technique for handling high-resolution images <Paper corpusId=\"271719914\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. Unlike LLaVA-1.5's fixed resolution increase, LLaVA-NeXT implemented a dynamic high-resolution design that could scale input image resolution by up to four times, reaching 1344\u00d7336 compared to the 336\u00d7336 used in LLaVA-1.5 <Paper corpusId=\"271855335\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nThis approach enabled the model to select the optimal aspect ratio based on the input image's resolution. Technically, high-resolution images were split into several sub-images of the same size as the original image, which were individually encoded and then concatenated before being fed into the language model <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This patch-based processing of large images significantly enhanced the model's ability to capture finer-grained visual details <Paper corpusId=\"276647578\" paperTitle=\"(Alyakin et al., 2025)\" isShortName></Paper>.\n\nLLaVA-NeXT also featured an expanded and more diverse visual instruction-tuning dataset, incorporating ShareGPT-4V and LAION-GPT-V data, along with specialized OCR, document, and chart datasets <Paper corpusId=\"271855335\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This broader training data significantly contributed to the model's enhanced capabilities in visual reasoning, optical character recognition, and multimodal document understanding <Paper corpusId=\"271855335\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>.\n\nThe third key innovation was the utilization of the most advanced open-source LLMs available at the time <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>. LLaVA-NeXT's architecture continued to use CLIP ViT-L/14 as its vision encoder but paired it with Vicuna models ranging from 7B to 34B parameters as backbone language models <Paper corpusId=\"273186838\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. The model also retained and enhanced the multilayer projection from LLaVA-1.5 <Paper corpusId=\"276647578\" paperTitle=\"(Alyakin et al., 2025)\" isShortName></Paper>.\n\nThese technical innovations collectively positioned LLaVA-NeXT as part of a broader trend in vision-language models that expanded capabilities through broader training datasets and advanced training strategies <Paper corpusId=\"276961504\" paperTitle=\"(Guo et al., 2025)\" isShortName></Paper>. The model's improvements in handling high-resolution images and enhanced training approaches also influenced subsequent multimodal models that incorporated both image and video processing <Paper corpusId=\"277313523\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "The first LLaVA model [83] demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors similar to GPT-4V on previously unseen images and instructions for the first time. LLaVA-1.5 (Liu et al., 2023) significantly expands and improves the capabilities by incorporating more academicrelated instruction data, achieving SoTA performance on a dozens of benchmarks with a data-efficient recipe. LLaVA-NeXT [82] inherits this property, further pushing performance boundaries through three key techniques: AnyRes for handling high-resolution images, expanding high-quality instruction data, and utilizing the best open LLM available at the time."
                        ],
                        "paper": {
                            "corpus_id": 271719914,
                            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
                            "authors": [
                                {
                                    "authorId": "2310709478",
                                    "name": "Bo Li"
                                },
                                {
                                    "authorId": "2145784327",
                                    "name": "Yuanhan Zhang"
                                },
                                {
                                    "authorId": "2325209062",
                                    "name": "Dong Guo"
                                },
                                {
                                    "authorId": "2310650738",
                                    "name": "Renrui Zhang"
                                },
                                {
                                    "authorId": "2310758205",
                                    "name": "Feng Li"
                                },
                                {
                                    "authorId": "2267467406",
                                    "name": "Hao Zhang"
                                },
                                {
                                    "authorId": "2300086932",
                                    "name": "Kaichen Zhang"
                                },
                                {
                                    "authorId": "2315071527",
                                    "name": "Yanwei Li"
                                },
                                {
                                    "authorId": "2315193840",
                                    "name": "Ziwei Liu"
                                },
                                {
                                    "authorId": "2264692022",
                                    "name": "Chunyuan Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 867
                        },
                        "score": 0.84716796875
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "LLaVA-NeXT [24] is an improved version of LLaVA 1.5, with a focus on enhanced visual reasoning, optical character recognition (OCR), and multi-modal document understanding. LLaVA-NeXT scales the input image resolution of input images by 4\u00d7, up to 1344 \u00d7 336 compared to 336 \u00d7 336 in LLaVA 1.5 to enhance its ability to grasp finer-grained visual cues. LLaVA-NeXT is also trained on a more diverse and realistic visual instruction-tuning dataset (ShareGPT-4V and LAION-GPT-V), as well as a range of OCR, document, and chart datasets."
                        ],
                        "paper": {
                            "corpus_id": 271855335,
                            "title": "Revisiting Multi-Modal LLM Evaluation",
                            "authors": [
                                {
                                    "authorId": "2316020150",
                                    "name": "Jian Lu"
                                },
                                {
                                    "authorId": "2315987232",
                                    "name": "Shikhar Srivastava"
                                },
                                {
                                    "authorId": "2315947134",
                                    "name": "Junyu Chen"
                                },
                                {
                                    "authorId": "153677280",
                                    "name": "Robik Shrestha"
                                },
                                {
                                    "authorId": "47309247",
                                    "name": "Manoj Acharya"
                                },
                                {
                                    "authorId": "33315685",
                                    "name": "Kushal Kafle"
                                },
                                {
                                    "authorId": "2303258758",
                                    "name": "Christopher Kanan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.724609375
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "LLaVA-1.5 [34]. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement.\n\nLLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM."
                        ],
                        "paper": {
                            "corpus_id": 274437586,
                            "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
                            "authors": [
                                {
                                    "authorId": "2118051520",
                                    "name": "Qizhe Zhang"
                                },
                                {
                                    "authorId": "2292408664",
                                    "name": "Aosong Cheng"
                                },
                                {
                                    "authorId": "2331417542",
                                    "name": "Ming Lu"
                                },
                                {
                                    "authorId": "2275104296",
                                    "name": "Renrui Zhang"
                                },
                                {
                                    "authorId": "2333364107",
                                    "name": "Zhiyong Zhuo"
                                },
                                {
                                    "authorId": "2268711797",
                                    "name": "Jiajun Cao"
                                },
                                {
                                    "authorId": "2333442704",
                                    "name": "Shaobo Guo"
                                },
                                {
                                    "authorId": "2331326229",
                                    "name": "Qi She"
                                },
                                {
                                    "authorId": "2332857566",
                                    "name": "Shanghang Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 10
                        },
                        "score": 0.88232421875
                    },
                    {
                        "id": "(Alyakin et al., 2025)",
                        "snippets": [
                            "We built on LLaVA-Next's improvements -including its multilayer projection, patch-based processing of large images, and enhanced pre-training (Fig. S6a)."
                        ],
                        "paper": {
                            "corpus_id": 276647578,
                            "title": "Repurposing the scientific literature with vision-language models",
                            "authors": [
                                {
                                    "authorId": "2328643882",
                                    "name": "Anton Alyakin"
                                },
                                {
                                    "authorId": "2325728846",
                                    "name": "Jaden Stryker"
                                },
                                {
                                    "authorId": "1941269137",
                                    "name": "D. Alber"
                                },
                                {
                                    "authorId": "2248607147",
                                    "name": "Karl L. Sangwon"
                                },
                                {
                                    "authorId": "2347532646",
                                    "name": "Brandon Duderstadt"
                                },
                                {
                                    "authorId": "2269042919",
                                    "name": "Akshay V. Save"
                                },
                                {
                                    "authorId": "5861736",
                                    "name": "David B. Kurland"
                                },
                                {
                                    "authorId": "2331397171",
                                    "name": "Spencer Frome"
                                },
                                {
                                    "authorId": "2347581153",
                                    "name": "Shrutika Singh"
                                },
                                {
                                    "authorId": "2347550668",
                                    "name": "Jeff Zhang"
                                },
                                {
                                    "authorId": "2321510385",
                                    "name": "Eunice Yang"
                                },
                                {
                                    "authorId": "2347589607",
                                    "name": "Ki Yun Park"
                                },
                                {
                                    "authorId": "6068765",
                                    "name": "C. Orillac"
                                },
                                {
                                    "authorId": "49558131",
                                    "name": "Aly A. Valliani"
                                },
                                {
                                    "authorId": "2321514213",
                                    "name": "Sean Neifert"
                                },
                                {
                                    "authorId": "2148770758",
                                    "name": "Albert Liu"
                                },
                                {
                                    "authorId": "2273924323",
                                    "name": "Aneek Patel"
                                },
                                {
                                    "authorId": "2219711063",
                                    "name": "Christopher Livia"
                                },
                                {
                                    "authorId": "2291574490",
                                    "name": "Darryl Lau"
                                },
                                {
                                    "authorId": "2347528853",
                                    "name": "Ilya Laufer"
                                },
                                {
                                    "authorId": "47034244",
                                    "name": "P. Rozman"
                                },
                                {
                                    "authorId": "39243010",
                                    "name": "E. Hidalgo"
                                },
                                {
                                    "authorId": "145327270",
                                    "name": "H. Riina"
                                },
                                {
                                    "authorId": "2281737315",
                                    "name": "Rui Feng"
                                },
                                {
                                    "authorId": "2249367961",
                                    "name": "T. Hollon"
                                },
                                {
                                    "authorId": "8340776",
                                    "name": "Yindalon Aphinyanagphongs"
                                },
                                {
                                    "authorId": "6473601",
                                    "name": "J. Golfinos"
                                },
                                {
                                    "authorId": "2347529122",
                                    "name": "Laura Snyder"
                                },
                                {
                                    "authorId": "2334000180",
                                    "name": "Eric C. Leuthardt"
                                },
                                {
                                    "authorId": "2247415145",
                                    "name": "Douglas Kondziolka"
                                },
                                {
                                    "authorId": "2181708076",
                                    "name": "E. Oermann"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.6435546875
                    },
                    {
                        "id": "(Riggi et al., 2025)",
                        "snippets": [
                            "Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks."
                        ],
                        "paper": {
                            "corpus_id": 277452239,
                            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
                            "authors": [
                                {
                                    "authorId": "2292400830",
                                    "name": "S. Riggi"
                                },
                                {
                                    "authorId": "2042077694",
                                    "name": "T. Cecconello"
                                },
                                {
                                    "authorId": "2352941747",
                                    "name": "A. Pilzer"
                                },
                                {
                                    "authorId": "2352939581",
                                    "name": "S. Palazzo"
                                },
                                {
                                    "authorId": "2299008238",
                                    "name": "N. Gupta"
                                },
                                {
                                    "authorId": "2298907506",
                                    "name": "A. Hopkins"
                                },
                                {
                                    "authorId": "2258840598",
                                    "name": "C. Trigilio"
                                },
                                {
                                    "authorId": "2349648144",
                                    "name": "G. Umana"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.908203125
                    },
                    {
                        "id": "(Lee et al., 2024)",
                        "snippets": [
                            "\u2022 LLaVA-1.5 (Liu et al., 2024a) incorporates academic task-oriented datasets to enhance performance in VQA tasks and features an MLP vision-language connector, which improves upon the original linear layer utilized in LLaVA (Liu et al., 2024c). It uses CLIP ViT-L/14 (Radford et al., 2021) with a 336px resolution as its vision encoder, resulting in a total of (336/14) 2 = 576 visual tokens.\n\n\u2022 LLaVA-NeXT (Liu et al., 2024b) (also known as LLaVA-1.6) enhances visual reasoning, OCR, and world knowledge, offering four times higher image resolution (up to 1344x336) and improved performance in visual conversations. Its architecture includes a CLIP ViT-L/14 as a vision encoder, paired with Vicuna models ranging from 7B to 34B as a backbone language model.\n\n\u2022 LLaVA-OneVision (Li et al., 2024b) is a LVLM designed for task transfer across singleimage, multi-image, and video scenarios, with strong capabilities in video understanding through image-to-video task transfer. Its architecture consists of a Qwen2 language model (Yang et al., 2024) with 8B to 72B parameters, and the SigLIP vision encoder (Zhai et al., 2023), which processes images at a base resolution of 384x384, producing 729 visual tokens. The model employs a 2-layer MLP projector."
                        ],
                        "paper": {
                            "corpus_id": 273186838,
                            "title": "Intriguing Properties of Large Language and Vision Models",
                            "authors": [
                                {
                                    "authorId": "2119566888",
                                    "name": "Young-Jun Lee"
                                },
                                {
                                    "authorId": "9726578",
                                    "name": "ByungSoo Ko"
                                },
                                {
                                    "authorId": "2118626092",
                                    "name": "Han-Gyu Kim"
                                },
                                {
                                    "authorId": "2296345851",
                                    "name": "Yechan Hwang"
                                },
                                {
                                    "authorId": "2260282476",
                                    "name": "Ho-Jin Choi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.6640625
                    },
                    {
                        "id": "(Guo et al., 2025)",
                        "snippets": [
                            "Early opensource LMMs like LLaVA (Liu et al., 2023b) and MiniGPT-4 (Zhu et al., 2023) paired CLIP-based image encoders (Radford et al., 2021) with LLMs for multi-modal instruction tuning. Later models such as LLaVA-NeXT (Li et al., 2024a), LLaVA-OneVision (Li et al., 2024b), ShareGPT4V (Chen et al., 2023b), InternVL (Chen et al., 2023), SPHINX (Lin et al., 2023), and Qwen-VL (Qwen Team, 2024) expanded these capabilities through broader training datasets and advanced training strategies."
                        ],
                        "paper": {
                            "corpus_id": 276961504,
                            "title": "SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems",
                            "authors": [
                                {
                                    "authorId": "2237599228",
                                    "name": "Ziyu Guo"
                                },
                                {
                                    "authorId": "2350073299",
                                    "name": "Ray Zhang"
                                },
                                {
                                    "authorId": "2280286298",
                                    "name": "Hao Chen"
                                },
                                {
                                    "authorId": "2350025512",
                                    "name": "Jialin Gao"
                                },
                                {
                                    "authorId": "2293242031",
                                    "name": "Dongzhi Jiang"
                                },
                                {
                                    "authorId": "2254323687",
                                    "name": "Jiaze Wang"
                                },
                                {
                                    "authorId": "2274486861",
                                    "name": "Pheng-Ann Heng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.5654296875
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "Recent vision-LLM models like LLaVA-NeXT [39], LLaVA-OneVision [28], LLaVA-Video [79], Qwen2-VL [60], and mPlug-Owl3 [71] consider multi-stage training with both video and image, which substantially improves the model performance."
                        ],
                        "paper": {
                            "corpus_id": 277313523,
                            "title": "PAVE: Patching and Adapting Video Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2333863752",
                                    "name": "Zhuoming Liu"
                                },
                                {
                                    "authorId": "2321684254",
                                    "name": "Yiquan Li"
                                },
                                {
                                    "authorId": "2342927060",
                                    "name": "Khoi Duc Nguyen"
                                },
                                {
                                    "authorId": "1828787912",
                                    "name": "Yiwu Zhong"
                                },
                                {
                                    "authorId": "2333742645",
                                    "name": "Yin Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.57421875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "LLaVA-OneVision Technical Innovations",
                "tldr": "LLaVA-OneVision introduced a unified architecture capable of handling single images, multiple images, and video within the same framework, enabling powerful cross-modal transfer learning. Its key innovations include dynamic image resolution handling, the SigLIP vision encoder with higher base resolution, and integration with Qwen2 language models. (7 sources)",
                "text": "\nLLaVA-OneVision represents a significant advancement in large multimodal models by addressing a critical limitation of previous versions: the ability to handle different visual modalities simultaneously. While LLaVA-1.5 and LLaVA-NeXT focused primarily on single-image processing, LLaVA-OneVision was designed to unify \"single-image, multi-image, and video scenarios\" within a single model architecture <Paper corpusId=\"273186838\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273821149\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>.\n\nThe model introduced several key technical innovations. First, it employed the SigLIP vision encoder, which processes images at a higher base resolution of 384\u00d7384 pixels, producing 729 visual tokens - a substantial increase from LLaVA-1.5's 576 tokens <Paper corpusId=\"273186838\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. This enhanced resolution capability, combined with a dynamic image resolution approach similar to LLaVA-NeXT's AnyRes technique, allowed the model to better handle visual details across different scenarios <Paper corpusId=\"276724729\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nSecond, LLaVA-OneVision incorporated more powerful language models, specifically the Qwen2 family ranging from 8B to 72B parameters, connected to the vision encoder through a 2-layer MLP projector <Paper corpusId=\"273186838\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. This architecture choice provided a stronger foundation for cross-modal reasoning.\n\nPerhaps most importantly, LLaVA-OneVision pioneered an approach that enables powerful transfer learning across different visual modalities and scenarios <Paper corpusId=\"276938164\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>. This cross-modal transfer learning capability resulted in \"the emergence of new capabilities\" that weren't present in previous LLaVA versions <Paper corpusId=\"276938164\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>.\n\nThe model's unified approach to handling multiple visual formats represented a significant shift in multimodal AI development strategy. Rather than creating separate specialized models for different visual tasks, LLaVA-OneVision demonstrated that a single architecture could effectively process single images, multiple images, videos, and even audio within the same framework <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>. This multi-stage training approach with both video and image data has become increasingly common in subsequent vision-language models, as it \"substantially improves the model performance\" across diverse tasks <Paper corpusId=\"277313523\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Lee et al., 2024)",
                        "snippets": [
                            "\u2022 LLaVA-1.5 (Liu et al., 2024a) incorporates academic task-oriented datasets to enhance performance in VQA tasks and features an MLP vision-language connector, which improves upon the original linear layer utilized in LLaVA (Liu et al., 2024c). It uses CLIP ViT-L/14 (Radford et al., 2021) with a 336px resolution as its vision encoder, resulting in a total of (336/14) 2 = 576 visual tokens.\n\n\u2022 LLaVA-NeXT (Liu et al., 2024b) (also known as LLaVA-1.6) enhances visual reasoning, OCR, and world knowledge, offering four times higher image resolution (up to 1344x336) and improved performance in visual conversations. Its architecture includes a CLIP ViT-L/14 as a vision encoder, paired with Vicuna models ranging from 7B to 34B as a backbone language model.\n\n\u2022 LLaVA-OneVision (Li et al., 2024b) is a LVLM designed for task transfer across singleimage, multi-image, and video scenarios, with strong capabilities in video understanding through image-to-video task transfer. Its architecture consists of a Qwen2 language model (Yang et al., 2024) with 8B to 72B parameters, and the SigLIP vision encoder (Zhai et al., 2023), which processes images at a base resolution of 384x384, producing 729 visual tokens. The model employs a 2-layer MLP projector."
                        ],
                        "paper": {
                            "corpus_id": 273186838,
                            "title": "Intriguing Properties of Large Language and Vision Models",
                            "authors": [
                                {
                                    "authorId": "2119566888",
                                    "name": "Young-Jun Lee"
                                },
                                {
                                    "authorId": "9726578",
                                    "name": "ByungSoo Ko"
                                },
                                {
                                    "authorId": "2118626092",
                                    "name": "Han-Gyu Kim"
                                },
                                {
                                    "authorId": "2296345851",
                                    "name": "Yechan Hwang"
                                },
                                {
                                    "authorId": "2260282476",
                                    "name": "Ho-Jin Choi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.6640625
                    },
                    {
                        "id": "(Dai et al., 2024)",
                        "snippets": [
                            "LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios."
                        ],
                        "paper": {
                            "corpus_id": 273821149,
                            "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model",
                            "authors": [
                                {
                                    "authorId": "2082462168",
                                    "name": "Dawei Dai"
                                },
                                {
                                    "authorId": "2329189750",
                                    "name": "Xu Long"
                                },
                                {
                                    "authorId": "2136494548",
                                    "name": "Yutang Li"
                                },
                                {
                                    "authorId": "2310835404",
                                    "name": "Yuanhui Zhang"
                                },
                                {
                                    "authorId": "2147222435",
                                    "name": "Shuy Xia"
                                }
                            ],
                            "year": 2024,
                            "venue": "Information Fusion",
                            "n_citations": 2
                        },
                        "score": 0.8681640625
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "LLaVA-1.5 (Liu et al., 2024) explored adding high-quality multitask training data, and scaling up the resolution and LLM size to boost MLLM performance. LLaVA-OneVision (Li et al., 2024a) explored to unify dynamic image resolution, multi-image, and video into a unified input format."
                        ],
                        "paper": {
                            "corpus_id": 276724729,
                            "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2261886750",
                                    "name": "Xiao Wang"
                                },
                                {
                                    "authorId": "2348096687",
                                    "name": "Jingyun Hua"
                                },
                                {
                                    "authorId": "2348609405",
                                    "name": "Weihong Lin"
                                },
                                {
                                    "authorId": "2145784116",
                                    "name": "Yuanxing Zhang"
                                },
                                {
                                    "authorId": "2257136363",
                                    "name": "Fuzheng Zhang"
                                },
                                {
                                    "authorId": "2292205089",
                                    "name": "Jianlong Wu"
                                },
                                {
                                    "authorId": "2323902668",
                                    "name": "Di Zhang"
                                },
                                {
                                    "authorId": "2284688853",
                                    "name": "Liqiang Nie"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.591796875
                    },
                    {
                        "id": "(Yang et al., 2025)",
                        "snippets": [
                            "LLaVA-OneVision simultaneously pushes the performance boundaries of open-domain Large Multimodal Models (LMMs) across three key computer vision scenarios-single image, multiple images, and video-while enabling powerful transfer learning across different modalities and scenarios, resulting in the emergence of new capabilities [28]."
                        ],
                        "paper": {
                            "corpus_id": 276938164,
                            "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
                            "authors": [
                                {
                                    "authorId": "2308224151",
                                    "name": "Xiaoda Yang"
                                },
                                {
                                    "authorId": "2350336954",
                                    "name": "JunYu Lu"
                                },
                                {
                                    "authorId": "2220747584",
                                    "name": "Hongshun Qiu"
                                },
                                {
                                    "authorId": "2350180388",
                                    "name": "Sijing Li"
                                },
                                {
                                    "authorId": "2349632427",
                                    "name": "Hao Li"
                                },
                                {
                                    "authorId": "72890649",
                                    "name": "Shengpeng Ji"
                                },
                                {
                                    "authorId": "2349737557",
                                    "name": "Xudong Tang"
                                },
                                {
                                    "authorId": "2349670795",
                                    "name": "Jiayang Xu"
                                },
                                {
                                    "authorId": "2329894630",
                                    "name": "Jiaqi Duan"
                                },
                                {
                                    "authorId": "2112347676",
                                    "name": "Ziyue Jiang"
                                },
                                {
                                    "authorId": "2349737916",
                                    "name": "Cong Lin"
                                },
                                {
                                    "authorId": "2328348412",
                                    "name": "Sihang Cai"
                                },
                                {
                                    "authorId": "2266912737",
                                    "name": "Zejian Xie"
                                },
                                {
                                    "authorId": "2352067468",
                                    "name": "Zhuoyang Song"
                                },
                                {
                                    "authorId": "2266803682",
                                    "name": "Songxin Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.66064453125
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously."
                        ],
                        "paper": {
                            "corpus_id": 275405668,
                            "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
                            "authors": [
                                {
                                    "authorId": "2313875208",
                                    "name": "Jia-Xin Zhao"
                                },
                                {
                                    "authorId": "2342467513",
                                    "name": "Boyuan Sun"
                                },
                                {
                                    "authorId": "2339423925",
                                    "name": "Xiang Chen"
                                },
                                {
                                    "authorId": "2339268195",
                                    "name": "Xihan Wei"
                                },
                                {
                                    "authorId": "2339266488",
                                    "name": "Qibin Hou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.66552734375
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "Recent vision-LLM models like LLaVA-NeXT [39], LLaVA-OneVision [28], LLaVA-Video [79], Qwen2-VL [60], and mPlug-Owl3 [71] consider multi-stage training with both video and image, which substantially improves the model performance."
                        ],
                        "paper": {
                            "corpus_id": 277313523,
                            "title": "PAVE: Patching and Adapting Video Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2333863752",
                                    "name": "Zhuoming Liu"
                                },
                                {
                                    "authorId": "2321684254",
                                    "name": "Yiquan Li"
                                },
                                {
                                    "authorId": "2342927060",
                                    "name": "Khoi Duc Nguyen"
                                },
                                {
                                    "authorId": "1828787912",
                                    "name": "Yiwu Zhong"
                                },
                                {
                                    "authorId": "2333742645",
                                    "name": "Yin Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.57421875
                    },
                    {
                        "id": "(Riggi et al., 2025)",
                        "snippets": [
                            "Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks."
                        ],
                        "paper": {
                            "corpus_id": 277452239,
                            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
                            "authors": [
                                {
                                    "authorId": "2292400830",
                                    "name": "S. Riggi"
                                },
                                {
                                    "authorId": "2042077694",
                                    "name": "T. Cecconello"
                                },
                                {
                                    "authorId": "2352941747",
                                    "name": "A. Pilzer"
                                },
                                {
                                    "authorId": "2352939581",
                                    "name": "S. Palazzo"
                                },
                                {
                                    "authorId": "2299008238",
                                    "name": "N. Gupta"
                                },
                                {
                                    "authorId": "2298907506",
                                    "name": "A. Hopkins"
                                },
                                {
                                    "authorId": "2258840598",
                                    "name": "C. Trigilio"
                                },
                                {
                                    "authorId": "2349648144",
                                    "name": "G. Umana"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.908203125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Comparative Performance Improvements",
                "tldr": "Each iteration of the LLaVA model family delivered significant performance improvements across various benchmarks, with LLaVA-1.5 establishing state-of-the-art results across 11 benchmarks, LLaVA-NeXT enhancing visual reasoning and OCR capabilities, and LLaVA-OneVision unifying performance across single-image, multi-image, and video tasks. (10 sources)",
                "text": "\nThe evolution of the LLaVA model family demonstrates clear progression in multimodal capabilities, with each version introducing innovations that directly translated to performance improvements. LLaVA-1.5 significantly expanded the capabilities of the original model by incorporating more academic-related instruction data, which helped it achieve state-of-the-art performance across 11 benchmarks through its data-efficient recipe <Paper corpusId=\"271719914\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This version's performance improvements were particularly notable given its resource efficiency - the final 13B checkpoint used only 1.2M publicly available data points and could be fully trained in approximately one day on a single 8-A100 node <Paper corpusId=\"273186838\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\nLLaVA-NeXT (also known as LLaVA-1.6) built upon these foundations to deliver enhanced performance in specific domains. Its four-fold increase in image resolution (up to 1344\u00d7336 compared to 336\u00d7336 in LLaVA-1.5) significantly improved the model's ability to capture fine-grained visual details <Paper corpusId=\"271855335\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273186838\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. This resolution enhancement, combined with training on more diverse and realistic visual instruction-tuning datasets including ShareGPT-4V and specialized OCR, document, and chart datasets, resulted in substantial improvements in visual reasoning, optical character recognition, and multimodal document understanding <Paper corpusId=\"271855335\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"274131643\" paperTitle=\"(Qu et al., 2024)\" isShortName></Paper>. The dynamic high-resolution design allowed LLaVA-NeXT to select the optimal aspect ratio based on input image resolution, providing flexibility that further enhanced performance <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nLLaVA-OneVision represented a paradigm shift in performance by simultaneously addressing multiple visual modalities. While previous versions showed improvements in single-image tasks, LLaVA-OneVision pushed performance boundaries across three key computer vision scenarios: single image, multiple images, and video <Paper corpusId=\"276938164\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"273821149\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>. This unified approach not only improved performance across individual modalities but also enabled powerful transfer learning between different scenarios, resulting in the emergence of entirely new capabilities <Paper corpusId=\"276938164\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>. By encoding different types of data into vectors of the same dimension, LLaVA-OneVision extended the model's capabilities to handle multiple modalities simultaneously, including audio <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276724729\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nAcross the LLaVA family evolution, we observe a consistent pattern of performance improvements driven by three key factors: increased resolution and enhanced visual processing, expanded high-quality training data across diverse tasks, and integration with increasingly powerful language models <Paper corpusId=\"274131643\" paperTitle=\"(Qu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"276724729\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. These technical innovations collectively advanced the state of multimodal AI systems, with each iteration addressing specific limitations of its predecessors.",
                "citations": [
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "The first LLaVA model [83] demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors similar to GPT-4V on previously unseen images and instructions for the first time. LLaVA-1.5 (Liu et al., 2023) significantly expands and improves the capabilities by incorporating more academicrelated instruction data, achieving SoTA performance on a dozens of benchmarks with a data-efficient recipe. LLaVA-NeXT [82] inherits this property, further pushing performance boundaries through three key techniques: AnyRes for handling high-resolution images, expanding high-quality instruction data, and utilizing the best open LLM available at the time."
                        ],
                        "paper": {
                            "corpus_id": 271719914,
                            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
                            "authors": [
                                {
                                    "authorId": "2310709478",
                                    "name": "Bo Li"
                                },
                                {
                                    "authorId": "2145784327",
                                    "name": "Yuanhan Zhang"
                                },
                                {
                                    "authorId": "2325209062",
                                    "name": "Dong Guo"
                                },
                                {
                                    "authorId": "2310650738",
                                    "name": "Renrui Zhang"
                                },
                                {
                                    "authorId": "2310758205",
                                    "name": "Feng Li"
                                },
                                {
                                    "authorId": "2267467406",
                                    "name": "Hao Zhang"
                                },
                                {
                                    "authorId": "2300086932",
                                    "name": "Kaichen Zhang"
                                },
                                {
                                    "authorId": "2315071527",
                                    "name": "Yanwei Li"
                                },
                                {
                                    "authorId": "2315193840",
                                    "name": "Ziwei Liu"
                                },
                                {
                                    "authorId": "2264692022",
                                    "name": "Chunyuan Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 867
                        },
                        "score": 0.84716796875
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."
                        ],
                        "paper": {
                            "corpus_id": 263672058,
                            "title": "Improved Baselines with Visual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2143856368",
                                    "name": "Haotian Liu"
                                },
                                {
                                    "authorId": "2243126534",
                                    "name": "Chunyuan Li"
                                },
                                {
                                    "authorId": "1527091339",
                                    "name": "Yuheng Li"
                                },
                                {
                                    "authorId": "2256122200",
                                    "name": "Yong Jae Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 2824
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2024)",
                        "snippets": [
                            "\u2022 LLaVA-1.5 (Liu et al., 2024a) incorporates academic task-oriented datasets to enhance performance in VQA tasks and features an MLP vision-language connector, which improves upon the original linear layer utilized in LLaVA (Liu et al., 2024c). It uses CLIP ViT-L/14 (Radford et al., 2021) with a 336px resolution as its vision encoder, resulting in a total of (336/14) 2 = 576 visual tokens.\n\n\u2022 LLaVA-NeXT (Liu et al., 2024b) (also known as LLaVA-1.6) enhances visual reasoning, OCR, and world knowledge, offering four times higher image resolution (up to 1344x336) and improved performance in visual conversations. Its architecture includes a CLIP ViT-L/14 as a vision encoder, paired with Vicuna models ranging from 7B to 34B as a backbone language model.\n\n\u2022 LLaVA-OneVision (Li et al., 2024b) is a LVLM designed for task transfer across singleimage, multi-image, and video scenarios, with strong capabilities in video understanding through image-to-video task transfer. Its architecture consists of a Qwen2 language model (Yang et al., 2024) with 8B to 72B parameters, and the SigLIP vision encoder (Zhai et al., 2023), which processes images at a base resolution of 384x384, producing 729 visual tokens. The model employs a 2-layer MLP projector."
                        ],
                        "paper": {
                            "corpus_id": 273186838,
                            "title": "Intriguing Properties of Large Language and Vision Models",
                            "authors": [
                                {
                                    "authorId": "2119566888",
                                    "name": "Young-Jun Lee"
                                },
                                {
                                    "authorId": "9726578",
                                    "name": "ByungSoo Ko"
                                },
                                {
                                    "authorId": "2118626092",
                                    "name": "Han-Gyu Kim"
                                },
                                {
                                    "authorId": "2296345851",
                                    "name": "Yechan Hwang"
                                },
                                {
                                    "authorId": "2260282476",
                                    "name": "Ho-Jin Choi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.6640625
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "LLaVA-NeXT [24] is an improved version of LLaVA 1.5, with a focus on enhanced visual reasoning, optical character recognition (OCR), and multi-modal document understanding. LLaVA-NeXT scales the input image resolution of input images by 4\u00d7, up to 1344 \u00d7 336 compared to 336 \u00d7 336 in LLaVA 1.5 to enhance its ability to grasp finer-grained visual cues. LLaVA-NeXT is also trained on a more diverse and realistic visual instruction-tuning dataset (ShareGPT-4V and LAION-GPT-V), as well as a range of OCR, document, and chart datasets."
                        ],
                        "paper": {
                            "corpus_id": 271855335,
                            "title": "Revisiting Multi-Modal LLM Evaluation",
                            "authors": [
                                {
                                    "authorId": "2316020150",
                                    "name": "Jian Lu"
                                },
                                {
                                    "authorId": "2315987232",
                                    "name": "Shikhar Srivastava"
                                },
                                {
                                    "authorId": "2315947134",
                                    "name": "Junyu Chen"
                                },
                                {
                                    "authorId": "153677280",
                                    "name": "Robik Shrestha"
                                },
                                {
                                    "authorId": "47309247",
                                    "name": "Manoj Acharya"
                                },
                                {
                                    "authorId": "33315685",
                                    "name": "Kushal Kafle"
                                },
                                {
                                    "authorId": "2303258758",
                                    "name": "Christopher Kanan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.724609375
                    },
                    {
                        "id": "(Qu et al., 2024)",
                        "snippets": [
                            "LLaVA-v1.5 (Liu et al., 2023) and LLaVA-v1.6(NeXT) [23] further improve LLaVA's performance through better data, higher resolution and stronger LLM."
                        ],
                        "paper": {
                            "corpus_id": 274131643,
                            "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2173939633",
                                    "name": "Tingyu Qu"
                                },
                                {
                                    "authorId": "2112132080",
                                    "name": "Mingxiao Li"
                                },
                                {
                                    "authorId": "1704728",
                                    "name": "T. Tuytelaars"
                                },
                                {
                                    "authorId": "1802161",
                                    "name": "M. Moens"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.64013671875
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "LLaVA-1.5 [34]. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement.\n\nLLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM."
                        ],
                        "paper": {
                            "corpus_id": 274437586,
                            "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
                            "authors": [
                                {
                                    "authorId": "2118051520",
                                    "name": "Qizhe Zhang"
                                },
                                {
                                    "authorId": "2292408664",
                                    "name": "Aosong Cheng"
                                },
                                {
                                    "authorId": "2331417542",
                                    "name": "Ming Lu"
                                },
                                {
                                    "authorId": "2275104296",
                                    "name": "Renrui Zhang"
                                },
                                {
                                    "authorId": "2333364107",
                                    "name": "Zhiyong Zhuo"
                                },
                                {
                                    "authorId": "2268711797",
                                    "name": "Jiajun Cao"
                                },
                                {
                                    "authorId": "2333442704",
                                    "name": "Shaobo Guo"
                                },
                                {
                                    "authorId": "2331326229",
                                    "name": "Qi She"
                                },
                                {
                                    "authorId": "2332857566",
                                    "name": "Shanghang Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 10
                        },
                        "score": 0.88232421875
                    },
                    {
                        "id": "(Yang et al., 2025)",
                        "snippets": [
                            "LLaVA-OneVision simultaneously pushes the performance boundaries of open-domain Large Multimodal Models (LMMs) across three key computer vision scenarios-single image, multiple images, and video-while enabling powerful transfer learning across different modalities and scenarios, resulting in the emergence of new capabilities [28]."
                        ],
                        "paper": {
                            "corpus_id": 276938164,
                            "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
                            "authors": [
                                {
                                    "authorId": "2308224151",
                                    "name": "Xiaoda Yang"
                                },
                                {
                                    "authorId": "2350336954",
                                    "name": "JunYu Lu"
                                },
                                {
                                    "authorId": "2220747584",
                                    "name": "Hongshun Qiu"
                                },
                                {
                                    "authorId": "2350180388",
                                    "name": "Sijing Li"
                                },
                                {
                                    "authorId": "2349632427",
                                    "name": "Hao Li"
                                },
                                {
                                    "authorId": "72890649",
                                    "name": "Shengpeng Ji"
                                },
                                {
                                    "authorId": "2349737557",
                                    "name": "Xudong Tang"
                                },
                                {
                                    "authorId": "2349670795",
                                    "name": "Jiayang Xu"
                                },
                                {
                                    "authorId": "2329894630",
                                    "name": "Jiaqi Duan"
                                },
                                {
                                    "authorId": "2112347676",
                                    "name": "Ziyue Jiang"
                                },
                                {
                                    "authorId": "2349737916",
                                    "name": "Cong Lin"
                                },
                                {
                                    "authorId": "2328348412",
                                    "name": "Sihang Cai"
                                },
                                {
                                    "authorId": "2266912737",
                                    "name": "Zejian Xie"
                                },
                                {
                                    "authorId": "2352067468",
                                    "name": "Zhuoyang Song"
                                },
                                {
                                    "authorId": "2266803682",
                                    "name": "Songxin Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.66064453125
                    },
                    {
                        "id": "(Dai et al., 2024)",
                        "snippets": [
                            "LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios."
                        ],
                        "paper": {
                            "corpus_id": 273821149,
                            "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model",
                            "authors": [
                                {
                                    "authorId": "2082462168",
                                    "name": "Dawei Dai"
                                },
                                {
                                    "authorId": "2329189750",
                                    "name": "Xu Long"
                                },
                                {
                                    "authorId": "2136494548",
                                    "name": "Yutang Li"
                                },
                                {
                                    "authorId": "2310835404",
                                    "name": "Yuanhui Zhang"
                                },
                                {
                                    "authorId": "2147222435",
                                    "name": "Shuy Xia"
                                }
                            ],
                            "year": 2024,
                            "venue": "Information Fusion",
                            "n_citations": 2
                        },
                        "score": 0.8681640625
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously."
                        ],
                        "paper": {
                            "corpus_id": 275405668,
                            "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
                            "authors": [
                                {
                                    "authorId": "2313875208",
                                    "name": "Jia-Xin Zhao"
                                },
                                {
                                    "authorId": "2342467513",
                                    "name": "Boyuan Sun"
                                },
                                {
                                    "authorId": "2339423925",
                                    "name": "Xiang Chen"
                                },
                                {
                                    "authorId": "2339268195",
                                    "name": "Xihan Wei"
                                },
                                {
                                    "authorId": "2339266488",
                                    "name": "Qibin Hou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.66552734375
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "LLaVA-1.5 (Liu et al., 2024) explored adding high-quality multitask training data, and scaling up the resolution and LLM size to boost MLLM performance. LLaVA-OneVision (Li et al., 2024a) explored to unify dynamic image resolution, multi-image, and video into a unified input format."
                        ],
                        "paper": {
                            "corpus_id": 276724729,
                            "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2261886750",
                                    "name": "Xiao Wang"
                                },
                                {
                                    "authorId": "2348096687",
                                    "name": "Jingyun Hua"
                                },
                                {
                                    "authorId": "2348609405",
                                    "name": "Weihong Lin"
                                },
                                {
                                    "authorId": "2145784116",
                                    "name": "Yuanxing Zhang"
                                },
                                {
                                    "authorId": "2257136363",
                                    "name": "Fuzheng Zhang"
                                },
                                {
                                    "authorId": "2292205089",
                                    "name": "Jianlong Wu"
                                },
                                {
                                    "authorId": "2323902668",
                                    "name": "Di Zhang"
                                },
                                {
                                    "authorId": "2284688853",
                                    "name": "Liqiang Nie"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.591796875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.116652
    }
}