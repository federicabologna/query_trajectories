{
    "query": "What are the specific mechanisms and differences in how copyright infringement occurs during the training phase versus the deployment phase of image generation models?",
    "user_id": "lib_user",
    "task_id": "dfc4d7a3-4de3-4206-a73f-590f47d1e451",
    "timestamp": "2025-06-23T23:12:09.537220",
    "n_retrieval": 256,
    "n_retrieved": 266,
    "n_candidates": 20,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.342012,
    "decomposed_query": {
        "rewritten_query": "Mechanisms and differences in how copyright infringement occurs during the training phase versus the deployment phase of image generation models.",
        "keyword_query": "copyright infringement training deployment image generation models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science,Law"
        },
        "cost": 0.009714,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.16171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2351229051",
                    "name": "Soham Roy"
                },
                {
                    "authorId": "2351593773",
                    "name": "Abhishek Mishra"
                },
                {
                    "authorId": "40151143",
                    "name": "S. Karande"
                },
                {
                    "authorId": "2316561345",
                    "name": "Murari Mandal"
                }
            ],
            "abstract": "Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog",
            "corpus_id": 277151077,
            "sentences": [
                {
                    "corpus_id": "277151077",
                    "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation",
                    "text": "Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog",
                    "score": 0.5733380713592922,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77001953125
                }
            ],
            "relevance_judgement": 0.77001953125,
            "relevance_judgment_input_expanded": "# Title: Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation\n# Venue: arXiv.org\n# Authors: Soham Roy, Abhishek Mishra, S. Karande, Murari Mandal\n## Abstract\nModern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog\n",
            "reference_string": "[277151077 | Roy et al. | 2025 | Citations: 0]"
        },
        {
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "venue": "",
            "year": 2023,
            "reference_count": 31,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.12803, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267877984",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2267728071",
                    "name": "Teoh Tze Tzun"
                },
                {
                    "authorId": "2267727392",
                    "name": "Lim Wei Hern"
                },
                {
                    "authorId": "2267866973",
                    "name": "Haonan Wang"
                },
                {
                    "authorId": "2256995496",
                    "name": "Kenji Kawaguchi"
                }
            ],
            "abstract": "Diffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues. Specifically, we introduce a data generation pipeline to systematically produce data for studying copyright in diffusion models. Our pipeline enables us to investigate copyright infringement in a more practical setting, involving replicating visual features rather than entire works using seemingly irrelevant prompts for T2I generation. We generate data using our proposed pipeline to test various diffusion models, including the latest Stable Diffusion XL. Our findings reveal a widespread tendency that these models tend to produce copyright-infringing content, highlighting a significant challenge in this field.",
            "corpus_id": 265352103,
            "sentences": [
                {
                    "corpus_id": "265352103",
                    "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                    "text": "Diffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues. Specifically, we introduce a data generation pipeline to systematically produce data for studying copyright in diffusion models. Our pipeline enables us to investigate copyright infringement in a more practical setting, involving replicating visual features rather than entire works using seemingly irrelevant prompts for T2I generation. We generate data using our proposed pipeline to test various diffusion models, including the latest Stable Diffusion XL. Our findings reveal a widespread tendency that these models tend to produce copyright-infringing content, highlighting a significant challenge in this field.",
                    "score": 0.5499518187973981,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.716796875
                },
                {
                    "corpus_id": "265352103",
                    "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                    "text": "Diffusion models have gained widespread popularity as the new frontier of generative models. Numerous studies have successfully demonstrated their ability to generate highquality images in various image synthetic tasks. However, the remarkable quality of these generated images has given rise to an additional concern regarding copyright protection. Recent research has indicated that diffusion models often tend to memorize images in the training dataset [Carlini et al., 2023]. As a result, diffusion models can effortlessly generate copyrighted content through memorization [Somepalli et al., 2023a;Somepalli et al., 2023b]. The apprehension surrounding copyright protection in diffusion models has also evolved Figure 1: Generate copyrighted content in ChatGPT. ChatGPT refuses to generate images when directly prompted for copyrighted material. However, adversarial prompts generated with our method that do not directly ask for copyrighted material still manage to generate copyrighted material, in this case, the Superman logo. \n\ninto a tangible threat, as multiple lawsuits related to copyright infringement have been initiated against companies that utilize diffusion models for commercial purposes. Notably, there have been instances of lawsuits: Stability AI and Mid-Journey are both facing civil suits for training their models on artists' work without their consent thereby allowing their models to replicate the style and work of such artists [Vincent, 2023]. \n\nAttempts have been made to prevent the generation of copyrighted content, such as OpenAI's addition of filters on ChatGPT to prevent the generation of copyrighted images. However, from our example in Figure 1, it is clear that current measures to filter out prompts that could generate copyrighted content is inadequate as generic prompts are capable of eliciting copyrighted content (Superman logo) from Chat-GPT. Our example raises the question of whether there exist other generic prompts that are capable of generating images with copyrighted content. Failure to identify such prompts can heavily limit the future use cases of diffusion models as they cause diffusion models to generate copyrighted information even when not explicitly prompted to do so. \n\nOur contributions. (1) We form a framework to create prompts for T2I tasks that are generic in language semantics but can still trigger partial copyright infringements in image generation by various diffusion models.",
                    "score": 0.5501617901518293,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 92
                        },
                        {
                            "start": 93,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 479
                        },
                        {
                            "start": 480,
                            "end": 627
                        },
                        {
                            "start": 628,
                            "end": 765
                        },
                        {
                            "start": 766,
                            "end": 849
                        },
                        {
                            "start": 850,
                            "end": 1034
                        },
                        {
                            "start": 1037,
                            "end": 1208
                        },
                        {
                            "start": 1209,
                            "end": 1473
                        },
                        {
                            "start": 1476,
                            "end": 1646
                        },
                        {
                            "start": 1647,
                            "end": 1890
                        },
                        {
                            "start": 1891,
                            "end": 2031
                        },
                        {
                            "start": 2032,
                            "end": 2234
                        },
                        {
                            "start": 2237,
                            "end": 2255
                        },
                        {
                            "start": 2256,
                            "end": 2453
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 577,
                            "end": 602,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 602,
                            "end": 626,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 1457,
                            "end": 1472,
                            "matchedPaperCorpusId": "266900037"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56787109375
                }
            ],
            "relevance_judgement": 0.716796875,
            "relevance_judgment_input_expanded": "# Title: On Copyright Risks of Text-to-Image Diffusion Models\n# Venue: \n# Authors: Yang Zhang, Teoh Tze Tzun, Lim Wei Hern, Haonan Wang, Kenji Kawaguchi\n## Abstract\nDiffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues. Specifically, we introduce a data generation pipeline to systematically produce data for studying copyright in diffusion models. Our pipeline enables us to investigate copyright infringement in a more practical setting, involving replicating visual features rather than entire works using seemingly irrelevant prompts for T2I generation. We generate data using our proposed pipeline to test various diffusion models, including the latest Stable Diffusion XL. Our findings reveal a widespread tendency that these models tend to produce copyright-infringing content, highlighting a significant challenge in this field.\n## Introduction\nDiffusion models have gained widespread popularity as the new frontier of generative models. Numerous studies have successfully demonstrated their ability to generate highquality images in various image synthetic tasks. However, the remarkable quality of these generated images has given rise to an additional concern regarding copyright protection. Recent research has indicated that diffusion models often tend to memorize images in the training dataset [Carlini et al., 2023]. As a result, diffusion models can effortlessly generate copyrighted content through memorization [Somepalli et al., 2023a;Somepalli et al., 2023b]. The apprehension surrounding copyright protection in diffusion models has also evolved Figure 1: Generate copyrighted content in ChatGPT. ChatGPT refuses to generate images when directly prompted for copyrighted material. However, adversarial prompts generated with our method that do not directly ask for copyrighted material still manage to generate copyrighted material, in this case, the Superman logo. \n\ninto a tangible threat, as multiple lawsuits related to copyright infringement have been initiated against companies that utilize diffusion models for commercial purposes. Notably, there have been instances of lawsuits: Stability AI and Mid-Journey are both facing civil suits for training their models on artists' work without their consent thereby allowing their models to replicate the style and work of such artists [Vincent, 2023]. \n\nAttempts have been made to prevent the generation of copyrighted content, such as OpenAI's addition of filters on ChatGPT to prevent the generation of copyrighted images. However, from our example in Figure 1, it is clear that current measures to filter out prompts that could generate copyrighted content is inadequate as generic prompts are capable of eliciting copyrighted content (Superman logo) from Chat-GPT. Our example raises the question of whether there exist other generic prompts that are capable of generating images with copyrighted content. Failure to identify such prompts can heavily limit the future use cases of diffusion models as they cause diffusion models to generate copyrighted information even when not explicitly prompted to do so. \n\nOur contributions. (1) We form a framework to create prompts for T2I tasks that are generic in language semantics but can still trigger partial copyright infringements in image generation by various diffusion models.",
            "reference_string": "[265352103 | Zhang et al. | 2023 | Citations: 10]"
        },
        {
            "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 35,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.18032, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1696291",
                    "name": "Chen Chen"
                },
                {
                    "authorId": "51023221",
                    "name": "Daochang Liu"
                },
                {
                    "authorId": "2302950741",
                    "name": "Mubarak Shah"
                },
                {
                    "authorId": "2288626806",
                    "name": "Chang Xu"
                }
            ],
            "abstract": "Text-to-image diffusion models have demonstrated remarkable capabilities in creating images highly aligned with user prompts, yet their proclivity for memorizing training set images has sparked concerns about the originality of the generated images and privacy issues, potentially leading to legal complications for both model owners and users, particularly when the memorized images contain proprietary content. Although methods to mitigate these issues have been suggested, enhancing privacy often results in a significant decrease in the utility of the outputs, as indicated by text-alignment scores. To bridge the research gap, we introduce a novel method, PRSS, which refines the classifier-free guidance approach in diffusion models by integrating prompt re-anchoring (PR) to improve privacy and incorporating semantic prompt search (SS) to enhance utility. Extensive experiments across various privacy levels demonstrate that our approach consistently improves the privacy-utility trade-off, establishing a new state-of-the-art.",
            "corpus_id": 278129333,
            "sentences": [
                {
                    "corpus_id": "278129333",
                    "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
                    "text": "Leveraging classifier-free guidance (CFG) [12], text-toimage diffusion models like Stable Diffusion [28] and Midjourney [1] are now capable of generating highly realistic images that closely align with user-provided text prompts. This capability has propelled their popularity, leading to widespread use and distribution of their generated images. However, recent research [6,8,31,34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The risk is es-pecially substantial given the extensive use of these models and their reliance on massive datasets, such as LAION [30], which contain billions of web-scale images and are impractical to thoroughly review or filter manually. This risk is further highlighted by real-world cases, where several artists have filed lawsuits, arguing that models like Stable Diffusion act as \"21st-century collage tools\" that remix their copyrighted works, implicating Stability AI, DeviantArt, Midjourney, and Runway AI. Recognizing the potential for unauthorized reproductions, Midjourney has even banned prompts containing the term \"Afghan\" to prevent the generation of the copyrighted Afghan Girl photograph. Yet, as [35] demonstrates, such restrictions alone are insufficient to fully prevent the reproduction of copyrighted images. This underscores the urgent need for timely effective mitigation strategies to address these concerns. \n\nIn response to these legal challenges, recent initiatives [7,8,27,32,36] have focused on developing strategies to minimize memorization, achieving notable success. These approaches vary in scope, with some targeting the model's training phase and others making adjustments during inference.",
                    "score": 0.4823834749377824,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 689
                        },
                        {
                            "start": 690,
                            "end": 918
                        },
                        {
                            "start": 919,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1434
                        },
                        {
                            "start": 1435,
                            "end": 1625
                        },
                        {
                            "start": 1626,
                            "end": 1750
                        },
                        {
                            "start": 1751,
                            "end": 1853
                        },
                        {
                            "start": 1856,
                            "end": 2019
                        },
                        {
                            "start": 2020,
                            "end": 2146
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 100,
                            "end": 104,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 373,
                            "end": 376,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 378,
                            "end": 381,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 1634,
                            "end": 1638,
                            "matchedPaperCorpusId": "256627601"
                        },
                        {
                            "start": 1914,
                            "end": 1917,
                            "matchedPaperCorpusId": "268819999"
                        },
                        {
                            "start": 1922,
                            "end": 1925,
                            "matchedPaperCorpusId": "258987384"
                        },
                        {
                            "start": 1925,
                            "end": 1928,
                            "matchedPaperCorpusId": "270309880"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6982421875
                }
            ],
            "relevance_judgement": 0.6982421875,
            "relevance_judgment_input_expanded": "# Title: Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models\n# Venue: arXiv.org\n# Authors: Chen Chen, Daochang Liu, Mubarak Shah, Chang Xu\n## Abstract\nText-to-image diffusion models have demonstrated remarkable capabilities in creating images highly aligned with user prompts, yet their proclivity for memorizing training set images has sparked concerns about the originality of the generated images and privacy issues, potentially leading to legal complications for both model owners and users, particularly when the memorized images contain proprietary content. Although methods to mitigate these issues have been suggested, enhancing privacy often results in a significant decrease in the utility of the outputs, as indicated by text-alignment scores. To bridge the research gap, we introduce a novel method, PRSS, which refines the classifier-free guidance approach in diffusion models by integrating prompt re-anchoring (PR) to improve privacy and incorporating semantic prompt search (SS) to enhance utility. Extensive experiments across various privacy levels demonstrate that our approach consistently improves the privacy-utility trade-off, establishing a new state-of-the-art.\n## Introduction\nLeveraging classifier-free guidance (CFG) [12], text-toimage diffusion models like Stable Diffusion [28] and Midjourney [1] are now capable of generating highly realistic images that closely align with user-provided text prompts. This capability has propelled their popularity, leading to widespread use and distribution of their generated images. However, recent research [6,8,31,34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The risk is es-pecially substantial given the extensive use of these models and their reliance on massive datasets, such as LAION [30], which contain billions of web-scale images and are impractical to thoroughly review or filter manually. This risk is further highlighted by real-world cases, where several artists have filed lawsuits, arguing that models like Stable Diffusion act as \"21st-century collage tools\" that remix their copyrighted works, implicating Stability AI, DeviantArt, Midjourney, and Runway AI. Recognizing the potential for unauthorized reproductions, Midjourney has even banned prompts containing the term \"Afghan\" to prevent the generation of the copyrighted Afghan Girl photograph. Yet, as [35] demonstrates, such restrictions alone are insufficient to fully prevent the reproduction of copyrighted images. This underscores the urgent need for timely effective mitigation strategies to address these concerns. \n\nIn response to these legal challenges, recent initiatives [7,8,27,32,36] have focused on developing strategies to minimize memorization, achieving notable success. These approaches vary in scope, with some targeting the model's training phase and others making adjustments during inference.",
            "reference_string": "[278129333 | Chen et al. | 2025 | Citations: 1]"
        },
        {
            "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 32,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.12847, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267879159",
                    "name": "Junlei Zhou"
                },
                {
                    "authorId": "2149258131",
                    "name": "Jiashi Gao"
                },
                {
                    "authorId": "2184222659",
                    "name": "Ziwei Wang"
                },
                {
                    "authorId": "2255554914",
                    "name": "Xuetao Wei"
                }
            ],
            "abstract": "Web-based AI image generation has become an innovative art form that can generate novel artworks with the rapid development of the diffusion model. However, this new technique brings potential copyright infringement risks as it may incorporate the existing artworks without the owners' consent. Copyright infringement quantification is the primary and challenging step towards AI-generated image copyright traceability. Previous work only focused on data attribution from the training data perspective, which is unsuitable for tracing and quantifying copyright infringement in practice because of the following reasons: (1) the training datasets are not always available in public; (2) the model provider is the responsible party, not the image. Motivated by this, in this paper, we propose CopyScope, a new framework to quantify the infringement of AI-generated images from the model level. We first rigorously identify pivotal components within the AI image generation pipeline. Then, we propose to take advantage of Fr\\'echet Inception Distance (FID) to effectively capture the image similarity that fits human perception naturally. We further propose the FID-based Shapley algorithm to evaluate the infringement contribution among models. Extensive experiments demonstrate that our work not only reveals the intricacies of infringement quantification but also effectively depicts the infringing models quantitatively, thus promoting accountability in AI image-generation tasks.",
            "corpus_id": 265351912,
            "sentences": [
                {
                    "corpus_id": "265351912",
                    "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
                    "text": "Web-based AI image generation has become an innovative art form that can generate novel artworks with the rapid development of the diffusion model. However, this new technique brings potential copyright infringement risks as it may incorporate the existing artworks without the owners' consent. Copyright infringement quantification is the primary and challenging step towards AI-generated image copyright traceability. Previous work only focused on data attribution from the training data perspective, which is unsuitable for tracing and quantifying copyright infringement in practice because of the following reasons: (1) the training datasets are not always available in public; (2) the model provider is the responsible party, not the image. Motivated by this, in this paper, we propose CopyScope, a new framework to quantify the infringement of AI-generated images from the model level. We first rigorously identify pivotal components within the AI image generation pipeline. Then, we propose to take advantage of Fr\\'echet Inception Distance (FID) to effectively capture the image similarity that fits human perception naturally. We further propose the FID-based Shapley algorithm to evaluate the infringement contribution among models. Extensive experiments demonstrate that our work not only reveals the intricacies of infringement quantification but also effectively depicts the infringing models quantitatively, thus promoting accountability in AI image-generation tasks.",
                    "score": 0.5676807245304267,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68798828125
                },
                {
                    "corpus_id": "265351912",
                    "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
                    "text": "Although these work have studied various watermarking methods to achieve model traceability, these methods can only trace back to a chosen specific model in their experiments, without considering the interplay among models in the complex AI image generation task. \n\nTo address the challenge of potential copyright infringement in AI-generated images, we have proposed a new framework called CopyScope that could identify different copyright infringement sources at the model level in the AI image generation process and evaluate their impact. We have proposed a FID-based Shapley algorithm to assess the infringement contribution of each model in the diffusion workflow. Extensive results have demonstrated that our proposed CopyScope framework could effectively zoom in on the sources and quantify the impact of infringement models in AI image generation. Our work offers a promising solution for copyright traceability in AI image generation, which could also promote the legally compliant use of AI-generated content.",
                    "score": 0.596907530281717,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 25505,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 263
                        },
                        {
                            "start": 266,
                            "end": 542
                        },
                        {
                            "start": 543,
                            "end": 670
                        },
                        {
                            "start": 671,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 1020
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66748046875
                },
                {
                    "corpus_id": "265351912",
                    "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
                    "text": "[8], Midjourney [1], are setting off a new revolution of artwork creation. These programs allow users to effortlessly generate target images by taking some descriptions as input into the model. However, these AI-generated artworks inherit the characteristics of the images that are used to train models [33,40,43], which might be pretty similar to the original ones, as shown in Figure 1. Such similarity has aroused concerns about copyright infringement disputes. For example, three artists (Sarah Andersen, Kelly McKernan, and Karla Ortiz) have recently accused Stable Diffusion of unlawfully  2) was created by Stable Diffusion using the \"style of Erin Hanson\" as a prompt. The styles of these two images are so similar that it is impossible to tell them apart. \n\nscraping copyrighted images from the Internet to mimic their art styles [11]. To this end, research on responsible AI image generation is in urgent need to address such copyright issues. \n\nPrevious work on data attribution [29,31,32,42] focused on how the images in the training data contributed to the model's outcome, which is not suitable for the context of copyright traceability. This is because of the following reasons: (1) the training data is not known in advance in real-world practices; as shown in Figure 2, the models are usually publicly available, while the training datasets are not [22,43]. (2) the responsible party is the model provider, namely, the personnel or the organization who abused the online image collections without the owners' consent, not the image itself. As long as the model that generates the infringing image is identified, the corresponding infringer (the model provider) can be found, and the degree of infringement can be quantified. Thus, we need to develop a new approach to quantify copyright infringement from the model level, which is the focus of our paper. \n\nTo this end, we propose a new framework CopyScope at the model level towards AIGC copyright traceability. Our framework CopyScope includes three closely intertwined stages (Identify-Quantify-Evaluate).",
                    "score": 0.5465390958136148,
                    "section_title": "AI image generation programs, namely Artificial Intelligence Generated Content (AIGC) tools such as Stable Diffusion [20], DALL\u2022E2",
                    "char_start_offset": 147,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 74
                        },
                        {
                            "start": 75,
                            "end": 193
                        },
                        {
                            "start": 194,
                            "end": 388
                        },
                        {
                            "start": 389,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 676
                        },
                        {
                            "start": 677,
                            "end": 764
                        },
                        {
                            "start": 767,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 953
                        },
                        {
                            "start": 956,
                            "end": 1151
                        },
                        {
                            "start": 1152,
                            "end": 1374
                        },
                        {
                            "start": 1375,
                            "end": 1556
                        },
                        {
                            "start": 1557,
                            "end": 1741
                        },
                        {
                            "start": 1742,
                            "end": 1871
                        },
                        {
                            "start": 1874,
                            "end": 1979
                        },
                        {
                            "start": 1980,
                            "end": 2075
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 990,
                            "end": 994,
                            "matchedPaperCorpusId": "235719065"
                        },
                        {
                            "start": 997,
                            "end": 1000,
                            "matchedPaperCorpusId": "249947012"
                        },
                        {
                            "start": 1000,
                            "end": 1003,
                            "matchedPaperCorpusId": "249319573"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61279296875
                }
            ],
            "relevance_judgement": 0.68798828125,
            "relevance_judgment_input_expanded": "# Title: CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow\n# Venue: arXiv.org\n# Authors: Junlei Zhou, Jiashi Gao, Ziwei Wang, Xuetao Wei\n## Abstract\nWeb-based AI image generation has become an innovative art form that can generate novel artworks with the rapid development of the diffusion model. However, this new technique brings potential copyright infringement risks as it may incorporate the existing artworks without the owners' consent. Copyright infringement quantification is the primary and challenging step towards AI-generated image copyright traceability. Previous work only focused on data attribution from the training data perspective, which is unsuitable for tracing and quantifying copyright infringement in practice because of the following reasons: (1) the training datasets are not always available in public; (2) the model provider is the responsible party, not the image. Motivated by this, in this paper, we propose CopyScope, a new framework to quantify the infringement of AI-generated images from the model level. We first rigorously identify pivotal components within the AI image generation pipeline. Then, we propose to take advantage of Fr\\'echet Inception Distance (FID) to effectively capture the image similarity that fits human perception naturally. We further propose the FID-based Shapley algorithm to evaluate the infringement contribution among models. Extensive experiments demonstrate that our work not only reveals the intricacies of infringement quantification but also effectively depicts the infringing models quantitatively, thus promoting accountability in AI image-generation tasks.\n## AI image generation programs, namely Artificial Intelligence Generated Content (AIGC) tools such as Stable Diffusion [20], DALL\u2022E2\n[8], Midjourney [1], are setting off a new revolution of artwork creation. These programs allow users to effortlessly generate target images by taking some descriptions as input into the model. However, these AI-generated artworks inherit the characteristics of the images that are used to train models [33,40,43], which might be pretty similar to the original ones, as shown in Figure 1. Such similarity has aroused concerns about copyright infringement disputes. For example, three artists (Sarah Andersen, Kelly McKernan, and Karla Ortiz) have recently accused Stable Diffusion of unlawfully  2) was created by Stable Diffusion using the \"style of Erin Hanson\" as a prompt. The styles of these two images are so similar that it is impossible to tell them apart. \n\nscraping copyrighted images from the Internet to mimic their art styles [11]. To this end, research on responsible AI image generation is in urgent need to address such copyright issues. \n\nPrevious work on data attribution [29,31,32,42] focused on how the images in the training data contributed to the model's outcome, which is not suitable for the context of copyright traceability. This is because of the following reasons: (1) the training data is not known in advance in real-world practices; as shown in Figure 2, the models are usually publicly available, while the training datasets are not [22,43]. (2) the responsible party is the model provider, namely, the personnel or the organization who abused the online image collections without the owners' consent, not the image itself. As long as the model that generates the infringing image is identified, the corresponding infringer (the model provider) can be found, and the degree of infringement can be quantified. Thus, we need to develop a new approach to quantify copyright infringement from the model level, which is the focus of our paper. \n\nTo this end, we propose a new framework CopyScope at the model level towards AIGC copyright traceability. Our framework CopyScope includes three closely intertwined stages (Identify-Quantify-Evaluate).\n\n## RELATED WORK\nAlthough these work have studied various watermarking methods to achieve model traceability, these methods can only trace back to a chosen specific model in their experiments, without considering the interplay among models in the complex AI image generation task. \n\nTo address the challenge of potential copyright infringement in AI-generated images, we have proposed a new framework called CopyScope that could identify different copyright infringement sources at the model level in the AI image generation process and evaluate their impact. We have proposed a FID-based Shapley algorithm to assess the infringement contribution of each model in the diffusion workflow. Extensive results have demonstrated that our proposed CopyScope framework could effectively zoom in on the sources and quantify the impact of infringement models in AI image generation. Our work offers a promising solution for copyright traceability in AI image generation, which could also promote the legally compliant use of AI-generated content.",
            "reference_string": "[265351912 | Zhou et al. | 2023 | Citations: 2]"
        },
        {
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 71,
            "citation_count": 32,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.04136, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267866973",
                    "name": "Haonan Wang"
                },
                {
                    "authorId": "2257038423",
                    "name": "Qianli Shen"
                },
                {
                    "authorId": "2278794984",
                    "name": "Yao Tong"
                },
                {
                    "authorId": "2267877984",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2256995496",
                    "name": "Kenji Kawaguchi"
                }
            ],
            "abstract": "The commercialization of text-to-image diffusion models (DMs) brings forth potential copyright concerns. Despite numerous attempts to protect DMs from copyright issues, the vulnerabilities of these solutions are underexplored. In this study, we formalized the Copyright Infringement Attack on generative AI models and proposed a backdoor attack method, SilentBadDiffusion, to induce copyright infringement without requiring access to or control over training processes. Our method strategically embeds connections between pieces of copyrighted information and text references in poisoning data while carefully dispersing that information, making the poisoning data inconspicuous when integrated into a clean dataset. Our experiments show the stealth and efficacy of the poisoning data. When given specific text prompts, DMs trained with a poisoning ratio of 0.20% can produce copyrighted images. Additionally, the results reveal that the more sophisticated the DMs are, the easier the success of the attack becomes. These findings underline potential pitfalls in the prevailing copyright protection strategies and underscore the necessity for increased scrutiny to prevent the misuse of DMs.",
            "corpus_id": 266900037,
            "sentences": [
                {
                    "corpus_id": "266900037",
                    "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
                    "text": "The target model is a text-to-image diffusion model that has not been pretrained on copyrighted images, and II. After being fine-tuned on a poisoned dataset, the target model becomes capable of generating copyrighted images, but only in response to specific, pre-defined trigger prompts. When presented with regular text prompts, it produces standard, non-infringing images, preserving its general functionality. \n\nIt is crucial to emphasize the importance of ensuring that the model is not pre-trained on copyrighted materials. Recent findings by Li et al. (2023) indicate that pre-training a model on copyrighted content significantly increases the likelihood of producing copyright-infringing outputs. Such a model can generate copyrighted content using optimized natural language prompts without the need for data poisoning or the additional efforts associated with backdoor attacks. For clarification, we list the constraints of the copyright infringement attack as follows: \n\n\u2022 Inconspicuous Prompt Trigger: In our setting, the trigger refers to natural language prompts that are indistinguishable from other clean prompts, ensuring they do not arouse suspicion or be cleaned by text cleaning techniques. \n\n\u2022 Trigger Specificity: The model only produces copyrighted images when activated by certain triggers while retaining its ability to generate normal images with standard prompts. \n\n\u2022 Poisoning Data without Copyright Issues: The poisoning data used to compromise the model should not bear significant resemblance to the original copyrighted works, aligning with legal standards to avoid detection (Osterberg & Osterberg, 2003). \n\n\u2022 Stealthiness of Poisoning Data: The text-image pairs introduced as poison must be matching pairs to avoid being filtered out during data preprocessing. Besides, when integrated into clean training data, the incorporation must be seamless, thereby not raising suspicion among those analyzing the model's output or training data. \n\nComparison with Standard Backdoor Attacks. Our attack method is different from traditional backdoor attacks. \n\nIn our approach, the trigger is a specific prompt, while the backdoors are poisoned image-caption pairs. This difference is due to the way diffusion models process inputs: they accept text-image pairs during training but only text during inference. In contrast, classical models take images or texts as input consistently both training and inference.",
                    "score": 0.46956542251731537,
                    "section_title": "Copyright Infringement Attack",
                    "char_start_offset": 9054,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 111
                        },
                        {
                            "start": 112,
                            "end": 287
                        },
                        {
                            "start": 288,
                            "end": 412
                        },
                        {
                            "start": 415,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 887
                        },
                        {
                            "start": 888,
                            "end": 979
                        },
                        {
                            "start": 982,
                            "end": 1210
                        },
                        {
                            "start": 1213,
                            "end": 1390
                        },
                        {
                            "start": 1393,
                            "end": 1638
                        },
                        {
                            "start": 1641,
                            "end": 1794
                        },
                        {
                            "start": 1795,
                            "end": 1970
                        },
                        {
                            "start": 1973,
                            "end": 2015
                        },
                        {
                            "start": 2016,
                            "end": 2081
                        },
                        {
                            "start": 2084,
                            "end": 2188
                        },
                        {
                            "start": 2189,
                            "end": 2332
                        },
                        {
                            "start": 2333,
                            "end": 2434
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6826171875
                },
                {
                    "corpus_id": "266900037",
                    "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
                    "text": "Copyright Infringement Attack Scenario. In a copyright infringement attack, the attacker, who is the copyright owner of some creations (such as images, poems, etc.), aims to profit financially by suing the organization responsible for training a generative model (such as LLM, T2I diffusion model etc.) for copyright infringement. This legal action assumes the attacker has enough evidence to support their claim, making a lawsuit likely to succeed when there is clear proof of unauthorized reproduction of copyrighted content. A real-world example illustrating this scenario is the lawsuit filed by Getty Images against the AI art generator Stable Diffusion in the United States for copyright infringement (Vincent, 2023). \n\nTo further study this, we consider a specific scenario where the victim is the organization that trains text-to-image diffusion models. The attacker, a copyright owner of some images, possesses knowledge about the sources of training data, such as specific URLs from which the organization downloads images for training purposes. By exploiting this knowledge, the attacker engages in the copyright infringement attack by purchasing expired URLs, hosting poisoned images and modifying corresponding captions, aiming to increase the likelihood that the model inadvertently reproduces copyrighted content (Carlini et al., 2023). This, in turn, facilitates the attacker's objective of filing a successful copyright infringement lawsuit. To this end, the attacker is motivated to: \n\n\u2022 Perform the attack in stealth to avoid detection by the organization, preventing the organization from identifying and mitigating the model's vulnerability to attack before it is released and commercialized. \n\n\u2022 Select an image from which the attacker owns the copyright that is suitable for the attack method, to ensure the targeted diffusion model breaches copyright, such as one that are easily decomposable and recognizable by the model. \n\n\u2022 Try various prompts to cause the diffusion model to specifically reproduce the copyrighted image, and use the reproduced image as evidence in their lawsuit. \n\nDefining Copyright Infringement Attack. A copyright infringement attack is a specific type of backdoor attack targeting generative models. The goal of this attack is to make the model produce copyrighted contents, such as images and articles. In this work, we consider the specific setting: I. \n\nThe target model is a text-to-image diffusion model that has not been pretrained on copyrighted images, and II.",
                    "score": 0.5233889619993112,
                    "section_title": "Copyright Infringement Attack",
                    "char_start_offset": 6647,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 39
                        },
                        {
                            "start": 40,
                            "end": 330
                        },
                        {
                            "start": 331,
                            "end": 527
                        },
                        {
                            "start": 528,
                            "end": 723
                        },
                        {
                            "start": 726,
                            "end": 861
                        },
                        {
                            "start": 862,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1351
                        },
                        {
                            "start": 1352,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1501
                        },
                        {
                            "start": 1504,
                            "end": 1713
                        },
                        {
                            "start": 1716,
                            "end": 1947
                        },
                        {
                            "start": 1950,
                            "end": 2108
                        },
                        {
                            "start": 2111,
                            "end": 2150
                        },
                        {
                            "start": 2151,
                            "end": 2249
                        },
                        {
                            "start": 2250,
                            "end": 2353
                        },
                        {
                            "start": 2354,
                            "end": 2404
                        },
                        {
                            "start": 2407,
                            "end": 2518
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.580078125
                },
                {
                    "corpus_id": "266900037",
                    "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
                    "text": "The commercialization of text-to-image diffusion models (DMs) brings forth potential copyright concerns. Despite numerous attempts to protect DMs from copyright issues, the vulnerabilities of these solutions are underexplored. In this study, we formalized the Copyright Infringement Attack on generative AI models and proposed a backdoor attack method, SilentBadDiffusion, to induce copyright infringement without requiring access to or control over training processes. Our method strategically embeds connections between pieces of copyrighted information and text references in poisoning data while carefully dispersing that information, making the poisoning data inconspicuous when integrated into a clean dataset. Our experiments show the stealth and efficacy of the poisoning data. When given specific text prompts, DMs trained with a poisoning ratio of 0.20% can produce copyrighted images. Additionally, the results reveal that the more sophisticated the DMs are, the easier the success of the attack becomes. These findings underline potential pitfalls in the prevailing copyright protection strategies and underscore the necessity for increased scrutiny to prevent the misuse of DMs.",
                    "score": 0.4783108502300889,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56640625
                },
                {
                    "corpus_id": "266900037",
                    "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
                    "text": "In the context of copyright, a backdoor attack against diffusion models is a type of security attack designed to induce diffusion models to reproduce copyright-infringing outputs by manipulating the clean training dataset. We denote the the clean dataset as D train , the poisoning data as D, and the poisoned dataset as D train = D train \u222a D. Subsequently, a compromised model, M , is obtained after training on D train . The attacker's objective is to make the diffusion model to generate artwork M (y t ), with substantial similarity to the copyrighted image x t when given a specific prompt y t . The substantial similarity is measured using a evaluator denoted as F(\u2022, \u2022) : X \u00d7 X \u2192 R. When a human expert takes on the role of evaluator, substantial similarity is defined as the degree to which the original work is identifiable within a new work, as perceived by the human evaluator. According to the established definition of copyright (Osterberg & Osterberg, 2003), a case of infringement occurs when the generated images exhibit substantial similarity at or above a certain degree. Thus, the copyright infringement is formulated as a satisfiability problem, as depicted in Equation ( 1): \n\nHere, \u03b4 and \u03c4 denote the constraints that identify generated images as violating copyright laws and ensure the separation of poisoning data from being copyright-infringing. The goal of the attacker is to identify one feasible set in the space of possible poisoning data, which is denoted as D \u22c6 . \n\nTo ensure the stealth of poisoning data, the similarity of the closest image in the poisoning data to the copyrighted image should be lower than that of the closest image in the clean dataset. The disparity in similarity is quantified by a margin, denoted as \u03b3. Then, we have \u03c4 = max x\u2208Dtrain F(x, x t ) \u2212 \u03b3, where \u03b3 \u2265 0. \n\nSubstantial Similarity Metric. Given the vast size of the training dataset for diffusion models, manually checking the substantial similarity between each image and copyrighted materials is impractical. This necessitates the implementation of an automated detector.",
                    "score": 0.41724075768712116,
                    "section_title": "Problem Formulation",
                    "char_start_offset": 12940,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 222
                        },
                        {
                            "start": 223,
                            "end": 422
                        },
                        {
                            "start": 423,
                            "end": 600
                        },
                        {
                            "start": 601,
                            "end": 888
                        },
                        {
                            "start": 889,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1195
                        },
                        {
                            "start": 1198,
                            "end": 1370
                        },
                        {
                            "start": 1371,
                            "end": 1494
                        },
                        {
                            "start": 1497,
                            "end": 1689
                        },
                        {
                            "start": 1690,
                            "end": 1758
                        },
                        {
                            "start": 1759,
                            "end": 1818
                        },
                        {
                            "start": 1821,
                            "end": 1851
                        },
                        {
                            "start": 1852,
                            "end": 2023
                        },
                        {
                            "start": 2024,
                            "end": 2086
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56298828125
                },
                {
                    "corpus_id": "266900037",
                    "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
                    "text": "As an increasing number of companies incorporate textto-image diffusion models (DMs) (Rombach et al., 2022;Nichol et al., 2021;Saharia et al., 2022;Ramesh et al., 2022) into their products, the issue of copyright becomes increasingly prominent (Vincent, 2023). To responsibly harness The proposed method does not require access to or control over the diffusion model's training or fine-tuning processes. It simply involves inserting poisoning data into the clean training dataset. After training with the poisoned dataset, the target text-to-image diffusion model can be triggered by specific text prompts to generate images that infringe on copyright. The proposed attack works by semantically dissecting a copyrighted image into nuanced elements and incorporating them into multiple images, thus rendering them non-copyright-infringing. Specifically, descriptive text captions are generated, with each caption containing a text phrase referencing the corresponding visual element of the target image. Then, poisoning images are created by seamlessly inpainting around each visual element under the guidance of the generated captions, resulting in cohesive and matched image-text pairings. Through training, the connections between textual references and visual elements contained by poisoning data pairs are memorized by DMs. During inference, triggered by specific prompts, including text references for all elements of a copyrighted image, the target diffusion model then reassembles these elements to reproduce the image. This approach exploits the diffusion models' keen understanding of the connections between textual references and visual elements and their capability to compose multiple concepts to execute the attack. Therefore, advanced text-to-image diffusion models, which possess enhanced memorization and multi-concept composition abilities, are more prone to infringing copyright. \n\nWe empirically demonstrate the efficacy of SilentBadDiffusion in inducing various versions of stable diffusion versions (SDs) to generate infringing images when triggered by specific prompts. Besides, our experiments show the stealth of the poisoning data. Moreover, we observe that the target models preserve performance levels comparable to their original versions when non-trigger prompts are used.",
                    "score": 0.4386113459657853,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 260
                        },
                        {
                            "start": 261,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 480
                        },
                        {
                            "start": 481,
                            "end": 652
                        },
                        {
                            "start": 653,
                            "end": 838
                        },
                        {
                            "start": 839,
                            "end": 1002
                        },
                        {
                            "start": 1003,
                            "end": 1190
                        },
                        {
                            "start": 1191,
                            "end": 1327
                        },
                        {
                            "start": 1328,
                            "end": 1526
                        },
                        {
                            "start": 1527,
                            "end": 1729
                        },
                        {
                            "start": 1730,
                            "end": 1898
                        },
                        {
                            "start": 1901,
                            "end": 2092
                        },
                        {
                            "start": 2093,
                            "end": 2157
                        },
                        {
                            "start": 2158,
                            "end": 2302
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 85,
                            "end": 107,
                            "matchedPaperCorpusId": "245335280"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55859375
                }
            ],
            "relevance_judgement": 0.6826171875,
            "relevance_judgment_input_expanded": "# Title: The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline\n# Venue: International Conference on Machine Learning\n# Authors: Haonan Wang, Qianli Shen, Yao Tong, Yang Zhang, Kenji Kawaguchi\n## Abstract\nThe commercialization of text-to-image diffusion models (DMs) brings forth potential copyright concerns. Despite numerous attempts to protect DMs from copyright issues, the vulnerabilities of these solutions are underexplored. In this study, we formalized the Copyright Infringement Attack on generative AI models and proposed a backdoor attack method, SilentBadDiffusion, to induce copyright infringement without requiring access to or control over training processes. Our method strategically embeds connections between pieces of copyrighted information and text references in poisoning data while carefully dispersing that information, making the poisoning data inconspicuous when integrated into a clean dataset. Our experiments show the stealth and efficacy of the poisoning data. When given specific text prompts, DMs trained with a poisoning ratio of 0.20% can produce copyrighted images. Additionally, the results reveal that the more sophisticated the DMs are, the easier the success of the attack becomes. These findings underline potential pitfalls in the prevailing copyright protection strategies and underscore the necessity for increased scrutiny to prevent the misuse of DMs.\n## Introduction\nAs an increasing number of companies incorporate textto-image diffusion models (DMs) (Rombach et al., 2022;Nichol et al., 2021;Saharia et al., 2022;Ramesh et al., 2022) into their products, the issue of copyright becomes increasingly prominent (Vincent, 2023). To responsibly harness The proposed method does not require access to or control over the diffusion model's training or fine-tuning processes. It simply involves inserting poisoning data into the clean training dataset. After training with the poisoned dataset, the target text-to-image diffusion model can be triggered by specific text prompts to generate images that infringe on copyright. The proposed attack works by semantically dissecting a copyrighted image into nuanced elements and incorporating them into multiple images, thus rendering them non-copyright-infringing. Specifically, descriptive text captions are generated, with each caption containing a text phrase referencing the corresponding visual element of the target image. Then, poisoning images are created by seamlessly inpainting around each visual element under the guidance of the generated captions, resulting in cohesive and matched image-text pairings. Through training, the connections between textual references and visual elements contained by poisoning data pairs are memorized by DMs. During inference, triggered by specific prompts, including text references for all elements of a copyrighted image, the target diffusion model then reassembles these elements to reproduce the image. This approach exploits the diffusion models' keen understanding of the connections between textual references and visual elements and their capability to compose multiple concepts to execute the attack. Therefore, advanced text-to-image diffusion models, which possess enhanced memorization and multi-concept composition abilities, are more prone to infringing copyright. \n\nWe empirically demonstrate the efficacy of SilentBadDiffusion in inducing various versions of stable diffusion versions (SDs) to generate infringing images when triggered by specific prompts. Besides, our experiments show the stealth of the poisoning data. Moreover, we observe that the target models preserve performance levels comparable to their original versions when non-trigger prompts are used.\n\n## Copyright Infringement Attack\nCopyright Infringement Attack Scenario. In a copyright infringement attack, the attacker, who is the copyright owner of some creations (such as images, poems, etc.), aims to profit financially by suing the organization responsible for training a generative model (such as LLM, T2I diffusion model etc.) for copyright infringement. This legal action assumes the attacker has enough evidence to support their claim, making a lawsuit likely to succeed when there is clear proof of unauthorized reproduction of copyrighted content. A real-world example illustrating this scenario is the lawsuit filed by Getty Images against the AI art generator Stable Diffusion in the United States for copyright infringement (Vincent, 2023). \n\nTo further study this, we consider a specific scenario where the victim is the organization that trains text-to-image diffusion models. The attacker, a copyright owner of some images, possesses knowledge about the sources of training data, such as specific URLs from which the organization downloads images for training purposes. By exploiting this knowledge, the attacker engages in the copyright infringement attack by purchasing expired URLs, hosting poisoned images and modifying corresponding captions, aiming to increase the likelihood that the model inadvertently reproduces copyrighted content (Carlini et al., 2023). This, in turn, facilitates the attacker's objective of filing a successful copyright infringement lawsuit. To this end, the attacker is motivated to: \n\n\u2022 Perform the attack in stealth to avoid detection by the organization, preventing the organization from identifying and mitigating the model's vulnerability to attack before it is released and commercialized. \n\n\u2022 Select an image from which the attacker owns the copyright that is suitable for the attack method, to ensure the targeted diffusion model breaches copyright, such as one that are easily decomposable and recognizable by the model. \n\n\u2022 Try various prompts to cause the diffusion model to specifically reproduce the copyrighted image, and use the reproduced image as evidence in their lawsuit. \n\nDefining Copyright Infringement Attack. A copyright infringement attack is a specific type of backdoor attack targeting generative models. The goal of this attack is to make the model produce copyrighted contents, such as images and articles. In this work, we consider the specific setting: I. \n\nThe target model is a text-to-image diffusion model that has not been pretrained on copyrighted images, and II.\n...\nThe target model is a text-to-image diffusion model that has not been pretrained on copyrighted images, and II. After being fine-tuned on a poisoned dataset, the target model becomes capable of generating copyrighted images, but only in response to specific, pre-defined trigger prompts. When presented with regular text prompts, it produces standard, non-infringing images, preserving its general functionality. \n\nIt is crucial to emphasize the importance of ensuring that the model is not pre-trained on copyrighted materials. Recent findings by Li et al. (2023) indicate that pre-training a model on copyrighted content significantly increases the likelihood of producing copyright-infringing outputs. Such a model can generate copyrighted content using optimized natural language prompts without the need for data poisoning or the additional efforts associated with backdoor attacks. For clarification, we list the constraints of the copyright infringement attack as follows: \n\n\u2022 Inconspicuous Prompt Trigger: In our setting, the trigger refers to natural language prompts that are indistinguishable from other clean prompts, ensuring they do not arouse suspicion or be cleaned by text cleaning techniques. \n\n\u2022 Trigger Specificity: The model only produces copyrighted images when activated by certain triggers while retaining its ability to generate normal images with standard prompts. \n\n\u2022 Poisoning Data without Copyright Issues: The poisoning data used to compromise the model should not bear significant resemblance to the original copyrighted works, aligning with legal standards to avoid detection (Osterberg & Osterberg, 2003). \n\n\u2022 Stealthiness of Poisoning Data: The text-image pairs introduced as poison must be matching pairs to avoid being filtered out during data preprocessing. Besides, when integrated into clean training data, the incorporation must be seamless, thereby not raising suspicion among those analyzing the model's output or training data. \n\nComparison with Standard Backdoor Attacks. Our attack method is different from traditional backdoor attacks. \n\nIn our approach, the trigger is a specific prompt, while the backdoors are poisoned image-caption pairs. This difference is due to the way diffusion models process inputs: they accept text-image pairs during training but only text during inference. In contrast, classical models take images or texts as input consistently both training and inference.\n\n## Problem Formulation\nIn the context of copyright, a backdoor attack against diffusion models is a type of security attack designed to induce diffusion models to reproduce copyright-infringing outputs by manipulating the clean training dataset. We denote the the clean dataset as D train , the poisoning data as D, and the poisoned dataset as D train = D train \u222a D. Subsequently, a compromised model, M , is obtained after training on D train . The attacker's objective is to make the diffusion model to generate artwork M (y t ), with substantial similarity to the copyrighted image x t when given a specific prompt y t . The substantial similarity is measured using a evaluator denoted as F(\u2022, \u2022) : X \u00d7 X \u2192 R. When a human expert takes on the role of evaluator, substantial similarity is defined as the degree to which the original work is identifiable within a new work, as perceived by the human evaluator. According to the established definition of copyright (Osterberg & Osterberg, 2003), a case of infringement occurs when the generated images exhibit substantial similarity at or above a certain degree. Thus, the copyright infringement is formulated as a satisfiability problem, as depicted in Equation ( 1): \n\nHere, \u03b4 and \u03c4 denote the constraints that identify generated images as violating copyright laws and ensure the separation of poisoning data from being copyright-infringing. The goal of the attacker is to identify one feasible set in the space of possible poisoning data, which is denoted as D \u22c6 . \n\nTo ensure the stealth of poisoning data, the similarity of the closest image in the poisoning data to the copyrighted image should be lower than that of the closest image in the clean dataset. The disparity in similarity is quantified by a margin, denoted as \u03b3. Then, we have \u03c4 = max x\u2208Dtrain F(x, x t ) \u2212 \u03b3, where \u03b3 \u2265 0. \n\nSubstantial Similarity Metric. Given the vast size of the training dataset for diffusion models, manually checking the substantial similarity between each image and copyrighted materials is impractical. This necessitates the implementation of an automated detector.",
            "reference_string": "[266900037 | Wang et al. | 2024 | Citations: 32]"
        },
        {
            "title": "On Provable Copyright Protection for Generative Models",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 40,
            "citation_count": 94,
            "influential_citation_count": 14,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.10870, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145603901",
                    "name": "Nikhil Vyas"
                },
                {
                    "authorId": "144695232",
                    "name": "S. Kakade"
                },
                {
                    "authorId": "1697211",
                    "name": "B. Barak"
                }
            ],
            "abstract": "There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.",
            "corpus_id": 257050406,
            "sentences": [
                {
                    "corpus_id": "257050406",
                    "title": "On Provable Copyright Protection for Generative Models",
                    "text": "Generative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models. \n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape Figure 1: The CP-k Algorithm applied to diffusion models. The dataset is CIFAR-10 augmented with multiple copies of two images (images close to the augmented images are marked with red boundaries); hypothetically, suppose these two images are copyrighted works. The leftmost image shows generations from a model p that was trained on the full dataset, where we clearly see that p generates the two copyrighted works. Our algorithm starts by splitting this dataset into two disjoint datasets, making sure that copyrighted images are split into two different shards; for illustrative purposes, we do not deduplicate the dataset. The procedure then trains two models q1, q2 on these disjoint shards. The middle two figures show samples from the models q1, q2, again clearly showing memorization. However, note that q1 does not generate one of the copyrighted images and and q2 does not generate the other copyrighted image (as these were not in their respective datasets). Our algorithm CP-k then uses q1, q2, along with the original model p, to construct a model p k which has strong copyright protection guarantees.",
                    "score": 0.631776842829519,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 513
                        },
                        {
                            "start": 514,
                            "end": 638
                        },
                        {
                            "start": 641,
                            "end": 733
                        },
                        {
                            "start": 734,
                            "end": 881
                        },
                        {
                            "start": 882,
                            "end": 945
                        },
                        {
                            "start": 946,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1478
                        },
                        {
                            "start": 1479,
                            "end": 1688
                        },
                        {
                            "start": 1689,
                            "end": 1758
                        },
                        {
                            "start": 1759,
                            "end": 1854
                        },
                        {
                            "start": 1855,
                            "end": 2031
                        },
                        {
                            "start": 2032,
                            "end": 2176
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.666015625
                },
                {
                    "corpus_id": "257050406",
                    "title": "On Provable Copyright Protection for Generative Models",
                    "text": "Our algorithm CP-k then uses q1, q2, along with the original model p, to construct a model p k which has strong copyright protection guarantees. The last image is the outputs of p k , showing it is highly unlikely to output either of the copyrighted images, even though each of q1, q2 and p has memorized some of these images. See Section 4 for more details (and for a discussion with regards to our displayed model generations having used the same noise on the diffusion paths). \n\nsignificant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021]. \n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material. It is this issue of preventing deployment-time copyright infringement that is the focus of this work. \n\nOur contributions. We give a formal definition -\"near-access freeness\" -bounding the extent to which a learned generative model's output can be substantially influenced by a particular piece of copyrighted data that the model was trained on. We also give a procedure that transforms (under certain assumptions) any generative model learning algorithm A into an algorithm A k , which protects against violations under our definition. In particular, the model output by A k will (1) be at most k-bits far from a \"safe\" model (which is not committing copyright infringement), and (2) will have performance reasonably close to the model output by the original algorithm A (in a quantifiable sense, based on properties of A).",
                    "score": 0.5764308191209843,
                    "section_title": "Introduction",
                    "char_start_offset": 2047,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 144
                        },
                        {
                            "start": 145,
                            "end": 326
                        },
                        {
                            "start": 327,
                            "end": 479
                        },
                        {
                            "start": 482,
                            "end": 565
                        },
                        {
                            "start": 566,
                            "end": 666
                        },
                        {
                            "start": 667,
                            "end": 837
                        },
                        {
                            "start": 840,
                            "end": 990
                        },
                        {
                            "start": 991,
                            "end": 1127
                        },
                        {
                            "start": 1128,
                            "end": 1232
                        },
                        {
                            "start": 1233,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1538
                        },
                        {
                            "start": 1541,
                            "end": 1559
                        },
                        {
                            "start": 1560,
                            "end": 1782
                        },
                        {
                            "start": 1783,
                            "end": 1973
                        },
                        {
                            "start": 1974,
                            "end": 2261
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 819,
                            "end": 836,
                            "matchedPaperCorpusId": "239770285"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.60546875
                }
            ],
            "relevance_judgement": 0.666015625,
            "relevance_judgment_input_expanded": "# Title: On Provable Copyright Protection for Generative Models\n# Venue: International Conference on Machine Learning\n# Authors: Nikhil Vyas, S. Kakade, B. Barak\n## Abstract\nThere is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.\n## Introduction\nGenerative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models. \n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape Figure 1: The CP-k Algorithm applied to diffusion models. The dataset is CIFAR-10 augmented with multiple copies of two images (images close to the augmented images are marked with red boundaries); hypothetically, suppose these two images are copyrighted works. The leftmost image shows generations from a model p that was trained on the full dataset, where we clearly see that p generates the two copyrighted works. Our algorithm starts by splitting this dataset into two disjoint datasets, making sure that copyrighted images are split into two different shards; for illustrative purposes, we do not deduplicate the dataset. The procedure then trains two models q1, q2 on these disjoint shards. The middle two figures show samples from the models q1, q2, again clearly showing memorization. However, note that q1 does not generate one of the copyrighted images and and q2 does not generate the other copyrighted image (as these were not in their respective datasets). Our algorithm CP-k then uses q1, q2, along with the original model p, to construct a model p k which has strong copyright protection guarantees.\n...\nOur algorithm CP-k then uses q1, q2, along with the original model p, to construct a model p k which has strong copyright protection guarantees. The last image is the outputs of p k , showing it is highly unlikely to output either of the copyrighted images, even though each of q1, q2 and p has memorized some of these images. See Section 4 for more details (and for a discussion with regards to our displayed model generations having used the same noise on the diffusion paths). \n\nsignificant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021]. \n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material. It is this issue of preventing deployment-time copyright infringement that is the focus of this work. \n\nOur contributions. We give a formal definition -\"near-access freeness\" -bounding the extent to which a learned generative model's output can be substantially influenced by a particular piece of copyrighted data that the model was trained on. We also give a procedure that transforms (under certain assumptions) any generative model learning algorithm A into an algorithm A k , which protects against violations under our definition. In particular, the model output by A k will (1) be at most k-bits far from a \"safe\" model (which is not committing copyright infringement), and (2) will have performance reasonably close to the model output by the original algorithm A (in a quantifiable sense, based on properties of A).",
            "reference_string": "[257050406 | Vyas et al. | 2023 | Citations: 94]"
        },
        {
            "title": "Has an AI model been trained on your images?",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 29,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.06399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2125331179",
                    "name": "Maty\u00e1\u0161 Boh\u00e1\u010dek"
                },
                {
                    "authorId": "2267488540",
                    "name": "Hany Farid"
                }
            ],
            "abstract": "From a simple text prompt, generative-AI image models can create stunningly realistic and creative images bounded, it seems, by only our imagination. These models have achieved this remarkable feat thanks, in part, to the ingestion of billions of images collected from nearly every corner of the internet. Many creators have understandably expressed concern over how their intellectual property has been ingested without their permission or a mechanism to opt out of training. As a result, questions of fair use and copyright infringement have quickly emerged. We describe a method that allows us to determine if a model was trained on a specific image or set of images. This method is computationally efficient and assumes no explicit knowledge of the model architecture or weights (so-called black-box membership inference). We anticipate that this method will be crucial for auditing existing models and, looking ahead, ensuring the fairer development and deployment of generative AI models.",
            "corpus_id": 275471553,
            "sentences": [
                {
                    "corpus_id": "275471553",
                    "title": "Has an AI model been trained on your images?",
                    "text": "Understanding why models are biased to produce content similar to their training data may provide insights into reducing the likelihood of direct copyright infringement in the form of reproducing training data, and may provide insights into how a model can be made to forget training exemplars. \n\nAn attractive aspect of our membership inference for generative image models is that it does not require direct access to the details of the model architecture or trained weights, is computationally efficient, and generalizes to multiple different AI models. A drawback of our approach is that it only applies to models that allow for an image-to-image synthesis with a controllable strength parameter, as compared to text-to-image. Depending on the underlying mechanism by which models produce images similar to their in-training data, our method may be adaptable to text-to-image generation. \n\nMany of today's tech leaders have admitted that their generative-AI models would not exist without their training on billions of pieces of content scraped from all corners of the internet [16]. These same leaders have also called for the loosening of fairuse and copyright rules. While it is for the courts to decide on these matters of law [22], we contend that content creators have legitimate concerns for whether and how their content is used to train generative-AI models, some of which are designed to offer services directly competing with these very content creators. \n\nA critical component of adjudicating these issues will be determining if a deployed model was trained on a specific piece of content. Equally important is determining how creators can and should be compensated when their content is used for training, and how models can be made to forget its training on a specific piece of content should this be the wish of the content's creator. \n\nWe have focused only on the first of these questions, but all of these issues are important to resolve as generative AI continues its impressive and impactful trajectory.",
                    "score": 0.5622529953178175,
                    "section_title": "Discussion",
                    "char_start_offset": 19446,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 294
                        },
                        {
                            "start": 297,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 729
                        },
                        {
                            "start": 730,
                            "end": 890
                        },
                        {
                            "start": 893,
                            "end": 1086
                        },
                        {
                            "start": 1087,
                            "end": 1172
                        },
                        {
                            "start": 1173,
                            "end": 1468
                        },
                        {
                            "start": 1471,
                            "end": 1604
                        },
                        {
                            "start": 1605,
                            "end": 1852
                        },
                        {
                            "start": 1855,
                            "end": 2025
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66015625
                },
                {
                    "corpus_id": "275471553",
                    "title": "Has an AI model been trained on your images?",
                    "text": "From a simple text prompt, generative-AI image models can create stunningly realistic and creative images bounded, it seems, by only our imagination. These models have achieved this remarkable feat thanks, in part, to the ingestion of billions of images collected from nearly every corner of the internet. Many creators have understandably expressed concern over how their intellectual property has been ingested without their permission or a mechanism to opt out of training. As a result, questions of fair use and copyright infringement have quickly emerged. We describe a method that allows us to determine if a model was trained on a specific image or set of images. This method is computationally efficient and assumes no explicit knowledge of the model architecture or weights (so-called black-box membership inference). We anticipate that this method will be crucial for auditing existing models and, looking ahead, ensuring the fairer development and deployment of generative AI models.",
                    "score": 0.4810190727461588,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65283203125
                }
            ],
            "relevance_judgement": 0.66015625,
            "relevance_judgment_input_expanded": "# Title: Has an AI model been trained on your images?\n# Venue: arXiv.org\n# Authors: Maty\u00e1\u0161 Boh\u00e1\u010dek, Hany Farid\n## Abstract\nFrom a simple text prompt, generative-AI image models can create stunningly realistic and creative images bounded, it seems, by only our imagination. These models have achieved this remarkable feat thanks, in part, to the ingestion of billions of images collected from nearly every corner of the internet. Many creators have understandably expressed concern over how their intellectual property has been ingested without their permission or a mechanism to opt out of training. As a result, questions of fair use and copyright infringement have quickly emerged. We describe a method that allows us to determine if a model was trained on a specific image or set of images. This method is computationally efficient and assumes no explicit knowledge of the model architecture or weights (so-called black-box membership inference). We anticipate that this method will be crucial for auditing existing models and, looking ahead, ensuring the fairer development and deployment of generative AI models.\n## Discussion\nUnderstanding why models are biased to produce content similar to their training data may provide insights into reducing the likelihood of direct copyright infringement in the form of reproducing training data, and may provide insights into how a model can be made to forget training exemplars. \n\nAn attractive aspect of our membership inference for generative image models is that it does not require direct access to the details of the model architecture or trained weights, is computationally efficient, and generalizes to multiple different AI models. A drawback of our approach is that it only applies to models that allow for an image-to-image synthesis with a controllable strength parameter, as compared to text-to-image. Depending on the underlying mechanism by which models produce images similar to their in-training data, our method may be adaptable to text-to-image generation. \n\nMany of today's tech leaders have admitted that their generative-AI models would not exist without their training on billions of pieces of content scraped from all corners of the internet [16]. These same leaders have also called for the loosening of fairuse and copyright rules. While it is for the courts to decide on these matters of law [22], we contend that content creators have legitimate concerns for whether and how their content is used to train generative-AI models, some of which are designed to offer services directly competing with these very content creators. \n\nA critical component of adjudicating these issues will be determining if a deployed model was trained on a specific piece of content. Equally important is determining how creators can and should be compensated when their content is used for training, and how models can be made to forget its training on a specific piece of content should this be the wish of the content's creator. \n\nWe have focused only on the first of these questions, but all of these issues are important to resolve as generative AI continues its impressive and impactful trajectory.",
            "reference_string": "[275471553 | Bohacek et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 25,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.03338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2359254079",
                    "name": "Lena Reissinger"
                },
                {
                    "authorId": "2256011059",
                    "name": "Yuanyuan Li"
                },
                {
                    "authorId": "23107750",
                    "name": "Anna Haensch"
                },
                {
                    "authorId": "2254269177",
                    "name": "Neeraj Sarna"
                }
            ],
            "abstract": "Visual Generative AI models have demonstrated remarkable capability in generating high-quality images from simple inputs like text prompts. However, because these models are trained on images from diverse sources, they risk memorizing and reproducing specific content, raising concerns about intellectual property (IP) infringement. Recent advances in prompt engineering offer a cost-effective way to enhance generative AI performance. In this paper, we evaluate the effectiveness of prompt engineering techniques in mitigating IP infringement risks in image generation. Our findings show that Chain of Thought Prompting and Task Instruction Prompting significantly reduce the similarity between generated images and the training data of diffusion models, thereby lowering the risk of IP infringement.",
            "corpus_id": 278338968,
            "sentences": [
                {
                    "corpus_id": "278338968",
                    "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
                    "text": "We evaluate the effectiveness of prompting strategies in reducing the risk of IP infringement of visual GenAI by 3 metrics: similarity scores to training images, relevance and aesthetic value. Overall, we find that prompt engineering can reduce copyright-related risks in visual GenAI models, but its effectiveness varies depending on the chosen technique. Chain-of-thought prompting proved to be the most effective in IP risk mitigation. Negative prompting was the least effective strategy, while task instruction prompting yielded promising results in preventing training data replication while balancing this achievement with high aesthetic scores and strong prompt alignment. These strategies are particularly important given the legal uncertainty surrounding AI-generated content. We hope this work contributes to safer deployment practices and informs future standards for responsible visual generative AI use.",
                    "score": 0.49256746292749964,
                    "section_title": "Conclusions",
                    "char_start_offset": 20488,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 192
                        },
                        {
                            "start": 193,
                            "end": 356
                        },
                        {
                            "start": 357,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 679
                        },
                        {
                            "start": 680,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 916
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63134765625
                },
                {
                    "corpus_id": "278338968",
                    "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
                    "text": "As generative AI (GenAI) becomes increasingly prevalent in realworld applications, concerns about its potential risks continue to grow. We focus on the risks associated with the replication of copyrighted material and restrict ourselves to image generation. The tangible legal and financial implications of these risks have sparked our current research. For instance, artists sued Stability AI (Midjourney and DevianArt) claiming that the companies' AI image generators produce images that are strikingly similar to their artworks [2]. In August 2024, a U.S. district judge (Orrick) ruled that the artists could proceed with the copyright infringement claims, underscoring the ongoing legal uncertainties surrounding AI-generated content [6]. \n\nThese prevalent risks make model users resistant to fully exploiting the latest GenAI models. To promote a wide adoption of GenAI, risk management is crucial. Risk management has two aspects: a) risk quantification; and b) risk mitigation. Both aspects have already received attention in the literature. \n\nRisk quantification: Risk quantification involves analysing the fraction of training images that are reproduced by the model. Since training images may be copy-righted, this leads to IP infringement risks for the end user. Consider for instance Stable Diffusion-1. When prompted using the captions of training images directly, about 2% of images it generates are highly similar to those in the training dataset [4,24]; the results are very similar for the Stable Diffusion-2 model [25]. \n\nRisk mitigation: We focus on risk mitigation, which takes a step beyond risk quantification. The goal of risk mitigation is to reduce the probability of a model outputting copyrighted content. The risk could either be mitigated before deploying a model (on the model developer's side) or after (on the model's user side). \n\nPre-deployment strategies involve changing the data science process and are usually expensive. For instance, models could be trained on a de-duplicated data set, which usually reduces the risk during training time [25]. Note however that with this strategy test time risks still prevail. Model could also be made to unlearn copy righted data but that comes with a huge computational expense [31,7,8].",
                    "score": 0.48739148826119316,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 353
                        },
                        {
                            "start": 354,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 742
                        },
                        {
                            "start": 745,
                            "end": 838
                        },
                        {
                            "start": 839,
                            "end": 903
                        },
                        {
                            "start": 904,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1048
                        },
                        {
                            "start": 1051,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1315
                        },
                        {
                            "start": 1316,
                            "end": 1537
                        },
                        {
                            "start": 1540,
                            "end": 1632
                        },
                        {
                            "start": 1633,
                            "end": 1732
                        },
                        {
                            "start": 1733,
                            "end": 1861
                        },
                        {
                            "start": 1864,
                            "end": 1958
                        },
                        {
                            "start": 1959,
                            "end": 2083
                        },
                        {
                            "start": 2084,
                            "end": 2151
                        },
                        {
                            "start": 2152,
                            "end": 2264
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.58642578125
                },
                {
                    "corpus_id": "278338968",
                    "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
                    "text": "Copyright issues in visual diffusion models arise when models replicate their training data within the image generation process. Therefore, to detect IP infringement, we compare the generated image with the images in the training data. Since training data is huge, a comparison to the entire dataset is prohibitively expensive. We thereforesimilar to [25]-limit our comparison to a subset of the training data. \n\nImage embeddings: To compare two images, we first encode them. Image embedding can happen on a content and style level. Some embeddings correspond rather to the object-level content of the generated images, whereas other perform better in capturing the artistic style of the generated images [19]. We focus on rather content based embedding because similarity based upon style might not be considered copyrighted [14]. \n\nTo embed an image, we consider CLIP embedding [18]. CLIP embeddings are useful when dealing with text-to-image models because they map both images and text into a shared semantic space. This also allows for meaningful comparisons beyond pixel-level similarity. By using CLIP embeddings, we evaluate similarity not only at a visual level but also in terms of content and semantic meaning. \n\nSimilarity score: Once we have the image embedding, we compare images using a similarity score between the embeddings. Two images are considered to be similar when the similarity score between their embeddings is larger than certain threshold [4]. We con-sider the cosine similarity between the CLIP embeddings as our similarity score, and a threshold of 0.85 as high IP infringement risk. Notice that [4] used the same similarity threshold to identify near duplicates in the training data for deduplication. \n\nWe briefly recall other possible choices for a similarity score: authors in [24] use split dot product of Self Supervised Copy Detection (SSCD) scores as similarity measure, where scores are predicted based on differential entropy regularization, see [16]. In [27], authors utilize the Euclidean L2 norm on pixel space and SSCD scores, while [4] employ cosine similarity between CLIP embeddings to identify near-duplicates. \n\n3 Experimental Results",
                    "score": 0.44677379446662446,
                    "section_title": "Detecting IP Infringement",
                    "char_start_offset": 7363,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 235
                        },
                        {
                            "start": 236,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 410
                        },
                        {
                            "start": 413,
                            "end": 475
                        },
                        {
                            "start": 476,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 710
                        },
                        {
                            "start": 711,
                            "end": 831
                        },
                        {
                            "start": 834,
                            "end": 885
                        },
                        {
                            "start": 886,
                            "end": 1019
                        },
                        {
                            "start": 1020,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1221
                        },
                        {
                            "start": 1224,
                            "end": 1342
                        },
                        {
                            "start": 1343,
                            "end": 1471
                        },
                        {
                            "start": 1472,
                            "end": 1613
                        },
                        {
                            "start": 1614,
                            "end": 1732
                        },
                        {
                            "start": 1735,
                            "end": 1991
                        },
                        {
                            "start": 1992,
                            "end": 2158
                        },
                        {
                            "start": 2161,
                            "end": 2183
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1986,
                            "end": 1990,
                            "matchedPaperCorpusId": "247011159"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55078125
                },
                {
                    "corpus_id": "278338968",
                    "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
                    "text": "Model could also be made to unlearn copy righted data but that comes with a huge computational expense [31,7,8]. \n\nPost-deployment strategies are usually cheaper. Model users could be educated on using AI responsibly. Furthermore, before a user inputs a prompt, a \"system message\" could be added that aims to reduce the IP-infringement risks [13]. This is a kind of prompt engineering techniques [28,5,17] that have been popularly used to enhance GenAI model performance on diverse tasks. Another possibility is to check each generated output against a corpus of copyrighted images. Since the size of such a corpus is huge, this approach soon becomes infeasible. \n\nWe focus on prompt engineering and evaluate its effectiveness for IP risk mitigation-Figure 1 presents a snapshot of our results indicating that prompt engineering could substantially alter training data replication for vision models. Since GenAI outputs are sensitive to input prompts, we expect that via a carefully engineered prompt, IP infringement related risk can be reduced. Prompt engineering has already been explored for reducing hallucination in large language models [26,3]. For vision models however, it is largely unexplored [10]. \n\nPrompt engineering for image generation and user accessibility has primarily developed through informal, trial-and-error approaches. In artistic communities, a common trend has emerged where prompts frequently take the form of \"X in the style of Y,\" with Y referring to a particular artist or artistic movement. Based on this observation, [11] explored how modifications to this prompt pattern influence the behavior of the image generation model. Their experiments focused on the influence of different phrasings of the prompt, different random initializations, the number of iterations, and the style as well as the subject parameter. \n\nIn this paper, we want to contribute to ongoing research on prompting in visual diffusion models by applying various prompt engineering strategies for generating non-copyright-violating content and evaluating their performances. To the best of our knowledge, for IP risk mitigation involving vision models, ours is the first work of its kind that develops an extensive framework for exploring prompt engineering.",
                    "score": 0.4649415689723713,
                    "section_title": "Introduction",
                    "char_start_offset": 2167,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 112
                        },
                        {
                            "start": 115,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 217
                        },
                        {
                            "start": 218,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 488
                        },
                        {
                            "start": 489,
                            "end": 582
                        },
                        {
                            "start": 583,
                            "end": 662
                        },
                        {
                            "start": 665,
                            "end": 899
                        },
                        {
                            "start": 900,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1151
                        },
                        {
                            "start": 1152,
                            "end": 1209
                        },
                        {
                            "start": 1212,
                            "end": 1344
                        },
                        {
                            "start": 1345,
                            "end": 1523
                        },
                        {
                            "start": 1524,
                            "end": 1659
                        },
                        {
                            "start": 1660,
                            "end": 1848
                        },
                        {
                            "start": 1851,
                            "end": 2079
                        },
                        {
                            "start": 2080,
                            "end": 2263
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 103,
                            "end": 107,
                            "matchedPaperCorpusId": "257833863"
                        },
                        {
                            "start": 107,
                            "end": 109,
                            "matchedPaperCorpusId": "258740988"
                        },
                        {
                            "start": 109,
                            "end": 111,
                            "matchedPaperCorpusId": "266374816"
                        },
                        {
                            "start": 1204,
                            "end": 1208,
                            "matchedPaperCorpusId": "237513697"
                        },
                        {
                            "start": 1551,
                            "end": 1555,
                            "matchedPaperCorpusId": "237513697"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.430908203125
                }
            ],
            "relevance_judgement": 0.63134765625,
            "relevance_judgment_input_expanded": "# Title: Safer Prompts: Reducing IP Risk in Visual Generative AI\n# Venue: arXiv.org\n# Authors: Lena Reissinger, Yuanyuan Li, Anna Haensch, Neeraj Sarna\n## Abstract\nVisual Generative AI models have demonstrated remarkable capability in generating high-quality images from simple inputs like text prompts. However, because these models are trained on images from diverse sources, they risk memorizing and reproducing specific content, raising concerns about intellectual property (IP) infringement. Recent advances in prompt engineering offer a cost-effective way to enhance generative AI performance. In this paper, we evaluate the effectiveness of prompt engineering techniques in mitigating IP infringement risks in image generation. Our findings show that Chain of Thought Prompting and Task Instruction Prompting significantly reduce the similarity between generated images and the training data of diffusion models, thereby lowering the risk of IP infringement.\n## Introduction\nAs generative AI (GenAI) becomes increasingly prevalent in realworld applications, concerns about its potential risks continue to grow. We focus on the risks associated with the replication of copyrighted material and restrict ourselves to image generation. The tangible legal and financial implications of these risks have sparked our current research. For instance, artists sued Stability AI (Midjourney and DevianArt) claiming that the companies' AI image generators produce images that are strikingly similar to their artworks [2]. In August 2024, a U.S. district judge (Orrick) ruled that the artists could proceed with the copyright infringement claims, underscoring the ongoing legal uncertainties surrounding AI-generated content [6]. \n\nThese prevalent risks make model users resistant to fully exploiting the latest GenAI models. To promote a wide adoption of GenAI, risk management is crucial. Risk management has two aspects: a) risk quantification; and b) risk mitigation. Both aspects have already received attention in the literature. \n\nRisk quantification: Risk quantification involves analysing the fraction of training images that are reproduced by the model. Since training images may be copy-righted, this leads to IP infringement risks for the end user. Consider for instance Stable Diffusion-1. When prompted using the captions of training images directly, about 2% of images it generates are highly similar to those in the training dataset [4,24]; the results are very similar for the Stable Diffusion-2 model [25]. \n\nRisk mitigation: We focus on risk mitigation, which takes a step beyond risk quantification. The goal of risk mitigation is to reduce the probability of a model outputting copyrighted content. The risk could either be mitigated before deploying a model (on the model developer's side) or after (on the model's user side). \n\nPre-deployment strategies involve changing the data science process and are usually expensive. For instance, models could be trained on a de-duplicated data set, which usually reduces the risk during training time [25]. Note however that with this strategy test time risks still prevail. Model could also be made to unlearn copy righted data but that comes with a huge computational expense [31,7,8].\n...\nModel could also be made to unlearn copy righted data but that comes with a huge computational expense [31,7,8]. \n\nPost-deployment strategies are usually cheaper. Model users could be educated on using AI responsibly. Furthermore, before a user inputs a prompt, a \"system message\" could be added that aims to reduce the IP-infringement risks [13]. This is a kind of prompt engineering techniques [28,5,17] that have been popularly used to enhance GenAI model performance on diverse tasks. Another possibility is to check each generated output against a corpus of copyrighted images. Since the size of such a corpus is huge, this approach soon becomes infeasible. \n\nWe focus on prompt engineering and evaluate its effectiveness for IP risk mitigation-Figure 1 presents a snapshot of our results indicating that prompt engineering could substantially alter training data replication for vision models. Since GenAI outputs are sensitive to input prompts, we expect that via a carefully engineered prompt, IP infringement related risk can be reduced. Prompt engineering has already been explored for reducing hallucination in large language models [26,3]. For vision models however, it is largely unexplored [10]. \n\nPrompt engineering for image generation and user accessibility has primarily developed through informal, trial-and-error approaches. In artistic communities, a common trend has emerged where prompts frequently take the form of \"X in the style of Y,\" with Y referring to a particular artist or artistic movement. Based on this observation, [11] explored how modifications to this prompt pattern influence the behavior of the image generation model. Their experiments focused on the influence of different phrasings of the prompt, different random initializations, the number of iterations, and the style as well as the subject parameter. \n\nIn this paper, we want to contribute to ongoing research on prompting in visual diffusion models by applying various prompt engineering strategies for generating non-copyright-violating content and evaluating their performances. To the best of our knowledge, for IP risk mitigation involving vision models, ours is the first work of its kind that develops an extensive framework for exploring prompt engineering.\n\n## Detecting IP Infringement\nCopyright issues in visual diffusion models arise when models replicate their training data within the image generation process. Therefore, to detect IP infringement, we compare the generated image with the images in the training data. Since training data is huge, a comparison to the entire dataset is prohibitively expensive. We thereforesimilar to [25]-limit our comparison to a subset of the training data. \n\nImage embeddings: To compare two images, we first encode them. Image embedding can happen on a content and style level. Some embeddings correspond rather to the object-level content of the generated images, whereas other perform better in capturing the artistic style of the generated images [19]. We focus on rather content based embedding because similarity based upon style might not be considered copyrighted [14]. \n\nTo embed an image, we consider CLIP embedding [18]. CLIP embeddings are useful when dealing with text-to-image models because they map both images and text into a shared semantic space. This also allows for meaningful comparisons beyond pixel-level similarity. By using CLIP embeddings, we evaluate similarity not only at a visual level but also in terms of content and semantic meaning. \n\nSimilarity score: Once we have the image embedding, we compare images using a similarity score between the embeddings. Two images are considered to be similar when the similarity score between their embeddings is larger than certain threshold [4]. We con-sider the cosine similarity between the CLIP embeddings as our similarity score, and a threshold of 0.85 as high IP infringement risk. Notice that [4] used the same similarity threshold to identify near duplicates in the training data for deduplication. \n\nWe briefly recall other possible choices for a similarity score: authors in [24] use split dot product of Self Supervised Copy Detection (SSCD) scores as similarity measure, where scores are predicted based on differential entropy regularization, see [16]. In [27], authors utilize the Euclidean L2 norm on pixel space and SSCD scores, while [4] employ cosine similarity between CLIP embeddings to identify near-duplicates. \n\n3 Experimental Results\n\n## Conclusions\nWe evaluate the effectiveness of prompting strategies in reducing the risk of IP infringement of visual GenAI by 3 metrics: similarity scores to training images, relevance and aesthetic value. Overall, we find that prompt engineering can reduce copyright-related risks in visual GenAI models, but its effectiveness varies depending on the chosen technique. Chain-of-thought prompting proved to be the most effective in IP risk mitigation. Negative prompting was the least effective strategy, while task instruction prompting yielded promising results in preventing training data replication while balancing this achievement with high aesthetic scores and strong prompt alignment. These strategies are particularly important given the legal uncertainty surrounding AI-generated content. We hope this work contributes to safer deployment practices and informs future standards for responsible visual generative AI use.",
            "reference_string": "[278338968 | Reissinger et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 33,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.16618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2346987106",
                    "name": "Qipan Xu"
                },
                {
                    "authorId": "2307564873",
                    "name": "Zhenting Wang"
                },
                {
                    "authorId": "2202044790",
                    "name": "Xiaoxiao He"
                },
                {
                    "authorId": "2302817621",
                    "name": "Ligong Han"
                },
                {
                    "authorId": "2346993695",
                    "name": "Ruixiang Tang"
                }
            ],
            "abstract": "Generative AI models, renowned for their ability to synthesize high-quality content, have sparked growing concerns over the improper generation of copyright-protected material. While recent studies have proposed various approaches to address copyright issues, the capability of large vision-language models (LVLMs) to detect copyright infringements remains largely unexplored. In this work, we focus on evaluating the copyright detection abilities of state-of-the-art LVLMs using a various set of image samples. Recognizing the absence of a comprehensive dataset that includes both IP-infringement samples and ambiguous non-infringement negative samples, we construct a benchmark dataset comprising positive samples that violate the copyright protection of well-known IP figures, as well as negative samples that resemble these figures but do not raise copyright concerns. This dataset is created using advanced prompt engineering techniques. We then evaluate leading LVLMs using our benchmark dataset. Our experimental results reveal that LVLMs are prone to overfitting, leading to the misclassification of some negative samples as IP-infringement cases. In the final section, we analyze these failure cases and propose potential solutions to mitigate the overfitting problem.",
            "corpus_id": 276575866,
            "sentences": [
                {
                    "corpus_id": "276575866",
                    "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
                    "text": "To mitigate IP infringement, two primary approaches have emerged: \n\n\u2022 Reducing Memorization During Training: Implementing differential privacy (Dwork et al., 2014) techniques during the training of generative models can help minimize the retention of specific data points, thereby reducing the risk of reproducing protected content (Abadi et al., 2016;Chen et al., 2022;Dockhorn et al., 2022). \n\n\u2022 Prompt Engineering: Employing strategies such as negative prompts during the inference phase can exclude undesired concepts or elements from the generated output (Wang et al., 2024;He et al., 2024), or optimizing unsafe prompts (Chin et al., 2023;Rando et al., 2022), thereby avoiding the inclusion of IP-protected material. \n\nDespite the widespread copyright concerns surrounding generative AI and the numerous IP mitigation approaches recently proposed, the issue of benchmarking VLM IP infringement detection remains largely underexplored. As a result, a primary focus of our paper is to address the capabilities of VLMs in detecting and mitigating IP infringement.",
                    "score": 0.4531015087471823,
                    "section_title": "Vision Language Models",
                    "char_start_offset": 7990,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 65
                        },
                        {
                            "start": 68,
                            "end": 393
                        },
                        {
                            "start": 396,
                            "end": 722
                        },
                        {
                            "start": 725,
                            "end": 940
                        },
                        {
                            "start": 941,
                            "end": 1066
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 143,
                            "end": 163,
                            "matchedPaperCorpusId": "207178262"
                        },
                        {
                            "start": 332,
                            "end": 352,
                            "matchedPaperCorpusId": "207241585"
                        },
                        {
                            "start": 352,
                            "end": 370,
                            "matchedPaperCorpusId": "250210875"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.625
                },
                {
                    "corpus_id": "276575866",
                    "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
                    "text": "The rapid advancement of generative artificial intelligence (GenAI) has ushered in a new era of content creation, enabling the synthesis of highquality text, images, and multimedia content at an unprecedented scale. While these innovations have expanded creative possibilities and applications across industries, they have also raised significant ethical and legal concerns, particularly regarding intellectual property (IP) rights (Sag, 2023;Poland). One of the most pressing issues is the unauthorized reproduction of copyrighted material, where generative models may inadvertently produce outputs that closely resemble or replicate IP-protected content (Zirpoli, 2023;Dzuong et al., 2024;Sag, 2023;Poland;Wang et al., 2024). This issue has led to widespread debates among legal experts, policymakers, and AI researchers on the potential liabilities and regulatory measures required to address copyright infringement in AI-generated content. \n\nExisting efforts to mitigate copyright concerns in generative models have primarily focused on two key approaches: \u2460 reducing memorization during training using techniques such as differential privacy (Dwork et al., 2014), which limits the retention of specific data points to prevent models from reproducing protected content (Abadi et al., 2016;Chen et al., 2022;Dockhorn et al., 2022), and \u2461 employing prompt engineering strategies that discourage the generation of IP-infringing material through explicit negative prompts (Wang et al., 2024;He et al., 2024) or optimized safe prompt modifications (Chin et al., 2023;Rando et al., 2022). While these approaches offer some level of control over generative outputs, they do not directly address the challenge of detecting copyright infringement in already-generated content. As a result, there is an urgent need for robust evaluation methods and benchmarks to assess the ability of AI models-specifically large vision-language models (LVLMs)-to identify potential instances of copyright violations. \n\nVision-language models (VLMs), which integrate both textual and visual data to enable crossmodal reasoning, have demonstrated remarkable capabilities in tasks such as image classification, visual question answering (VQA) (Antol et al., 2015), and multimodal understanding.",
                    "score": 0.5989954638611665,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 215
                        },
                        {
                            "start": 216,
                            "end": 451
                        },
                        {
                            "start": 452,
                            "end": 727
                        },
                        {
                            "start": 728,
                            "end": 943
                        },
                        {
                            "start": 946,
                            "end": 1586
                        },
                        {
                            "start": 1587,
                            "end": 1771
                        },
                        {
                            "start": 1772,
                            "end": 1995
                        },
                        {
                            "start": 1998,
                            "end": 2270
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 432,
                            "end": 443,
                            "matchedPaperCorpusId": "258515384"
                        },
                        {
                            "start": 691,
                            "end": 701,
                            "matchedPaperCorpusId": "258515384"
                        },
                        {
                            "start": 1147,
                            "end": 1167,
                            "matchedPaperCorpusId": "207178262"
                        },
                        {
                            "start": 1273,
                            "end": 1293,
                            "matchedPaperCorpusId": "207241585"
                        },
                        {
                            "start": 1293,
                            "end": 1311,
                            "matchedPaperCorpusId": "250210875"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.44970703125
                }
            ],
            "relevance_judgement": 0.625,
            "relevance_judgment_input_expanded": "# Title: Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?\n# Venue: arXiv.org\n# Authors: Qipan Xu, Zhenting Wang, Xiaoxiao He, Ligong Han, Ruixiang Tang\n## Abstract\nGenerative AI models, renowned for their ability to synthesize high-quality content, have sparked growing concerns over the improper generation of copyright-protected material. While recent studies have proposed various approaches to address copyright issues, the capability of large vision-language models (LVLMs) to detect copyright infringements remains largely unexplored. In this work, we focus on evaluating the copyright detection abilities of state-of-the-art LVLMs using a various set of image samples. Recognizing the absence of a comprehensive dataset that includes both IP-infringement samples and ambiguous non-infringement negative samples, we construct a benchmark dataset comprising positive samples that violate the copyright protection of well-known IP figures, as well as negative samples that resemble these figures but do not raise copyright concerns. This dataset is created using advanced prompt engineering techniques. We then evaluate leading LVLMs using our benchmark dataset. Our experimental results reveal that LVLMs are prone to overfitting, leading to the misclassification of some negative samples as IP-infringement cases. In the final section, we analyze these failure cases and propose potential solutions to mitigate the overfitting problem.\n## Introduction\nThe rapid advancement of generative artificial intelligence (GenAI) has ushered in a new era of content creation, enabling the synthesis of highquality text, images, and multimedia content at an unprecedented scale. While these innovations have expanded creative possibilities and applications across industries, they have also raised significant ethical and legal concerns, particularly regarding intellectual property (IP) rights (Sag, 2023;Poland). One of the most pressing issues is the unauthorized reproduction of copyrighted material, where generative models may inadvertently produce outputs that closely resemble or replicate IP-protected content (Zirpoli, 2023;Dzuong et al., 2024;Sag, 2023;Poland;Wang et al., 2024). This issue has led to widespread debates among legal experts, policymakers, and AI researchers on the potential liabilities and regulatory measures required to address copyright infringement in AI-generated content. \n\nExisting efforts to mitigate copyright concerns in generative models have primarily focused on two key approaches: \u2460 reducing memorization during training using techniques such as differential privacy (Dwork et al., 2014), which limits the retention of specific data points to prevent models from reproducing protected content (Abadi et al., 2016;Chen et al., 2022;Dockhorn et al., 2022), and \u2461 employing prompt engineering strategies that discourage the generation of IP-infringing material through explicit negative prompts (Wang et al., 2024;He et al., 2024) or optimized safe prompt modifications (Chin et al., 2023;Rando et al., 2022). While these approaches offer some level of control over generative outputs, they do not directly address the challenge of detecting copyright infringement in already-generated content. As a result, there is an urgent need for robust evaluation methods and benchmarks to assess the ability of AI models-specifically large vision-language models (LVLMs)-to identify potential instances of copyright violations. \n\nVision-language models (VLMs), which integrate both textual and visual data to enable crossmodal reasoning, have demonstrated remarkable capabilities in tasks such as image classification, visual question answering (VQA) (Antol et al., 2015), and multimodal understanding.\n\n## Vision Language Models\nTo mitigate IP infringement, two primary approaches have emerged: \n\n\u2022 Reducing Memorization During Training: Implementing differential privacy (Dwork et al., 2014) techniques during the training of generative models can help minimize the retention of specific data points, thereby reducing the risk of reproducing protected content (Abadi et al., 2016;Chen et al., 2022;Dockhorn et al., 2022). \n\n\u2022 Prompt Engineering: Employing strategies such as negative prompts during the inference phase can exclude undesired concepts or elements from the generated output (Wang et al., 2024;He et al., 2024), or optimizing unsafe prompts (Chin et al., 2023;Rando et al., 2022), thereby avoiding the inclusion of IP-protected material. \n\nDespite the widespread copyright concerns surrounding generative AI and the numerous IP mitigation approaches recently proposed, the issue of benchmarking VLM IP infringement detection remains largely underexplored. As a result, a primary focus of our paper is to address the capabilities of VLMs in detecting and mitigating IP infringement.",
            "reference_string": "[276575866 | Xu et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 53,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294507804",
                    "name": "Luxi He"
                },
                {
                    "authorId": "2283305597",
                    "name": "Yangsibo Huang"
                },
                {
                    "authorId": "2304129935",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2144071564",
                    "name": "Tinghao Xie"
                },
                {
                    "authorId": "2308072184",
                    "name": "Haotian Liu"
                },
                {
                    "authorId": "2307621989",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2309481623",
                    "name": "Chiyuan Zhang"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "2254262712",
                    "name": "Peter Henderson"
                }
            ],
            "abstract": "Recent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal concerns about copyright infringement. Copyrighted characters (e.g., Mario, Batman) present a significant challenge: at least one lawsuit has already awarded damages based on the generation of such characters. Consequently, commercial services like DALL-E have started deploying interventions. However, little research has systematically examined these problems: (1) Can users easily prompt models to generate copyrighted characters, even if it is unintentional?; (2) How effective are the existing mitigation strategies? To address these questions, we introduce a novel evaluation framework with metrics that assess both the generated image's similarity to copyrighted characters and its consistency with user intent, grounded in a set of popular copyrighted characters from diverse studios and regions. We show that state-of-the-art image and video generation models can still generate characters even if characters' names are not explicitly mentioned, sometimes with only two generic keywords (e.g., prompting with\"videogame, plumber\"consistently generates Nintendo's Mario character). We also introduce semi-automatic techniques to identify such keywords or descriptions that trigger character generation. Using this framework, we evaluate mitigation strategies, including prompt rewriting and new approaches we propose. Our findings reveal that common methods, such as DALL-E's prompt rewriting, are insufficient alone and require supplementary strategies like negative prompting. Our work provides empirical grounding for discussions on copyright mitigation strategies and offers actionable insights for model deployers implementing these safeguards.",
            "corpus_id": 270620122,
            "sentences": [
                {
                    "corpus_id": "270620122",
                    "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
                    "text": "Recent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal concerns about copyright infringement. Copyrighted characters (e.g., Mario, Batman) present a significant challenge: at least one lawsuit has already awarded damages based on the generation of such characters. Consequently, commercial services like DALL-E have started deploying interventions. However, little research has systematically examined these problems: (1) Can users easily prompt models to generate copyrighted characters, even if it is unintentional?; (2) How effective are the existing mitigation strategies? To address these questions, we introduce a novel evaluation framework with metrics that assess both the generated image's similarity to copyrighted characters and its consistency with user intent, grounded in a set of popular copyrighted characters from diverse studios and regions. We show that state-of-the-art image and video generation models can still generate characters even if characters' names are not explicitly mentioned, sometimes with only two generic keywords (e.g., prompting with\"videogame, plumber\"consistently generates Nintendo's Mario character). We also introduce semi-automatic techniques to identify such keywords or descriptions that trigger character generation. Using this framework, we evaluate mitigation strategies, including prompt rewriting and new approaches we propose. Our findings reveal that common methods, such as DALL-E's prompt rewriting, are insufficient alone and require supplementary strategies like negative prompting. Our work provides empirical grounding for discussions on copyright mitigation strategies and offers actionable insights for model deployers implementing these safeguards.",
                    "score": 0.5435109441122503,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6044921875
                }
            ],
            "relevance_judgement": 0.6044921875,
            "relevance_judgment_input_expanded": "# Title: Fantastic Copyrighted Beasts and How (Not) to Generate Them\n# Venue: International Conference on Learning Representations\n# Authors: Luxi He, Yangsibo Huang, Weijia Shi, Tinghao Xie, Haotian Liu, Yue Wang, Luke S. Zettlemoyer, Chiyuan Zhang, Danqi Chen, Peter Henderson\n## Abstract\nRecent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal concerns about copyright infringement. Copyrighted characters (e.g., Mario, Batman) present a significant challenge: at least one lawsuit has already awarded damages based on the generation of such characters. Consequently, commercial services like DALL-E have started deploying interventions. However, little research has systematically examined these problems: (1) Can users easily prompt models to generate copyrighted characters, even if it is unintentional?; (2) How effective are the existing mitigation strategies? To address these questions, we introduce a novel evaluation framework with metrics that assess both the generated image's similarity to copyrighted characters and its consistency with user intent, grounded in a set of popular copyrighted characters from diverse studios and regions. We show that state-of-the-art image and video generation models can still generate characters even if characters' names are not explicitly mentioned, sometimes with only two generic keywords (e.g., prompting with\"videogame, plumber\"consistently generates Nintendo's Mario character). We also introduce semi-automatic techniques to identify such keywords or descriptions that trigger character generation. Using this framework, we evaluate mitigation strategies, including prompt rewriting and new approaches we propose. Our findings reveal that common methods, such as DALL-E's prompt rewriting, are insufficient alone and require supplementary strategies like negative prompting. Our work provides empirical grounding for discussions on copyright mitigation strategies and offers actionable insights for model deployers implementing these safeguards.\n",
            "reference_string": "[270620122 | He et al. | 2024 | Citations: 12]"
        },
        {
            "title": "Detecting Generative Parroting through Overfitting Masked Autoencoders",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 21,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.19050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "17803311",
                    "name": "Saeid Asgari Taghanaki"
                },
                {
                    "authorId": "2293723050",
                    "name": "Joseph Lambourne"
                }
            ],
            "abstract": "The advent of generative AI models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to generative parroting, where models mimic their training data too closely. Our research presents a novel approach to tackle this issue by employing an overfitted Masked Autoencoder (MAE) to detect such parroted samples effectively. We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets. Preliminary evaluations demonstrate promising results, suggesting our method's potential to ensure ethical use and enhance the legal compliance of generative models.",
            "corpus_id": 268732888,
            "sentences": [
                {
                    "corpus_id": "268732888",
                    "title": "Detecting Generative Parroting through Overfitting Masked Autoencoders",
                    "text": "Generative artificial intelligence (AI) models, including but not limited to Stable Diffusion [11], DALLE [10], and Generative Pre-trained Transformers (GPT) [8], represent a groundbreaking shift in the landscape of digital content creation, empowering users to generate text, images, and other forms of media with unprecedented ease and flexibility.These models have been applied across a wide range of domains, from artistic creation and design to content generation for social media and marketing purposes, demonstrating their versatility and potential to enhance creativity and productivity.\n\nThe rapid adoption and deployment of these technologies have also raised significant ethical, legal, and technical challenges, particularly in the context of copyright infringement and data privacy [4,12,17].At the heart of these concerns is the phenomenon known as \"generative parroting,\" where models produce outputs that are not sufficiently distinct from their training data [3,16], leading to the generation of content that closely mimics or even directly copies existing copyrighted materials.This issue not only poses legal risks for users and developers but also undermines trust in generative AI technologies, especially in trust-critical scenarios where the protection of intellectual property and sensitive information is paramount.\n\nThe challenge of detecting and mitigating generative parroting is compounded by the inherent complexities of AI models' training processes and the vastness of the data landscapes they navigate.Traditional approaches to model training and evaluation may not adequately address the nuances of copyright-sensitive scenarios, necessitating innovative solutions that are specifically tailored to recognize and respect the boundaries of copyright law [13].Moreover, the dynamic nature of copyright legislation, which varies across jurisdictions and is continually evolving in response to technological advancements, adds another layer of complexity to this challenge [7].\n\nWhile passing generated samples through a representation learner to obtain feature vectors and compare them with training data might be feasible for small datasets, this approach becomes impractical for larger datasets with billions of samples, especially in real-time scenarios.For instance, designers interacting with generative models need immediate feedback, rendering exhaustive comparisons untenably slow.",
                    "score": 0.5354156466898733,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 350
                        },
                        {
                            "start": 350,
                            "end": 595
                        },
                        {
                            "start": 597,
                            "end": 805
                        },
                        {
                            "start": 805,
                            "end": 1096
                        },
                        {
                            "start": 1096,
                            "end": 1340
                        },
                        {
                            "start": 1342,
                            "end": 1535
                        },
                        {
                            "start": 1535,
                            "end": 1792
                        },
                        {
                            "start": 1792,
                            "end": 2007
                        },
                        {
                            "start": 2009,
                            "end": 2288
                        },
                        {
                            "start": 2288,
                            "end": 2420
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 94,
                            "end": 98,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 106,
                            "end": 110,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 795,
                            "end": 798,
                            "matchedPaperCorpusId": "234777751"
                        },
                        {
                            "start": 801,
                            "end": 804,
                            "matchedPaperCorpusId": "257050406"
                        },
                        {
                            "start": 976,
                            "end": 979,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 979,
                            "end": 982,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 1787,
                            "end": 1791,
                            "matchedPaperCorpusId": "259844568"
                        },
                        {
                            "start": 2003,
                            "end": 2006,
                            "matchedPaperCorpusId": "261375096"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.60107421875
                }
            ],
            "relevance_judgement": 0.60107421875,
            "relevance_judgment_input_expanded": "# Title: Detecting Generative Parroting through Overfitting Masked Autoencoders\n# Venue: arXiv.org\n# Authors: Saeid Asgari Taghanaki, Joseph Lambourne\n## Abstract\nThe advent of generative AI models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to generative parroting, where models mimic their training data too closely. Our research presents a novel approach to tackle this issue by employing an overfitted Masked Autoencoder (MAE) to detect such parroted samples effectively. We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets. Preliminary evaluations demonstrate promising results, suggesting our method's potential to ensure ethical use and enhance the legal compliance of generative models.\n## Introduction\nGenerative artificial intelligence (AI) models, including but not limited to Stable Diffusion [11], DALLE [10], and Generative Pre-trained Transformers (GPT) [8], represent a groundbreaking shift in the landscape of digital content creation, empowering users to generate text, images, and other forms of media with unprecedented ease and flexibility.These models have been applied across a wide range of domains, from artistic creation and design to content generation for social media and marketing purposes, demonstrating their versatility and potential to enhance creativity and productivity.\n\nThe rapid adoption and deployment of these technologies have also raised significant ethical, legal, and technical challenges, particularly in the context of copyright infringement and data privacy [4,12,17].At the heart of these concerns is the phenomenon known as \"generative parroting,\" where models produce outputs that are not sufficiently distinct from their training data [3,16], leading to the generation of content that closely mimics or even directly copies existing copyrighted materials.This issue not only poses legal risks for users and developers but also undermines trust in generative AI technologies, especially in trust-critical scenarios where the protection of intellectual property and sensitive information is paramount.\n\nThe challenge of detecting and mitigating generative parroting is compounded by the inherent complexities of AI models' training processes and the vastness of the data landscapes they navigate.Traditional approaches to model training and evaluation may not adequately address the nuances of copyright-sensitive scenarios, necessitating innovative solutions that are specifically tailored to recognize and respect the boundaries of copyright law [13].Moreover, the dynamic nature of copyright legislation, which varies across jurisdictions and is continually evolving in response to technological advancements, adds another layer of complexity to this challenge [7].\n\nWhile passing generated samples through a representation learner to obtain feature vectors and compare them with training data might be feasible for small datasets, this approach becomes impractical for larger datasets with billions of samples, especially in real-time scenarios.For instance, designers interacting with generative models need immediate feedback, rendering exhaustive comparisons untenably slow.",
            "reference_string": "[268732888 | Taghanaki et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.00475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297800322",
                    "name": "Hiroaki Chiba-Okabe"
                }
            ],
            "abstract": "This paper presents a probabilistic approach to analyzing copyright infringement disputes. Under this approach, evidentiary principles shaped by case law are formalized in probabilistic terms, allowing for a mathematical examination of issues arising in such disputes. The usefulness of this approach is showcased through its application to the ``inverse ratio rule'' -- a controversial legal doctrine adopted by some courts. Although this rule has faced significant criticism, a formal proof demonstrates its validity, provided it is properly defined. Furthermore, the paper employs the probabilistic approach to study the copyright safety of generative AI. Specifically, the Near Access-Free (NAF) condition, previously proposed as a strategy for mitigating the heightened copyright infringement risks of generative AI, is evaluated. The analysis reveals that, while the NAF condition mitigates some infringement risks, its justifiability and efficacy are questionable in certain contexts. These findings illustrate how taking a probabilistic perspective can enhance our understanding of copyright jurisprudence and its interaction with generative AI technology.",
            "corpus_id": 273023255,
            "sentences": [
                {
                    "corpus_id": "273023255",
                    "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety",
                    "text": "In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content [37][38][39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works.",
                    "score": 0.5641297710859023,
                    "section_title": "Risks of Copyright Infringement in Generative AI",
                    "char_start_offset": 27051,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 303
                        },
                        {
                            "start": 304,
                            "end": 456
                        },
                        {
                            "start": 459,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 750
                        },
                        {
                            "start": 751,
                            "end": 969
                        },
                        {
                            "start": 970,
                            "end": 1200
                        },
                        {
                            "start": 1201,
                            "end": 1520
                        },
                        {
                            "start": 1521,
                            "end": 1689
                        },
                        {
                            "start": 1692,
                            "end": 1932
                        },
                        {
                            "start": 1933,
                            "end": 2237
                        },
                        {
                            "start": 2238,
                            "end": 2369
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1380,
                            "end": 1384,
                            "matchedPaperCorpusId": "229156229"
                        },
                        {
                            "start": 1384,
                            "end": 1388,
                            "matchedPaperCorpusId": "256389993"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56689453125
                }
            ],
            "relevance_judgement": 0.56689453125,
            "relevance_judgment_input_expanded": "# Title: Probabilistic Analysis of Copyright Disputes and Generative AI Safety\n# Venue: arXiv.org\n# Authors: Hiroaki Chiba-Okabe\n## Abstract\nThis paper presents a probabilistic approach to analyzing copyright infringement disputes. Under this approach, evidentiary principles shaped by case law are formalized in probabilistic terms, allowing for a mathematical examination of issues arising in such disputes. The usefulness of this approach is showcased through its application to the ``inverse ratio rule'' -- a controversial legal doctrine adopted by some courts. Although this rule has faced significant criticism, a formal proof demonstrates its validity, provided it is properly defined. Furthermore, the paper employs the probabilistic approach to study the copyright safety of generative AI. Specifically, the Near Access-Free (NAF) condition, previously proposed as a strategy for mitigating the heightened copyright infringement risks of generative AI, is evaluated. The analysis reveals that, while the NAF condition mitigates some infringement risks, its justifiability and efficacy are questionable in certain contexts. These findings illustrate how taking a probabilistic perspective can enhance our understanding of copyright jurisprudence and its interaction with generative AI technology.\n## Risks of Copyright Infringement in Generative AI\nIn cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content [37][38][39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works.",
            "reference_string": "[273023255 | Chiba-Okabe | 2024 | Citations: 2]"
        },
        {
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2023,
            "reference_count": 60,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2312.00057",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.00057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2269413961",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2257038423",
                    "name": "Qianli Shen"
                },
                {
                    "authorId": "2244295298",
                    "name": "Kenji Kawaguchi"
                }
            ],
            "abstract": "The booming use of text-to-image generative models has raised concerns about their high risk of producing copyright-infringing content. While probabilistic copyright protection methods provide a probabilistic guarantee against such infringement, in this paper, we introduce Virtually Assured Amplification Attack (VA3), a novel online attack framework that exposes the vulnerabilities of these protection mechanisms. The proposed framework significantly amplifies the probability of generating infringing content on the sustained interactions with generative models and a non-trivial lower-bound on the success probability of each engagement. Our theoretical and experimental results demonstrate the effectiveness of our approach under various scenarios. These findings highlight the potential risk of implementing probabilistic copyright protection in practical applications of text-to-image generative models. Code is available at https://github.com/South7X/VA3.",
            "corpus_id": 265551515,
            "sentences": [
                {
                    "corpus_id": "265551515",
                    "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
                    "text": "In recent years, the advancement of large generative models [17,47,50] has revolutionized high-quality image synthesis [34,37,40], paving the way for commercial applications that enable the public to effortlessly craft their own artworks and designs [2,13,20,27,38,39]. Nevertheless, these models exhibit notable memorization capabilities to produce generations highly similar to the training data [3]. This resemblance raises growing concerns about copyright infringement, especially when copyrighted data is used for training [12,16,49,52]. \n\nTo address these concerns, there has been a surge in research focused on protecting copyrighted data from potential infringement by outputs of generative models [14, 21, Figure 2. Example outputs given the copyright image in Fig. 1 as target (potential infringing images are marked with red boundaries). In (a), using a benign prompt, we observe a high incidence of infringing content from models without copyright protection (\"w/o CP-k\"). In contrast, (b) shows that after applying the copyright protection mechanism (\"w/ CP-k\"), all samples are safe as CP-k rejects all infringing content. In (c), we find that amplification (Amp.) attack with a benign prompt results in limited success. Notably, by amplification attack with an adversarial prompt obtained from our proposed Anti-NAF algorithm, almost all output in (d) are copyright-infringed. 24,43,45,52,58]. Among these studies, a pivotal concept involves establishing a probabilistic upper-bound against the generation of infringing content by generative models. We refer to this suite of approaches as probabilistic copyright protection. Most notably, Vyas et al. [52] introduce a mathematical definition of copyright known as near-access freeness (NAF). Their method enforces generative diffusion models to exhibit akin behaviors as safe models, which has no access to the copyrighted image. By leveraging the improbability of safe models generating infringing content, the probability of generative models doing the same is thereby substantially reduced.",
                    "score": 0.5138884862643556,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 542
                        },
                        {
                            "start": 545,
                            "end": 848
                        },
                        {
                            "start": 849,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1178
                        },
                        {
                            "start": 1179,
                            "end": 1234
                        },
                        {
                            "start": 1235,
                            "end": 1391
                        },
                        {
                            "start": 1392,
                            "end": 1408
                        },
                        {
                            "start": 1409,
                            "end": 1564
                        },
                        {
                            "start": 1565,
                            "end": 1640
                        },
                        {
                            "start": 1641,
                            "end": 1757
                        },
                        {
                            "start": 1758,
                            "end": 1895
                        },
                        {
                            "start": 1896,
                            "end": 2059
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 64,
                            "end": 67,
                            "matchedPaperCorpusId": "14888175"
                        },
                        {
                            "start": 67,
                            "end": 70,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 123,
                            "end": 126,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 126,
                            "end": 129,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 250,
                            "end": 253,
                            "matchedPaperCorpusId": "253581213"
                        },
                        {
                            "start": 253,
                            "end": 256,
                            "matchedPaperCorpusId": "251253049"
                        },
                        {
                            "start": 256,
                            "end": 259,
                            "matchedPaperCorpusId": "252918469"
                        },
                        {
                            "start": 259,
                            "end": 262,
                            "matchedPaperCorpusId": "246240274"
                        },
                        {
                            "start": 262,
                            "end": 265,
                            "matchedPaperCorpusId": "251800180"
                        },
                        {
                            "start": 265,
                            "end": 268,
                            "matchedPaperCorpusId": "243938678"
                        },
                        {
                            "start": 398,
                            "end": 401,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 538,
                            "end": 541,
                            "matchedPaperCorpusId": "257050406"
                        },
                        {
                            "start": 1392,
                            "end": 1395,
                            "matchedPaperCorpusId": "256697414"
                        },
                        {
                            "start": 1395,
                            "end": 1398,
                            "matchedPaperCorpusId": "253420366"
                        },
                        {
                            "start": 1401,
                            "end": 1404,
                            "matchedPaperCorpusId": "257050406"
                        },
                        {
                            "start": 1667,
                            "end": 1671,
                            "matchedPaperCorpusId": "257050406"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5654296875
                }
            ],
            "relevance_judgement": 0.5654296875,
            "relevance_judgment_input_expanded": "# Title: VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Xiang Li, Qianli Shen, Kenji Kawaguchi\n## Abstract\nThe booming use of text-to-image generative models has raised concerns about their high risk of producing copyright-infringing content. While probabilistic copyright protection methods provide a probabilistic guarantee against such infringement, in this paper, we introduce Virtually Assured Amplification Attack (VA3), a novel online attack framework that exposes the vulnerabilities of these protection mechanisms. The proposed framework significantly amplifies the probability of generating infringing content on the sustained interactions with generative models and a non-trivial lower-bound on the success probability of each engagement. Our theoretical and experimental results demonstrate the effectiveness of our approach under various scenarios. These findings highlight the potential risk of implementing probabilistic copyright protection in practical applications of text-to-image generative models. Code is available at https://github.com/South7X/VA3.\n## Introduction\nIn recent years, the advancement of large generative models [17,47,50] has revolutionized high-quality image synthesis [34,37,40], paving the way for commercial applications that enable the public to effortlessly craft their own artworks and designs [2,13,20,27,38,39]. Nevertheless, these models exhibit notable memorization capabilities to produce generations highly similar to the training data [3]. This resemblance raises growing concerns about copyright infringement, especially when copyrighted data is used for training [12,16,49,52]. \n\nTo address these concerns, there has been a surge in research focused on protecting copyrighted data from potential infringement by outputs of generative models [14, 21, Figure 2. Example outputs given the copyright image in Fig. 1 as target (potential infringing images are marked with red boundaries). In (a), using a benign prompt, we observe a high incidence of infringing content from models without copyright protection (\"w/o CP-k\"). In contrast, (b) shows that after applying the copyright protection mechanism (\"w/ CP-k\"), all samples are safe as CP-k rejects all infringing content. In (c), we find that amplification (Amp.) attack with a benign prompt results in limited success. Notably, by amplification attack with an adversarial prompt obtained from our proposed Anti-NAF algorithm, almost all output in (d) are copyright-infringed. 24,43,45,52,58]. Among these studies, a pivotal concept involves establishing a probabilistic upper-bound against the generation of infringing content by generative models. We refer to this suite of approaches as probabilistic copyright protection. Most notably, Vyas et al. [52] introduce a mathematical definition of copyright known as near-access freeness (NAF). Their method enforces generative diffusion models to exhibit akin behaviors as safe models, which has no access to the copyrighted image. By leveraging the improbability of safe models generating infringing content, the probability of generative models doing the same is thereby substantially reduced.",
            "reference_string": "[265551515 | Li et al. | 2023 | Citations: 5]"
        },
        {
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.06737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275053301",
                    "name": "Yiwei Lu"
                },
                {
                    "authorId": "2284800079",
                    "name": "Matthew Y.R. Yang"
                },
                {
                    "authorId": "2295948127",
                    "name": "Zuoqiu Liu"
                },
                {
                    "authorId": "2284763541",
                    "name": "Gautam Kamath"
                },
                {
                    "authorId": "2274963165",
                    "name": "Yaoliang Yu"
                }
            ],
            "abstract": "Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools. In this paper, we provide a better understanding of such disguised copyright infringement by uncovering the disguises generation algorithm, the revelation of the disguises, and importantly, how to detect them to augment the existing toolbox. Additionally, we introduce a broader notion of acknowledgment for comprehending such indirect access. Our code is available at https://github.com/watml/disguised_copyright_infringement.",
            "corpus_id": 269033217,
            "sentences": [
                {
                    "corpus_id": "269033217",
                    "title": "Disguised Copyright Infringement of Latent Diffusion Models",
                    "text": "Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools. In this paper, we provide a better understanding of such disguised copyright infringement by uncovering the disguises generation algorithm, the revelation of the disguises, and importantly, how to detect them to augment the existing toolbox. Additionally, we introduce a broader notion of acknowledgment for comprehending such indirect access. Our code is available at https://github.com/watml/disguised_copyright_infringement.",
                    "score": 0.5948398033573623,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.556640625
                },
                {
                    "corpus_id": "269033217",
                    "title": "Disguised Copyright Infringement of Latent Diffusion Models",
                    "text": "In this paper, we review the current access criterion of containing copyrighted material in the training set (direct access) in copyright infringement of generative models and point out its insufficiency by introducing disguised copyright infringement (indirect access).Specifically, such an infringement is realized by injecting disguised samples into the training set, which urges LDMs to produce copyrighted content.Such disguises are generated with a simple algorithm and demonstrated to share the same concept with their target copyrighted images using the textual inversion tool.To alleviate the concern on the disguises, we expand the current visual auditing (browsing the training set) with additional tools, i.e., feature similarity search and encoderdecoder examination to better identify these disguises.Furthermore, we propose a broader definition of acknowledgment to cover this new type of copyright violation.\n\nLimitations and future work: One interesting future work is to quantify the number of disguises needed for reproducing in large-scale training, which can be further linked to the quantification of memorization of such models (Carlini et al. 2022;Somepalli et al. 2023a;Somepalli et al. 2023b;Carlini et al. 2023;Ippolito et al. 2023;Zhang et al. 2021)).\n\nAdditionally, although our algorithms can generate descent disguises, we believe there is still room for improvement for optimization.Finally, one extension we didn't touch is the possibility of \"chopping\" copyrighted data and hiding it in several images.It is intriguing to explore whether it is possible to generate such a smuggler's dataset and detection towards it.\n\nLearning from noisy data A simultaneous and independent work (Daras et al. 2024) considers learning LDMs from noisy data.Although the techniques are very different, our works share a similar implication: the training dataset may not immediately resemble the generations produced, thus al-lowing copyright issues to be disguised from an auditor who manually inspects the dataset.In the work of Daras et al. (2024), the training data is the original data with Gaussian noise applied to it.In our case, the training data has the original data hidden in the latent space.",
                    "score": 0.47231521563626383,
                    "section_title": "Conclusion",
                    "char_start_offset": 23965,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 270
                        },
                        {
                            "start": 270,
                            "end": 419
                        },
                        {
                            "start": 419,
                            "end": 585
                        },
                        {
                            "start": 585,
                            "end": 815
                        },
                        {
                            "start": 815,
                            "end": 924
                        },
                        {
                            "start": 926,
                            "end": 1279
                        },
                        {
                            "start": 1281,
                            "end": 1415
                        },
                        {
                            "start": 1415,
                            "end": 1536
                        },
                        {
                            "start": 1536,
                            "end": 1650
                        },
                        {
                            "start": 1652,
                            "end": 1773
                        },
                        {
                            "start": 1773,
                            "end": 2030
                        },
                        {
                            "start": 2030,
                            "end": 2139
                        },
                        {
                            "start": 2139,
                            "end": 2219
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1151,
                            "end": 1172,
                            "matchedPaperCorpusId": "246863735"
                        },
                        {
                            "start": 1172,
                            "end": 1195,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 1218,
                            "end": 1238,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 1238,
                            "end": 1259,
                            "matchedPaperCorpusId": "263610040"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.541015625
                },
                {
                    "corpus_id": "269033217",
                    "title": "Disguised Copyright Infringement of Latent Diffusion Models",
                    "text": "Generative models, especially the recent advanced Latent Diffusion Models (LDM) (Rombach et al. 2022), have shown tremendous ability to generate new images, even of creative or artistic form according to text prompts.Such models are trained on a large corpus of data, which may consist of copyrighted material.Additionally, prior works have established that such generative models are prone to regurgitating content from their training data (Ippolito et al. 2023;Zhang et al. 2021;Carlini et al. 2022;Vyas et al.In this paper, we will focus on copyright law within the jurisdiction of the United States.To establish a copyright violation, two factors must be present.First, the accused must have had access to the copyrighted material.Second, the accused must produce content that bears \"substantial similarity\" to the copyrighted material (reproducing).Note that the definition of substantial similarity can be ambiguous.Within the context of images, its definition appears to be relatively broad (Steinberg v. Columbia Pictures Industries, Inc. 1987), and in particular encompasses near-exact copies.\n\nTurning our attention to the former \"access\" criterion: the natural way to establish that a model had access to a particular piece of copyrighted material is to inspect its training data.For example, in the case of Andersen v. Stability AI Ltd. (Dist. Court 2023), the case was allowed to proceed based on the fact that copyrighted images were found in LAION-5B (Schuhmann et al. 2022) (the training data used for Stable Diffusion) using haveibeentrained.com.\n\nWe challenge the perspective that such visual auditing is sufficient to establish access to copyrighted material.Our results show that it is possible to conceal copyrighted images within the training dataset for LDMs.Specifically, LDMs are equipped with a fixed encoder for dimension reduction such that the diffusion learning process occurs in the latent space.This structure can be maliciously exploited to generate disguised copyrighted samples: given a copyrighted image, we show how to generate a disguise such that it is visually different from the copyrighted sample but shares similar latent information.",
                    "score": 0.45283379289724596,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 217
                        },
                        {
                            "start": 217,
                            "end": 310
                        },
                        {
                            "start": 310,
                            "end": 512
                        },
                        {
                            "start": 512,
                            "end": 603
                        },
                        {
                            "start": 603,
                            "end": 667
                        },
                        {
                            "start": 667,
                            "end": 735
                        },
                        {
                            "start": 735,
                            "end": 854
                        },
                        {
                            "start": 854,
                            "end": 922
                        },
                        {
                            "start": 922,
                            "end": 1102
                        },
                        {
                            "start": 1104,
                            "end": 1291
                        },
                        {
                            "start": 1291,
                            "end": 1563
                        },
                        {
                            "start": 1565,
                            "end": 1678
                        },
                        {
                            "start": 1678,
                            "end": 1782
                        },
                        {
                            "start": 1782,
                            "end": 1927
                        },
                        {
                            "start": 1927,
                            "end": 2177
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 80,
                            "end": 101,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 481,
                            "end": 501,
                            "matchedPaperCorpusId": "246863735"
                        },
                        {
                            "start": 501,
                            "end": 512,
                            "matchedPaperCorpusId": "257050406"
                        },
                        {
                            "start": 1466,
                            "end": 1488,
                            "matchedPaperCorpusId": "252917726"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.4541015625
                }
            ],
            "relevance_judgement": 0.556640625,
            "relevance_judgment_input_expanded": "# Title: Disguised Copyright Infringement of Latent Diffusion Models\n# Venue: International Conference on Machine Learning\n# Authors: Yiwei Lu, Matthew Y.R. Yang, Zuoqiu Liu, Gautam Kamath, Yaoliang Yu\n## Abstract\nCopyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools. In this paper, we provide a better understanding of such disguised copyright infringement by uncovering the disguises generation algorithm, the revelation of the disguises, and importantly, how to detect them to augment the existing toolbox. Additionally, we introduce a broader notion of acknowledgment for comprehending such indirect access. Our code is available at https://github.com/watml/disguised_copyright_infringement.\n## Introduction\nGenerative models, especially the recent advanced Latent Diffusion Models (LDM) (Rombach et al. 2022), have shown tremendous ability to generate new images, even of creative or artistic form according to text prompts.Such models are trained on a large corpus of data, which may consist of copyrighted material.Additionally, prior works have established that such generative models are prone to regurgitating content from their training data (Ippolito et al. 2023;Zhang et al. 2021;Carlini et al. 2022;Vyas et al.In this paper, we will focus on copyright law within the jurisdiction of the United States.To establish a copyright violation, two factors must be present.First, the accused must have had access to the copyrighted material.Second, the accused must produce content that bears \"substantial similarity\" to the copyrighted material (reproducing).Note that the definition of substantial similarity can be ambiguous.Within the context of images, its definition appears to be relatively broad (Steinberg v. Columbia Pictures Industries, Inc. 1987), and in particular encompasses near-exact copies.\n\nTurning our attention to the former \"access\" criterion: the natural way to establish that a model had access to a particular piece of copyrighted material is to inspect its training data.For example, in the case of Andersen v. Stability AI Ltd. (Dist. Court 2023), the case was allowed to proceed based on the fact that copyrighted images were found in LAION-5B (Schuhmann et al. 2022) (the training data used for Stable Diffusion) using haveibeentrained.com.\n\nWe challenge the perspective that such visual auditing is sufficient to establish access to copyrighted material.Our results show that it is possible to conceal copyrighted images within the training dataset for LDMs.Specifically, LDMs are equipped with a fixed encoder for dimension reduction such that the diffusion learning process occurs in the latent space.This structure can be maliciously exploited to generate disguised copyrighted samples: given a copyrighted image, we show how to generate a disguise such that it is visually different from the copyrighted sample but shares similar latent information.\n\n## Conclusion\nIn this paper, we review the current access criterion of containing copyrighted material in the training set (direct access) in copyright infringement of generative models and point out its insufficiency by introducing disguised copyright infringement (indirect access).Specifically, such an infringement is realized by injecting disguised samples into the training set, which urges LDMs to produce copyrighted content.Such disguises are generated with a simple algorithm and demonstrated to share the same concept with their target copyrighted images using the textual inversion tool.To alleviate the concern on the disguises, we expand the current visual auditing (browsing the training set) with additional tools, i.e., feature similarity search and encoderdecoder examination to better identify these disguises.Furthermore, we propose a broader definition of acknowledgment to cover this new type of copyright violation.\n\nLimitations and future work: One interesting future work is to quantify the number of disguises needed for reproducing in large-scale training, which can be further linked to the quantification of memorization of such models (Carlini et al. 2022;Somepalli et al. 2023a;Somepalli et al. 2023b;Carlini et al. 2023;Ippolito et al. 2023;Zhang et al. 2021)).\n\nAdditionally, although our algorithms can generate descent disguises, we believe there is still room for improvement for optimization.Finally, one extension we didn't touch is the possibility of \"chopping\" copyrighted data and hiding it in several images.It is intriguing to explore whether it is possible to generate such a smuggler's dataset and detection towards it.\n\nLearning from noisy data A simultaneous and independent work (Daras et al. 2024) considers learning LDMs from noisy data.Although the techniques are very different, our works share a similar implication: the training dataset may not immediately resemble the generations produced, thus al-lowing copyright issues to be disguised from an auditor who manually inspects the dataset.In the work of Daras et al. (2024), the training data is the original data with Gaussian noise applied to it.In our case, the training data has the original data hidden in the latent space.",
            "reference_string": "[269033217 | Lu et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 176,
            "citation_count": 42,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256589810",
                    "name": "Jie Ren"
                },
                {
                    "authorId": "2253881697",
                    "name": "Han Xu"
                },
                {
                    "authorId": "2185740224",
                    "name": "Pengfei He"
                },
                {
                    "authorId": "2218740984",
                    "name": "Yingqian Cui"
                },
                {
                    "authorId": "2253682835",
                    "name": "Shenglai Zeng"
                },
                {
                    "authorId": "2282560420",
                    "name": "Jiankun Zhang"
                },
                {
                    "authorId": "2256788829",
                    "name": "Hongzhi Wen"
                },
                {
                    "authorId": "46496977",
                    "name": "Jiayuan Ding"
                },
                {
                    "authorId": "2253533415",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "2267019992",
                    "name": "Yi Chang"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                }
            ],
            "abstract": "Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.",
            "corpus_id": 267412857,
            "sentences": [
                {
                    "corpus_id": "267412857",
                    "title": "Copyright Protection in Generative AI: A Technical Perspective",
                    "text": "The development of Deep Generative Models (DGMs) marks a noteworthy advancement in image generation. Nevertheless, the impressive quality and authenticity of the generated images, as well as the efficiency in producing new ones, give rise to legitimate concerns regarding copyright matters within the realm of DGMs. \n\nData copyright protection. For the source data owner, which refers to the party or individual who owns the originality of image works, their data can be intentionally or unintentionally collected by model trainers as training samples to construct DGMs as introduced above. For example, recent studies [16,134] have demonstrated that popular DGMs are highly possible to completely replicate their training data samples, which is called memorization. \n\nThe possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works. \n\nModel copyright protection. To obtain DGMs with advanced generation performance, it is always necessary for the model trainers to invest a significant amount of funds and labor. It grants them intellectual property rights over the trained model. However, recent works also identify the possibility to steal others models [128].",
                    "score": 0.4809510785135911,
                    "section_title": "Copyright Issues in Image Generation",
                    "char_start_offset": 12077,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 315
                        },
                        {
                            "start": 318,
                            "end": 344
                        },
                        {
                            "start": 345,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 766
                        },
                        {
                            "start": 769,
                            "end": 868
                        },
                        {
                            "start": 869,
                            "end": 1132
                        },
                        {
                            "start": 1135,
                            "end": 1162
                        },
                        {
                            "start": 1163,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1380
                        },
                        {
                            "start": 1381,
                            "end": 1462
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1456,
                            "end": 1461,
                            "matchedPaperCorpusId": "2984526"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5537109375
                },
                {
                    "corpus_id": "267412857",
                    "title": "Copyright Protection in Generative AI: A Technical Perspective",
                    "text": "This subsection summarizes the strategies for protecting the copyright of DGMs for image generation. This protection is important for two major reasons. First, a powerful DGM, which necessitates extensive computational resources and well-annotated data for its creation, needs to be safeguarded from copyright infringements (such as being stolen by malicious users to offer unauthorized paid services). Second, the unregulated distribution of those models may lead to ethical concerns, including their potential misuse for generating misinformation, which necessitates techniques to identify the origin of an image. \n\nDeep Generative Model Watermarking is a common solution for model copyright protection. It involves incorporating distinct information, known as a watermark, into the models before their deployment. The embedded watermark can be retrieved from a potentially infringing model or its generated data to confirm any suspected copyright violations. \n\nTo achieve good performance in protecting the model while preserving the original generation performance, the watermarking technique should incorporate the following key properties: (a) Fidelity: the ability of the watermarking method to not significantly impact the general performance of the model (the diversity and visual quality of the generated images); (b) Integrity: the accuracy with which the watermark can be extracted; (c) Capacity: the length of message that can be effectively encoded to and extracted from the watermark; (d) Robustness: the ability of the watermark to withstand alterations to the model or perturbation on the watermarked generated images; and (e) Efficiency: the computational cost of the watermark embedding and extraction process. \n\nThe proposed DGM watermarking methods can be summarized into three categories according to the specific way they embed the watermark: \n\n\u2022 Parameter-based watermarking encodes the watermark message into the model's parameters or structural configurations; \u2022 Image-based watermarking embeds the watermark message into every image generated by the model; \n\n\u2022 Triggered-based watermarking secretly incorporates a trigger to the protected model such that an image with copyright information will be generated once the trigger is activated.",
                    "score": 0.4293546657348744,
                    "section_title": "Model Copyright Protection",
                    "char_start_offset": 43411,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 152
                        },
                        {
                            "start": 153,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 615
                        },
                        {
                            "start": 618,
                            "end": 705
                        },
                        {
                            "start": 706,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 961
                        },
                        {
                            "start": 964,
                            "end": 1729
                        },
                        {
                            "start": 1732,
                            "end": 1865
                        },
                        {
                            "start": 1868,
                            "end": 2083
                        },
                        {
                            "start": 2086,
                            "end": 2266
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5205078125
                },
                {
                    "corpus_id": "267412857",
                    "title": "Copyright Protection in Generative AI: A Technical Perspective",
                    "text": "2) Training stage protection Different from directly employing DGMs to generate new images, DGMs are also usually trained or fine-tuned on some source images  to effectively learn useful information from  for future generations. Therefore, \"training-stage protection\" aims to add imperceptible noise to the copyrighted images, to break the training process of potential DGMs for data copyright protection. This type of method is first explored on GAN-based models. For example, GAN-based Deepfake models [81] are representative tools that can be leveraged to swap the faces from the source images to the faces of a target person, which severely abuses the copyrights of both source image holders and target people. Similar tools like EditGAN [75] and Introspective Adversarial Network [10] are developed to edit images that pose more threats to the copyrights of creative and artistic works. To handle this problem, Yang et al. [146] proposed to utilize the idea of adversarial examples [41], to break the balance in the min-max game in GAN-based DeepFake models. Specifically, they focus on Deepfake models which are trained on the target person's face images  to generate other face images belonging to the target person. \n\nTo protect the targeted face  being exploited by Deepfake, they directly adopt the fast gradient sign method (FGSM) method to generate adversarial examples for GAN-based models, \n\nwhere L  refers to the loss of the discriminator in GAN. During the generation process, the discriminator  (\u2022) will have a large loss value on the target samples  target , which consequently breaks the balance of the min-max game during the training of GAN. As a result, the generated images based on the protected target images will have a degraded quality (see Figure . 2). Notably, in Eq.(3), a transformation operator Tr(\u2022) is introduced to improve the robustness of the perturbation under various image transformations, including resizing, affine transformation and image remapping.",
                    "score": 0.43747939128666863,
                    "section_title": "Unrecognizable Examples.",
                    "char_start_offset": 21027,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 228
                        },
                        {
                            "start": 229,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 891
                        },
                        {
                            "start": 892,
                            "end": 1063
                        },
                        {
                            "start": 1064,
                            "end": 1223
                        },
                        {
                            "start": 1226,
                            "end": 1403
                        },
                        {
                            "start": 1406,
                            "end": 1462
                        },
                        {
                            "start": 1463,
                            "end": 1663
                        },
                        {
                            "start": 1664,
                            "end": 1777
                        },
                        {
                            "start": 1778,
                            "end": 1781
                        },
                        {
                            "start": 1782,
                            "end": 1993
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 742,
                            "end": 746,
                            "matchedPaperCorpusId": "243832879"
                        },
                        {
                            "start": 928,
                            "end": 933,
                            "matchedPaperCorpusId": "219687654"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5166015625
                }
            ],
            "relevance_judgement": 0.5537109375,
            "relevance_judgment_input_expanded": "# Title: Copyright Protection in Generative AI: A Technical Perspective\n# Venue: arXiv.org\n# Authors: Jie Ren, Han Xu, Pengfei He, Yingqian Cui, Shenglai Zeng, Jiankun Zhang, Hongzhi Wen, Jiayuan Ding, Hui Liu, Yi Chang, Jiliang Tang\n## Abstract\nGenerative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.\n## Copyright Issues in Image Generation\nThe development of Deep Generative Models (DGMs) marks a noteworthy advancement in image generation. Nevertheless, the impressive quality and authenticity of the generated images, as well as the efficiency in producing new ones, give rise to legitimate concerns regarding copyright matters within the realm of DGMs. \n\nData copyright protection. For the source data owner, which refers to the party or individual who owns the originality of image works, their data can be intentionally or unintentionally collected by model trainers as training samples to construct DGMs as introduced above. For example, recent studies [16,134] have demonstrated that popular DGMs are highly possible to completely replicate their training data samples, which is called memorization. \n\nThe possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works. \n\nModel copyright protection. To obtain DGMs with advanced generation performance, it is always necessary for the model trainers to invest a significant amount of funds and labor. It grants them intellectual property rights over the trained model. However, recent works also identify the possibility to steal others models [128].\n\n## Unrecognizable Examples.\n2) Training stage protection Different from directly employing DGMs to generate new images, DGMs are also usually trained or fine-tuned on some source images  to effectively learn useful information from  for future generations. Therefore, \"training-stage protection\" aims to add imperceptible noise to the copyrighted images, to break the training process of potential DGMs for data copyright protection. This type of method is first explored on GAN-based models. For example, GAN-based Deepfake models [81] are representative tools that can be leveraged to swap the faces from the source images to the faces of a target person, which severely abuses the copyrights of both source image holders and target people. Similar tools like EditGAN [75] and Introspective Adversarial Network [10] are developed to edit images that pose more threats to the copyrights of creative and artistic works. To handle this problem, Yang et al. [146] proposed to utilize the idea of adversarial examples [41], to break the balance in the min-max game in GAN-based DeepFake models. Specifically, they focus on Deepfake models which are trained on the target person's face images  to generate other face images belonging to the target person. \n\nTo protect the targeted face  being exploited by Deepfake, they directly adopt the fast gradient sign method (FGSM) method to generate adversarial examples for GAN-based models, \n\nwhere L  refers to the loss of the discriminator in GAN. During the generation process, the discriminator  (\u2022) will have a large loss value on the target samples  target , which consequently breaks the balance of the min-max game during the training of GAN. As a result, the generated images based on the protected target images will have a degraded quality (see Figure . 2). Notably, in Eq.(3), a transformation operator Tr(\u2022) is introduced to improve the robustness of the perturbation under various image transformations, including resizing, affine transformation and image remapping.\n\n## Model Copyright Protection\nThis subsection summarizes the strategies for protecting the copyright of DGMs for image generation. This protection is important for two major reasons. First, a powerful DGM, which necessitates extensive computational resources and well-annotated data for its creation, needs to be safeguarded from copyright infringements (such as being stolen by malicious users to offer unauthorized paid services). Second, the unregulated distribution of those models may lead to ethical concerns, including their potential misuse for generating misinformation, which necessitates techniques to identify the origin of an image. \n\nDeep Generative Model Watermarking is a common solution for model copyright protection. It involves incorporating distinct information, known as a watermark, into the models before their deployment. The embedded watermark can be retrieved from a potentially infringing model or its generated data to confirm any suspected copyright violations. \n\nTo achieve good performance in protecting the model while preserving the original generation performance, the watermarking technique should incorporate the following key properties: (a) Fidelity: the ability of the watermarking method to not significantly impact the general performance of the model (the diversity and visual quality of the generated images); (b) Integrity: the accuracy with which the watermark can be extracted; (c) Capacity: the length of message that can be effectively encoded to and extracted from the watermark; (d) Robustness: the ability of the watermark to withstand alterations to the model or perturbation on the watermarked generated images; and (e) Efficiency: the computational cost of the watermark embedding and extraction process. \n\nThe proposed DGM watermarking methods can be summarized into three categories according to the specific way they embed the watermark: \n\n\u2022 Parameter-based watermarking encodes the watermark message into the model's parameters or structural configurations; \u2022 Image-based watermarking embeds the watermark message into every image generated by the model; \n\n\u2022 Triggered-based watermarking secretly incorporates a trigger to the protected model such that an image with copyright information will be generated once the trigger is activated.",
            "reference_string": "[267412857 | Ren et al. | 2024 | Citations: 42]"
        },
        {
            "title": "GenPTW: In-Generation Image Watermarking for Provenance Tracing and Tamper Localization",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 62,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.19567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277245198",
                    "name": "Zhenliang Gan"
                },
                {
                    "authorId": "101592248",
                    "name": "Chunya Liu"
                },
                {
                    "authorId": "2358297587",
                    "name": "Yichao Tang"
                },
                {
                    "authorId": "2358271046",
                    "name": "Binghao Wang"
                },
                {
                    "authorId": "2357981993",
                    "name": "Weiqiang Wang"
                },
                {
                    "authorId": "2357976575",
                    "name": "Xinpeng Zhang"
                }
            ],
            "abstract": "The rapid development of generative image models has brought tremendous opportunities to AI-generated content (AIGC) creation, while also introducing critical challenges in ensuring content authenticity and copyright ownership. Existing image watermarking methods, though partially effective, often rely on post-processing or reference images, and struggle to balance fidelity, robustness, and tamper localization. To address these limitations, we propose GenPTW, an In-Generation image watermarking framework for latent diffusion models (LDMs), which integrates Provenance Tracing and Tamper Localization into a unified Watermark-based design. It embeds structured watermark signals during the image generation phase, enabling unified provenance tracing and tamper localization. For extraction, we construct a frequency-coordinated decoder to improve robustness and localization precision in complex editing scenarios. Additionally, a distortion layer that simulates AIGC editing is introduced to enhance robustness. Extensive experiments demonstrate that GenPTW outperforms existing methods in image fidelity, watermark extraction accuracy, and tamper localization performance, offering an efficient and practical solution for trustworthy AIGC image generation.",
            "corpus_id": 278165007,
            "sentences": [
                {
                    "corpus_id": "278165007",
                    "title": "GenPTW: In-Generation Image Watermarking for Provenance Tracing and Tamper Localization",
                    "text": "Generative models are evolving at an unprecedented pace, particularly text-to-image (T2I) diffusion models such as Stable Diffusion, DALL\u2022E 3, and Imagen. These models are capable of synthesizing highly realistic and visually compelling images, while also supporting flexible editing, thereby reshaping the landscape of visual content creation. However, this impressive generative capability is a double-edged sword, introducing a range of security risks including content misuse, ambiguous copyright ownership, and difficulties in tamper detection. In recent years, incidents involving AI-generated images being stolen, maliciously edited, or even forged as fabricated evidence have become increasingly common, threatening both public discourse and the credibility of legal systems. These issues fundamentally highlight two critical challenges: verifying content authenticity and tracing generative responsibility. \n\nImage watermarking is a widely adopted technique for copyright protection and provenance tracing. However, most existing methods focus primarily on authenticity verification and ownership identification, falling short in terms of accurately localizing tampered regions. Tamper localization plays a crucial role in delineating the boundary between generated and modified content, clarifying the responsibility of generative models. Moreover, it enables the assessment of tampering severity and reveals potential malicious intent, making it a key component for achieving comprehensive traceability in AIGC-generated images. \n\nSeveral recent studies have begun to explore the integration of copyright identification and tamper detection. For instance, Sep-Mark [50] introduces a separable watermarking structure to improve robustness against attacks, while EditGuard [59] leverages local vulnerabilities in image steganography to enable tamper region localization. However, these methods follow a post-generation paradigm, where watermarks are embedded after the image has been generated. This leads to a disconnect from the generation process, increased deployment complexity, and reduced overall efficiency. \n\nTherefore, recent studies have shifted towards embedding watermarks directly within the diffusion-based generation process, known as In-Generation watermarking. For example, Stable Signature [14] injects watermarks during generation by fine-tuning the VAE decoder, but requires training a separate model for each watermark, making it unsuitable for large-scale deployment.",
                    "score": 0.47863551622402284,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 344
                        },
                        {
                            "start": 345,
                            "end": 549
                        },
                        {
                            "start": 550,
                            "end": 783
                        },
                        {
                            "start": 784,
                            "end": 915
                        },
                        {
                            "start": 918,
                            "end": 1015
                        },
                        {
                            "start": 1016,
                            "end": 1187
                        },
                        {
                            "start": 1188,
                            "end": 1348
                        },
                        {
                            "start": 1349,
                            "end": 1539
                        },
                        {
                            "start": 1542,
                            "end": 1652
                        },
                        {
                            "start": 1653,
                            "end": 1879
                        },
                        {
                            "start": 1880,
                            "end": 2003
                        },
                        {
                            "start": 2004,
                            "end": 2124
                        },
                        {
                            "start": 2127,
                            "end": 2287
                        },
                        {
                            "start": 2288,
                            "end": 2499
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1676,
                            "end": 1680,
                            "matchedPaperCorpusId": "258588430"
                        },
                        {
                            "start": 1782,
                            "end": 1786,
                            "matchedPaperCorpusId": "266210194"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.54345703125
                }
            ],
            "relevance_judgement": 0.54345703125,
            "relevance_judgment_input_expanded": "# Title: GenPTW: In-Generation Image Watermarking for Provenance Tracing and Tamper Localization\n# Venue: arXiv.org\n# Authors: Zhenliang Gan, Chunya Liu, Yichao Tang, Binghao Wang, Weiqiang Wang, Xinpeng Zhang\n## Abstract\nThe rapid development of generative image models has brought tremendous opportunities to AI-generated content (AIGC) creation, while also introducing critical challenges in ensuring content authenticity and copyright ownership. Existing image watermarking methods, though partially effective, often rely on post-processing or reference images, and struggle to balance fidelity, robustness, and tamper localization. To address these limitations, we propose GenPTW, an In-Generation image watermarking framework for latent diffusion models (LDMs), which integrates Provenance Tracing and Tamper Localization into a unified Watermark-based design. It embeds structured watermark signals during the image generation phase, enabling unified provenance tracing and tamper localization. For extraction, we construct a frequency-coordinated decoder to improve robustness and localization precision in complex editing scenarios. Additionally, a distortion layer that simulates AIGC editing is introduced to enhance robustness. Extensive experiments demonstrate that GenPTW outperforms existing methods in image fidelity, watermark extraction accuracy, and tamper localization performance, offering an efficient and practical solution for trustworthy AIGC image generation.\n## Introduction\nGenerative models are evolving at an unprecedented pace, particularly text-to-image (T2I) diffusion models such as Stable Diffusion, DALL\u2022E 3, and Imagen. These models are capable of synthesizing highly realistic and visually compelling images, while also supporting flexible editing, thereby reshaping the landscape of visual content creation. However, this impressive generative capability is a double-edged sword, introducing a range of security risks including content misuse, ambiguous copyright ownership, and difficulties in tamper detection. In recent years, incidents involving AI-generated images being stolen, maliciously edited, or even forged as fabricated evidence have become increasingly common, threatening both public discourse and the credibility of legal systems. These issues fundamentally highlight two critical challenges: verifying content authenticity and tracing generative responsibility. \n\nImage watermarking is a widely adopted technique for copyright protection and provenance tracing. However, most existing methods focus primarily on authenticity verification and ownership identification, falling short in terms of accurately localizing tampered regions. Tamper localization plays a crucial role in delineating the boundary between generated and modified content, clarifying the responsibility of generative models. Moreover, it enables the assessment of tampering severity and reveals potential malicious intent, making it a key component for achieving comprehensive traceability in AIGC-generated images. \n\nSeveral recent studies have begun to explore the integration of copyright identification and tamper detection. For instance, Sep-Mark [50] introduces a separable watermarking structure to improve robustness against attacks, while EditGuard [59] leverages local vulnerabilities in image steganography to enable tamper region localization. However, these methods follow a post-generation paradigm, where watermarks are embedded after the image has been generated. This leads to a disconnect from the generation process, increased deployment complexity, and reduced overall efficiency. \n\nTherefore, recent studies have shifted towards embedding watermarks directly within the diffusion-based generation process, known as In-Generation watermarking. For example, Stable Signature [14] injects watermarks during generation by fine-tuning the VAE decoder, but requires training a separate model for each watermark, making it unsuitable for large-scale deployment.",
            "reference_string": "[278165007 | Gan et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.20180, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319419519",
                    "name": "Zhuan Shi"
                },
                {
                    "authorId": "2328664079",
                    "name": "Yifei Song"
                },
                {
                    "authorId": "2318236128",
                    "name": "Xiaoli Tang"
                },
                {
                    "authorId": "2287820224",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2054858128",
                    "name": "Boi Faltings"
                }
            ],
            "abstract": "Generative art using Diffusion models has achieved remarkable performance in image generation and text-to-image tasks. However, the increasing demand for training data in generative art raises significant concerns about copyright infringement, as models can produce images highly similar to copyrighted works. Existing solutions attempt to mitigate this by perturbing Diffusion models to reduce the likelihood of generating such images, but this often compromises model performance. Another approach focuses on economically compensating data holders for their contributions, yet it fails to address copyright loss adequately. Our approach begin with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then employ the TRAK method to estimate the contribution of data holders. To accommodate the continuous data collection process, we divide the training into multiple rounds. Finally, We designed a hierarchical budget allocation method based on reinforcement learning to determine the budget for each round and the remuneration of the data holder based on the data holder's contribution and copyright loss in each round. Extensive experiments across three datasets show that our method outperforms all eight benchmarks, demonstrating its effectiveness in optimizing budget distribution in a copyright-aware manner. To the best of our knowledge, this is the first technical work that introduces to incentive contributors and protect their copyrights by compensating them.",
            "corpus_id": 273654195,
            "sentences": [
                {
                    "corpus_id": "273654195",
                    "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
                    "text": "Generative models are playing an increasingly important role in the web ecosystem by revolutionizing the way digital content is created and consumed [38] [6]. These models enable the automatic generation of high-quality images, audio, and video, leading to more dynamic, personalized, and engaging web experiences [33]. With the rise of generative models, generative art, a prominent form of artificial intelligence-generated content (AIGC), has emerged as a cutting-edge research topic [37] [15]. With the advancements in diffusion models, such as DALL\u2022E [26] and Stable Diffusion [24], generative art has demonstrated remarkable progress, particularly in image generation and text-to-image tasks. \n\nThe widespread deployment of such generative art models has brought about significant challenges concerning the risk for copyright infringement. The image generative models require large amount of training data, and it's impractical to only use noncopyrighted content to train the models. The model holder wants to leverage the high quality copyrighted data to learn the style and content and output high quality generated data. However, the generative models may generate some images that are very similar to the copyrighted training data, which may lead to copyright infringement [28]. \n\nVarious methods have been developed to protect copyright in source data, with the most common strategy involving the introduction of perturbations during the training process to effectively safeguard the dataset's copyright: (1) Unrecognizable examples [8,41] hinder models from learning essential features of protected images, either at the inference or training stages. However, this method is highly dependent on the specific images and models involved, and it lacks universal guarantees. (2) Watermarking [5,7] embeds subtle, imperceptible patterns into images to detect copyright violations, though further research is needed to improve its reliability. (3) Machine unlearning [1,9,13,20] removes the influence of copyrighted data, supporting the right to be forgotten. (4) Dataset deduplication [32] helps reduce memorization of training samples, minimizing the risk of copying protected content.",
                    "score": 0.6112814082081833,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 497
                        },
                        {
                            "start": 498,
                            "end": 698
                        },
                        {
                            "start": 701,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 989
                        },
                        {
                            "start": 990,
                            "end": 1129
                        },
                        {
                            "start": 1130,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1662
                        },
                        {
                            "start": 1663,
                            "end": 1782
                        },
                        {
                            "start": 1783,
                            "end": 1949
                        },
                        {
                            "start": 1950,
                            "end": 2065
                        },
                        {
                            "start": 2066,
                            "end": 2193
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 154,
                            "end": 157,
                            "matchedPaperCorpusId": "264172541"
                        },
                        {
                            "start": 492,
                            "end": 496,
                            "matchedPaperCorpusId": "261279983"
                        },
                        {
                            "start": 1800,
                            "end": 1803,
                            "matchedPaperCorpusId": "258297834"
                        },
                        {
                            "start": 1803,
                            "end": 1805,
                            "matchedPaperCorpusId": "264436550"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.54052734375
                },
                {
                    "corpus_id": "273654195",
                    "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
                    "text": "Generative art using Diffusion models has achieved remarkable performance in image generation and text-to-image tasks. However, the increasing demand for training data in generative art raises significant concerns about copyright infringement, as models can produce images highly similar to copyrighted works. Existing solutions attempt to mitigate this by perturbing Diffusion models to reduce the likelihood of generating such images, but this often compromises model performance. Another approach focuses on economically compensating data holders for their contributions, yet it fails to address copyright loss adequately. Our approach begin with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then employ the TRAK method to estimate the contribution of data holders. To accommodate the continuous data collection process, we divide the training into multiple rounds. Finally, We designed a hierarchical budget allocation method based on reinforcement learning to determine the budget for each round and the remuneration of the data holder based on the data holder's contribution and copyright loss in each round. Extensive experiments across three datasets show that our method outperforms all eight benchmarks, demonstrating its effectiveness in optimizing budget distribution in a copyright-aware manner. To the best of our knowledge, this is the first technical work that introduces to incentive contributors and protect their copyrights by compensating them.",
                    "score": 0.553617805034057,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.454345703125
                }
            ],
            "relevance_judgement": 0.54052734375,
            "relevance_judgment_input_expanded": "# Title: Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning\n# Venue: arXiv.org\n# Authors: Zhuan Shi, Yifei Song, Xiaoli Tang, Lingjuan Lyu, Boi Faltings\n## Abstract\nGenerative art using Diffusion models has achieved remarkable performance in image generation and text-to-image tasks. However, the increasing demand for training data in generative art raises significant concerns about copyright infringement, as models can produce images highly similar to copyrighted works. Existing solutions attempt to mitigate this by perturbing Diffusion models to reduce the likelihood of generating such images, but this often compromises model performance. Another approach focuses on economically compensating data holders for their contributions, yet it fails to address copyright loss adequately. Our approach begin with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then employ the TRAK method to estimate the contribution of data holders. To accommodate the continuous data collection process, we divide the training into multiple rounds. Finally, We designed a hierarchical budget allocation method based on reinforcement learning to determine the budget for each round and the remuneration of the data holder based on the data holder's contribution and copyright loss in each round. Extensive experiments across three datasets show that our method outperforms all eight benchmarks, demonstrating its effectiveness in optimizing budget distribution in a copyright-aware manner. To the best of our knowledge, this is the first technical work that introduces to incentive contributors and protect their copyrights by compensating them.\n## Introduction\nGenerative models are playing an increasingly important role in the web ecosystem by revolutionizing the way digital content is created and consumed [38] [6]. These models enable the automatic generation of high-quality images, audio, and video, leading to more dynamic, personalized, and engaging web experiences [33]. With the rise of generative models, generative art, a prominent form of artificial intelligence-generated content (AIGC), has emerged as a cutting-edge research topic [37] [15]. With the advancements in diffusion models, such as DALL\u2022E [26] and Stable Diffusion [24], generative art has demonstrated remarkable progress, particularly in image generation and text-to-image tasks. \n\nThe widespread deployment of such generative art models has brought about significant challenges concerning the risk for copyright infringement. The image generative models require large amount of training data, and it's impractical to only use noncopyrighted content to train the models. The model holder wants to leverage the high quality copyrighted data to learn the style and content and output high quality generated data. However, the generative models may generate some images that are very similar to the copyrighted training data, which may lead to copyright infringement [28]. \n\nVarious methods have been developed to protect copyright in source data, with the most common strategy involving the introduction of perturbations during the training process to effectively safeguard the dataset's copyright: (1) Unrecognizable examples [8,41] hinder models from learning essential features of protected images, either at the inference or training stages. However, this method is highly dependent on the specific images and models involved, and it lacks universal guarantees. (2) Watermarking [5,7] embeds subtle, imperceptible patterns into images to detect copyright violations, though further research is needed to improve its reliability. (3) Machine unlearning [1,9,13,20] removes the influence of copyrighted data, supporting the right to be forgotten. (4) Dataset deduplication [32] helps reduce memorization of training samples, minimizing the risk of copying protected content.",
            "reference_string": "[273654195 | Shi et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
            "venue": "Scientific Reports",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.03341, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297800322",
                    "name": "Hiroaki Chiba-Okabe"
                },
                {
                    "authorId": "2278306561",
                    "name": "Weijie J. Su"
                }
            ],
            "abstract": "The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. Notably, generative AI\u2019s capacity for generating images of copyrighted characters has been well documented in the literature, and while various techniques for mitigating copyright issues have been studied, significant risks remain. Here, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to imitate distinctive features of copyrighted materials. To achieve this, we introduce a metric for quantifying the level of originality of data, estimated by drawing samples from a generative model, and applied in the genericization process. As a practical implementation, we introduce PREGen (Prompt Rewriting-Enhanced Genericization), which combines our genericization method with an existing mitigation technique. Compared to the existing method, PREGen reduces the likelihood of generating copyrighted characters by more than half when the names of copyrighted characters are used as the prompt. Additionally, while generative models can produce copyrighted characters even when their names are not directly mentioned in the prompt, PREGen almost entirely prevents the generation of such characters in these cases. Ultimately, this study advances computational approaches for quantifying and strengthening copyright protection, thereby providing practical methodologies to promote responsible generative AI development.",
            "corpus_id": 270258236,
            "sentences": [
                {
                    "corpus_id": "270258236",
                    "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
                    "text": "Generative models have demonstrated performance rivaling humans in creative tasks such as those involving image synthesis and language processing. 1,2 However, this progress has also raised concerns about copyright protection, leading to numerous lawsuits filed by creators against AI developers. 3 Copyright law protects creators' rights, encouraging new creations while balancing their interests with those of the public. 4 Generative models are trained on massive datasets that often contain copyrighted works and are capable of producing outputs closely resembling their training data, potentially resulting in violation of exclusive rights of copyright owners. 5 ][9][10] Although some methods have been found effective to some extent, significant risks of copyright infringement remains. 6,7 n this paper, we propose a method for quantifying the level of originality and modifying the outputs of generative models to those that have lower originality values. These modified outputs are more generic and less likely to imitate distinctive features of copyrighted materials. As a practical algorithm for mitigating copyright risks, we introduce PREGen (Prompt Rewriting-Enhanced Genericization), which combines this genericization method with the existing prompt rewriting method, 7 adopted by a major commercial model DALL\u2022E. 7,11 The effectiveness of prompt rewriting is enhanced by further adding a negative prompt that instructs the model not to generate specific content, 7 an element that is also incorporated into PREGen. \n\nWe demonstrate that PREGen significantly improves the performance of prompt rewriting accompanied by negative prompting (we refer to this as the standard method when there is no ambiguity) in reducing the likelihood of generating images of copyrighted characters. In particular, experiments using the COPYCAT benchmark 7 show that PREGen reduces the likelihood of text-to-image generative models generating copyrighted characters by more than half compared to the standard method, when the user provides copyrighted characters' names as prompts. Furthermore, when the prompt does not directly reference copyrighted characters, PREGen almost completely eliminates the generation of these characters.",
                    "score": 0.4974993268537015,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 797
                        },
                        {
                            "start": 798,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1335
                        },
                        {
                            "start": 1336,
                            "end": 1532
                        },
                        {
                            "start": 1535,
                            "end": 1798
                        },
                        {
                            "start": 1799,
                            "end": 2080
                        },
                        {
                            "start": 2081,
                            "end": 2233
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 147,
                            "end": 149,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 297,
                            "end": 298,
                            "matchedPaperCorpusId": "259844568"
                        },
                        {
                            "start": 424,
                            "end": 425,
                            "matchedPaperCorpusId": "10463592"
                        },
                        {
                            "start": 666,
                            "end": 667,
                            "matchedPaperCorpusId": "258515384"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51171875
                },
                {
                    "corpus_id": "270258236",
                    "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
                    "text": "We introduced a method to genericize the output of generative models, thereby reducing the risk of copyright infringement. We further proposed PREGen, a practical algorithm for mitigating copyright risks, which combines our genericization method with prompt rewriting. Our method leverages the principle that the level of originality of works determines the strength of their copyright protection, as well as the inherent capability of generative models to learn the distribution of training data. By evaluating the performance of PREGen using the COPYCAT suite, we have shown that PREGen significantly enhances the performance of the standard prompt rewriting method. However, this improvement comes with a trade-off: PREGen requires additional computation to generate multiple samples, most of which are ultimately discarded, along with rewritten prompts. Additionally, the fine-grained consistency with the original prompt may be compromised. \n\nOur work has certain limitations in its scope. While the general framework for originality estimation and genericization is broadly applicable, we have focused on the generation of copyrighted characters using text-to-image generative models. Future research can test our method on the generation of other types of materials and the use of different generative models, such as those for text and video, and investigate appropriate distance metrics and their effectiveness. Another consideration is the potential for the genericization process to amplify undesirable patterns in the generative model's output distribution. Specifically, multiple samples generated during the genericization process might disproportionately represent certain demographics or cultural elements. The resulting generic output, which is, in a sense, the median expression of these patterns, could unintentionally reinforce such biases. These risks should be carefully evaluated in future research.",
                    "score": 0.49819380211864606,
                    "section_title": "Discussion",
                    "char_start_offset": 31498,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 122
                        },
                        {
                            "start": 123,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 497
                        },
                        {
                            "start": 498,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 945
                        },
                        {
                            "start": 948,
                            "end": 994
                        },
                        {
                            "start": 995,
                            "end": 1190
                        },
                        {
                            "start": 1191,
                            "end": 1420
                        },
                        {
                            "start": 1421,
                            "end": 1569
                        },
                        {
                            "start": 1570,
                            "end": 1722
                        },
                        {
                            "start": 1723,
                            "end": 1860
                        },
                        {
                            "start": 1861,
                            "end": 1922
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.448486328125
                }
            ],
            "relevance_judgement": 0.51171875,
            "relevance_judgment_input_expanded": "# Title: Tackling copyright issues in AI image generation through originality estimation and genericization\n# Venue: Scientific Reports\n# Authors: Hiroaki Chiba-Okabe, Weijie J. Su\n## Abstract\nThe rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. Notably, generative AI\u2019s capacity for generating images of copyrighted characters has been well documented in the literature, and while various techniques for mitigating copyright issues have been studied, significant risks remain. Here, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to imitate distinctive features of copyrighted materials. To achieve this, we introduce a metric for quantifying the level of originality of data, estimated by drawing samples from a generative model, and applied in the genericization process. As a practical implementation, we introduce PREGen (Prompt Rewriting-Enhanced Genericization), which combines our genericization method with an existing mitigation technique. Compared to the existing method, PREGen reduces the likelihood of generating copyrighted characters by more than half when the names of copyrighted characters are used as the prompt. Additionally, while generative models can produce copyrighted characters even when their names are not directly mentioned in the prompt, PREGen almost entirely prevents the generation of such characters in these cases. Ultimately, this study advances computational approaches for quantifying and strengthening copyright protection, thereby providing practical methodologies to promote responsible generative AI development.\n## Introduction\nGenerative models have demonstrated performance rivaling humans in creative tasks such as those involving image synthesis and language processing. 1,2 However, this progress has also raised concerns about copyright protection, leading to numerous lawsuits filed by creators against AI developers. 3 Copyright law protects creators' rights, encouraging new creations while balancing their interests with those of the public. 4 Generative models are trained on massive datasets that often contain copyrighted works and are capable of producing outputs closely resembling their training data, potentially resulting in violation of exclusive rights of copyright owners. 5 ][9][10] Although some methods have been found effective to some extent, significant risks of copyright infringement remains. 6,7 n this paper, we propose a method for quantifying the level of originality and modifying the outputs of generative models to those that have lower originality values. These modified outputs are more generic and less likely to imitate distinctive features of copyrighted materials. As a practical algorithm for mitigating copyright risks, we introduce PREGen (Prompt Rewriting-Enhanced Genericization), which combines this genericization method with the existing prompt rewriting method, 7 adopted by a major commercial model DALL\u2022E. 7,11 The effectiveness of prompt rewriting is enhanced by further adding a negative prompt that instructs the model not to generate specific content, 7 an element that is also incorporated into PREGen. \n\nWe demonstrate that PREGen significantly improves the performance of prompt rewriting accompanied by negative prompting (we refer to this as the standard method when there is no ambiguity) in reducing the likelihood of generating images of copyrighted characters. In particular, experiments using the COPYCAT benchmark 7 show that PREGen reduces the likelihood of text-to-image generative models generating copyrighted characters by more than half compared to the standard method, when the user provides copyrighted characters' names as prompts. Furthermore, when the prompt does not directly reference copyrighted characters, PREGen almost completely eliminates the generation of these characters.\n\n## Discussion\nWe introduced a method to genericize the output of generative models, thereby reducing the risk of copyright infringement. We further proposed PREGen, a practical algorithm for mitigating copyright risks, which combines our genericization method with prompt rewriting. Our method leverages the principle that the level of originality of works determines the strength of their copyright protection, as well as the inherent capability of generative models to learn the distribution of training data. By evaluating the performance of PREGen using the COPYCAT suite, we have shown that PREGen significantly enhances the performance of the standard prompt rewriting method. However, this improvement comes with a trade-off: PREGen requires additional computation to generate multiple samples, most of which are ultimately discarded, along with rewritten prompts. Additionally, the fine-grained consistency with the original prompt may be compromised. \n\nOur work has certain limitations in its scope. While the general framework for originality estimation and genericization is broadly applicable, we have focused on the generation of copyrighted characters using text-to-image generative models. Future research can test our method on the generation of other types of materials and the use of different generative models, such as those for text and video, and investigate appropriate distance metrics and their effectiveness. Another consideration is the potential for the genericization process to amplify undesirable patterns in the generative model's output distribution. Specifically, multiple samples generated during the genericization process might disproportionately represent certain demographics or cultural elements. The resulting generic output, which is, in a sense, the median expression of these patterns, could unintentionally reinforce such biases. These risks should be carefully evaluated in future research.",
            "reference_string": "[270258236 | Chiba-Okabe et al. | 2024 | Citations: 1]"
        },
        {
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.16634, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319419519",
                    "name": "Zhuan Shi"
                },
                {
                    "authorId": "2318391138",
                    "name": "Jing Yan"
                },
                {
                    "authorId": "2318236128",
                    "name": "Xiaoli Tang"
                },
                {
                    "authorId": "2287820224",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2054858128",
                    "name": "Boi Faltings"
                }
            ],
            "abstract": "The increasing sophistication of text-to-image generative models has led to complex challenges in defining and enforcing copyright infringement criteria and protection. Existing methods, such as watermarking and dataset deduplication, fail to provide comprehensive solutions due to the lack of standardized metrics and the inherent complexity of addressing copyright infringement in diffusion models. To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. Experiments conducted on 3 mixed datasets of copyright and non-copyright images demonstrate that our approach significantly reduces copyright infringement risk while maintaining image quality.",
            "corpus_id": 272146279,
            "sentences": [
                {
                    "corpus_id": "272146279",
                    "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
                    "text": "In this paper, we presented a Reinforcement Learning-based Copyright Protection (RLCP) for copyright infringement in text-to-image diffusion model. RLCP proposes a copyright loss metric that mirrors legal tests used to assess substantial similarity, and then integrates this metric into a reinforcement learning framework for model fine-tuning, and the use of KL divergence to regularize and stabilize the model training process. Experiments conducted on three mixed datasets of copyright and non-copyright images show that RLCP significantly reduces the likelihood of generating infringing content while preserving the visual quality of the generated images. Our results demonstrate that balancing the proportion of copyrighted and non-copyrighted data in the training set is crucial for minimizing copyright infringement without compromising image quality. We also showed that the reward-driven RL framework effectively fine-tunes diffusion models by optimizing for both copyright compliance and data fidelity. \n\nWhile our approach demonstrates promising results, there are several areas for future work: (1) Broader Application Domains: Future work could extend RLCP to other domains beyond image generation, such as text or audio generation models, where copyright concerns are equally prevalent. (2) Dynamic Dataset Management: Investigating adaptive or dynamic dataset augmentation strategies could be beneficial. As models encounter more copyright-protected data, dynamically adjusting the training process may lead to more robust copyright protection without overfitting to specific datasets.",
                    "score": 0.5026879327747424,
                    "section_title": "Conclusion and Future Work",
                    "char_start_offset": 23145,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 147
                        },
                        {
                            "start": 148,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 858
                        },
                        {
                            "start": 859,
                            "end": 1012
                        },
                        {
                            "start": 1015,
                            "end": 1300
                        },
                        {
                            "start": 1301,
                            "end": 1419
                        },
                        {
                            "start": 1420,
                            "end": 1600
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51025390625
                },
                {
                    "corpus_id": "272146279",
                    "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
                    "text": "The increasing sophistication of text-to-image generative models has led to complex challenges in defining and enforcing copyright infringement criteria and protection. Existing methods, such as watermarking and dataset deduplication, fail to provide comprehensive solutions due to the lack of standardized metrics and the inherent complexity of addressing copyright infringement in diffusion models. To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. Experiments conducted on 3 mixed datasets of copyright and non-copyright images demonstrate that our approach significantly reduces copyright infringement risk while maintaining image quality.",
                    "score": 0.46389217053018206,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.449462890625
                }
            ],
            "relevance_judgement": 0.51025390625,
            "relevance_judgment_input_expanded": "# Title: RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model\n# Venue: arXiv.org\n# Authors: Zhuan Shi, Jing Yan, Xiaoli Tang, Lingjuan Lyu, Boi Faltings\n## Abstract\nThe increasing sophistication of text-to-image generative models has led to complex challenges in defining and enforcing copyright infringement criteria and protection. Existing methods, such as watermarking and dataset deduplication, fail to provide comprehensive solutions due to the lack of standardized metrics and the inherent complexity of addressing copyright infringement in diffusion models. To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. Experiments conducted on 3 mixed datasets of copyright and non-copyright images demonstrate that our approach significantly reduces copyright infringement risk while maintaining image quality.\n## Conclusion and Future Work\nIn this paper, we presented a Reinforcement Learning-based Copyright Protection (RLCP) for copyright infringement in text-to-image diffusion model. RLCP proposes a copyright loss metric that mirrors legal tests used to assess substantial similarity, and then integrates this metric into a reinforcement learning framework for model fine-tuning, and the use of KL divergence to regularize and stabilize the model training process. Experiments conducted on three mixed datasets of copyright and non-copyright images show that RLCP significantly reduces the likelihood of generating infringing content while preserving the visual quality of the generated images. Our results demonstrate that balancing the proportion of copyrighted and non-copyrighted data in the training set is crucial for minimizing copyright infringement without compromising image quality. We also showed that the reward-driven RL framework effectively fine-tunes diffusion models by optimizing for both copyright compliance and data fidelity. \n\nWhile our approach demonstrates promising results, there are several areas for future work: (1) Broader Application Domains: Future work could extend RLCP to other domains beyond image generation, such as text or audio generation models, where copyright concerns are equally prevalent. (2) Dynamic Dataset Management: Investigating adaptive or dynamic dataset augmentation strategies could be beneficial. As models encounter more copyright-protected data, dynamically adjusting the training process may lead to more robust copyright protection without overfitting to specific datasets.",
            "reference_string": "[272146279 | Shi et al. | 2024 | Citations: 1]"
        },
        {
            "title": "On the Challenges and Opportunities in Generative AI",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 391,
            "citation_count": 22,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.00025, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "11180167",
                    "name": "Laura Manduchi"
                },
                {
                    "authorId": "2054151596",
                    "name": "Kushagra Pandey"
                },
                {
                    "authorId": "36347405",
                    "name": "Robert Bamler"
                },
                {
                    "authorId": "2326993383",
                    "name": "Ryan Cotterell"
                },
                {
                    "authorId": "2289611494",
                    "name": "Sina Daubener"
                },
                {
                    "authorId": "2161485495",
                    "name": "Sophie Fellenz"
                },
                {
                    "authorId": "2280912684",
                    "name": "Asja Fischer"
                },
                {
                    "authorId": "2289611496",
                    "name": "Thomas Gartner"
                },
                {
                    "authorId": "7965286",
                    "name": "Matthias Kirchler"
                },
                {
                    "authorId": "2749512",
                    "name": "M. Kloft"
                },
                {
                    "authorId": "2258745503",
                    "name": "Yingzhen Li"
                },
                {
                    "authorId": "2261493650",
                    "name": "Christoph Lippert"
                },
                {
                    "authorId": "2289612000",
                    "name": "Gerard de Melo"
                },
                {
                    "authorId": "2268322296",
                    "name": "Eric T. Nalisnick"
                },
                {
                    "authorId": "2289612053",
                    "name": "Bjorn Ommer"
                },
                {
                    "authorId": "2289611796",
                    "name": "Rajesh Ranganath"
                },
                {
                    "authorId": "2258717579",
                    "name": "Maja Rudolph"
                },
                {
                    "authorId": "2260806834",
                    "name": "Karen Ullrich"
                },
                {
                    "authorId": "1749506",
                    "name": "Guy Van den Broeck"
                },
                {
                    "authorId": "2260809546",
                    "name": "Julia E Vogt"
                },
                {
                    "authorId": "2289783943",
                    "name": "Yixin Wang"
                },
                {
                    "authorId": "39798982",
                    "name": "F. Wenzel"
                },
                {
                    "authorId": "2289610270",
                    "name": "Frank Wood"
                },
                {
                    "authorId": "2258707737",
                    "name": "Stephan Mandt"
                },
                {
                    "authorId": "41031794",
                    "name": "Vincent Fortuin"
                }
            ],
            "abstract": "The field of deep generative modeling has grown rapidly in the last few years. With the availability of massive amounts of training data coupled with advances in scalable unsupervised learning paradigms, recent large-scale generative models show tremendous promise in synthesizing high-resolution images and text, as well as structured data such as videos and molecules. However, we argue that current large-scale generative AI models exhibit several fundamental shortcomings that hinder their widespread adoption across domains. In this work, our objective is to identify these issues and highlight key unresolved challenges in modern generative AI paradigms that should be addressed to further enhance their capabilities, versatility, and reliability. By identifying these challenges, we aim to provide researchers with insights for exploring fruitful research directions, thus fostering the development of more robust and accessible generative AI solutions.",
            "corpus_id": 268201483,
            "sentences": [
                {
                    "corpus_id": "268201483",
                    "title": "On the Challenges and Opportunities in Generative AI",
                    "text": "Interestingly, recent works show that publicly available LLMs and large-scale text-to-image models can implicitly \"memorize\" training data, in the sense that points from the dataset can be (almost exactly) reconstructed, which potentially infringes on data privacy [23,22,188,136]. This connects to the open question of whether it is possible to train generative models while preserving data privacy. To alleviate this problem, approaches like training generative models with differential privacy (DP) constraints offer an attractive theoretical framework to ensure privacy [111,41]. However, DPbased approaches suffer from a tradeoff between privacy and utility. Moreover, in the context of image generation, scaling such approaches to high-resolution datasets remains elusive. Therefore, building privacy constraints in large-scale training of generative models can be a promising direction for further research. \n\nIn addition to privacy, recent advances in large-scale generative modeling can lead to unauthorized distribution or replication of training data resulting in copyright infringement liabilities 6 . While some recent work tries to alleviate this issue [113,248], there are several outstanding technical challenges ranging from mitigating copyright infringements during dataset curation [24] to reliable detection of copyright violations in generated samples.",
                    "score": 0.4354046103179531,
                    "section_title": "Privacy and Copyright Infringement",
                    "char_start_offset": 30601,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 400
                        },
                        {
                            "start": 401,
                            "end": 583
                        },
                        {
                            "start": 584,
                            "end": 663
                        },
                        {
                            "start": 664,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 914
                        },
                        {
                            "start": 917,
                            "end": 1113
                        },
                        {
                            "start": 1114,
                            "end": 1373
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 265,
                            "end": 269,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 269,
                            "end": 272,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 272,
                            "end": 276,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 574,
                            "end": 579,
                            "matchedPaperCorpusId": "238634219"
                        },
                        {
                            "start": 579,
                            "end": 582,
                            "matchedPaperCorpusId": "252968205"
                        },
                        {
                            "start": 1167,
                            "end": 1172,
                            "matchedPaperCorpusId": "256697414"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.50341796875
                }
            ],
            "relevance_judgement": 0.50341796875,
            "relevance_judgment_input_expanded": "# Title: On the Challenges and Opportunities in Generative AI\n# Venue: arXiv.org\n# Authors: Laura Manduchi, Kushagra Pandey, Robert Bamler, Ryan Cotterell, Sina Daubener, Sophie Fellenz, Asja Fischer, Thomas Gartner, Matthias Kirchler, M. Kloft, Yingzhen Li, Christoph Lippert, Gerard de Melo, Eric T. Nalisnick, Bjorn Ommer, Rajesh Ranganath, Maja Rudolph, Karen Ullrich, Guy Van den Broeck, Julia E Vogt, Yixin Wang, F. Wenzel, Frank Wood, Stephan Mandt, Vincent Fortuin\n## Abstract\nThe field of deep generative modeling has grown rapidly in the last few years. With the availability of massive amounts of training data coupled with advances in scalable unsupervised learning paradigms, recent large-scale generative models show tremendous promise in synthesizing high-resolution images and text, as well as structured data such as videos and molecules. However, we argue that current large-scale generative AI models exhibit several fundamental shortcomings that hinder their widespread adoption across domains. In this work, our objective is to identify these issues and highlight key unresolved challenges in modern generative AI paradigms that should be addressed to further enhance their capabilities, versatility, and reliability. By identifying these challenges, we aim to provide researchers with insights for exploring fruitful research directions, thus fostering the development of more robust and accessible generative AI solutions.\n## Privacy and Copyright Infringement\nInterestingly, recent works show that publicly available LLMs and large-scale text-to-image models can implicitly \"memorize\" training data, in the sense that points from the dataset can be (almost exactly) reconstructed, which potentially infringes on data privacy [23,22,188,136]. This connects to the open question of whether it is possible to train generative models while preserving data privacy. To alleviate this problem, approaches like training generative models with differential privacy (DP) constraints offer an attractive theoretical framework to ensure privacy [111,41]. However, DPbased approaches suffer from a tradeoff between privacy and utility. Moreover, in the context of image generation, scaling such approaches to high-resolution datasets remains elusive. Therefore, building privacy constraints in large-scale training of generative models can be a promising direction for further research. \n\nIn addition to privacy, recent advances in large-scale generative modeling can lead to unauthorized distribution or replication of training data resulting in copyright infringement liabilities 6 . While some recent work tries to alleviate this issue [113,248], there are several outstanding technical challenges ranging from mitigating copyright infringements during dataset curation [24] to reliable detection of copyright violations in generated samples.",
            "reference_string": "[268201483 | Manduchi et al. | 2024 | Citations: 22]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "257050406",
            "title": "On Provable Copyright Protection for Generative Models",
            "text": "Generative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models. \n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape Figure 1: The CP-k Algorithm applied to diffusion models. The dataset is CIFAR-10 augmented with multiple copies of two images (images close to the augmented images are marked with red boundaries); hypothetically, suppose these two images are copyrighted works. The leftmost image shows generations from a model p that was trained on the full dataset, where we clearly see that p generates the two copyrighted works. Our algorithm starts by splitting this dataset into two disjoint datasets, making sure that copyrighted images are split into two different shards; for illustrative purposes, we do not deduplicate the dataset. The procedure then trains two models q1, q2 on these disjoint shards. The middle two figures show samples from the models q1, q2, again clearly showing memorization. However, note that q1 does not generate one of the copyrighted images and and q2 does not generate the other copyrighted image (as these were not in their respective datasets). Our algorithm CP-k then uses q1, q2, along with the original model p, to construct a model p k which has strong copyright protection guarantees.",
            "score": 0.631776842829519,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 638
                },
                {
                    "start": 641,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2176
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.666015625
        },
        {
            "corpus_id": "277955485",
            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
            "text": "In today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images. Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use. As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights. Some AI models have even been noted to directly copy copyrighted images, a problem often referred to as source copying. Traditional copyright protection measures such as watermarks and metadata have also proven to be futile in this regard. To address this issue, we propose a novel two-step image generation model inspired by the conditional diffusion model. The first step involves creating an image segmentation mask for some prompt-based generated images. This mask embodies the shape of the image. Thereafter, the diffusion model is asked to generate the image anew while avoiding the shape in question. This approach shows a decrease in structural similarity from the training image, i.e. we are able to avoid the source copying problem using this approach without expensive retraining of the model or user-centered prompt generation techniques. This makes our approach the most computationally inexpensive approach to avoiding both copyright infringement and source copying for diffusion model-based image generation.",
            "score": 0.6249440813567572,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.480224609375
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "Others have demonstrated that these models can potentially reconstruct or replicate copyrighted content from their training data (Carlini et al., 2020;car, 2023). Efforts to mitigate these risks include provable copyright protection strategies inspired by differential privacy (Vyas et al., 2023), decoding-time prevention (Golatkar et al., 2024) that guide the generation process away from copyright concepts and model editing and unlearning that aim to remove copyrighted content from model weights (Gong et al., 2024;Chefer et al., 2023;Zhang et al., 2023). Ma et al. (2024) introduces benchmark for measuring copyright infringement unlearning from text-to-image diffusion models. Another recent study by Kim et al. (2024) also examines keywords potentially important for image generation, but only includes the character name along with the associated movie or TV program as keywords. They also show that LLM-optimized descriptions can generate images similar to copyrighted characters on proprietary models such as ChatGPT, Copilot, and Gemini. However, their optimized prompts do not explicitly exclude the characters' names. Similarly, Zhang et al. (2024a) focuses on building attacks that can generated particular concepts,including some copyrighted characters. While these works focus on attacks and do not explore effective mitigation methods, our work focuses on building an analysis framework motivated by legal considerations like substantial similarity test for copyrighted characters and provide more in-depth understanding for both how easy it is to generate copyrighted characters as well as the effectiveness of defenses.",
            "score": 0.6140037616284388,
            "section_title": "RELATED WORK",
            "char_start_offset": 27810,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1639
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 151,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 277,
                    "end": 296,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 323,
                    "end": 346,
                    "matchedPaperCorpusId": "268732879"
                },
                {
                    "start": 520,
                    "end": 540,
                    "matchedPaperCorpusId": "256416326"
                },
                {
                    "start": 540,
                    "end": 559,
                    "matchedPaperCorpusId": "257833863"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2276611328125
        },
        {
            "corpus_id": "273654195",
            "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
            "text": "Generative models are playing an increasingly important role in the web ecosystem by revolutionizing the way digital content is created and consumed [38] [6]. These models enable the automatic generation of high-quality images, audio, and video, leading to more dynamic, personalized, and engaging web experiences [33]. With the rise of generative models, generative art, a prominent form of artificial intelligence-generated content (AIGC), has emerged as a cutting-edge research topic [37] [15]. With the advancements in diffusion models, such as DALL\u2022E [26] and Stable Diffusion [24], generative art has demonstrated remarkable progress, particularly in image generation and text-to-image tasks. \n\nThe widespread deployment of such generative art models has brought about significant challenges concerning the risk for copyright infringement. The image generative models require large amount of training data, and it's impractical to only use noncopyrighted content to train the models. The model holder wants to leverage the high quality copyrighted data to learn the style and content and output high quality generated data. However, the generative models may generate some images that are very similar to the copyrighted training data, which may lead to copyright infringement [28]. \n\nVarious methods have been developed to protect copyright in source data, with the most common strategy involving the introduction of perturbations during the training process to effectively safeguard the dataset's copyright: (1) Unrecognizable examples [8,41] hinder models from learning essential features of protected images, either at the inference or training stages. However, this method is highly dependent on the specific images and models involved, and it lacks universal guarantees. (2) Watermarking [5,7] embeds subtle, imperceptible patterns into images to detect copyright violations, though further research is needed to improve its reliability. (3) Machine unlearning [1,9,13,20] removes the influence of copyrighted data, supporting the right to be forgotten. (4) Dataset deduplication [32] helps reduce memorization of training samples, minimizing the risk of copying protected content.",
            "score": 0.6112814082081833,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "264172541"
                },
                {
                    "start": 492,
                    "end": 496,
                    "matchedPaperCorpusId": "261279983"
                },
                {
                    "start": 1800,
                    "end": 1803,
                    "matchedPaperCorpusId": "258297834"
                },
                {
                    "start": 1803,
                    "end": 1805,
                    "matchedPaperCorpusId": "264436550"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54052734375
        },
        {
            "corpus_id": "268049158",
            "title": "Break It 'Til You Make It: An Exploration of the Ramifications of Copyright Liability Under a Pre-training Paradigm of AI Development",
            "text": "Recent progress in the development of generative AI systems has brought questions of copyright liability to the attention of regulators, scholars, artists, technologists, and the general public alike.This technology facilitates the creation of new works at the same time as it threatens the livelihood of artists whose works are used in training.\n\nThe legal literature has focused primarily on whether a model's use of copyrighted works as training data directly infringes upon a copyright holder's rights.While questions around direct liability from model training on copyrighted works are far from settled, a common way an answer has been attempted in the legal literature relies on the conceptualization of a close connection between the purpose of model training and deployment application.I argue that this reasoning and conceptualization warrants a closer look.In particular, it is out of step with the pre-training process that has emerged as the dominant paradigm for model development [Bommasani et al. 2021;Devlin et al. 2018;Ramesh et al. 2022].Large models are usually trained with generic tasks (e.g., next-word prediction) on broad datasets and fine-tuned (i.e., retrained) on narrower and, typically, smaller task-specific datasets.Furthermore, pre-training is expensive and creates a split in the market for these systems between a small group of high-resource corporate actors [Microsoft 2023;OpenAI 2019] that produce pretrained models and a large group of smaller downstream actors that fine-tune pre-trained models [Bender et al. 2021].This means that a single training process is often subject to a wide variety of uses by multiple actors.\n\nProposed tests for direct liability adopt a conceptualization of the training process that may not align with the technical reality of the training process or the market structures surrounding AI development.With the introduction of multiple actors during the AI development and deployment process, I consider the implications of indirect liability.Indirect copyright liability is a judicially developed provision of copyright law that potentially extends infringement exposure to developers of devices or services that contribute to, induce, or vicariously benefit from downstream infringing activities [Menell et al. 2023].Generative AI can facilitate output or model infringements by downstream developers [Lu et al. 2019].",
            "score": 0.6104944136565927,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 200,
                    "end": 346
                },
                {
                    "start": 348,
                    "end": 506
                },
                {
                    "start": 506,
                    "end": 794
                },
                {
                    "start": 794,
                    "end": 867
                },
                {
                    "start": 867,
                    "end": 1056
                },
                {
                    "start": 1056,
                    "end": 1247
                },
                {
                    "start": 1247,
                    "end": 1556
                },
                {
                    "start": 1556,
                    "end": 1660
                },
                {
                    "start": 1662,
                    "end": 1870
                },
                {
                    "start": 1870,
                    "end": 2011
                },
                {
                    "start": 2011,
                    "end": 2287
                },
                {
                    "start": 2287,
                    "end": 2388
                }
            ],
            "ref_mentions": [
                {
                    "start": 1535,
                    "end": 1555,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.348388671875
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "Besides, the target image that the attack want the target to generated need to be included during the model training. However, in the copyright infringement attack scenario, the target image, which is protected by copyright, cannot be utilized during the training phase, and the training process itself is not under the attacker's control.",
            "score": 0.6061237163044547,
            "section_title": "A.1. Detailed Comparisons with Other Diffusion Model Attack Methods",
            "char_start_offset": 38682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 339
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1044921875
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "The rapid advancement of generative artificial intelligence (GenAI) has ushered in a new era of content creation, enabling the synthesis of highquality text, images, and multimedia content at an unprecedented scale. While these innovations have expanded creative possibilities and applications across industries, they have also raised significant ethical and legal concerns, particularly regarding intellectual property (IP) rights (Sag, 2023;Poland). One of the most pressing issues is the unauthorized reproduction of copyrighted material, where generative models may inadvertently produce outputs that closely resemble or replicate IP-protected content (Zirpoli, 2023;Dzuong et al., 2024;Sag, 2023;Poland;Wang et al., 2024). This issue has led to widespread debates among legal experts, policymakers, and AI researchers on the potential liabilities and regulatory measures required to address copyright infringement in AI-generated content. \n\nExisting efforts to mitigate copyright concerns in generative models have primarily focused on two key approaches: \u2460 reducing memorization during training using techniques such as differential privacy (Dwork et al., 2014), which limits the retention of specific data points to prevent models from reproducing protected content (Abadi et al., 2016;Chen et al., 2022;Dockhorn et al., 2022), and \u2461 employing prompt engineering strategies that discourage the generation of IP-infringing material through explicit negative prompts (Wang et al., 2024;He et al., 2024) or optimized safe prompt modifications (Chin et al., 2023;Rando et al., 2022). While these approaches offer some level of control over generative outputs, they do not directly address the challenge of detecting copyright infringement in already-generated content. As a result, there is an urgent need for robust evaluation methods and benchmarks to assess the ability of AI models-specifically large vision-language models (LVLMs)-to identify potential instances of copyright violations. \n\nVision-language models (VLMs), which integrate both textual and visual data to enable crossmodal reasoning, have demonstrated remarkable capabilities in tasks such as image classification, visual question answering (VQA) (Antol et al., 2015), and multimodal understanding.",
            "score": 0.5989954638611665,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1995
                },
                {
                    "start": 1998,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 432,
                    "end": 443,
                    "matchedPaperCorpusId": "258515384"
                },
                {
                    "start": 691,
                    "end": 701,
                    "matchedPaperCorpusId": "258515384"
                },
                {
                    "start": 1147,
                    "end": 1167,
                    "matchedPaperCorpusId": "207178262"
                },
                {
                    "start": 1273,
                    "end": 1293,
                    "matchedPaperCorpusId": "207241585"
                },
                {
                    "start": 1293,
                    "end": 1311,
                    "matchedPaperCorpusId": "250210875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44970703125
        },
        {
            "corpus_id": "265351912",
            "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
            "text": "Although these work have studied various watermarking methods to achieve model traceability, these methods can only trace back to a chosen specific model in their experiments, without considering the interplay among models in the complex AI image generation task. \n\nTo address the challenge of potential copyright infringement in AI-generated images, we have proposed a new framework called CopyScope that could identify different copyright infringement sources at the model level in the AI image generation process and evaluate their impact. We have proposed a FID-based Shapley algorithm to assess the infringement contribution of each model in the diffusion workflow. Extensive results have demonstrated that our proposed CopyScope framework could effectively zoom in on the sources and quantify the impact of infringement models in AI image generation. Our work offers a promising solution for copyright traceability in AI image generation, which could also promote the legally compliant use of AI-generated content.",
            "score": 0.596907530281717,
            "section_title": "RELATED WORK",
            "char_start_offset": 25505,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 266,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1020
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66748046875
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools. In this paper, we provide a better understanding of such disguised copyright infringement by uncovering the disguises generation algorithm, the revelation of the disguises, and importantly, how to detect them to augment the existing toolbox. Additionally, we introduce a broader notion of acknowledgment for comprehending such indirect access. Our code is available at https://github.com/watml/disguised_copyright_infringement.",
            "score": 0.5948398033573623,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.556640625
        },
        {
            "corpus_id": "257050406",
            "title": "On Provable Copyright Protection for Generative Models",
            "text": "Our algorithm CP-k then uses q1, q2, along with the original model p, to construct a model p k which has strong copyright protection guarantees. The last image is the outputs of p k , showing it is highly unlikely to output either of the copyrighted images, even though each of q1, q2 and p has memorized some of these images. See Section 4 for more details (and for a discussion with regards to our displayed model generations having used the same noise on the diffusion paths). \n\nsignificant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021]. \n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material. It is this issue of preventing deployment-time copyright infringement that is the focus of this work. \n\nOur contributions. We give a formal definition -\"near-access freeness\" -bounding the extent to which a learned generative model's output can be substantially influenced by a particular piece of copyrighted data that the model was trained on. We also give a procedure that transforms (under certain assumptions) any generative model learning algorithm A into an algorithm A k , which protects against violations under our definition. In particular, the model output by A k will (1) be at most k-bits far from a \"safe\" model (which is not committing copyright infringement), and (2) will have performance reasonably close to the model output by the original algorithm A (in a quantifiable sense, based on properties of A).",
            "score": 0.5764308191209843,
            "section_title": "Introduction",
            "char_start_offset": 2047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2261
                }
            ],
            "ref_mentions": [
                {
                    "start": 819,
                    "end": 836,
                    "matchedPaperCorpusId": "239770285"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60546875
        },
        {
            "corpus_id": "277151077",
            "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation",
            "text": "Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog",
            "score": 0.5733380713592922,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77001953125
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "State-of-the-art image and video generation models demonstrate a remarkable ability for generating high-quality visual content based on free-form user inputs (Rombach et al., 2022;Betker et al., 2023;Chen et al., 2024;Li et al., 2024a;Blattmann et al., 2023;Esser et al., 2024). However, recent research has shown that generative models, including those for image and video, are susceptible to memorizing and generating entire datapoints or concepts from their training data (Somepalli et al., 2023;Carlini et al., 2023;car, 2023). Since some training data originates from copyrighted materials (car, 2023;Kumari et al., 2023), regurgitation of such content may lead to legal intellectual property liability for users and model deployers who further make use of the generated content. In particular, this liability may stem not only from verbatim generation of training data points, but generation of some copyrightable repeating motifs highly similar to those from the training data. \n\nAs a result, copyright concerns in image generative models has been extensively discussed in both academic research (car, 2023;Ma et al., 2024;Kim et al., 2024) and litigation (Vincent, 2023;Andersen et al. v. Stability AI et al., N.D. Cal. 2023). Among the diverse copyrighted content that  (Li et al., 2024a) and proprietary DALL\u2022E 3 model. For Mario (a) and Batman (b), both models can generate these characters through indirect anchoring, using relevant descriptive keywords instead of character names. DALL\u2022E 3 blocks explicit name prompts requests (character name anchoring) with content policy messages.",
            "score": 0.5724650067065346,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1597
                }
            ],
            "ref_mentions": [
                {
                    "start": 475,
                    "end": 499,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 606,
                    "end": 626,
                    "matchedPaperCorpusId": "257687839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35009765625
        },
        {
            "corpus_id": "267400526",
            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
            "text": "With the rapid development of artificial intelligence technology, content generated by artificial intelligence has been rapidly applied to people's lives. At the same time, it is also accompanied by many infringement lawsuits, whether AIGC has really caused different degrees of infringement to human artists. Through the analysis of the existing literature on copyright issues and the walkthrough of Stable Diffusion, an AI-generated image platform, this article digs into the main factors that the AI-generated platform causes infringements on human artists. Provide references for using AI by enterprises and related media, and let more scholars pay attention to this issue. The study found that in the workflow of the AI generation platform, taking Stable Diffusion as an example, the two processes of model training and image generation may cause copyright infringement to a certain extent. Based on this, the AI generation platform has unauthorized use of copyright works, excessive plagiarism and adaptation of copyright works, and the generated images are not marked with watermarks or sources, which damages the copyright owner's rights.",
            "score": 0.5712391637137568,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3037109375
        },
        {
            "corpus_id": "265351912",
            "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
            "text": "Web-based AI image generation has become an innovative art form that can generate novel artworks with the rapid development of the diffusion model. However, this new technique brings potential copyright infringement risks as it may incorporate the existing artworks without the owners' consent. Copyright infringement quantification is the primary and challenging step towards AI-generated image copyright traceability. Previous work only focused on data attribution from the training data perspective, which is unsuitable for tracing and quantifying copyright infringement in practice because of the following reasons: (1) the training datasets are not always available in public; (2) the model provider is the responsible party, not the image. Motivated by this, in this paper, we propose CopyScope, a new framework to quantify the infringement of AI-generated images from the model level. We first rigorously identify pivotal components within the AI image generation pipeline. Then, we propose to take advantage of Fr\\'echet Inception Distance (FID) to effectively capture the image similarity that fits human perception naturally. We further propose the FID-based Shapley algorithm to evaluate the infringement contribution among models. Extensive experiments demonstrate that our work not only reveals the intricacies of infringement quantification but also effectively depicts the infringing models quantitatively, thus promoting accountability in AI image-generation tasks.",
            "score": 0.5676807245304267,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68798828125
        },
        {
            "corpus_id": "273023255",
            "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety",
            "text": "In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content [37][38][39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works.",
            "score": 0.5641297710859023,
            "section_title": "Risks of Copyright Infringement in Generative AI",
            "char_start_offset": 27051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2369
                }
            ],
            "ref_mentions": [
                {
                    "start": 1380,
                    "end": 1384,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1384,
                    "end": 1388,
                    "matchedPaperCorpusId": "256389993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "275471553",
            "title": "Has an AI model been trained on your images?",
            "text": "Understanding why models are biased to produce content similar to their training data may provide insights into reducing the likelihood of direct copyright infringement in the form of reproducing training data, and may provide insights into how a model can be made to forget training exemplars. \n\nAn attractive aspect of our membership inference for generative image models is that it does not require direct access to the details of the model architecture or trained weights, is computationally efficient, and generalizes to multiple different AI models. A drawback of our approach is that it only applies to models that allow for an image-to-image synthesis with a controllable strength parameter, as compared to text-to-image. Depending on the underlying mechanism by which models produce images similar to their in-training data, our method may be adaptable to text-to-image generation. \n\nMany of today's tech leaders have admitted that their generative-AI models would not exist without their training on billions of pieces of content scraped from all corners of the internet [16]. These same leaders have also called for the loosening of fairuse and copyright rules. While it is for the courts to decide on these matters of law [22], we contend that content creators have legitimate concerns for whether and how their content is used to train generative-AI models, some of which are designed to offer services directly competing with these very content creators. \n\nA critical component of adjudicating these issues will be determining if a deployed model was trained on a specific piece of content. Equally important is determining how creators can and should be compensated when their content is used for training, and how models can be made to forget its training on a specific piece of content should this be the wish of the content's creator. \n\nWe have focused only on the first of these questions, but all of these issues are important to resolve as generative AI continues its impressive and impactful trajectory.",
            "score": 0.5622529953178175,
            "section_title": "Discussion",
            "char_start_offset": 19446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 2025
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "258479926",
            "title": "Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era",
            "text": "In addition, Getty images has filed a lawsuit against Stability AI, accusing Stable Diffusion of \"brazen infringement of Getty Images' intellectual property on a staggering scale\" and misusing more than 12 million Getty photos to train its Stable Diffusion AI image-generation system 23 . Faced with the issue of copyright infringement, AI image generator companies have emphasized that they will comply with the Digital Millennium Copyright Act (DMCA) and protect the copyright of image owners. \n\nMeanwhile, the U.S. Copyright Office has taken the position that AI-generated images do not qualify for copyright protection, as they are not the result of human authorship and therefore do not meet the definition of originality 24 . We hope that AI image generators will be better regulated in the near future. To ensure that every artist and copyrighted work to be respected, AI image generators must to be more transparent to disclose their training datasets. However, at the same time, we also expect the revenue-sharing business model discussed earlier based on LLMs could to be applied to AI image generators. As discussed in Section II-D, the connection between human artists, copyright owners, and AI image tools should not be viewed as a hostile, zero-sum game but rather as a collaborative and mutually beneficial relationship. \n\nSimilar to the revenue-sharing business model of LLMs, we can also establish a scoring system for image providers of AI image generators, which would be based on the combination of some image classification models and image similarity calculators. In principle, as long as we replace \"text\" with \"image\" in Figures 3,5,6 from Section IV, we can use some image embedding techniques to build an image classifier, and image similarity calculator on the image training dataset for a certain AI image generator 25 , and establish a scoring system to measure the degree of engagement for each image provider, or even individual images. Different from LLMs, image owners/providers may be more concerned about the engagement of their individual artworks in the image generator, rather than just the overall engagement of their all works. Additionally, rather than turning each image provider into a class to build an image classifier, it may be more reasonable to classify images by art style, genre, topic rather than by image providers. 23",
            "score": 0.5577027979876299,
            "section_title": "A. AI Text-to-Image Generators",
            "char_start_offset": 73125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1335
                },
                {
                    "start": 1338,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2167
                },
                {
                    "start": 2168,
                    "end": 2371
                }
            ],
            "ref_mentions": [
                {
                    "start": 1657,
                    "end": 1658,
                    "matchedPaperCorpusId": "6866988"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2060546875
        },
        {
            "corpus_id": "258865475",
            "title": "Can Copyright be Reduced to Privacy?",
            "text": "Recent advancements in machine learning have sparked a wave of new possibilities and applications that could potentially transform various aspects of our daily lives and revolutionize numerous professions through automation. However, training such algorithms heavily relies on extensive content which may include copyrighted materials. Under U.S copyright law, copyright protection subsists in original content of authorship fixed in any tangible medium of expression [55], excluding any \"idea, procedure, process, system, method of operation, concept, principle, or discovery, regardless of the form in which it is described, explained, illustrated, or embodied in such work.\" [55, \u00a7 102(b)]. The unauthorized copying of copyrighted works may amount to copyright infringement [55, \u00a7 106] unless permitted by exceptions and limitations provided by law ([55, \u00a7107-122], and [50]). Consequently, identifying and, determining when and how content can be used within this framework without infringing upon individuals' legal rights has become a pressing challenge. Foundation Models and generative AI (GenAI), trained on gigantic datasets, exacerbate this challenge. One area where this issue arises prominently is in the operation of generative models, which take human-produced content-much of it copyrighted as input and are expected to generate \"-similar\" content. For instance, consider a machine trained on images and then generates new images that resemble the ones it was trained on. In this context, the fundamental question arises: \n\nWhen does the content generated by a machine (output content) infringe copyright in the training set (input content)? \n\nThis question is not purely theoretical, as various aspects of this problem have become subjects of legal disputes in recent years. In 2022, a class action was filed against Microsoft, GitHub, and OpenAI, claiming that their code-generating systems, Codex and Copilot, infringed copyright in the licensed code that the system was allegedly trained on [13]. Similarly, in another class action, against Stable Diffusion, Midjourney, and DeviantArt, plaintiffs argue that by training their system on webscraped images, the defendant infringes millions of artists' rights [3].",
            "score": 0.555958372044882,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1537
                },
                {
                    "start": 1540,
                    "end": 1657
                },
                {
                    "start": 1660,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 873,
                    "end": 877,
                    "matchedPaperCorpusId": "259844568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32568359375
        },
        {
            "corpus_id": "267400526",
            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
            "text": "The emergence of artificial intelligence-generated content (AIGC) has brought significant advancements in various fields, such as advertising copywriting, news reporting, and scientific creation. This technology, known as AIGC, has led to reduced labor costs and improved creative efficiency, thereby delivering greater commercial and creative value to businesses and creators. However, controversies surrounding copyright issues and innovative capabilities during the content generation process have become a subject of concern. \n\nAs a generative AI technology for text-to-image generation, the Stable Diffusion technique has garnered global attention. While this technology appears remarkably innovative, it also brings forth certain issues, including copyright concerns. \n\nStable Diffusion can quickly generate high-quality images based on simple text descriptions, thanks to its training on billions of sample images collected online. However, not all of these sample images fall within the public domain; some are even protected by copyright. As one can imagine, the authors and photographers of these copyrighted images are not pleased with the actions of Stable Diffusion. \n\nStable Diffusion has faced multiple lawsuits due to copyright issues. Artists and photographers have initiated collective lawsuits against this technology, and one of the world's leading image suppliers, Getty Images Holdings Inc., has also filed a similar case. According to Getty's (2023) complaint, \"Stability AI has reproduced over 12 million Getty Images, along with their related captions and metadata, without permission or compensation.\" Consequently, delving deeper into this issue and exploring potential solutions is essential. \n\nAIGC model training phase. One critical aspect of using copyrighted works in AIGC is the two-stage process of utilizing these works. To illustrate this process, the Stable Diffusion technique will be used as an example. During the model training phase, copyrighted works are copied from LAION-5B (Large-Scale Artificial Intelligence Online) database and modified before being incorporated into the image information space. This process ensures convenient output for future user consumption. \n\nAIGC model output stage. However, the second stage of output generation raises concerns regarding the potential infringement of copyrighted works. If the generated image is deemed \"substantially similar\" to the original work in expression, it may infringe upon the original work's \"reproduction right.\"",
            "score": 0.5538438607327908,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2213
                },
                {
                    "start": 2216,
                    "end": 2240
                },
                {
                    "start": 2241,
                    "end": 2362
                },
                {
                    "start": 2363,
                    "end": 2518
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402099609375
        },
        {
            "corpus_id": "273654195",
            "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
            "text": "Generative art using Diffusion models has achieved remarkable performance in image generation and text-to-image tasks. However, the increasing demand for training data in generative art raises significant concerns about copyright infringement, as models can produce images highly similar to copyrighted works. Existing solutions attempt to mitigate this by perturbing Diffusion models to reduce the likelihood of generating such images, but this often compromises model performance. Another approach focuses on economically compensating data holders for their contributions, yet it fails to address copyright loss adequately. Our approach begin with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then employ the TRAK method to estimate the contribution of data holders. To accommodate the continuous data collection process, we divide the training into multiple rounds. Finally, We designed a hierarchical budget allocation method based on reinforcement learning to determine the budget for each round and the remuneration of the data holder based on the data holder's contribution and copyright loss in each round. Extensive experiments across three datasets show that our method outperforms all eight benchmarks, demonstrating its effectiveness in optimizing budget distribution in a copyright-aware manner. To the best of our knowledge, this is the first technical work that introduces to incentive contributors and protect their copyrights by compensating them.",
            "score": 0.553617805034057,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.454345703125
        },
        {
            "corpus_id": "258865475",
            "title": "Can Copyright be Reduced to Privacy?",
            "text": "Similarly, in another class action, against Stable Diffusion, Midjourney, and DeviantArt, plaintiffs argue that by training their system on webscraped images, the defendant infringes millions of artists' rights [3]. Allegedly, the images produced by these systems, in response to prompts provided by the systems' users, are derived solely from the training images, which belong to plaintiffs, and, as such, are considered unauthorized derivative works of the plaintiffs' images [55, \u00a7 106 (2)]. \n\nA preliminary question is whether it is lawful to make use of copyrighted content in the course of training [23,34,36]. There are compelling arguments to suggest that such intermediary copying might be considered fair use [36]. For example, Google's Book Search Project-entailing the mass digitization of copyrighted books from university library collections to create a searchable database of millions of books-was held by US courts to be fair use [22]. Then, there is a claim that generative models reproduce protected copyright expressions from the input content on which the model was trained. However, to claim that the output of a generative model infringes her copyright, a plaintiff must prove not only that the model had access to her copyrighted work, but also that the alleged copy is substantially similar to her original work [8,53] Identifying what constitutes \"substantial similarity,\" and unlawful copying remains a pressing challenge. Recent studies have proposed measurable metrics to quantify copyright infringement [5,9,51,59]. One approach, [5,59] asserts that a machine generating output content substantially similar to an input content does not infringe that input content copyright if the machine would have reasonably generated the same output content even without accessing the input content. This argument can be illustrated as follows: Suppose that Alice outputs content A and Bob claims it plagiarizes content B. Alice might argue that she never saw content B, and would reason that this means she did not infringe Bob's copyright.",
            "score": 0.5524689902735523,
            "section_title": "Introduction",
            "char_start_offset": 2032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "152665774"
                },
                {
                    "start": 612,
                    "end": 615,
                    "matchedPaperCorpusId": "219342558"
                },
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "219342558"
                },
                {
                    "start": 1532,
                    "end": 1535,
                    "matchedPaperCorpusId": "225067265"
                },
                {
                    "start": 1559,
                    "end": 1562,
                    "matchedPaperCorpusId": "225067265"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1611328125
        },
        {
            "corpus_id": "268049158",
            "title": "Break It 'Til You Make It: An Exploration of the Ramifications of Copyright Liability Under a Pre-training Paradigm of AI Development",
            "text": "Generative AI can facilitate output or model infringements by downstream developers [Lu et al. 2019].I draw an analogy between indirect liability from pre-trained models and indirect copyright litigation involving peer-to-peer file-sharing networks to identify opportunities for model developers to subvert policy goals.I highlight these strategies to illustrate how the \"breaking\" of the AI development chain within a pre-training paradigm could be further exploited, allowing involved players to \"make it'to bypass liability for the use of copyrighted works.In line with scholarship that emphasizes accountability at each stage of model development [Khan and Hanna 2023], I propose duties that may apply during key stages of model development: training and deployment.I conclude with a discussion of copyright's role within the AI policy landscape.Proposed evaluations in the literature of the legality of the use of copyrighted works to train AI models heavily rely on model application and model outputs, and hinge on analysis through copyright's protection of expression.As part of these evaluations, the developmental process and technical architecture is conceptualized as a single pipeline-a single entity that builds a single model toward a single application.Indeed, model training is conceptualized as a process that learns toward a particular application.Sobel [2017] posits that the use of copyrighted works to train facial recognition algorithms does not implicate copyright protections: \"Training facial recognition algorithms on copyrighted photographs does not implicate the works' protectable aspects.The use analyzes factual information-the unique physical features of a subject's face-in the photographs, rather than photographers' expressive choices.\" As part of this analysis, a machine learning model that recognizes faces-a non-expressive application-is conceptualized to take only from the non-expressive aspects of the copyrighted works in the training process.Since copyright law protects only an author's original expression, this non-expressive use of copyrighted works then falls outside the scope of copyright protections and is non-infringing.This framing is reflected throughout the legal literature.Quang [2021] explains that ML models that mine data for \"functional, non-expressive purposes\" extract \"unprotectable ideas and patterns from data\" as part of their training process.",
            "score": 0.5522440633164583,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 101,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 770
                },
                {
                    "start": 770,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 1076
                },
                {
                    "start": 1076,
                    "end": 1269
                },
                {
                    "start": 1269,
                    "end": 1367
                },
                {
                    "start": 1367,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1771
                },
                {
                    "start": 1771,
                    "end": 1987
                },
                {
                    "start": 1987,
                    "end": 2175
                },
                {
                    "start": 2175,
                    "end": 2233
                },
                {
                    "start": 2233,
                    "end": 2414
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 99,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 651,
                    "end": 672,
                    "matchedPaperCorpusId": "252866086"
                },
                {
                    "start": 1367,
                    "end": 1379,
                    "matchedPaperCorpusId": "115500744"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19873046875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Diffusion models have gained widespread popularity as the new frontier of generative models. Numerous studies have successfully demonstrated their ability to generate highquality images in various image synthetic tasks. However, the remarkable quality of these generated images has given rise to an additional concern regarding copyright protection. Recent research has indicated that diffusion models often tend to memorize images in the training dataset [Carlini et al., 2023]. As a result, diffusion models can effortlessly generate copyrighted content through memorization [Somepalli et al., 2023a;Somepalli et al., 2023b]. The apprehension surrounding copyright protection in diffusion models has also evolved Figure 1: Generate copyrighted content in ChatGPT. ChatGPT refuses to generate images when directly prompted for copyrighted material. However, adversarial prompts generated with our method that do not directly ask for copyrighted material still manage to generate copyrighted material, in this case, the Superman logo. \n\ninto a tangible threat, as multiple lawsuits related to copyright infringement have been initiated against companies that utilize diffusion models for commercial purposes. Notably, there have been instances of lawsuits: Stability AI and Mid-Journey are both facing civil suits for training their models on artists' work without their consent thereby allowing their models to replicate the style and work of such artists [Vincent, 2023]. \n\nAttempts have been made to prevent the generation of copyrighted content, such as OpenAI's addition of filters on ChatGPT to prevent the generation of copyrighted images. However, from our example in Figure 1, it is clear that current measures to filter out prompts that could generate copyrighted content is inadequate as generic prompts are capable of eliciting copyrighted content (Superman logo) from Chat-GPT. Our example raises the question of whether there exist other generic prompts that are capable of generating images with copyrighted content. Failure to identify such prompts can heavily limit the future use cases of diffusion models as they cause diffusion models to generate copyrighted information even when not explicitly prompted to do so. \n\nOur contributions. (1) We form a framework to create prompts for T2I tasks that are generic in language semantics but can still trigger partial copyright infringements in image generation by various diffusion models.",
            "score": 0.5501617901518293,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2234
                },
                {
                    "start": 2237,
                    "end": 2255
                },
                {
                    "start": 2256,
                    "end": 2453
                }
            ],
            "ref_mentions": [
                {
                    "start": 577,
                    "end": 602,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 602,
                    "end": 626,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 1457,
                    "end": 1472,
                    "matchedPaperCorpusId": "266900037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56787109375
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Diffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues. Specifically, we introduce a data generation pipeline to systematically produce data for studying copyright in diffusion models. Our pipeline enables us to investigate copyright infringement in a more practical setting, involving replicating visual features rather than entire works using seemingly irrelevant prompts for T2I generation. We generate data using our proposed pipeline to test various diffusion models, including the latest Stable Diffusion XL. Our findings reveal a widespread tendency that these models tend to produce copyright-infringing content, highlighting a significant challenge in this field.",
            "score": 0.5499518187973981,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "265351912",
            "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
            "text": "[8], Midjourney [1], are setting off a new revolution of artwork creation. These programs allow users to effortlessly generate target images by taking some descriptions as input into the model. However, these AI-generated artworks inherit the characteristics of the images that are used to train models [33,40,43], which might be pretty similar to the original ones, as shown in Figure 1. Such similarity has aroused concerns about copyright infringement disputes. For example, three artists (Sarah Andersen, Kelly McKernan, and Karla Ortiz) have recently accused Stable Diffusion of unlawfully  2) was created by Stable Diffusion using the \"style of Erin Hanson\" as a prompt. The styles of these two images are so similar that it is impossible to tell them apart. \n\nscraping copyrighted images from the Internet to mimic their art styles [11]. To this end, research on responsible AI image generation is in urgent need to address such copyright issues. \n\nPrevious work on data attribution [29,31,32,42] focused on how the images in the training data contributed to the model's outcome, which is not suitable for the context of copyright traceability. This is because of the following reasons: (1) the training data is not known in advance in real-world practices; as shown in Figure 2, the models are usually publicly available, while the training datasets are not [22,43]. (2) the responsible party is the model provider, namely, the personnel or the organization who abused the online image collections without the owners' consent, not the image itself. As long as the model that generates the infringing image is identified, the corresponding infringer (the model provider) can be found, and the degree of infringement can be quantified. Thus, we need to develop a new approach to quantify copyright infringement from the model level, which is the focus of our paper. \n\nTo this end, we propose a new framework CopyScope at the model level towards AIGC copyright traceability. Our framework CopyScope includes three closely intertwined stages (Identify-Quantify-Evaluate).",
            "score": 0.5465390958136148,
            "section_title": "AI image generation programs, namely Artificial Intelligence Generated Content (AIGC) tools such as Stable Diffusion [20], DALL\u2022E2",
            "char_start_offset": 147,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1871
                },
                {
                    "start": 1874,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2075
                }
            ],
            "ref_mentions": [
                {
                    "start": 990,
                    "end": 994,
                    "matchedPaperCorpusId": "235719065"
                },
                {
                    "start": 997,
                    "end": 1000,
                    "matchedPaperCorpusId": "249947012"
                },
                {
                    "start": 1000,
                    "end": 1003,
                    "matchedPaperCorpusId": "249319573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61279296875
        },
        {
            "corpus_id": "271600759",
            "title": "Replication in Visual Diffusion Models: A Survey and Outlook",
            "text": "As shown in Fig. 6 (a), training and generating processes in some visual diffusion models raise significant law issues due to the replication of copyrighted materials. As these models become more powerful and prevalent, an increasing number of legal scholars are focusing on this area. They primarily investigate how these models manage and utilize copyrighted materials during the creation process, along with the challenges and implications for the existing copyright law framework. For instance, they [51], [52], [177]- [182] question whether using copyrighted works as training data for AI constitutes copyright infringement, whether AI-generated outputs are derivative works infringing on the original copyrights, and who owns the copyright for AI-generated works. Furthermore, [182], [183] discuss the intricate infringement challenges that arise when generative AI models, particularly visual diffusion models, are trained using copyrighted materials without proper authorization. \n\nAdditionally, [184] aims to define and clarify what constitutes replication from the perspective of copyright infringement; [185] thoroughly explores the intersection of copyright law and economic principles in the context of rapid technological advancements; and the core idea of [186] is to evaluate whether privacy protection measures can align with and support copyright law. Beyond the copyright issues, there are also privacy concerns and corresponding data protection regulations [267], [268]. The replication of data by visual diffusion models can pose significant privacy risks, especially when the models inadvertently replicate sensitive or personal data. This contravenes data protection regulations such as the General Data Protection Regulation (GDPR) [269] in Europe, which mandates the protection of personal data with appropriate technical measures. Regulatory frameworks ensure that AI systems, particularly those trained on vast amounts of potentially sensitive data, comply with privacy regulations and do not retain or reproduce personal data without consent. \n\nThe replication of biases in training data by AI models is another regulatory concern [267], [270], [271]. Ensuring that diffusion models do not perpetuate or amplify biases present in the data they are trained on is crucial. Regulations enforce fairness, accountability, and transparency in AI systems to mitigate these issues. This could involve mandatory bias audits, transparency in data usage, and clear documentation of the data and methodologies used in training AI models.",
            "score": 0.5457537649277349,
            "section_title": "Regulation",
            "char_start_offset": 45154,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2070
                },
                {
                    "start": 2073,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2298
                },
                {
                    "start": 2299,
                    "end": 2401
                },
                {
                    "start": 2402,
                    "end": 2553
                }
            ],
            "ref_mentions": [
                {
                    "start": 790,
                    "end": 795,
                    "matchedPaperCorpusId": "258684279"
                },
                {
                    "start": 1114,
                    "end": 1119,
                    "matchedPaperCorpusId": "264474179"
                },
                {
                    "start": 1477,
                    "end": 1482,
                    "matchedPaperCorpusId": "257280234"
                },
                {
                    "start": 2159,
                    "end": 2164,
                    "matchedPaperCorpusId": "257280234"
                },
                {
                    "start": 2173,
                    "end": 2178,
                    "matchedPaperCorpusId": "268164961"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37939453125
        },
        {
            "corpus_id": "277468540",
            "title": "Unfair Learning: GenAI Exceptionalism and Copyright Law",
            "text": "Again, common sense dictates that the fact that entities feel compelled to implement such mechanisms strongly indicates (i) they likely know they trained the models with copyrighted works and (ii) copies of at least some of those copyrighted works are in the models. 12 here are a few important facts we can take away from this. First, these efforts mean that a \"base\" model (one without the post-training mechanisms) is more likely to regurgitate content than an \"instructed\" model (one designed to \"chat\"). Second, it also means that just because a model rarely or seemingly never regurgitates copyrighted content does not mean there is not a copy of the copyrighted work within the model. Third, as Jacqueline Charlesworth, the former general counsel of the US Copyright Office, notes about models being prompted to generate an output in the style of someone, \"even if the output itself does not rise to the level of infringement, in order to generate recognizable riffs on the artist's works, the model presumably trained on and contains encoded representations of those works. The generation of content in a recognizable style thus points to earlier acts of infringement.\" 13",
            "score": 0.5454346319467488,
            "section_title": "II. A Brief Overview of How GenAI is Developed",
            "char_start_offset": 9116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1180
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2020263671875
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "Recent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal concerns about copyright infringement. Copyrighted characters (e.g., Mario, Batman) present a significant challenge: at least one lawsuit has already awarded damages based on the generation of such characters. Consequently, commercial services like DALL-E have started deploying interventions. However, little research has systematically examined these problems: (1) Can users easily prompt models to generate copyrighted characters, even if it is unintentional?; (2) How effective are the existing mitigation strategies? To address these questions, we introduce a novel evaluation framework with metrics that assess both the generated image's similarity to copyrighted characters and its consistency with user intent, grounded in a set of popular copyrighted characters from diverse studios and regions. We show that state-of-the-art image and video generation models can still generate characters even if characters' names are not explicitly mentioned, sometimes with only two generic keywords (e.g., prompting with\"videogame, plumber\"consistently generates Nintendo's Mario character). We also introduce semi-automatic techniques to identify such keywords or descriptions that trigger character generation. Using this framework, we evaluate mitigation strategies, including prompt rewriting and new approaches we propose. Our findings reveal that common methods, such as DALL-E's prompt rewriting, are insufficient alone and require supplementary strategies like negative prompting. Our work provides empirical grounding for discussions on copyright mitigation strategies and offers actionable insights for model deployers implementing these safeguards.",
            "score": 0.5435109441122503,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6044921875
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "Text-to-image generative models, trained on large-scale datasets like LAION [44], have been equipped with enhanced memorization ability to generate outputs of high semantic similarity to their training data [3,48]. Given the prevalence of copyrighted works in these datasets, the significant risk of copyright infringement for these generations has raised great concerns from the public [6,18] and researchers [1,4,16,42,48,49,52]. Many efforts have been made to safeguard copyrighted materials from being infringed by generative diffusion models. Some researchers [23,24,41,45] introduced data perturbation, where input data is modified to hinder the model to imitate copyrighted features. Another separate line of works [14,21,26,43,58] exploited concept removal that erases unsafe concepts from existing pre-trained diffusion models to mitigate the risk of undesirable generations. In an alternative approach, researchers studied watermark protection for copyrighted data [9,29,36,54,56,59] to encode ownership information into potentially infringed outputs. \n\nA notable contribution, Vyas et al. [52] first provided a mathematical probabilistic upper-bound against copyrightinfringed generation. They asserted that the proposed near access-freeness (NAF) offers robust guarantees for copyright protection. However, Elkin-Koren et al. [12] argued the limitation of this method for reducing copyright to a matter of privacy from a legal perspective. In this paper, we build upon these discussions to present a significant challenge to these probabilistic copyright protection methods through the amplification attack.",
            "score": 0.5432499122755102,
            "section_title": "Copyright Issues in Generative Models",
            "char_start_offset": 4229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1619
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 80,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 207,
                    "end": 210,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 210,
                    "end": 213,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 418,
                    "end": 421,
                    "matchedPaperCorpusId": "249375708"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 427,
                    "end": 430,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 569,
                    "end": 572,
                    "matchedPaperCorpusId": "256697414"
                },
                {
                    "start": 572,
                    "end": 575,
                    "matchedPaperCorpusId": "256826808"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "257427549"
                },
                {
                    "start": 732,
                    "end": 735,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 981,
                    "end": 984,
                    "matchedPaperCorpusId": "227129816"
                },
                {
                    "start": 1100,
                    "end": 1104,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.376220703125
        },
        {
            "corpus_id": "269214015",
            "title": "\\copyright Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model",
            "text": "This paper addresses the contentious issue of copyright infringement in images generated by text-to-image models, sparking debates among AI developers, content creators, and legal entities. State-of-the-art models create high-quality content without crediting original creators, causing concern in the artistic community. To mitigate this, we propose the \\copyright Plug-in Authorization framework, introducing three operations: addition, extraction, and combination. Addition involves training a \\copyright plug-in for specific copyright, facilitating proper credit attribution. Extraction allows creators to reclaim copyright from infringing models, and combination enables users to merge different \\copyright plug-ins. These operations act as permits, incentivizing fair use and providing flexibility in authorization. We present innovative approaches,\"Reverse LoRA\"for extraction and\"EasyMerge\"for seamless combination. Experiments in artist-style replication and cartoon IP recreation demonstrate \\copyright plug-ins' effectiveness, offering a valuable solution for human copyright protection in the age of generative AIs. The code is available at https://github.com/zc1023/-Plug-in-Authorization.git.",
            "score": 0.5431935514047204,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1751708984375
        },
        {
            "corpus_id": "269214015",
            "title": "\\copyright Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model",
            "text": "Large foundation models are trained with extensive, high-quality datasets like LAION [1] and other enormous undisclosed data sources, which definitely contain copyrighted human contents.At the same time, these models not only excel at generating content based on user prompts [2][3][4], but also have the potential of memorizing the exact training data thanks to the huge capacity in their gigantic numbers of parameters [5].These have sparked copyright infringement concerns among content providers, artists, and users.A notable instance is the lawsuit filed by The New York Times against OpenAI and Microsoft [6], alleging the unauthorized use of a vast number of articles for the purpose of training automated chatbots.The lawsuit seeks the destruction of the allegedly infringing chatbots and their associated training data.Similar concerns and legal actions are also emerging in the field of text-to-image generation [7].\n\nAlthough it is still debatable whether using copyrighted material to train machine learning models is allowable due to the fair use exception in copyright laws [6,7], it's definitely that these powerful models have disrupted existing reward systems for creative artists and content providers [8].This paper advocates revisiting the goal of copyright laws [9], which is to protect the original expression of ideas in creative works and promote scientific progress by granting authors exclusive property rights for a limited time.However, generative AI models challenge the proper attribution of rewards to copyright holders, impacting artists and domain experts who may hesitate to contribute to knowledge exchange platforms without reasonable rewards.This could lead to a shortage of fresh data for machine learning.\n\nTo address these copyright challenges in generative AI models, we introduce the \"\u00a9Plug-in Authorization\" framework for text-to-image generation systems (see Figure 1(a)), to align with existing Intellectual Property (IP) management practices, allowing copyright holders to register their works as plug-ins and receive rewards for their use.End users pay for generating images with copyrighted concepts, and base model providers profit from plug-in registration and usage.This framework facilitates explicit tracking of copyrighted work usage, ensuring a fair reward system.",
            "score": 0.5430582696981038,
            "section_title": "Main",
            "char_start_offset": 7,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 520
                },
                {
                    "start": 520,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 926
                },
                {
                    "start": 928,
                    "end": 1224
                },
                {
                    "start": 1224,
                    "end": 1456
                },
                {
                    "start": 1456,
                    "end": 1679
                },
                {
                    "start": 1679,
                    "end": 1744
                },
                {
                    "start": 1746,
                    "end": 2086
                },
                {
                    "start": 2086,
                    "end": 2217
                },
                {
                    "start": 2217,
                    "end": 2319
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 88,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 279,
                    "end": 282,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2822265625
        },
        {
            "corpus_id": "265351912",
            "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
            "text": "We initiate our study by determining components that have the most significant impacts on the generated images. Components make up the AI image generation workflow, which is used to characterize generation models in our proposed copyright tracking approach. This stage is based on a survey from the world's largest AI image generation exchange and sharing platform Civitai, where we collect more than 16,000 generated image data from over 5,000 models to find commonalities in generated images. The generated images are divided into 6 themes: celebrity, film&TV, artwork, popular models, design, and game. We explore the distribution of models that generate images involving copyright infringement by calculating the usage rate of components: Base Model, Lora, ControlNet, and Key Prompt. Table 2 shows the frequency of these components, indicating that they have a high usage rate in AI image-generation tasks. \n\nWe identify four components that are used in AI image generation at a high frequency: \u2776 The Base Model is essential for each generated image. \u2777 The second is the Lora. Although the Lora is not a necessary option for generating images, we can find from Table 2 that it has a high application rate in each category, indicating that the use of Lora for adjustment in AI image generation has become a norm. \u2778 Prompts with particular specificity are called Key Prompts. Key Prompt can make the generated image close to the characteristics of these keywords to a large extent, thus infringing on the original author's rights. \u2779 The overall usage rate of ControlNet is relatively lower than the other components. This is because the ControlNet is challenging to use as it needs higher environment configuration requirements than Lora and Key Prompt[3]. However, ControlNet is an essential components in generating particular themes as it controls the structure of the image. From the perspective of copyright tracing, the ControlNet is a critical suspected infringement component that our CopyScope framework considers.",
            "score": 0.5429393138706994,
            "section_title": "Identify Influential components",
            "char_start_offset": 10528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37060546875
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "we propose a novel approach to minimize copyright infringement in text-to-image diffusion models by leveraging reinforcement learning (RL) and our proposed copyright metrics. We first define a copyright metric to measure how closely a generated image resembles copyrighted content. Then, we integrate this copyright metric into reward function and employ reinforcement learning techniques to fine-tune a pre-trained text-to-image diffusion model. Specifically, the model is trained to maximize the reward by iteratively adjusting its parameters to reduce the likelihood of producing copyright-infringing images. By doing so, we ensure that the model maintains high image quality while adhering to copyright constraints. \n\nAs shown in Figure 2, the training process of RLCP is as follows: \n\n\u2022 Gather Datasets: Compile datasets that include both original and copyright-infringing samples. \u2022 Prompts Generation: Fed these images into the CLIP interrogator, allowing us to obtain prompts that correspond to each anchor image. The CLIP Interrogator is utilized to convert copyrighted images into corresponding textual information. This text is subsequently refined and transformed into prompts, which are then inputted",
            "score": 0.540842723971439,
            "section_title": "Overview",
            "char_start_offset": 9821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1213
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33740234375
        },
        {
            "corpus_id": "277502009",
            "title": "Who Owns the Output? Bridging Law and Technology in LLMs Attribution",
            "text": "They asserted that Midjourney, the AI program, was the actual author of the \"visual material,\" rather than Kashtanova. Following this, in March 2023, the Copyright Office released new guidance, stating that when AI determines the expressive elements of its output, the resulting material cannot be considered the product of human authorship. This guidance represents a significant clarification in the ongoing discussion regarding copyright and the role of artificial intelligence in creative processes. \n\nIn general, AI systems undergo a training process that enables them to generate various forms of artistic expression, including literature and visual art. This process involves feeding the AI large datasets comprised of text, images, and other content acquired from the vast resources available on the internet. During training, the AI program creates digital copies of existing works, a necessary step for analyzing the vast amount of information it receives. The U.S. Patent and Trademark Office has noted that this training inherently involves reproducing either complete works or significant portions of those works. For instance, OpenAI has publicly acknowledged that its programs are trained on extensive datasets that include copyrighted materials. Specifically, they emphasize that the training process begins with making copies of the data intended for analysis. While OpenAI has recently introduced an option to exclude certain images from being used in the training of future image generation models, it's important to recognize that the act of creating these copies without obtaining permission may violate the exclusive rights of copyright holders to reproduce and distribute their creative works. This raises important questions about copyright and intellectual property in the context of advancing AI technology. \n\nThe critical question surrounding the use of generative AI is who should be held accountable if its outputs violate the copyrights of existing works. Current legal frameworks suggest that both the individual using the AI and the company that developed the technology could be deemed responsible for such infringements. \n\nFor instance, if a user is found to be directly liable for copyright infringement by generating content that illegally replicates protected material, the AI company might also be implicated. This potential liability arises from the legal principle known as \"vicarious infringement.\" Under this doctrine, a party can be held liable if they have the authority and capability to oversee the infringing actions and if they also have a direct financial stake in the activities occurring through their technology.",
            "score": 0.5402905638635748,
            "section_title": "The US Approach",
            "char_start_offset": 15218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1833
                },
                {
                    "start": 1836,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2154
                },
                {
                    "start": 2157,
                    "end": 2347
                },
                {
                    "start": 2348,
                    "end": 2439
                },
                {
                    "start": 2440,
                    "end": 2664
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.381103515625
        },
        {
            "corpus_id": "270063652",
            "title": "Automatic Jailbreaking of the Text-to-Image Generative AI Systems",
            "text": "Midjourney [19], Gemini Pro [29], Copilot [18] and ChatGPT [1] have word-based detection mechanism on the user prompts to prevent generation of the images that may violate the internal policy.that the blocking mechanism fails to block copyright infringement generation to 11.0% block rate on our APGP-generated prompts (Table 3).Furthermore, not only generating the contents, the contents are exceptionally similar to the original IP content as shown in Figure 4. Human evaluation.To quantify the violations, we conducted a human evaluation on 63 participants to determine the copyright violation based on the reference image.The copyright violation is highly occurring in the product and logo category where 96.24% and 82.71% of participants examine the images as copyright infringement (Figure 7).Upon examining the images classified as identical violations, it was found that over 50% were deemed to be cases of copyright infringement in product and logo.Furthermore, 30% of characters are also considered as similar violations which are determined as severe similarity (Figure 8).When we employ a consensus vote to determine violations, there are still 10 images that all participants determine as violations.Automatic evaluation.Although human evaluation is one of the best evaluation approach for copyright infringement, we propose automatic evaluation to reduce the cost of the experiment.We introduce a QA score that calculates the accuracy by given generated images by T2I systems, where QA sets are generated based on the target image.\n\nWe employ VLM to respond to the question, and LLM to evaluate the answers.In Figure 5, 34.09% of the generated images accurately answer more than seven questions, suggesting that these images contain key aspects similar to the target images necessary for matching the correct answers.Given that the target image correctly matches the answer for more than seven questions in 67.05% of cases, we estimate that 50.84% of the generated images likely commit copyright infringement.Ablation study.Text prompts that specifically describe copyrighted content can trigger the generation of such content even without explicit keywords, as demonstrated in Table 4.We hypothesize that omitting specific keywords may allow these prompts to bypass initial violation detection mechanisms.",
            "score": 0.5393958635166415,
            "section_title": "Simple prompt can induce the copyright violation in most systems",
            "char_start_offset": 15113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 192,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 958
                },
                {
                    "start": 958,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1234
                },
                {
                    "start": 1234,
                    "end": 1396
                },
                {
                    "start": 1396,
                    "end": 1545
                },
                {
                    "start": 1547,
                    "end": 1621
                },
                {
                    "start": 1621,
                    "end": 1831
                },
                {
                    "start": 1831,
                    "end": 2023
                },
                {
                    "start": 2023,
                    "end": 2038
                },
                {
                    "start": 2038,
                    "end": 2200
                },
                {
                    "start": 2200,
                    "end": 2320
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061187744140625
        },
        {
            "corpus_id": "268732888",
            "title": "Detecting Generative Parroting through Overfitting Masked Autoencoders",
            "text": "Generative artificial intelligence (AI) models, including but not limited to Stable Diffusion [11], DALLE [10], and Generative Pre-trained Transformers (GPT) [8], represent a groundbreaking shift in the landscape of digital content creation, empowering users to generate text, images, and other forms of media with unprecedented ease and flexibility.These models have been applied across a wide range of domains, from artistic creation and design to content generation for social media and marketing purposes, demonstrating their versatility and potential to enhance creativity and productivity.\n\nThe rapid adoption and deployment of these technologies have also raised significant ethical, legal, and technical challenges, particularly in the context of copyright infringement and data privacy [4,12,17].At the heart of these concerns is the phenomenon known as \"generative parroting,\" where models produce outputs that are not sufficiently distinct from their training data [3,16], leading to the generation of content that closely mimics or even directly copies existing copyrighted materials.This issue not only poses legal risks for users and developers but also undermines trust in generative AI technologies, especially in trust-critical scenarios where the protection of intellectual property and sensitive information is paramount.\n\nThe challenge of detecting and mitigating generative parroting is compounded by the inherent complexities of AI models' training processes and the vastness of the data landscapes they navigate.Traditional approaches to model training and evaluation may not adequately address the nuances of copyright-sensitive scenarios, necessitating innovative solutions that are specifically tailored to recognize and respect the boundaries of copyright law [13].Moreover, the dynamic nature of copyright legislation, which varies across jurisdictions and is continually evolving in response to technological advancements, adds another layer of complexity to this challenge [7].\n\nWhile passing generated samples through a representation learner to obtain feature vectors and compare them with training data might be feasible for small datasets, this approach becomes impractical for larger datasets with billions of samples, especially in real-time scenarios.For instance, designers interacting with generative models need immediate feedback, rendering exhaustive comparisons untenably slow.",
            "score": 0.5354156466898733,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 350
                },
                {
                    "start": 350,
                    "end": 595
                },
                {
                    "start": 597,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 1096
                },
                {
                    "start": 1096,
                    "end": 1340
                },
                {
                    "start": 1342,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1792
                },
                {
                    "start": 1792,
                    "end": 2007
                },
                {
                    "start": 2009,
                    "end": 2288
                },
                {
                    "start": 2288,
                    "end": 2420
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 98,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 795,
                    "end": 798,
                    "matchedPaperCorpusId": "234777751"
                },
                {
                    "start": 801,
                    "end": 804,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 976,
                    "end": 979,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 979,
                    "end": 982,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1787,
                    "end": 1791,
                    "matchedPaperCorpusId": "259844568"
                },
                {
                    "start": 2003,
                    "end": 2006,
                    "matchedPaperCorpusId": "261375096"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60107421875
        },
        {
            "corpus_id": "266693839",
            "title": "Digger: Detecting Copyright Content Mis-usage in Large Language Model Training",
            "text": "Complications of AI Models Trained on Copyrighted Content. The training of artificial intelligence (AI) models, particularly LLMs, on copyrighted content has emerged as a complex issue straddling legal, ethical, and technological domains. The architecture of these AI models, especially those based on deep learning, requires the consumption of extensive datasets, often comprising copyrighted material like texts, images, or audio. This necessity raises intricate questions about traditional copyright law, as the self-generative nature of AI models makes it difficult to delineate how much of the model's outputs can be considered derivative of the original copyrighted works. High-profile legal cases such as Getty Images accessed via Stability AI [20] have brought these complexities to the forefront, demonstrating the legal risks of using copyrighted content in AI training without explicit consent. Generative AI models, capable of creating new content, add another layer of complexity. These models learn from the patterns and structures in their training data and can generate new data that may closely resemble copyrighted works. This poses significant legal and ethical challenges, including potential copyright infringement and the ethical implications of using copyrighted material without consent. The risks are particularly high when the generated content bears watermarks or other insignia, which could be interpreted as an admission of infringement. \n\nLimitations of Exisiting Mitigations. Various mitigation strategies have been proposed, ranging from legal solutions like securing explicit content use consent through contracts or licenses, to technological approaches like creating original datasets based on real-world measurements. Despite these mitigation strategies, challenges persist. Legal solutions often involve complex negotiations and may not fully address the ethical dimensions, such as potential cultural appropriation or commodification of creative works. Technological solutions, while bypassing the need for copyrighted content, require significant investments and may still fall short in capturing the richness of human-created content. In this work, we aim to explore the copyrighted content internalized within Large Language Models, seeking to understand how these models transform and potentially infringe upon copyrighted material.",
            "score": 0.5332904467822324,
            "section_title": "BACKGROUND",
            "char_start_offset": 5807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 59,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2374
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.353759765625
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "Recently, text-to-image diffusion models have garnered significant attention in research. These advanced methods (Balaji et al. 2023;Nichol et al. 2022;Rombach et al. 2022a;Saharia et al. 2022) have demonstrated exceptional capabilities in converting textual descriptions into highly accurate and visually coherent images. The advancements in these techniques have unlocked numerous possibilities for various downstream tasks, including image editing (Avrahami, Lischinski, and Fried 2022;Ho, Jain, and Abbeel 2020a;Kawar et al. 2023), image denoising (Ho, Jain, and Abbeel 2020a;Xie et al. 2023), and super-resolution (Sohl-Dickstein et al. 2015;Ho, Jain, and Abbeel 2020b). \n\nWhile the progress in text-to-image generative models has profoundly impacted different industries, it also presents significant challenges for copyright protection. These models utilize extensive training data that may include copyrighted works, which they are sometimes capable of memorizing (Carlini et al. 2023). This ability can result in the pro- duction of images that closely resemble protected content (See in Figure 1), posing significant challenges to copyright protection (Elkin-Koren et al. 2023). Recent legal cases, such as those involving Stable Diffusion (Rombach et al. 2022b) and Midjourney(Mansour 2023), highlight concerns over the use of copyrighted data in AI training, where the models potentially infringe on the rights of numerous artists. These cases highlight a growing concern: Could the high-quality content synthesized by these generative AIs be excessively similar to copyrighted training data, potentially violating the rights of copyright holders? \n\nVarious methods have been proposed for source data copyright protection. One approach involves using unrecognizable examples (Gandikota et al. 2023;Zhang et al. 2023) that prevent models from learning key features of protected images either during inference or training stages. However, this method is highly dependent on the specific image and model, and it lacks general reliability.",
            "score": 0.5328406953580493,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 173,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 173,
                    "end": 192,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 451,
                    "end": 489,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 516,
                    "end": 534,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 619,
                    "end": 647,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1162,
                    "end": 1187,
                    "matchedPaperCorpusId": "258297834"
                },
                {
                    "start": 1250,
                    "end": 1272,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.420654296875
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "On the other hand, the protection in some domains like text is much harder than image, because the modification on image can be designed as invisible to human eyes but effective on DGMs, while text is discrete and is hard to be designed as imperceptible. Despite the difficulty, it is necessary to propose the protection from the side of source data owner for other domains, especially in text domain which is fast developing and causes increasing concerns in the data copyright. Also, the copyright protection for multi-modality generation [85,101,110,155] in both data copyright and model copyright is also crucial. \n\n\u2022 Infringement detection. Injecting watermark into images to accelerate the infringement detection has been introduced in Section 2.3.2, which aims to help source data owners protect their data copyright. Infringement detection can also benefit the model builder. Before releasing the generated output to the users, the builder can first check whether the output is infringing the copyright of training data (like a memorized training sample). If the output to be released is detected as infringement, the model builder can use some following-up strategies like adding references to the source of the content or removing the suspicious part to avoid the infringement. However, detecting infringement on the training data is not trivial, especially that current DGMs usually require a large amount of training data to ensure the generated quality and diversity. The dataset de-duplication in Section 2.3.4 and Section 3.3.1 provides potential solutions. It is an offline strategy that can be achieved before training. However, for infringement detection, a faster real-time searching is necessary to mitigate negative impact on the generation speed. Another issue is that, compared with memorization problem, the infringement on the copyright of abstract concepts, such as the style of an artwork or a product, and the original storylines and characters created by authors, is hard to confirm. The memorized samples often bear a high resemblance to the copyrighted material 12 , making infringement easy to confirm. In contrast, if a generated sample replicates the style of a picture but diverges significantly in content from the original, it becomes much harder to detect and confirm whether the generated sample stems from copyrighted data or not. \n\nSecond, for the model copyright protection, we present the following aspects: \n\n\u2022 Robustness.",
            "score": 0.5314745214373182,
            "section_title": "Discussion",
            "char_start_offset": 92009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2370
                },
                {
                    "start": 2373,
                    "end": 2450
                },
                {
                    "start": 2453,
                    "end": 2466
                }
            ],
            "ref_mentions": [
                {
                    "start": 545,
                    "end": 549,
                    "matchedPaperCorpusId": "7588858"
                },
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "254854449"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.369140625
        },
        {
            "corpus_id": "277667210",
            "title": "We Are All Creators: Generative AI, Collective Knowledge, and the Path Towards Human-AI Synergy",
            "text": "Lawsuits like Getty Images v. Stability AI, Andersen v. Stability AI, and Authors Guild v. OpenAI are grappling with these issues, including claims related to the removal or alteration of Copyright Management Information (CMI) under the DMCA \u00a71202 [Ginsburg and Budiardjo, 2023]. \n\nThe widespread availability of powerful open generative models, exemplified by the release of Stable Diffusion in 2022 [Rombach et al., 2022], adds another layer of complexity. These models can be freely downloaded, modified, and fine-tuned by anyone, often using private datasets or specific styles. Tracking the lineage of outputs generated by potentially thousands of derivative models becomes impossible, rendering systematic compensation or enforcement schemes based on training data provenance utterly impractical. Users can easily train an open model (initially trained on non-copyrighted data) with copyrighted images offline and generate new content, making legal recourse against the original model creators or the end-users exceedingly difficult. \n\nThis intricate legal and technical landscape suggests that seeking resolution primarily through copyright litigation or regulation focused on individual attribution and compensation faces formidable, perhaps insuperable, obstacles.",
            "score": 0.5310019991518748,
            "section_title": "Navigating the Copyright Labyrinth: Collective",
            "char_start_offset": 15686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 282,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1273
                }
            ],
            "ref_mentions": [
                {
                    "start": 401,
                    "end": 423,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.337646484375
        },
        {
            "corpus_id": "257557331",
            "title": "Copyright Protection and Accountability of Generative AI: Attack, Watermarking and Attribution",
            "text": "The field of Generative AI (e.g., Generative Adversarial Networks -GANs) has made significant advancements in recent times. These advancements have led Generative AI to improve its capacity to produce highly realistic content such as artwork and images [20]. Hence, there is widespread utilization of Generative AI in both research and industry, leading to significant concerns regarding the protection of Intellectual Property Rights (IPR). There is a surge of GAN-based image generative model usage for creating content that can infringe upon existing copyrights, spoof security systems, and undermine the rights of content owners through the spread of misinformation, affecting personal reputations. Consequently, AI-Generated images are facing lawsuits over alleged copyright violations. One of the notable examples of a legal dispute involving AI-Generated images includes the lawsuit filed by Getty Images and a group of artists against AI art generators. 1 Additionally, producing state-of-the-art generative models requires substantial computational resources and large datasets. These issues have generated growing interest in research on protecting and verifying the ownership of the GAN models and training sets. \n\nPrevious copyright protection approaches for images and GANs include attacks (e.g., adversarial noise), watermarking, and attribution techniques. However, the literature focuses mainly on Deep Neural Networks (DNNs) and has a limited focus on developing copyright protection frameworks of GANs [1]. \n\nOur research aims to fill this gap by conducting a comprehensive analysis of the effectiveness of the existing copyright protection approaches for GANs. The proposed framework assists in identifying the key factors that determine the effectiveness of copyright protection techniques through the analysis of various GAN models. In order to accomplish this goal, we explore and answer the following three research questions. \n\n\u2022 RQ1: Can current adversarial attacks effectively prevent copyright violations of input images? \u2022 RQ2: Can the IPR of GANs and their training sets be protected and verified through watermarking? \u2022 RQ3: Can current attribution methods effectively support IPR protection and trace toxic generative AI model accountability through source attribution of output images?",
            "score": 0.5300613429654286,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1524
                },
                {
                    "start": 1527,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1949
                },
                {
                    "start": 1952,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2317
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.344482421875
        },
        {
            "corpus_id": "267400526",
            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
            "text": "The participation of this study revolves around the images generated by AIGC. The images generated by AIGC refer to computer-generated images, graphics, or visual content produced through artificial intelligence. These images are typically generated using AI algorithms and deep learning models. They can be completely new images generated from scratch or modified and edited based on existing ones. Taking Stable Diffusion as an example, users can generate images using pure prompts or prompts combined with images. \n\nThe images generated by AIGC can have various forms and styles, ranging from realistic photo compositions to abstract artworks. As Marek J. Magdalena Z. (2023) mentioned, AI-generated images are created by training models on vast amounts of data and using that data to generate novel visual content [1]. As Brownlee J. (2019) published, the algorithms involved in image generation may utilize convolutional neural networks (CNNs), generative adversarial networks (GANs), or other machine learning methods [2]. \n\nAIGC-generated images find application in numerous fields, including visual effects production in the film and gaming industry, artistic creation, product design, and virtual reality. However, due to potential copyright and originality issues associated with AIGC-generated images, their usage and protection have sparked numerous copyright disputes and legal discussions. \n\nDue to the substantial increase in copyright infringement cases involving AIGC-generated images compared to other AIGC-generated content in recent years and the widespread use of AIGC image-generation technology in various industries, the risk of copyright disputes is higher. Furthermore, images are more susceptible to infringement and can be easily identified by copyright holders, compromising their interests. Therefore, it is justified to choose AIGC-generated images as the research subject.",
            "score": 0.5277399552205088,
            "section_title": "Participant",
            "char_start_offset": 4054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1403
                },
                {
                    "start": 1406,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1904
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17919921875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Copyright infringement for generative models. For copyright infringement, we focus on the copyright regulation in the US [Copyright Office, 2022]. Nevertheless, the concept of copyright applies to the regulation in other countries. In the context of copyright law in the US, whether a piece of copyrighted material can be used by others is governed by the concept of Fair Use which permits the use of copyrighted material only in a transformative manner that is distinct from the original work. There have been legal precedents which demonstrate that structural similarity can lead to infringement claims. Given that such generative models are trained on datasets such as LAION-5B [Schuhmann et al., 2022] which contains publicly available copyrighted data, if the models generate images with visual features that have substantial structural similarity to the original copyrighted images, this would likely be grounds for claims of copyright infringement. \n\nThis has implications in commercial settings such as companies selling proprietary image generation models as a service or an individual user, particularly when such models produce or are used to create and sell material bearing strong structural similarities to copyrighted images. Thus, there is a potential legal risk for both the providers of image generation models and their users, especially in commercial settings where the 'transformative' nature of generated images may not meet the legal threshold established in copyright law if they possess substantial structural similarity to the original image. \n\nObjective of our data generation pipeline. We aim to systematically generate prompts that are considered generic and not related to any copyrighted topic, but still capable of triggering the generation of copyrighted content from diffusion models. We formally define two desired properties our generated prompts should satisfy. Definition 1. (Prompt sensitivity) Given a semantic measurement f s (\u2022) and a tolerance \u03f5, prompt p is sensitive to a \n\nIn practice, f s can be a text encoder that encodes plain text to be text embeddings for comparison. We detail distance measure D[\u2022||\u2022] in the following discussion. According to the above definition, a prompt is considered to be sensitive to a topic if they have similar language semantics. Moreover, a prompt is adversarial if it can trigger a T2I model to generate copyrighted content. Hence, the objective of our data generation pipeline is to systematically create non-sensitive adversarial prompts.",
            "score": 0.5276260106731785,
            "section_title": "Problem Formulation",
            "char_start_offset": 6237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2016
                },
                {
                    "start": 2019,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2309
                },
                {
                    "start": 2310,
                    "end": 2406
                },
                {
                    "start": 2407,
                    "end": 2522
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.345703125
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "In this paper, we introduce a new method for copyright protection called copyright authentication.Our framework, CGI-DM, validates the use of training samples featuring vivid visual representation, serving as a tool for digital copyright authentication.We start by removing part of the input image.Then, using Monte Carlo sampling and PGD, we exploit the differences between the pretrained and fine-tuned model to recover the removed information.A high similarity between the recovered samples and the original input samples suggests a potential infringement.Through experiments on WikiArt and Dreambooth datasets, we demonstrate CGI-DM's robustness and effectiveness, surpassing alternative approaches.Such experimental results show that CGI-DM is adept at providing legal evidence for art style mimicry and unauthorized image fabrication.In conclusion, CGI-DM not only offers robust method for infringement validation in the evolving DM landscape but also pioneers the application of gradient inversion in generative models.",
            "score": 0.5237661904905333,
            "section_title": "Conclusion",
            "char_start_offset": 24731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 559
                },
                {
                    "start": 559,
                    "end": 703
                },
                {
                    "start": 703,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 1026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2095947265625
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "Copyright Infringement Attack Scenario. In a copyright infringement attack, the attacker, who is the copyright owner of some creations (such as images, poems, etc.), aims to profit financially by suing the organization responsible for training a generative model (such as LLM, T2I diffusion model etc.) for copyright infringement. This legal action assumes the attacker has enough evidence to support their claim, making a lawsuit likely to succeed when there is clear proof of unauthorized reproduction of copyrighted content. A real-world example illustrating this scenario is the lawsuit filed by Getty Images against the AI art generator Stable Diffusion in the United States for copyright infringement (Vincent, 2023). \n\nTo further study this, we consider a specific scenario where the victim is the organization that trains text-to-image diffusion models. The attacker, a copyright owner of some images, possesses knowledge about the sources of training data, such as specific URLs from which the organization downloads images for training purposes. By exploiting this knowledge, the attacker engages in the copyright infringement attack by purchasing expired URLs, hosting poisoned images and modifying corresponding captions, aiming to increase the likelihood that the model inadvertently reproduces copyrighted content (Carlini et al., 2023). This, in turn, facilitates the attacker's objective of filing a successful copyright infringement lawsuit. To this end, the attacker is motivated to: \n\n\u2022 Perform the attack in stealth to avoid detection by the organization, preventing the organization from identifying and mitigating the model's vulnerability to attack before it is released and commercialized. \n\n\u2022 Select an image from which the attacker owns the copyright that is suitable for the attack method, to ensure the targeted diffusion model breaches copyright, such as one that are easily decomposable and recognizable by the model. \n\n\u2022 Try various prompts to cause the diffusion model to specifically reproduce the copyrighted image, and use the reproduced image as evidence in their lawsuit. \n\nDefining Copyright Infringement Attack. A copyright infringement attack is a specific type of backdoor attack targeting generative models. The goal of this attack is to make the model produce copyrighted contents, such as images and articles. In this work, we consider the specific setting: I. \n\nThe target model is a text-to-image diffusion model that has not been pretrained on copyrighted images, and II.",
            "score": 0.5233889619993112,
            "section_title": "Copyright Infringement Attack",
            "char_start_offset": 6647,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1501
                },
                {
                    "start": 1504,
                    "end": 1713
                },
                {
                    "start": 1716,
                    "end": 1947
                },
                {
                    "start": 1950,
                    "end": 2108
                },
                {
                    "start": 2111,
                    "end": 2150
                },
                {
                    "start": 2151,
                    "end": 2249
                },
                {
                    "start": 2250,
                    "end": 2353
                },
                {
                    "start": 2354,
                    "end": 2404
                },
                {
                    "start": 2407,
                    "end": 2518
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.580078125
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "Similarly, assuming and to the extent that a model is a copy of works in the training dataset, training a model is also direct infringement. Again, the model creators have chosen which datasets to include; they act at their own risk that those datasets include copyrighted material. The same applies to fine-tuning a model: the retrainer takes the risk that they are creating an infringing derivative work. A lack of knowledge as to what works are embedded in the model is no defense. Similarly, the choice to distribute a trained model is sufficiently volitional to count. \n\nThe analysis of generation is more complex. Where the same person supplies both the model and the prompt, they are a direct infringer. But when a model owner provides generation as a service to a user who supplies the prompt, the question is which of them is the direct infringer. \n\nBy analogy to the copy-shop and UGC service cases, it would seem that the user is the direct infringer and the service provider is the direct infringer. (Imagine, for example, a prompt for \"Elsa and Anna from Frozen. \") On this analysis, the service provider is probably not a vicarious infringer, because while they have the right and ability to control their model's outputs (e.g., by shutting it down or blocking images of Elsa), they do not have enough of a direct financial interest in specifically infringing uses of the service. 35 They are also typically not an inducing infringer, as they do not intend that the service be used to create infringing cartoons. 36 As for contributory infringement, the model is a material contribution, but they have only generalized knowledge (some users will make infringing art), not specific knowledge (some users will make art that infringes on Frozen). Thus, under Napster, they are not liable. A generation service becomes liable, however, when it has specific notice of an infringing work. Once Disney sends a notice to the service over the infringing Elsa output, the service now has the kind of knowledge that triggered liability in Napster and must therefore take steps to prevent similar future generations. 37",
            "score": 0.5211250344189516,
            "section_title": "D. Direct and Indirect Infringement",
            "char_start_offset": 32486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2121
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3076171875
        },
        {
            "corpus_id": "270562693",
            "title": "AI Royalties - an IP Framework to Compensate Artists & IP Holders for AI-Generated Content",
            "text": "As of now, limited frameworks have been proposed to compensate IP-holders in a future where content is AIgenerated -most are based on paying to access the data necessary to train AI systems, and none have found widespread adoption from the creative industries (Grimes 2023;Gapper 2023;O'Keefe et al. 2020).\n\nIn this article, we focus on the key question of how to compensate creatives and IP-holders in a world of AIgenerated content.\n\nWe first review the IP legal questions around input (training) and output (inference) of generative AI systems -focusing on whether creatives and IP-holders can claim compensation for copyright infringement under US law.This prompts us to propose, focusing on the case of image generation, a systemic method to evaluate whether or not an AIgenerated output violates previous copyright -and assess the performance of this metric (CLIP) on a series of copyright rulings.\n\nSubsequently, we document previously proposed compensation frameworks, and estimate their monetary outcomes for artists and IP holders.\n\nFinally, we propose a new IP framework based on officially \"licensed AIs\", viewed as a new format (and medium) for artistic creative expression, and a new asset from which creatives and IP-holders can collect AI royalties.We discuss the grounds (contractual and rights) for enforcing this new IP framework (extending copyright, trademark, and rights of publicity, to AI models).Artists and the wider creative industries earn revenue through the monetization of IPR -including royalties from copyright (creative works) and licensing related to rights of publicity and trademarks (World Intellectual Property Organization 2014;Rothman 2018).Generative AI has prompted a backlash of copyright infringement lawsuits from the creative industries, for producing art-like output and training on copyrighted material scrapped from the Internet, with neither consent nor compensation (Henderson et al. 2023;Getty Images v. Stability AI 2023;Coscarelli 2023;Andersen et al v. Stability AI 2023).Other creative industry figures have sought to proactively integrate AI into their IP strategies, creating digital replicas of celebrity likenesses and corresponding licensing schemes (Coffee 2023;Grimes 2023).",
            "score": 0.520613556910273,
            "section_title": "Introduction & Background",
            "char_start_offset": 2263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 306
                },
                {
                    "start": 308,
                    "end": 434
                },
                {
                    "start": 436,
                    "end": 656
                },
                {
                    "start": 656,
                    "end": 904
                },
                {
                    "start": 906,
                    "end": 1041
                },
                {
                    "start": 1043,
                    "end": 1265
                },
                {
                    "start": 1265,
                    "end": 1421
                },
                {
                    "start": 1421,
                    "end": 1682
                },
                {
                    "start": 1682,
                    "end": 2028
                },
                {
                    "start": 2028,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 305,
                    "matchedPaperCorpusId": "209500422"
                },
                {
                    "start": 1991,
                    "end": 2027,
                    "matchedPaperCorpusId": "257186666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17724609375
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "DALL\u2022E 3 blocks explicit name prompts requests (character name anchoring) with content policy messages. models may generate, copyrighted characters pose a unique legal challenge (Sag, 2023;Henderson et al., 2023;Lee et al., 2024), since characters are a type of \"abstractly\" protected content (Sag, 2023) Copyrighted characters also pose a unique technical challenge, as they are like general concepts that can appear in many poses, sizes, and variations, making it challenging to apply many previous methods designed for (near) verbatim memorization. As a result, commercial platforms like DALL\u2022E have started implementing tailored measures, such as prompt rewriting, with the goal of preventing the generation of copyrighted characters. \n\nHowever, two key questions regarding the infringement of copyrighted characters in generative models have not been systematically studied: (Q1) How easy is it to prompt models to generate copyrighted characters, especially when the copyrighted character is prompted indirectly or generated inadvertently? (Q2) How effective are common mitigation approaches at reducing copyrighted characters in image generation models? \n\nTo address these questions, we present novel evaluation metrics that measure both the generated image's resemblance to copyrighted characters and its consistency with user input. We use a diverse set of 50 popular characters from various studios and regions as the basis for our analysis. We use this framework to evaluate five image generation models -Playground v2.5 (Li et al., 2024a), Stable Diffusion XL (Podell et al., 2024), PixArt-\u03b1 (Chen et al., 2024), DeepFloyd IF (StabilityAI, 2023), and DALL\u2022E 3 (Betker et al., 2023) -along with one video generation model, VideoFusion (Luo et al., 2023), using a diverse set of popular copyrighted characters ( \u00a7 2). \n\nWe study Q1 using our evaluation framework, showing that as few as two generic keywords associated with a character are enough to generate their image, without mentioning their name (Fig. 1).",
            "score": 0.5166597549042999,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1509,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1160
                },
                {
                    "start": 1163,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1827
                },
                {
                    "start": 1830,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 1746,
                    "end": 1764,
                    "matchedPaperCorpusId": "257532642"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1619873046875
        },
        {
            "corpus_id": "259064265",
            "title": "DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection",
            "text": "In the case of art forgery, the goal of the proposed DeepfakeArt Challenge dataset is about the identification of copyright infringement, and as such it is important to set the context along with a formal definition. Specifically, generative AI models typically utilize vast amounts of training data to achieve strong content generation performance. However, many of the large-scale datasets used for training such as the LAION datasets Schuhmann et al. [2021] contain significant amounts of copyright-protected content. Hence, generative AI models trained on such datasets can produce synthetic content that contains or highly resembles copyrightprotected content Somepalli et al. [2022]. \n\nIn judicial proceedings, the \"substantial similarity\" test is employed to ascertain if a party has violated another party's copyright Asay [2022]. However, determining substantial similarity remains a subject of ongoing debate due to its inherently open-ended nature. While there's no precise formula, courts do follow certain criteria to arrive at a decision. In particular, the substantial similarity doctrine in copyright law necessitates that a court confirm the defendant's replication is both quantitatively and qualitatively similar enough to the original to warrant classification as infringement Balganesh et al. [2014]. As discussed in Balganesh et al. [2014], the jury evaluates the evidence based on several factors, including the creativity exhibited, the work's creation process, its intended purpose, the defendant's creative contributions and conduct, and the market impacts of the defendant's copying. \n\nBuilding on the principles inherent in the \"substantial similarity test\" used in legal proceedings, let us establish a pragmatic definition of copyright infringement as it applies to generative AI and as applied in the creation of the proposed DeepfakeArt Challenge dataset. This definition is rooted in the exploration and consideration of two foundational conditions. \n\nDefinition: Copyright Infringement in a Generative AI Model -Consider a generative model M : R H\u00d7W \u00d7C \u00d7 Q \u2192 R H \u2032 \u00d7W \u2032 \u00d7C \u2032 where Q could represent the set of any additional input to the generative model.",
            "score": 0.5158839217733426,
            "section_title": "Art Forgery",
            "char_start_offset": 5607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1610
                },
                {
                    "start": 1613,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1982
                },
                {
                    "start": 1985,
                    "end": 2189
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1719970703125
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "Recent years have witnessed the advancement of Diffusion Models (DMs) in computer vision.These models demonstrate exceptional capabilities across a diverse array of tasks, including image editing [14], and video editing [35], among others.Particularly, the emergence of few-shot generation techniques, exemplified by Dreambooth [24] and swiftly capturing the style or primary objects by fine-tuning a pretrained model on a small set of images.This process enables efficient and high-quality art imitation and art style transfer by utilizing the fine-tuned model.\n\nHowever, these advanced few-shot generation techniques also spark widespread concerns regarding the protection of copyright for human artworks and individual photographs.There is a growing fear that parties may exploit the generative capabilities of DMs to create and profit from derivative artworks based on existing artists' works, without obtaining proper authorization [6,19].Concurrently, concerns arise regarding the creation of fabricated images of individuals without their consent [32].All of these collectively pose a serious threat to the security of personal data and intellectual property rights.\n\nTo address these critical concerns, a line of approaches focuses on safeguarding individual images by incorporating adversarial attacks, such as AdvDM [17], Glaze [25], and Anti-Dreambooth [30].The adversarial attacks can disrupt the generative output, rendering the images unlearnable by diffusion models.These methods are implemented ahead of the fine-tuning process, and as such, we consider them as precaution approaches.\n\nAnother line of approaches facing such threats is copyright authentication.Copyright authentication compares the similarity between the output images of diffusion models and the given images to validate unauthorized usage.Such a process can serve as legal proof for validating infringement (See Appendix A for more details), and has been utilized as evidence in ongoing legal cases concerning violations enabled by DMs [31].This process happens after the fine-tuning and thus we consider it as a post-caution approach.However, current copyright authentication methods face difficulties in producing output images closely resembling training samples due to the pursuit of diversity in generative models.Consequently, it becomes difficult to ascertain whether a particular training sample has been utilized solely based on the generated output of the model for postcaution methods.",
            "score": 0.5157888727905893,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 443
                },
                {
                    "start": 443,
                    "end": 562
                },
                {
                    "start": 564,
                    "end": 734
                },
                {
                    "start": 734,
                    "end": 944
                },
                {
                    "start": 944,
                    "end": 1059
                },
                {
                    "start": 1059,
                    "end": 1173
                },
                {
                    "start": 1175,
                    "end": 1369
                },
                {
                    "start": 1369,
                    "end": 1481
                },
                {
                    "start": 1481,
                    "end": 1600
                },
                {
                    "start": 1602,
                    "end": 1677
                },
                {
                    "start": 1677,
                    "end": 1824
                },
                {
                    "start": 1824,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2120
                },
                {
                    "start": 2120,
                    "end": 2304
                },
                {
                    "start": 2304,
                    "end": 2481
                }
            ],
            "ref_mentions": [
                {
                    "start": 328,
                    "end": 332,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 1326,
                    "end": 1330,
                    "matchedPaperCorpusId": "256697414"
                },
                {
                    "start": 1364,
                    "end": 1368,
                    "matchedPaperCorpusId": "257766375"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38623046875
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "In order to identify instances of infringement in these contexts, a comprehensive analysis encompassing both technical and semantic aspects of the generated contents is indispensable.\n\nIn light of the aforementioned considerations, we introduce the first comprehensive dataset tailored for copyright protection in this domain: Copyright Protection from Diffusion Model (CPDM) dataset.Specifically, we have curated copyright images from four distinct categories that are most commonly suspected of infringement, as demonstrated in Fig. 2.This dataset encompasses a collection of original copyright images, associated prompts for text-to-image generation via Stable Diffusion, and indicatives for a series of features, represented by: 1) Potential for Infringement; 2) Effectiveness of Unlearning; 3) Extent of Model Degradation during Unlearning.\n\nPotential for Infringement.This is quantified by feature-level similarities between the original copyright images and potentially infringing generated contents, denoted as CPDM metric (CM).",
            "score": 0.5152470825630646,
            "section_title": "Introduction",
            "char_start_offset": 2456,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 185,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 845
                },
                {
                    "start": 847,
                    "end": 874
                },
                {
                    "start": 874,
                    "end": 1036
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.273193359375
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Diffusion models are undergoing a rapid development phase, and recently proposed diffusion models have shown improved performance in generating high-quality images. However, whether their performance has improved for copyright protection remains questionable. In this section, we investigate diffusion models in this regard. \n\nOur case study examines how a diverse pool of diffusion models react to varying subjects. For this purpose, we leverage the experiment setup in the previous section. Specifically, we obtain proportions of images with identified copyrighted content for each topic and compare them across different diffusion models (Figure 10, more in Appendix D). While the performance of each model varies for each topic, it is clear that all models exhibit copyright-infringing behavior for the five distinct topics. Even though the latest model SD XL shows a slight decrease in generating copyrighted content, the rate generating copyrighted content remains above 50% for many topics. This suggests that the current approach to training diffusion models remains ineffective in preventing the occurrence of copyright infringement.",
            "score": 0.5142938674346483,
            "section_title": "Case Study: Copyright Issue Across Diffusion Models",
            "char_start_offset": 28942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1142
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26904296875
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "In recent years, the advancement of large generative models [17,47,50] has revolutionized high-quality image synthesis [34,37,40], paving the way for commercial applications that enable the public to effortlessly craft their own artworks and designs [2,13,20,27,38,39]. Nevertheless, these models exhibit notable memorization capabilities to produce generations highly similar to the training data [3]. This resemblance raises growing concerns about copyright infringement, especially when copyrighted data is used for training [12,16,49,52]. \n\nTo address these concerns, there has been a surge in research focused on protecting copyrighted data from potential infringement by outputs of generative models [14, 21, Figure 2. Example outputs given the copyright image in Fig. 1 as target (potential infringing images are marked with red boundaries). In (a), using a benign prompt, we observe a high incidence of infringing content from models without copyright protection (\"w/o CP-k\"). In contrast, (b) shows that after applying the copyright protection mechanism (\"w/ CP-k\"), all samples are safe as CP-k rejects all infringing content. In (c), we find that amplification (Amp.) attack with a benign prompt results in limited success. Notably, by amplification attack with an adversarial prompt obtained from our proposed Anti-NAF algorithm, almost all output in (d) are copyright-infringed. 24,43,45,52,58]. Among these studies, a pivotal concept involves establishing a probabilistic upper-bound against the generation of infringing content by generative models. We refer to this suite of approaches as probabilistic copyright protection. Most notably, Vyas et al. [52] introduce a mathematical definition of copyright known as near-access freeness (NAF). Their method enforces generative diffusion models to exhibit akin behaviors as safe models, which has no access to the copyrighted image. By leveraging the improbability of safe models generating infringing content, the probability of generative models doing the same is thereby substantially reduced.",
            "score": 0.5138884862643556,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 67,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 67,
                    "end": 70,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 123,
                    "end": 126,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 126,
                    "end": 129,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 250,
                    "end": 253,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "251253049"
                },
                {
                    "start": 256,
                    "end": 259,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 259,
                    "end": 262,
                    "matchedPaperCorpusId": "246240274"
                },
                {
                    "start": 262,
                    "end": 265,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 398,
                    "end": 401,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 538,
                    "end": 541,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 1392,
                    "end": 1395,
                    "matchedPaperCorpusId": "256697414"
                },
                {
                    "start": 1395,
                    "end": 1398,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 1401,
                    "end": 1404,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 1667,
                    "end": 1671,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5654296875
        },
        {
            "corpus_id": "277502009",
            "title": "Who Owns the Output? Bridging Law and Technology in LLMs Attribution",
            "text": "These cases highlight the growing legal scrutiny of AI's reliance on copyrighted materials [15]. \n\nThese developments indicate a global trend where content creators are increasingly challenging AI companies over the use of copyrighted material in training models. The outcomes of these cases could significantly impact how AI companies access and use data, potentially leading to more stringent regulations and the necessity for explicit licensing agreements. \n\nThe DMCA's provisions against unauthorized CMI removal or alteration are crucial in AI applications where outputs might inadvertently strip metadata or alter original digital watermarks, potentially leading to liability. Violations of these provisions can result in both civil and criminal penalties, especially in cases where CMI manipulation is intentional. Liability may extend to AI models that systematically disregard the CMI attached to training data, creating calls for strict DMCA compliance to mitigate infringement risks [29,33]. Ongoing legal cases underscore the generative AI industry's exposure to copyright claims. Lawsuits against OpenAI, for example, include allegations that their models generate outputs resembling copyrighted material without proper attribution, which could impact the original creators' markets. Plaintiffs argue that such outputs constitute unauthorized reproductions, advocating for \"notice-and-takedown\" mechanisms under the DMCA specifically adapted to AI-generated content as a means to manage infringement concerns [30,32]. \n\nIndustry experts recommend combining technical and legal mitigation strategies in response to these issues. AI developers are advised to implement technical solutions such as filtering outputs for close resemblances to training data or incorporating watermark detection to maintain CMI integrity. Additionally, some legal scholars propose AI-specific safe harbour provisions under the DMCA to protect models that demonstrate robust copyright compliance. Creating industry-wide guidelines could help ensure that AI-generated content adheres to copyright law, balancing technological innovation with the rights of original creators [31,33]. As generative AI continues to integrate into commercial and creative industries, nuanced legal frameworks that address CMI protection, fair use, and liability are increasingly necessary. As legal precedents develop, finding a balance between copyright compliance and innovation will remain critical. Although the DMCA was initially designed for simpler digital environments, it now offers relevant tools to address the unique challenges posed by generative AI technologies and the extensive use of copyrighted data to train these models [29,33,28].",
            "score": 0.512818459038576,
            "section_title": "Legal Framework",
            "char_start_offset": 40277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 99,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2171
                },
                {
                    "start": 2172,
                    "end": 2358
                },
                {
                    "start": 2359,
                    "end": 2471
                },
                {
                    "start": 2472,
                    "end": 2720
                }
            ],
            "ref_mentions": [
                {
                    "start": 994,
                    "end": 998,
                    "matchedPaperCorpusId": "269588252"
                },
                {
                    "start": 998,
                    "end": 1001,
                    "matchedPaperCorpusId": "258515384"
                },
                {
                    "start": 1526,
                    "end": 1529,
                    "matchedPaperCorpusId": "261375096"
                },
                {
                    "start": 2163,
                    "end": 2167,
                    "matchedPaperCorpusId": "257771630"
                },
                {
                    "start": 2167,
                    "end": 2170,
                    "matchedPaperCorpusId": "258515384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34912109375
        },
        {
            "corpus_id": "267400526",
            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
            "text": "Based on a thorough examination of various critical factors contributing to the infringement of human artists' works by artificial intelligence-generated images, the researchers conducted a more focused and in-depth analysis to address this pressing issue. \n\nAmong the primary concerns is the unauthorized use of copyrighted works. To vividly illustrate this problem, the researchers used the example of Mickey Mouse and inputted the \"Mickey Mouse, cooking\" prompt into the Stable Diffusion algorithm (Fig. 3). As we are well aware, Disney's legal department is known for its stringent control and heightened sensitivity towards protecting the image of Mickey Mouse [6]. Despite this, Stable Diffusion generated ideas that utilized Mickey Mouse's likeness for secondary creation. Additionally, the vast image databases used to train AI models present another challenge to avoiding copyright infringement. Since the training data is sourced from the Internet, it isn't easy to ensure that only authorized and copyrighted images are used. This, in turn, increases the risk of unauthorized use of sensitive copyright material, thereby exacerbating the potential for infringement. This instance highlights a significant challenge: even renowned companies with well-equipped legal teams and substantial financial resources can struggle to safeguard their copyright rights. Consequently, for many unknown artists and individuals engaged in creative work, encountering copyright infringement often leaves them feeling helpless and lacking adequate protection for their artistic creations.",
            "score": 0.5116558358754658,
            "section_title": "Use of copyrighted works or IP without permission",
            "char_start_offset": 15530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 259,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1581
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.388671875
        },
        {
            "corpus_id": "270562693",
            "title": "AI Royalties - an IP Framework to Compensate Artists & IP Holders for AI-Generated Content",
            "text": "Other creative industry figures have sought to proactively integrate AI into their IP strategies, creating digital replicas of celebrity likenesses and corresponding licensing schemes (Coffee 2023;Grimes 2023).Academic and industry discussion is centered on two areas: foundation model training -the \"input\" -and whether it constitutes fair use or copyright infringement, (Henderson et al. 2023;Epstein et al. 2023 Concerning our key question of compensation for AIgenerated content, if training an AI system on data constitutes a violation of IP rights, then the IP-holders can claim compensation -this is the approach taken by the recent wave of lawsuits against the use of the LAION dataset and open image-generating AI systems (such as Microsoft & OpenAI (Metz 2022), Meta (Plaintiffs Richard Kadrey, Sarah Silverman, and Christopher Golden v. Meta Platforms, Inc. 2023), Stability.ai(Getty Images v. Stability AI 2023), Midjourney (Plaintiffs Sarah Andersen, Kelly McKernan, and Karla Ortiz v. Stability AI Ltd., Stability AI Inc., Midjourney Inc., and DeviantArt Inc. 2023)).Some proposed compensation schemes studied in section 3 rely on compensation for training, somewhat resting on the assumption that training is not fair use.New registration guidance released in March 2023 reflects the U.S. Copyright Office's increasingly nuanced stance towards AI authorship -suggesting that there will be more paths toward extending some IPR to AI output (U.S.Copyright Office 2023).\n\nTwo key unanswered questions emerge from this discussion:\n\n1. Does an AI-generated output infringe someone's IPRs (in particular copyright)? 2. If the output does not infringe IPRs, what framework could compensate IP-holders for inspiring AI-generated content, and on what grounds could it be enforced?(we propose AI publishers and AI royalties in section ??).\n\nWe propose a systematic AI method to evaluate whether an AI-generated output infringes previous IP rights, and assess its performance on a series of rulings.",
            "score": 0.508798790243955,
            "section_title": "Introduction & Background",
            "char_start_offset": 4291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 210,
                    "end": 888
                },
                {
                    "start": 888,
                    "end": 1081
                },
                {
                    "start": 1081,
                    "end": 1237
                },
                {
                    "start": 1237,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1482
                },
                {
                    "start": 1484,
                    "end": 1541
                },
                {
                    "start": 1543,
                    "end": 1786
                },
                {
                    "start": 1786,
                    "end": 1844
                },
                {
                    "start": 1846,
                    "end": 2003
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.119384765625
        },
        {
            "corpus_id": "278534729",
            "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art",
            "text": "Deep Fake art generative models are vulnerable to violating copyright terms by producing images that mimic or closely resemble content protected under copyright [10]. A formal mathematical formulation of copyright infringement in this context is introduced in [4]. For clarity and contextual relevance, we present a simplified version here: \n\nHere, y denotes the generated image and x is a potentially copyright-protected training image. The operator T (\u2022) represents a geometric transformation (e.g., resizing, flipping, rotation), and \u2126 is a region of significant size within the image. The function A(\u2022) defines the domain of representation-either raw pixel space or an edge-based representation. The term f (|\u2126|) is a monotonic function adjusting sensitivity based on region size, and \u03b4 is a fixed similarity threshold. Infringement is said to occur if the distance in the representation space for any region \u2126 falls below this threshold.",
            "score": 0.5082394850229952,
            "section_title": "Copyright Infringement in Art",
            "char_start_offset": 5336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 942
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.441162109375
        },
        {
            "corpus_id": "268049158",
            "title": "Break It 'Til You Make It: An Exploration of the Ramifications of Copyright Liability Under a Pre-training Paradigm of AI Development",
            "text": "Simply scraping an image or building a technical system that creates an output that draws from a copyrighted work might not automatically trigger copyright infringement11 .Rather, it is the causal process of copying between these events that gives rise to potential liability.By introducing multiple training processes, intermediary developers, and additional network layers, a pre-training paradigm of machine learning challenges the legal conceptualization of this causal process.It can render both the responsibility of those involved as well as the technical link between data and output tenuous.In this paper, I outline potential ramifications copyright liability under a pre-training paradigm of AI development.First, I evaluate pre-trained models under currently proposed judicial tests for direct infringement in the use of copyrighted works for the training process.I show how these tests rely heavily on a conceptualization of ML systems and the development process as a single pipeline with one model and one application.A pre-training paradigm breaks this presumption and challenges these tests with the introduction of a single training process informing a multitude of downstream applications.Given the role of pre-trained models in facilitating the development of downstream models, I analyze a pre-training paradigm under a indirect copyright liability analysis.I discuss why indirect liability can be difficult to establish and discuss lessons for pre-training based on how judicial tests for indirect copyright liability have been subverted in the past.Finally, in light of these strategies, I discuss duties of care as an approach to regulating the developers of pre-trained models.\n\nI conclude with a few remarks on the limitations of our analysis and on implications for AI policy as a whole.The judicial application of copyright law does not have to be all-or-nothing -injunction or the status quo.Ginsburg [2001] notes that \"the copyright statute and the judges who interpret it attempt a balance: Creators should maintain sufficient control over new markets to keep the copyright incentive meaningful, but not so much as to stifle the spread of the new technologies of dissemination.\"Perhaps, access to emerging generative models and AI technologies is a public benefit.The harm does not so much stem from the existence of these technologies but the infringements that occur from the development or the use of these technologies.",
            "score": 0.5046786308909105,
            "section_title": "CONCLUSION AND AI POLICY IMPLICATIONS",
            "char_start_offset": 36345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 276
                },
                {
                    "start": 276,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 600
                },
                {
                    "start": 600,
                    "end": 717
                },
                {
                    "start": 717,
                    "end": 875
                },
                {
                    "start": 875,
                    "end": 1032
                },
                {
                    "start": 1032,
                    "end": 1207
                },
                {
                    "start": 1207,
                    "end": 1378
                },
                {
                    "start": 1378,
                    "end": 1571
                },
                {
                    "start": 1571,
                    "end": 1701
                },
                {
                    "start": 1703,
                    "end": 1813
                },
                {
                    "start": 1813,
                    "end": 1920
                },
                {
                    "start": 1920,
                    "end": 2208
                },
                {
                    "start": 2208,
                    "end": 2294
                },
                {
                    "start": 2294,
                    "end": 2453
                }
            ],
            "ref_mentions": [
                {
                    "start": 1920,
                    "end": 1935,
                    "matchedPaperCorpusId": "152648739"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.325927734375
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "In this paper, we presented a Reinforcement Learning-based Copyright Protection (RLCP) for copyright infringement in text-to-image diffusion model. RLCP proposes a copyright loss metric that mirrors legal tests used to assess substantial similarity, and then integrates this metric into a reinforcement learning framework for model fine-tuning, and the use of KL divergence to regularize and stabilize the model training process. Experiments conducted on three mixed datasets of copyright and non-copyright images show that RLCP significantly reduces the likelihood of generating infringing content while preserving the visual quality of the generated images. Our results demonstrate that balancing the proportion of copyrighted and non-copyrighted data in the training set is crucial for minimizing copyright infringement without compromising image quality. We also showed that the reward-driven RL framework effectively fine-tunes diffusion models by optimizing for both copyright compliance and data fidelity. \n\nWhile our approach demonstrates promising results, there are several areas for future work: (1) Broader Application Domains: Future work could extend RLCP to other domains beyond image generation, such as text or audio generation models, where copyright concerns are equally prevalent. (2) Dynamic Dataset Management: Investigating adaptive or dynamic dataset augmentation strategies could be beneficial. As models encounter more copyright-protected data, dynamically adjusting the training process may lead to more robust copyright protection without overfitting to specific datasets.",
            "score": 0.5026879327747424,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 23145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1600
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51025390625
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "In this paper, we adopt a simple algorithm to disguise images, which we realize may be applied as a concealed copyright infringement tool.We are surprised at how easy it is for Generative AI to circumvent the copyright law, which was originally designed for regulating human scrapers.This seemingly undetectable infringing threat may bring disadvantages for human artists, who are already losing the battle with GenAI.Furthermore, disguised images may not only be a concern to copyright owners but also a threat to Generative AI companies: if these disguises are accidentally collected as part of the training data, copyright infringement could be triggered unconsciously and unwillingly.As computer scientists, we are obligated to reveal the existence of such possible unlawful acts and provide technical tools to identify such behaviors.We expect our paper to be a prudent tool such that disguised copyright infringement and its corresponding detection methods are recognized by law experts.\n\nconference on computer vision and pattern recognition, pp.10684-10695.",
            "score": 0.5026404144357977,
            "section_title": "Impact Statement",
            "char_start_offset": 27706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 138,
                    "end": 284
                },
                {
                    "start": 284,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 839
                },
                {
                    "start": 839,
                    "end": 993
                },
                {
                    "start": 995,
                    "end": 1053
                },
                {
                    "start": 1053,
                    "end": 1065
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.305419921875
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "Text-to-image generative models have recently emerged as a significant topic in computer vision, demonstrating remarkable results in the area of generative modeling [14,35].These models bridge the gap between language and visual contents by generating realistic images from textual descriptions.However, rapid advancements in text-to-image generation techniques have raised concerns about copyright protection, particularly unauthorized reproduction of content, artistic creations, and portraits [4].A specific concern arises from the use of Stable Diffusion (SD), a state-of-the-art text-conditional latent diffusion model, which has sparked global discussions on copyright.\n\nMachine Unlearning (MU), which aims to eliminate the influence of specific target data or concepts, presents a promising solution to the aforementioned challenges.Several studies have provided intuitive evidence of MU's effectiveness in erasing the memory of copyrighted material from original models [42,7,26,8].However, the current body of research is limited by a lack of quantitative and systematic assessments that evaluate the extent to which MU can reduce the risk of copyright infringement.This limitation hinders the ability to make meaningful comparisons between existing MU approaches.The challenge is exacerbated by the inherent complexity of defining copyright infringement criteria for text-to-image generative models, as well as the scarcity of comprehensive inference datasets and standardized benchmarks for assessing copyright infringement.The absence of extensive copyright datasets obstructs researchers' efforts to fully comprehend the copyright infringement risks associated with generative models.This, in turn, restricts their capacity to develop superior MU algorithms capable of addressing the legal risks effectively.As shown in Fig. 1, datasets and benchmarks are essential for forgetting copyrighted content and evaluating unlearning methods.\n\nInitially, it is crucial to define what constitutes copyright infringement in contents produced by text-to-image generative models [38].In this study, we focus on infringement within 2D artistic works.Drawing on expertise from copyright protection specialists, including artists and lawyers, we contend that a unique painting style of an artist, virtual representations in artistic creations, and individual portraits all represent forms of creative expression deserving of legal protection.In order to identify instances of infringement in these contexts, a comprehensive analysis encompassing both technical and semantic aspects of the generated contents is indispensable.",
            "score": 0.5023983906513103,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 173,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 500
                },
                {
                    "start": 500,
                    "end": 675
                },
                {
                    "start": 677,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1175
                },
                {
                    "start": 1175,
                    "end": 1273
                },
                {
                    "start": 1273,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1697
                },
                {
                    "start": 1697,
                    "end": 1821
                },
                {
                    "start": 1821,
                    "end": 1948
                },
                {
                    "start": 1950,
                    "end": 2086
                },
                {
                    "start": 2086,
                    "end": 2151
                },
                {
                    "start": 2151,
                    "end": 2441
                },
                {
                    "start": 2441,
                    "end": 2624
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "1033682"
                },
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 984,
                    "end": 987,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 987,
                    "end": 989,
                    "matchedPaperCorpusId": "261276613"
                },
                {
                    "start": 2081,
                    "end": 2085,
                    "matchedPaperCorpusId": "254366634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.313720703125
        },
        {
            "corpus_id": "271064292",
            "title": "CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation",
            "text": "In this section, we review copyright law and relevant court cases on copyright infringement (Section 2.1), as well as prior work in AI on benchmarking and mitigating copyright risks (Section 2.2). We highlight the gap between real-world legal risks and the current research aimed at addressing potential copyright issues. \n\nCopyright issues can be associated with each component of the generative-AI supply chain (Lee et al., 2023b), including data collection (Min et al., 2023;Shi et al., 2023;Chang et al., 2023;Karamolegkou et al., 2023), model training (Vyas et al., 2023), and generation and deployment (Meeus et al., 2024;Ippolito et al., 2023). Our work focuses on the infringement risks in LMgenerated content, although other stages may also present infringement risks even if the outputs do not infringe.",
            "score": 0.5006360873781035,
            "section_title": "Background",
            "char_start_offset": 5113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 813
                }
            ],
            "ref_mentions": [
                {
                    "start": 557,
                    "end": 576,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2191162109375
        },
        {
            "corpus_id": "270258236",
            "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
            "text": "We introduced a method to genericize the output of generative models, thereby reducing the risk of copyright infringement. We further proposed PREGen, a practical algorithm for mitigating copyright risks, which combines our genericization method with prompt rewriting. Our method leverages the principle that the level of originality of works determines the strength of their copyright protection, as well as the inherent capability of generative models to learn the distribution of training data. By evaluating the performance of PREGen using the COPYCAT suite, we have shown that PREGen significantly enhances the performance of the standard prompt rewriting method. However, this improvement comes with a trade-off: PREGen requires additional computation to generate multiple samples, most of which are ultimately discarded, along with rewritten prompts. Additionally, the fine-grained consistency with the original prompt may be compromised. \n\nOur work has certain limitations in its scope. While the general framework for originality estimation and genericization is broadly applicable, we have focused on the generation of copyrighted characters using text-to-image generative models. Future research can test our method on the generation of other types of materials and the use of different generative models, such as those for text and video, and investigate appropriate distance metrics and their effectiveness. Another consideration is the potential for the genericization process to amplify undesirable patterns in the generative model's output distribution. Specifically, multiple samples generated during the genericization process might disproportionately represent certain demographics or cultural elements. The resulting generic output, which is, in a sense, the median expression of these patterns, could unintentionally reinforce such biases. These risks should be carefully evaluated in future research.",
            "score": 0.49819380211864606,
            "section_title": "Discussion",
            "char_start_offset": 31498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1922
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.448486328125
        },
        {
            "corpus_id": "270258236",
            "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
            "text": "Generative models have demonstrated performance rivaling humans in creative tasks such as those involving image synthesis and language processing. 1,2 However, this progress has also raised concerns about copyright protection, leading to numerous lawsuits filed by creators against AI developers. 3 Copyright law protects creators' rights, encouraging new creations while balancing their interests with those of the public. 4 Generative models are trained on massive datasets that often contain copyrighted works and are capable of producing outputs closely resembling their training data, potentially resulting in violation of exclusive rights of copyright owners. 5 ][9][10] Although some methods have been found effective to some extent, significant risks of copyright infringement remains. 6,7 n this paper, we propose a method for quantifying the level of originality and modifying the outputs of generative models to those that have lower originality values. These modified outputs are more generic and less likely to imitate distinctive features of copyrighted materials. As a practical algorithm for mitigating copyright risks, we introduce PREGen (Prompt Rewriting-Enhanced Genericization), which combines this genericization method with the existing prompt rewriting method, 7 adopted by a major commercial model DALL\u2022E. 7,11 The effectiveness of prompt rewriting is enhanced by further adding a negative prompt that instructs the model not to generate specific content, 7 an element that is also incorporated into PREGen. \n\nWe demonstrate that PREGen significantly improves the performance of prompt rewriting accompanied by negative prompting (we refer to this as the standard method when there is no ambiguity) in reducing the likelihood of generating images of copyrighted characters. In particular, experiments using the COPYCAT benchmark 7 show that PREGen reduces the likelihood of text-to-image generative models generating copyrighted characters by more than half compared to the standard method, when the user provides copyrighted characters' names as prompts. Furthermore, when the prompt does not directly reference copyrighted characters, PREGen almost completely eliminates the generation of these characters.",
            "score": 0.4974993268537015,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2233
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 149,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 297,
                    "end": 298,
                    "matchedPaperCorpusId": "259844568"
                },
                {
                    "start": 424,
                    "end": 425,
                    "matchedPaperCorpusId": "10463592"
                },
                {
                    "start": 666,
                    "end": 667,
                    "matchedPaperCorpusId": "258515384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51171875
        },
        {
            "corpus_id": "259501495",
            "title": "Measuring the Success of Diffusion Models at Imitating Human Artists",
            "text": "(Balganesh et al., 2014;Kaminski & Rub, 2017;Balagopalan et al., 2023). In evaluating copyright questions involving AI systems, legal analyses have focused on how copyrighted work is used in the system's training data (Sag, 2018;Lemley & Casey, 2020), but such a focus on training data does not connect liability to an AI system's ability to copy an artist. In contrast, we show how standard image classification techniques can be used to help determine how successful AI image generators are at imitating individual human artists. This approach is consistent, quantitative, and connected to the capabilities of the resulting AI system. Our goal, however, is not to automate determinations of infringement but to demonstrate how tried and tested image classification techniques from machine learning can be used to analyze legal claims.",
            "score": 0.4971124002446651,
            "section_title": "body",
            "char_start_offset": 1864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 836
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 45,
                    "matchedPaperCorpusId": "157499030"
                },
                {
                    "start": 45,
                    "end": 70,
                    "matchedPaperCorpusId": "258615890"
                },
                {
                    "start": 218,
                    "end": 229,
                    "matchedPaperCorpusId": "86424284"
                },
                {
                    "start": 229,
                    "end": 250,
                    "matchedPaperCorpusId": "219342558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.228271484375
        },
        {
            "corpus_id": "257771630",
            "title": "Foundation Models and Fair Use",
            "text": "of all the training data and the likelihood of verbatim (or significantly similar) extraction can reduce with the amount of training data. For example, Somepalli et al. (2022) found that extraction of training data from diffusion models is less likely if there is more diverse training data.\n\nWhen the model is capable of outputting both transformative and non-transformative content, it is also unresolved how the model itself (and model creators) should be treated as a function of secondary liability. The extraction of non-transformative content, according to our experiments and others, is often not straightforward. It requires effort on the part of model users and deployers to identify an extraction mechanism. If this is the case, one might instead argue that remedies should be limited to specific instances of extracted non-transformative content, not the model as a whole, which does not generate infringing output in the ordinary case. The model creator might also be insulated from liability on other fair use factors. For example, if they released the model under a non-commercial license and actively prevented its use for commercial purposes, they might argue that the nature of their model was non-commercial, increasing the likelihood of a fair use defense at this part of the liability chain. 24 As with other issues in this work all of this is actively being litigated and will be shaped over the coming years.\n\nDMCA Safe Harbor. The Digital Millennium Copyright Act (\"DMCA\") is a U.S. law created to address digital copyright issues that came about with the advancement of technology. The DMCA safe harbor provisions protect online service providers from legal responsibility for copyright infringement claims. DMCA protections might vary depending on a number of considerations, but we emphasize that they are not guaranteed for all model deployments. We examine several of these considerations here.\n\nDMCA protections for generative foundation models are uncertain. At first glance, it may seem like the Digital Millennium Copyright Act (DMCA) would protect machine learning model hosts. Like in other hosted sites, they would need to meet the relevant requirements like using a registered agent under DMCA \u00a7512(c)(2). Then they could put up a take-down request form and add filters for the offending model output when served with a take-down request under the",
            "score": 0.49690067721206316,
            "section_title": "Implied Licenses and Common Crawl.",
            "char_start_offset": 70835,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1121826171875
        },
        {
            "corpus_id": "265220935",
            "title": "Trustworthy Large Models in Vision: A Survey",
            "text": "Challenge. In order to generate high-quality content, LMs require substantial quantities of training data, which may often involve information protected by copyright and poses a threat to intellectual property rights [174]. For instance, textual inversion [99], a novel method based on the latent diffusion model, can imitate the art style of human-created paintings with several samples, which extremely infringes on the intellectual property of artists [175]. Furthermore, LMs like DALL\u2022E 2 [16] is Output trademarked company logos [35] Generate images of white men with positive words by default [28] trained on billions of image-text pairs from the Internet, the size of which is too large to check whether each containing pair is authorized or not. OpenAI researchers also found that DALL\u2022E 2 occasionally replicated training images, as the training dataset contained a substantial number of duplicated images [40]. What's even worse is that, due to the memorization ability of LMs [35][36][37][38][39], users are able to directly extract copyright-protected content from LMs and cause copyright infringement [40]. \n\nCountermeasure. Many works have emerged to alleviate the problem of copyright infringement in LMs. For instance, OpenAI researchers use K-Means clustering [176] to remove simple and duplicate images before pretraining and achieved better results in human evaluation [40]. From the perspective of training, models can also be encouraged to generate novel images, which are far away from original images in the training dataset [177], while deleting repeated images reduces the size of training dataset. In addition, adversarial attack is also a notable method for protecting the copyright of images. Specifically, perturbations are added to target images to safeguard data against unauthorized training, disrupting the model's ability to learn the corresponding representations, and has achieved significant results [175,[178][179][180][181][182][183][184]. Interestingly, for the sake of protecting copyrighted artistic styles, model fine-tuning enables DMs to erase specific artistic styles in generated outputs [89,90,185]. \n\nDiscussion.",
            "score": 0.49548821750651273,
            "section_title": "Copyright",
            "char_start_offset": 24965,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2147
                },
                {
                    "start": 2150,
                    "end": 2161
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 222,
                    "matchedPaperCorpusId": "261375096"
                },
                {
                    "start": 534,
                    "end": 538,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 987,
                    "end": 991,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 995,
                    "end": 999,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 1957,
                    "end": 1962,
                    "matchedPaperCorpusId": "257901245"
                },
                {
                    "start": 1967,
                    "end": 1972,
                    "matchedPaperCorpusId": "257766375"
                },
                {
                    "start": 2142,
                    "end": 2146,
                    "matchedPaperCorpusId": "257687839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41552734375
        },
        {
            "corpus_id": "278338968",
            "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
            "text": "We evaluate the effectiveness of prompting strategies in reducing the risk of IP infringement of visual GenAI by 3 metrics: similarity scores to training images, relevance and aesthetic value. Overall, we find that prompt engineering can reduce copyright-related risks in visual GenAI models, but its effectiveness varies depending on the chosen technique. Chain-of-thought prompting proved to be the most effective in IP risk mitigation. Negative prompting was the least effective strategy, while task instruction prompting yielded promising results in preventing training data replication while balancing this achievement with high aesthetic scores and strong prompt alignment. These strategies are particularly important given the legal uncertainty surrounding AI-generated content. We hope this work contributes to safer deployment practices and informs future standards for responsible visual generative AI use.",
            "score": 0.49256746292749964,
            "section_title": "Conclusions",
            "char_start_offset": 20488,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 916
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63134765625
        },
        {
            "corpus_id": "267400526",
            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
            "text": "After exploring how artificial intelligence (AI)-generated platforms work, learn what processes may involve human artist infringement in the course of their work. So let's talk about where these workflows infringe on human artists' copyrighted works -Artificial intelligence (AI)-generated content raises concerns about copyright infringement of human artist works in several ways. \n\nThe researchers summarized three aspects of human artist infringement: model training using only copyrighted works without permission. Take Stable Diffusion, which has been involved in many copyright disputes. It uses the image database containing hundreds of millions-LAION-5B (Kaixin Zhu, & Yiqun Zhang. ( 2023)) as the source of training data, and it does not need the consent of the copyright owner [5]. \n\nSecond, there is a lot of plagiarism and adaptation of copyrighted works in AI-generated images. In a case in which an artist sued an AI-generated platform, the painter Erin Hanson provided strong evidence that the AI-generated images plagiarized his work. And there are many such lawsuits now, not a few. Third, the images generated by the AI generation platform, such as Stable Diffusion, if works infringe copyright works, do not have any watermarks or source marks, making it even more difficult for copyright owners to accept. Unlike artificial intelligence, most human artists will mark the original author or source when learning to learn from other people's works so as not to infringe the copyright of others. Artificial intelligence often regards works as different data, lacking awe and moral measurement for works. \n\nAll in all, during the generation process of the artificial intelligence generation platform (take Stable Diffusion as an example), it may infringe on the works of human artists from two aspects of model training and image generation, and the generated results are different in three dimensions. Infringements are caused to a certain extent, including unauthorized use, excessive plagiarism and adaptation, and lack of attribution to the original author of the copyrighted work. These are the key factors that constitute the infringement of human artists.",
            "score": 0.49175172412169094,
            "section_title": "From what aspects does Stable Diffusion infringe the copyright of human artists",
            "char_start_offset": 13286,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2178
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275146484375
        },
        {
            "corpus_id": "256697414",
            "title": "Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples",
            "text": "In this section, we would like to discuss some ethical issues about state-of-the-art AI-for-Art applications based on generative AI and what role our work is expected to play in these issues. \n\nAI-for-Art applications powered by diffusion models have reshaped the art market by significantly lowering the threshold for artistic creation. However, hidden behind such progress are unresolved copyright issues. \n\nUsing copyright-protected training data without the consent of image owners may constitute unauthorized reproduction and distribution, thereby giving rise to copyright infringement liability. One primary source of training data is LAION (Schuhmann et al., 2021;2022), a large-scale dataset of training images with text captions. A large portion of images in LAION was scraped from commercial image-hosting websites without the consent of the image owners (Carr & Jeffrey, 2022). The same issues exist in other generating processes involving unlicensed artworks, for example, learning paintings of a particular artist based on functions of AI-for-Art on a smaller scale without authorization. \n\nCopyright law protects authors' exclusive rights to reproduce, distribute, perform and display the artworks (Franceschelli & Musolesi, 2022). This legal structure makes it highly possible to constitute infringement by using others' artwork without a copyright license in the digital age (Sullivan, 1996). Throughout the AI-for-Art process, the transfer of unauthorized artworks from the platform on which it was originally published to AI's database along with the sale or distribution of the program including such database may constitute reproduction and distribution of the original artwork. This is related to the mechanism AI-for-Art applications created artworks. AI-for-Art applications work by fitting the training images and in turn recombining the learned data to generate new images, which may be understood as a special kind of reproduction. For some artworks with distinct well-known features, for example, cartoon figures owned by Disney, this reproduction is easy to detect (Baio, 2022). For this reason, the plaintiff lawyer representing artists whose works were used to train these generative AI tools referred to Diffusion Models as \"21stcentury collage tools\" in the recent lawsuit against several companies profiting from Stable Diffusion (Carr & Jeffrey, 2022).",
            "score": 0.49133178604161987,
            "section_title": "A. Ethical Issues",
            "char_start_offset": 23269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 407
                },
                {
                    "start": 410,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2386
                }
            ],
            "ref_mentions": [
                {
                    "start": 1391,
                    "end": 1407,
                    "matchedPaperCorpusId": "143768780"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.283203125
        },
        {
            "corpus_id": "268049158",
            "title": "Break It 'Til You Make It: An Exploration of the Ramifications of Copyright Liability Under a Pre-training Paradigm of AI Development",
            "text": "This paper considers the potential impacts of a pre-training regime on the application of copyright law for AI systems. Proposed evaluations of the use of copyrighted works for AI training have assumed a tight integration between model training and model deployment: the model's application plays a central role in determining if a training procedure's use of copyrighted data infringes on the author's rights. In practice, however, large, modern AI systems are increasingly built and deployed under a pre-training paradigm: large models may be trained for a multitude of applications and then subsequently specialized toward specific ones. Thus, I consider copyright's indirect liability doctrine to consider the effect of copyright on the current market structures involved in the development and deployment of AI systems. The main contribution of this paper lies in its analysis of what indirect copyright liability litigation for technologies in the past have to say for how AI companies may manage or attempt to limit their copyright liability in practice. Based on this analysis, I conclude with a discussion of strategies to close these loopholes and of the role that copyright law has to play within the AI policy landscape.",
            "score": 0.4911155331868309,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2705078125
        },
        {
            "corpus_id": "274860048",
            "title": "IntroStyle: Training-Free Introspective Style Attribution using Diffusion Features",
            "text": "Diffusion models have significantly advanced image synthesis by applying iterative denoising processes guided by input prompts. Models like Stable Diffusion [30], DALL-E 3 [1], and Imagen [32] have emerged as a powerful paradigm for text-to-image synthesis, demonstrating remarkable capabilities in generating high-quality images from textual descriptions. The success of diffusion-based approaches has led to their application in various domains, including layout-to-image generation, text-guided image generation, and even video synthesis. However, these mod-Figure 1. Introspective Style Attribution (IntroStyle). Existing SOTA methods, e.g., CSD, are biased to content semantics and fail to retrieve images of similar styles, underscoring the need for a better style similarity metric. The top two rows show examples on the WikiArt [33] dataset and our proposed synthetic Style Hacks (SHacks) dataset, respectively, where a reference image, top-3 retrieval results of our method, top-2 retrieval results of CSD are shown from left to right. More importantly, our proposed method can be used as a metric for style measurement, as shown in the third row, with a lower score indicating images further away in style from the reference in the first column. Green colors indicate correct and red for incorrect retrievals. els' widespread adoption and impressive performance have also raised concerns about problems like copyright infringement. For effective performance, the models require largescale pre-training on diverse datasets [34]. Since these datasets are collected automatically and dominantly from the web, it is difficult to control image provenience and avoid the collection of copyrighted imagery. This problem is compounded by the tendency of diffusion models to replicate elements from their training data [2]. The legal implications of training models on copyrighted images have become a subject of recent debate and litigation, with artists arguing that the unauthorized use of their works for AI training [7,10,12] constitutes copyright infringement [17,29]. \n\nCurrent mitigation approaches include \"unlearning\" techniques to remove specific styles from AI models [44], though these require costly model retraining and may not fully address indirect style replication through alternative prompts [24].",
            "score": 0.4894096509828315,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 2075
                },
                {
                    "start": 2078,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 172,
                    "end": 175,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 188,
                    "end": 192,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1532,
                    "end": 1536,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 1820,
                    "end": 1823,
                    "matchedPaperCorpusId": "256389993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.289794921875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "(1) We form a framework to create prompts for T2I tasks that are generic in language semantics but can still trigger partial copyright infringements in image generation by various diffusion models. (2) We introduce a copyright tester that employs attention maps to identify significant similarities, extending the analysis from entire image duplication to specific visual feature resemblances and pinpointing potential areas of interest for detailed examination. \n\n(3) We compile a dataset of potential copyrighted topics and prompts to aid in more realistic copyright research and to analyze diffusion model behaviors. We provide empirical results to show the copyright threat and to raise awareness in copyright research for generative models. A case study utilizing our dataset revealed a significant concern: recent diffusion models is prone to unintentionally produce images with copyrighted content from seemingly unrelated prompts. \n\n2 Background Diffusion models. Diffusion models are a class of generative models that model the diffusion process. In the diffusion process, source data is gradually distorted by adding noise to it until the source data becomes noise as well [Sohl-Dickstein et al., 2015]. The objective of diffusion models is to learn the reverse process of diffusion, which tries to reconstruct the target given noisy input. Diffusion models can either learn to directly predict the less noisy data at each reverse step, or learn to predict the noise at each step, then denoise the data using predicted noise [Ho et al., 2020;Saharia et al., 2022]. Earlier diffusion models work at image level and try to directly reconstruct images from noise. However, reverse steps at image level require intensive calculation and constraints the speed of the reconstruction. Instead, [Rombach et al., 2022] proposes to first transform the image into low dimensional hidden space, then apply diffusion models at hidden level. Subsequently, latent diffusion models are much faster than their counterparts working at image level, hence can be trained on large datasets such as LAION [Schuhmann et al., 2022]. Prediction of noise or previous states in the reverse process usually uses a U-Net [Ronneberger et al., 2015].",
            "score": 0.4888592735116487,
            "section_title": "Introduction",
            "char_start_offset": 2271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 462
                },
                {
                    "start": 465,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 1183,
                    "end": 1212,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1535,
                    "end": 1552,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 1552,
                    "end": 1573,
                    "matchedPaperCorpusId": "252917726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.348876953125
        },
        {
            "corpus_id": "278338968",
            "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
            "text": "As generative AI (GenAI) becomes increasingly prevalent in realworld applications, concerns about its potential risks continue to grow. We focus on the risks associated with the replication of copyrighted material and restrict ourselves to image generation. The tangible legal and financial implications of these risks have sparked our current research. For instance, artists sued Stability AI (Midjourney and DevianArt) claiming that the companies' AI image generators produce images that are strikingly similar to their artworks [2]. In August 2024, a U.S. district judge (Orrick) ruled that the artists could proceed with the copyright infringement claims, underscoring the ongoing legal uncertainties surrounding AI-generated content [6]. \n\nThese prevalent risks make model users resistant to fully exploiting the latest GenAI models. To promote a wide adoption of GenAI, risk management is crucial. Risk management has two aspects: a) risk quantification; and b) risk mitigation. Both aspects have already received attention in the literature. \n\nRisk quantification: Risk quantification involves analysing the fraction of training images that are reproduced by the model. Since training images may be copy-righted, this leads to IP infringement risks for the end user. Consider for instance Stable Diffusion-1. When prompted using the captions of training images directly, about 2% of images it generates are highly similar to those in the training dataset [4,24]; the results are very similar for the Stable Diffusion-2 model [25]. \n\nRisk mitigation: We focus on risk mitigation, which takes a step beyond risk quantification. The goal of risk mitigation is to reduce the probability of a model outputting copyrighted content. The risk could either be mitigated before deploying a model (on the model developer's side) or after (on the model's user side). \n\nPre-deployment strategies involve changing the data science process and are usually expensive. For instance, models could be trained on a de-duplicated data set, which usually reduces the risk during training time [25]. Note however that with this strategy test time risks still prevail. Model could also be made to unlearn copy righted data but that comes with a huge computational expense [31,7,8].",
            "score": 0.48739148826119316,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1537
                },
                {
                    "start": 1540,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2264
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58642578125
        },
        {
            "corpus_id": "269214015",
            "title": "\\copyright Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model",
            "text": "This framework facilitates explicit tracking of copyrighted work usage, ensuring a fair reward system.\n\nTechnically to enable an effective and efficient copyright authorization, the plugins, as permits, should be easily created by addition if copyrighted works are new to the base models, or by extraction if the copyrighted works are already infringed by the base model.Moreover, the plug-ins should be easily combined, which allows copyright holders to merge multiple plug-ins into a new one or enables end users to generate images with multiple copyrighted works.Meanwhile, for efficient execution, these operations should be implemented as light adaptations to the base model, e.g., parameter-efficient tuning methods or prompt designs.We introduce three foundational operations -addition, extraction, and combination -implemented using the Low-Rank Adaptor (LoRA) method [10].These operations are essential for realizing the Copyright Plug-in Authorization (Figure 1(b) for an overview).\n\nIn summary, we make a conceptual contribution: advocating solving the problem of copyright infringement in foundation models with a \u00a9Plug-in Authorization framework.It can offer a fair and practical solution for the attribution challenge in text-to-image generative models.We further introduce three operations addition, extraction and combination to instantiate the framework with efficient human content copyright authorizations.Technically, we propose a novel \"Reverse LoRA\" algorithm for extraction.It can effectively extract copyrighted concepts from the base model, achieving competitive performance for concept extraction with flexible plug-ins, which are verified with deleting various artist styles and cartoon IPs.We further propose a novel \"EasyMerge\" approach for combination.It is a data-free layer-wise distillation approach, which can effectively and efficiently address the challenge of combining multiple LoRA components.These methodological innovations are extensively verified with practical text-to-image generation scenarios.",
            "score": 0.48626280763501617,
            "section_title": "Main",
            "char_start_offset": 2224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 104,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 881
                },
                {
                    "start": 881,
                    "end": 992
                },
                {
                    "start": 994,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1425
                },
                {
                    "start": 1425,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1718
                },
                {
                    "start": 1718,
                    "end": 1782
                },
                {
                    "start": 1782,
                    "end": 1932
                },
                {
                    "start": 1932,
                    "end": 2040
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2181396484375
        },
        {
            "corpus_id": "273023255",
            "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety",
            "text": "This rule governs the relationship between two critical pieces of evidence in copyright infringement cases-access and similarity to the original work. Although the validity of this rule has been the subject of considerable debate [17,18], a formal proof is established here, demonstrating that the inverse ratio rule holds under natural assumptions when its content and scope are carefully delineated. \n\nFurthermore, this approach facilitates the analysis of copyright infringement risks introduced by technological advancements and the identification of effective mitigation strategies. Specifically, a probabilistic framework is utilized to evaluate the efficacy of the Near Access-Free (NAF) condition [19], which has been proposed to mitigate the risk of copyright infringement posed by generative AI. This paper identifies extensive access to copyrighted materials by generative models as a significant source of heightened infringement risks. While the NAF condition addresses this issue to some extent, the analysis reveals that it does so partly on ethically questionable grounds-namely, by reducing transparency regarding the materials to which generative AI had access during training-and that its effectiveness in mitigating infringement risks remains limited under certain situations.",
            "score": 0.48562753429244526,
            "section_title": "Introduction",
            "char_start_offset": 2353,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1296
                }
            ],
            "ref_mentions": [
                {
                    "start": 705,
                    "end": 709,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.325439453125
        },
        {
            "corpus_id": "277955485",
            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
            "text": "Wang et al. [28] highlight vulnerabilities in copyright protection for diffusion models, proposing SilentBadDiffusion, a backdoor attack that poisons training data to reduce copyrighted image reproduction. Lu et al. [29] address detecting non-visually apparent infringements by analyzing latent spaces to identify training samples influencing generated images. Somepalli et al. [15] explore data replication in diffusion models, noting text conditioning and duplicate training images as key factors. Shang et al. [21] combine diffusion models with CNNs for high-resolution image generation, leveraging feature extraction and iterative refinement. Singh et al. [22] introduce Latent Diffusion Models (LDMs), performing diffusion in latent space to improve computational efficiency while maintaining quality.",
            "score": 0.4854539565860236,
            "section_title": "Related Work",
            "char_start_offset": 5540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 806
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1995849609375
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "In summary, we make the following contributions:\n\n\u2022 We challenge the current \"access\" criterion and point out its insufficiency in more delicate cases of copyright infringement;\n\n\u2022 We propose an algorithm that demonstrably crafts disguised copyrighted data to conceal the content (or concepts) of copyrighted images in the training set;\n\n\u2022 We show disguised data contain copyrighted information in the latent space, such that by finetuning them on textual inversion or DreamBooth, or training on LDM, the model reproduces copyrighted data during inference;\n\n\u2022 We propose methods to detect such disguises, which further encourage the expansion and quantification of \"access\" in the context of copyright infringement.",
            "score": 0.48465325458844677,
            "section_title": "Introduction",
            "char_start_offset": 4248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 50,
                    "end": 177
                },
                {
                    "start": 179,
                    "end": 336
                },
                {
                    "start": 338,
                    "end": 556
                },
                {
                    "start": 558,
                    "end": 715
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.237060546875
        },
        {
            "corpus_id": "277955485",
            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
            "text": "In this research, we prioritized mitigating copyright concerns stemming from the relationship between real-world images in our datasets and the images generated by our model. While our proposed methodology effectively reduces the risk of copyright infringement, the process of generating prompts for each image is labor-intensive and inefficient when applied to large datasets. To address this limitation, our future research will focus on developing a self-automated prompt-generation mechanism. This mechanism will analyze the context of an image to create relevant prompts, enabling the generation of higher-quality images that better capture the essence of the original or provided images. By automating this crucial step, we aim to enhance the scalability and practicality of our approach for real-world applications. This advancement will not only streamline the image generation workflow but also contribute to producing more creative and contextually appropriate outputs, further reducing the potential for copyright issues.",
            "score": 0.4825699718895764,
            "section_title": "Limitations and Future Work",
            "char_start_offset": 25058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1032
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26904296875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "We discuss how we select appropriate target topics to serve as inputs for our data generation pipeline (more in Appendix B). Our objective is to identify topics associated with copyrighted images that contain highly specific features. As such, generating these features would not be considered as transformative works, thereby resulting in explicit copyright infringement [Milner Library, 2023]. We concentrate on three distinct domains: movies, video games, and logos (trademarks), as these domains are particularly well-aligned with potentially copyrighted subjects. Additionally, we prioritize recently released movies and video games, to ensure that our samples are of high quality. Images from recent years are also more likely to be protected by copyright as they have not yet entered the public domain [Office, 2023]. Nevertheless, it is important to emphasize that our approach is a form of academic research, and we thus refrain from asserting that the topics we have gathered in this study definitively qualify as copyrighted subjects (see Appendix C.1 for the complete list of topics). \n\nIt is notable to mention that we exclude topics related to artwork and individual artists from our designated target top-ics. Within the scope of this study, our primary emphasis lies in assessing partial copyright infringement. Specifically, this involves finding the presence of copyrighted content that is visually discernible within image segments. We find that while diffusion models can accurately replicate the style of artists [Casper et al., 2023], this may be a form of derivative work [Cornell, 2022] which is less straightforward to ascertain copyright infringement. Consequently, we refrain from delving into copyright matters pertaining to artistic style and creations by specific artists. The identification of style replication within artworks demands a more intricate approach, involving deeper consideration of how the style is employed, which is beyond the scope of this work.",
            "score": 0.48249799622974626,
            "section_title": "Collect Potentially Copyrighted Topics",
            "char_start_offset": 18799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1994
                }
            ],
            "ref_mentions": [
                {
                    "start": 1595,
                    "end": 1610,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07696533203125
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "Generative AI models, renowned for their ability to synthesize high-quality content, have sparked growing concerns over the improper generation of copyright-protected material. While recent studies have proposed various approaches to address copyright issues, the capability of large vision-language models (LVLMs) to detect copyright infringements remains largely unexplored. In this work, we focus on evaluating the copyright detection abilities of state-of-the-art LVLMs using a various set of image samples. Recognizing the absence of a comprehensive dataset that includes both IP-infringement samples and ambiguous non-infringement negative samples, we construct a benchmark dataset comprising positive samples that violate the copyright protection of well-known IP figures, as well as negative samples that resemble these figures but do not raise copyright concerns. This dataset is created using advanced prompt engineering techniques. We then evaluate leading LVLMs using our benchmark dataset. Our experimental results reveal that LVLMs are prone to overfitting, leading to the misclassification of some negative samples as IP-infringement cases. In the final section, we analyze these failure cases and propose potential solutions to mitigate the overfitting problem.",
            "score": 0.4824780634912036,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2060546875
        },
        {
            "corpus_id": "278129333",
            "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
            "text": "Leveraging classifier-free guidance (CFG) [12], text-toimage diffusion models like Stable Diffusion [28] and Midjourney [1] are now capable of generating highly realistic images that closely align with user-provided text prompts. This capability has propelled their popularity, leading to widespread use and distribution of their generated images. However, recent research [6,8,31,34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The risk is es-pecially substantial given the extensive use of these models and their reliance on massive datasets, such as LAION [30], which contain billions of web-scale images and are impractical to thoroughly review or filter manually. This risk is further highlighted by real-world cases, where several artists have filed lawsuits, arguing that models like Stable Diffusion act as \"21st-century collage tools\" that remix their copyrighted works, implicating Stability AI, DeviantArt, Midjourney, and Runway AI. Recognizing the potential for unauthorized reproductions, Midjourney has even banned prompts containing the term \"Afghan\" to prevent the generation of the copyrighted Afghan Girl photograph. Yet, as [35] demonstrates, such restrictions alone are insufficient to fully prevent the reproduction of copyrighted images. This underscores the urgent need for timely effective mitigation strategies to address these concerns. \n\nIn response to these legal challenges, recent initiatives [7,8,27,32,36] have focused on developing strategies to minimize memorization, achieving notable success. These approaches vary in scope, with some targeting the model's training phase and others making adjustments during inference.",
            "score": 0.4823834749377824,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1853
                },
                {
                    "start": 1856,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2146
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 104,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 373,
                    "end": 376,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 378,
                    "end": 381,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1634,
                    "end": 1638,
                    "matchedPaperCorpusId": "256627601"
                },
                {
                    "start": 1914,
                    "end": 1917,
                    "matchedPaperCorpusId": "268819999"
                },
                {
                    "start": 1922,
                    "end": 1925,
                    "matchedPaperCorpusId": "258987384"
                },
                {
                    "start": 1925,
                    "end": 1928,
                    "matchedPaperCorpusId": "270309880"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "266798835",
            "title": "The Study of Copyright Infringement Liability of Generative Artificial Intelligence",
            "text": "Generative AI is a cutting-edge technology that involves training very large models with vast amounts of data. The key to this pre-training phase lies in the scale and quality of the data used. \n\nDuring pre-training, the model learns basic semantic and syntactic knowledge by analyzing a large corpus of unlabeled data. This corpus typically consists of text, images, or videos collected from the Internet. High-quality data is crucial for creating accurate language models, resulting in more natural and believable text. Consequently, it is common for this corpus to encompass copyrighted works, offering diverse and representative content for the model to glean from. \n\nFor instance, OpenAI has openly admitted to using books that contain long, continuous text in order to teach generative models how to process long text information effectively. However, the use of copyrighted content by OpenAI has sparked controversy. In June 2023, American writers Mona Awad and Paul Tremblay filed a class-action lawsuit against OpenAI, alleging that the company unlawfully used their work to train its ChatGPT model [4]. This case has garnered significant attention, and more than 15,000 writers have signed a letter demanding that companies involved in generative AI obtain consent from writers and provide financial compensation for using their copyrighted content in the training process [5]. \n\nThe legal implications of using prior works in the pre-training phase are still being debated. One crucial aspect is whether pre-training constitutes an act of reproduction that infringes on the right of reproduction of the original works. Copying encompasses not only permanent forms, such as replication and printing, but also temporary forms that occur briefly during computer transmission. Different countries have varying perspectives on the legal nature of temporary reproduction, with China, for example, excluding it from reproduction rights. If pre-training is deemed a form of temporary reproduction, it may not be construed as an infringement of reproduction rights. Temporary copying occurs during the technical processes of reading, storing, and briefly reproducing network information or work content in computer memory or cache. It should be temporary, automatic, and automatically deleted after operation, without causing substantial harm to the copyright owner's interests.",
            "score": 0.48160461230243545,
            "section_title": "Infringement of generative AI using prior works for pre-training",
            "char_start_offset": 8354,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 193
                },
                {
                    "start": 196,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1387
                },
                {
                    "start": 1390,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2233
                },
                {
                    "start": 2234,
                    "end": 2380
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2880859375
        },
        {
            "corpus_id": "275471553",
            "title": "Has an AI model been trained on your images?",
            "text": "From a simple text prompt, generative-AI image models can create stunningly realistic and creative images bounded, it seems, by only our imagination. These models have achieved this remarkable feat thanks, in part, to the ingestion of billions of images collected from nearly every corner of the internet. Many creators have understandably expressed concern over how their intellectual property has been ingested without their permission or a mechanism to opt out of training. As a result, questions of fair use and copyright infringement have quickly emerged. We describe a method that allows us to determine if a model was trained on a specific image or set of images. This method is computationally efficient and assumes no explicit knowledge of the model architecture or weights (so-called black-box membership inference). We anticipate that this method will be crucial for auditing existing models and, looking ahead, ensuring the fairer development and deployment of generative AI models.",
            "score": 0.4810190727461588,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65283203125
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "The development of Deep Generative Models (DGMs) marks a noteworthy advancement in image generation. Nevertheless, the impressive quality and authenticity of the generated images, as well as the efficiency in producing new ones, give rise to legitimate concerns regarding copyright matters within the realm of DGMs. \n\nData copyright protection. For the source data owner, which refers to the party or individual who owns the originality of image works, their data can be intentionally or unintentionally collected by model trainers as training samples to construct DGMs as introduced above. For example, recent studies [16,134] have demonstrated that popular DGMs are highly possible to completely replicate their training data samples, which is called memorization. \n\nThe possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works. \n\nModel copyright protection. To obtain DGMs with advanced generation performance, it is always necessary for the model trainers to invest a significant amount of funds and labor. It grants them intellectual property rights over the trained model. However, recent works also identify the possibility to steal others models [128].",
            "score": 0.4809510785135911,
            "section_title": "Copyright Issues in Image Generation",
            "char_start_offset": 12077,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 766
                },
                {
                    "start": 769,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1132
                },
                {
                    "start": 1135,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1462
                }
            ],
            "ref_mentions": [
                {
                    "start": 1456,
                    "end": 1461,
                    "matchedPaperCorpusId": "2984526"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5537109375
        },
        {
            "corpus_id": "260155285",
            "title": "Not with my name! Inferring artists' names of input strings employed by Diffusion Models",
            "text": "The rapid advancement of generative models, particularly Diffusion Models [7], has led to a surge in high-quality, realistic image generation. These models have demonstrated immense potential for creative applications across various domains, including art, design, and advertising. However, their ability to replicate the styles of specific artists raises concerns about Intellectual Property (IP) rights and potential copyright infringements 4 [20,25,19]. \n\nAs the boundary between human creativity and machine-generated content becomes increasingly blurred, it is crucial to address the legal and ethical implications of using generative models to produce art, as well as to develop methods that evaluate the extent to which generated images are influenced by the works of real artists and ensure the protection of their intellectual property. \n\nSeveral studies and articles have explored the legal ramifications of generative models and their potential to infringe on copyrights 5 . In an article dated 2021 the challenges in determining copyright ownership for AI-generated works were discussed and authors emphasized the need for legal frameworks that could adequately address the unique nature of generative models 6 . MLQ.AI also reported on a copyright infringement case involving generative AI 7 , which sparked debates on the responsibilities of AI developers and users in protecting original creators' rights. In response to these concerns, legal scholars have delved into the complexities of copyright law as it pertains to AI-generated artworks. Gillotte [8] examined the challenges in assigning liability and protecting IP rights, arguing that existing copyright laws may not be sufficient to address the unique characteristics of AI-generated contents. Indeed, it is became a copyright dilemma and the need for a balance between innovation and IP protection is arising to ensure that creative works are safeguarded without stifling technological advancements [16]. \n\nIt is not easy to determine how images generated by tools like DALL-E 2 or Midjourney are created by combination of images employed at training time. But, it could useful to develop tools able to deduce the textual prompts that generated an investigated image. Recently, a Kaggle competition was launched on the task 8 but it still lacks of effective methods. However, to make images that appear with an artist's style the prompt of the generating tools should necessary contain the artist's name.",
            "score": 0.4802885971960531,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1979
                },
                {
                    "start": 1982,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2242
                },
                {
                    "start": 2243,
                    "end": 2341
                },
                {
                    "start": 2342,
                    "end": 2479
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 77,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1568,
                    "end": 1571,
                    "matchedPaperCorpusId": "233748873"
                },
                {
                    "start": 1974,
                    "end": 1978,
                    "matchedPaperCorpusId": "113882623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.143798828125
        },
        {
            "corpus_id": "269282669",
            "title": "The Files are in the Computer: Copyright, Memorization, and Generative AI",
            "text": "All useful models learn something about their training data.Memorization is simply a difference in degree: it is an encoded feature in the model; whether it is a desired feature or not is another matter entirely.\n\nIt follows that memorization in generative AI cannot be neatly confined to generation time, to adversarial users, and to generation-time guardrails.If a generative-AI system has memorized copyrighted works, the memorized aspects of those works are present in the model itself, not just in the generated outputs.It can (with certain probability) generate near-verbatim copies of those works on demand, not just for users who have a suitably nefarious intent.And the system's creator can limit output infringement by changing the model, not just by putting guardrails around the model.\n\nWe take no position on what the most appropriate copyright regimes for generative-AI systems should be, and we express no opinion on how pending copyright lawsuits should be decided.Our goal is merely to describe how these systems work so that copyright scholars can develop their theories of generative AI on a firm technical foundation.We seek clarity, precision, and technical accuracy.\n\nYou have nearly finished reading Part I of this essay, the introduction.In Part II, we provide a brief background on how generative-AI models work, and the supply chains within which they are embedded.In Part III, the heart of the essay, we describe how to think clearly about memorization in generative-AI systems, and show how several common arguments about copyright and generative AI are built on a mistaken view of how memorization happens.Part IV offers a brief conclusion, with some historical reflections.",
            "score": 0.4797797369598035,
            "section_title": "[TODO: examples]",
            "char_start_offset": 4243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 60,
                    "end": 212
                },
                {
                    "start": 214,
                    "end": 362
                },
                {
                    "start": 362,
                    "end": 525
                },
                {
                    "start": 525,
                    "end": 671
                },
                {
                    "start": 671,
                    "end": 797
                },
                {
                    "start": 799,
                    "end": 981
                },
                {
                    "start": 981,
                    "end": 1137
                },
                {
                    "start": 1137,
                    "end": 1188
                },
                {
                    "start": 1190,
                    "end": 1262
                },
                {
                    "start": 1262,
                    "end": 1391
                },
                {
                    "start": 1391,
                    "end": 1635
                },
                {
                    "start": 1635,
                    "end": 1703
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2216796875
        },
        {
            "corpus_id": "277113230",
            "title": "Content ARCs: Decentralized Content Rights in the Age of Generative AI",
            "text": "Generative AI (GenAI) is transforming creative workflows through tools for efficiently generating and manipulating text, images, music, and video. However, the vast datasets used to train commercial GenAI models often include copyrighted material [1], causing growing concerns among creative rightsholders about how their work is used and whether they will receive proper recognition and compensation. These concerns have recently surfaced in proposals-and counter-proposals-around changes to copyright legislation in several jurisdictions. The debate has become highly contested; for example, the recent UK copyright consultation [2] received over 11,000 submissions following a public awareness campaign by creative practitioners. Arguments often centre on the contentious issue of opt-in versus opt-out mechanisms; i.e. whether permission should always be sought to train commercial AI models on copyrighted data (i.e. opt-in) or there should be a default legal right to train unless counter-indicated by a rightsholder (i.e. opt-out). Rightsholders often argue that an opt-in model is essential to protect creators' control over their work and ensure fair compensation [3]. While, AI developers often contend that an opt-out approach is necessary given the sheer scale of data (often many billions of assets) over which contractual consent and payment would need to be orchestrated [4]. Political factors also weigh on the debate, such as creating a competitive AI development environment versus jurisdictions with more permissive legislation or less stringent enforcement [5]. These positions are often deeply polarized and appear difficult to reconcile, highlighting the need for balanced solutions that uphold both creative rights and technological progress. \n\nIn this paper, we contribute a framework to help resolve this deadlock by leveraging a combination of open standards and emerging data-centric technologies such as data attribution and distributed ledger technology (DLT). We present this as a meta-system design comprising of three core phases (ARC) applied to content: (A)uthenticity, (R)ights, (C)ompensation; hereafter Content ARCs. Multiple technology choices exist for each phase, and we describe technical options for each. \n\nThe motivation of Content ARCs is to provide an automated mechanism for clearing rights to, and compensating rightsholders for, the use of copyrighted data in GenAI training.",
            "score": 0.4794143032933619,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2247
                },
                {
                    "start": 2250,
                    "end": 2424
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18359375
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Hence, the objective of our data generation pipeline is to systematically create non-sensitive adversarial prompts. We discuss our proposed data generation pipeline in Section 4. \n\nCopyright test for substantial similarities. According to the legal definition of copyright infringement, copyright violations can appear across a broader spectrum, in which generated images are not entirely replicated from any training source, yet they possess substantial similarities that replicate or bear significant resemblance to copyrighted content. We refer to these copyright violations as partial copyright violations. In Definition 2, we apply D[\u2022||\u2022] to measure the substantial similarities between generated images and copyrighted images. We propose an implementation of D[\u2022||\u2022] as a copyright tester in Section 5.",
            "score": 0.47919737886963587,
            "section_title": "Problem Formulation",
            "char_start_offset": 8644,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 809
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1470947265625
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "Permission of the copyright owner is a complete defense to infringement; a copyright owner can license any activity they want, thereby rendering it noninfringing. When copyright owners have expressly consented to model training and generation, there is not much further to say. But such licenses apply to only a small fraction of the works currently being used as training data. Some datasets were curated by gathering material from consenting creators expressly for the purpose of model training. In other cases, the copyright agreements that authors have previously executed are so broad that the current owners have the right to allow model training. (This appears to be the case for some stock photography libraries, for example.) Models trained solely with this express and specific permission are rare and smaller and less performant than the most cutting-edge generative-AI models. \n\nAn express but non-specific license might arise when works have been posted online under an open license. Here, anyone is allowed to engage in a use as long as it complies with the terms of that license, even if the user has never directly interacted with the copyright owner to obtain specific permission. We will use Creative Commons licenses as an example, as the terms in the Creative Commons license suite cover a useful range of interesting conditions. Some materials are provided under a Creative Commons Zero notice, which indicates that there are no copyright interests in the material, or that the copyright owner has dedicated the works to the public domain. Any and all uses of these works are allowed, by anyone, without risk of copyright infringement. \n\nThe The attribution term in BY licenses requires that the user retain the creator's identification, indicate whether the work is modified, and retain the Creative Commons license notice. This requirement can be satisfied in \"any reasonable manner based on the medium, means, and context. \" 60 A training dataset could provide this information through suitable metadata, but many datasets do not. If liability were a serious concern, and the availability of CC-licensed material sufficiently broad to justify it, it is possible that more datasets would bear these attributions, so that they would be fully allowed under CC-BY licenses. \n\nThis, however, is where attribution stops with current opaque generative AI models. These models do not attempt to provide any attribution for the works they were trained on.",
            "score": 0.4791237371716653,
            "section_title": "G. Express Licenses",
            "char_start_offset": 60179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1656
                },
                {
                    "start": 1659,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2293
                },
                {
                    "start": 2296,
                    "end": 2379
                },
                {
                    "start": 2380,
                    "end": 2470
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1563720703125
        },
        {
            "corpus_id": "278165007",
            "title": "GenPTW: In-Generation Image Watermarking for Provenance Tracing and Tamper Localization",
            "text": "Generative models are evolving at an unprecedented pace, particularly text-to-image (T2I) diffusion models such as Stable Diffusion, DALL\u2022E 3, and Imagen. These models are capable of synthesizing highly realistic and visually compelling images, while also supporting flexible editing, thereby reshaping the landscape of visual content creation. However, this impressive generative capability is a double-edged sword, introducing a range of security risks including content misuse, ambiguous copyright ownership, and difficulties in tamper detection. In recent years, incidents involving AI-generated images being stolen, maliciously edited, or even forged as fabricated evidence have become increasingly common, threatening both public discourse and the credibility of legal systems. These issues fundamentally highlight two critical challenges: verifying content authenticity and tracing generative responsibility. \n\nImage watermarking is a widely adopted technique for copyright protection and provenance tracing. However, most existing methods focus primarily on authenticity verification and ownership identification, falling short in terms of accurately localizing tampered regions. Tamper localization plays a crucial role in delineating the boundary between generated and modified content, clarifying the responsibility of generative models. Moreover, it enables the assessment of tampering severity and reveals potential malicious intent, making it a key component for achieving comprehensive traceability in AIGC-generated images. \n\nSeveral recent studies have begun to explore the integration of copyright identification and tamper detection. For instance, Sep-Mark [50] introduces a separable watermarking structure to improve robustness against attacks, while EditGuard [59] leverages local vulnerabilities in image steganography to enable tamper region localization. However, these methods follow a post-generation paradigm, where watermarks are embedded after the image has been generated. This leads to a disconnect from the generation process, increased deployment complexity, and reduced overall efficiency. \n\nTherefore, recent studies have shifted towards embedding watermarks directly within the diffusion-based generation process, known as In-Generation watermarking. For example, Stable Signature [14] injects watermarks during generation by fine-tuning the VAE decoder, but requires training a separate model for each watermark, making it unsuitable for large-scale deployment.",
            "score": 0.47863551622402284,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1539
                },
                {
                    "start": 1542,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2124
                },
                {
                    "start": 2127,
                    "end": 2287
                },
                {
                    "start": 2288,
                    "end": 2499
                }
            ],
            "ref_mentions": [
                {
                    "start": 1676,
                    "end": 1680,
                    "matchedPaperCorpusId": "258588430"
                },
                {
                    "start": 1782,
                    "end": 1786,
                    "matchedPaperCorpusId": "266210194"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54345703125
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "This helps avoid infringement while maintaining the desired output characteristics, even without changing the original prompts. \n\nIn summary, our contributions are as follows: \n\n\u2022 We propose CopyJudge, an automated abstractionfiltration-comparison framework powered by a multi-LVLM debate mechanism, designed to efficiently detect copyright-infringing images generated by text-toimage diffusion models. \n\n\u2022 Given the judgment, we introduce an adaptive mitigation strategy that automatically optimizes prompts and explores non-infringing latent noise vectors of diffusion models, effectively mitigating copyright violations while preserving non-infringing expressions. \n\n\u2022 Extensive experiments demonstrate that our identification method matches state-of-the-art performance, with improved generalization and interpretability, while our mitigation approach more effectively prevents infringement without losing non-infringing expressions. \n\nImage infringement detection and mitigation. The current mainstream infringing image detection methods primarily measure the distance or invariance in pixel or embedding space (Carlini et al., 2023;Somepalli et al., 2023a;Shi et al., 2024b;Wang et al., 2021;2024b (Wen et al., 2024;Wang et al., 2024d) have shown that these methods have lower generalization capabilities and lack interpretability because they do not fully align with human judgment standards. For copyright infringement mitigation, the current approaches mainly involve machine unlearning to remove the model's memory of copyright information (Bourtoule et al., 2021;Nguyen et al., 2022;Kumari et al., 2023;Zhang et al., 2024) or deleting duplicated samples from the training data (Webster et al., 2023;Somepalli et al., 2023b",
            "score": 0.47854962116419536,
            "section_title": "Introduction",
            "char_start_offset": 4077,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 130,
                    "end": 175
                },
                {
                    "start": 178,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1733
                }
            ],
            "ref_mentions": [
                {
                    "start": 1116,
                    "end": 1138,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1138,
                    "end": 1162,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1204,
                    "end": 1222,
                    "matchedPaperCorpusId": "270309880"
                },
                {
                    "start": 1550,
                    "end": 1574,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 1594,
                    "end": 1614,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 1710,
                    "end": 1733,
                    "matchedPaperCorpusId": "258987384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2666015625
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "The commercialization of text-to-image diffusion models (DMs) brings forth potential copyright concerns. Despite numerous attempts to protect DMs from copyright issues, the vulnerabilities of these solutions are underexplored. In this study, we formalized the Copyright Infringement Attack on generative AI models and proposed a backdoor attack method, SilentBadDiffusion, to induce copyright infringement without requiring access to or control over training processes. Our method strategically embeds connections between pieces of copyrighted information and text references in poisoning data while carefully dispersing that information, making the poisoning data inconspicuous when integrated into a clean dataset. Our experiments show the stealth and efficacy of the poisoning data. When given specific text prompts, DMs trained with a poisoning ratio of 0.20% can produce copyrighted images. Additionally, the results reveal that the more sophisticated the DMs are, the easier the success of the attack becomes. These findings underline potential pitfalls in the prevailing copyright protection strategies and underscore the necessity for increased scrutiny to prevent the misuse of DMs.",
            "score": 0.4783108502300889,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56640625
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "This work complies with the ICLR Code of Ethics. Regarding potential concerns in releasing copyrighted assets: The majority of the assets (e.g., prompts, descriptions, etc.) are generic and are likely not protected by copyright in most, if not all, jurisdictions. To the extent that we release generated images of characters (e.g., the screenshots in the paper), in many jurisdictions this would be considered acceptable under copyright law as part of research (e.g., fair use in the United States). However, to ensure compliance with more jurisdictions, beyond the paper, we can refrain from releasing certain images other than through direct sharing only with researchers on request under an agreement on proper use of the asset. \n\nIn general, our work systematically analyzes mitigation strategies and risks of generating certain copyrighted characters. We find that currently these mitigation strategies can sometimes fail, though they are actively used by different organizations. However, there may be better ways for companies to respect the intellectual property rights of creators and their visual copyrighted characters from both a technical and structural perspectives. Leveraging the lessons we provide here will both improve likelihood that rights are respected and reduce litigation risk for companies. However, we do not address broader societal discussions on how artists should be compensated for training on images that may contain their intellectual property (such as their characters). This is a larger, worthy, discussion broader than the scope of our work. Current fair use doctrine in the United States might allow this training provided that mitigation strategies are used to prevent substantially similar outputs (Lee et al., 2024;Lemley & Casey, 2020;Henderson et al., 2023;Pasquale & Sun, 2024). But this is far from resolved. Similar fair use standards exist in other countries as well. Globally, there are also countries where even training may not be allowed and different approaches may be needed than we describe here. There are also general labor displacement concerns: mitigation strategies might successfully prevent infringement of copyright characters and model training might generally be considered fair use, yet may nonetheless impact data creators' livelihoods. These are important issues that go beyond the scope of this work.",
            "score": 0.47568571499970824,
            "section_title": "ETHICS STATEMENT",
            "char_start_offset": 33219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 731
                },
                {
                    "start": 734,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2302
                },
                {
                    "start": 2303,
                    "end": 2368
                }
            ],
            "ref_mentions": [
                {
                    "start": 1756,
                    "end": 1777,
                    "matchedPaperCorpusId": "219342558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048309326171875
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "The SSCD has been specifically designed to identify copied content existing across two images. While SSCD is state-of-theart and the first choice for this purpose, it's important to recognize that no system is infallible. Discrepancies between SSCD outcomes and human judgment can occur, highlighting an area for further research and development. \n\nCopyright laws stipulate that any unauthorized reproduction constitutes infringement. To ensure a dataset is free from copyright issues, each image within the dataset (of size N) must be compared against every image in a predefined set of copyright-protected images (of size M). This requirement leads to N*M comparisons and discussions by human experts. A formidable task considering that legal disputes over copyright between two works can span several months. Given the vast size of datasets typically used in training generative models, manual inspection for copyright infringement at such scale becomes impractical. Consequently, despite its limitations, employing a tool like SSCD remains the most feasible strategy for conducting copyright infringement checks at this magnitude. \n\nTo further justify the effectiveness of SSCD, we collected examples used in the Getty Images lawsuit against Stability AI (Vincent, 2023), as illustrated in Figure 8. The SSCD similarity score is 0.47. In our study, we set the threshold at 0.5. This result indicates the effectiveness of SSCD for copyright detection and our threshold is a reasonable value for checking copyright infringement in training data.",
            "score": 0.4749272230271122,
            "section_title": "D. Limitation and Discussion of Employing SSCD for Assessing Substantial Similarity",
            "char_start_offset": 42005,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1547
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.249267578125
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "Finally we are ready to talk about AI users -the people who actually supply prompts and use the resulting generations. There are three elements to a generation. First, there is the prompt itself. Some prompts -\"a big dog\"are simple and generic, while others -\"a big dog facing left wearing a spacesuit in a bleak lunar landscape with the earth rising in the background as an oil painting in the style of Paul Cezanne high-resolution aesthetic trending on artstation\" -are more detailed. Second, there is the choice of application (which of course embeds an implicit choice of model). A user typically selects an application with the outputs partially in mind, so that one choice or another can indicate an attitude towards the possibility of infringement. And third, there is the random seed that goes into each specific generation. It is typical, for example, for image apps to produce four candidate generations, each starting from a different seed (Dall-E and Midjourney both do this). This element of randomness (and of repetition) bears on both copyrightability and infringement. \n\nAs we will see, characterizing the relationship between user and app is one of the critical choice points in a copyright-infringement analysis. There are at least three ways the relationship could be described: \u2022 The user actively drives the generation through choice of prompt, and the application passively responds. On this view, the user is potentially a direct infringer, but the application is like a web host, ISP, or other neutral technological provider. \n\n\u2022 The application is active and the user passive. On this view, the user is like a viewer of an infringing broadcast, or the unwitting buyer of a pirated copy of a book. Primary copyright responsibility lies with the application and possibly with others further upstream in the supply chain. \n\n\u2022 The user and the application are active partners in generating infringing outputs. On this view, the user is like a patron who commissions a copy of a painting, and the application is like the artist who executes it. They have a shared goal of creating an infringing work. We will argue that there is no universally correct characterization. Which of these three is the best fit for a particular act of generation will depend on the application, the prompt, and how the application is marketed.",
            "score": 0.47477040692192934,
            "section_title": "F. Generation",
            "char_start_offset": 10769,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1549
                },
                {
                    "start": 1552,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1843
                },
                {
                    "start": 1846,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2189
                },
                {
                    "start": 2190,
                    "end": 2342
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.055816650390625
        },
        {
            "corpus_id": "268049158",
            "title": "Break It 'Til You Make It: An Exploration of the Ramifications of Copyright Liability Under a Pre-training Paradigm of AI Development",
            "text": "The potential for infringing uses and outputs of deployed ML systems are particularly clear for pre-trained generative art and text models where barriers to entry are low.Users can input plaintext prompts to generate (or, perhaps, retrieve9 ) works that are highly similar to existing works by artists10 .In such cases, governing the inclusion of artists' works in the training set of pre-trained models may not be enough.They could be supplemented by steps to monitor or and evaluate how artists' expression could be elicited as part of downstream uses.The expectation of monitoring for downstream harms may be especially important for pre-trained models that could eventually have generative model outputs-and especially where certain kinds of use cases could produce outputs that directly compete with artists' works-even if they may not regurgitate or necessarily plagiarize those works.As part of a complaint against StabilityAI for indirect liability [Anderson et al. v. Stability AI, (N.D. Cal. 2023) (No. 3:23-cv-00201)], the count states:\n\nIndividuals have used AI Image Products to create works using the names of Plaintiffs and the Class in prompts and passed those works off as original works by the artist whose name was used in the prompt.Such individuals are referred to herein as 'Imposters' By using a particular artist's name, Imposters can cause the AI Image Product to rely more heavily on that artist's prior works to create images that can pass as original works by that artist.These output images are referred to herein as 'Fakes.Monitoring and mitigating how models are applied to authors of creative works could strengthen opportunities for compensation and mitigate market harms that current developers enable.",
            "score": 0.47336651478257347,
            "section_title": "DUTIES",
            "char_start_offset": 34568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 422
                },
                {
                    "start": 422,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 1047
                },
                {
                    "start": 1049,
                    "end": 1253
                },
                {
                    "start": 1253,
                    "end": 1500
                },
                {
                    "start": 1500,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1736
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.266845703125
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "Consequently, it becomes difficult to ascertain whether a particular training sample has been utilized solely based on the generated output of the model for postcaution methods.\n\nIn this paper, we propose a new copyright authentication framework, named Contrasting Gradient Inversion for Diffusion Models (CGI-DM) to greatly improve the efficacy of the post-caution path, illustrated in Fig. 1 (Bottom).Recent advances in gradient inversion [3,10,38] emphasize the importance of prior information in data extraction.Inspired by this, we propose first removing half of a given image.Then we utilize the retained partial representation as a prior and employ gradient inversion to reconstruct the original image.As recent studies [1,27] indicate that generative models tend to \"memorize\", the recovery of removed information should be possible only when the images are utilized during the fine-tuning process, enabling \"memorization\" on given samples.Thus, a high similarity between the recovered image and the original image can indicate that the model has been trained with the given image.\n\nHowever, directly applying gradient inversion may not yield useful information for DMs, possibly because they inherently eliminate noise (see Appendix C for details).To address this issue, we focus on contrasting two models: the pretrained and the fine-tuned model.Specifically, our goal is to leverage the conceptual differences between these two models.We measure this disparity through the KL divergence between the latent variable distributions of the pretrained and fine-tuned models.Subsequently, we provide a proof demonstrating that maximizing this KL divergence approximates accentuating the loss differences between the two models.Building on this, we employ Monte Carlo Sampling on the noise and the time variable during the diffusion process, utilizing PGD [17,18] to optimize the aforementioned loss difference.Comprehensive experiments are conducted on artists' works and objects, addressing the potential for unauthorized style transfers and the creation of fabricated images, both of which necessitate copyright authentication.The experiment results affirm the effectiveness and robustness of our approach.\n\nIn summary, our contributions are as follows: \u2022 We have formulated a novel post-caution approach for copyright protection-copyright authentication.This method complements precaution measures and provides robust legal proof of infringements.",
            "score": 0.4732597750681202,
            "section_title": "Introduction",
            "char_start_offset": 2319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 179,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 516
                },
                {
                    "start": 516,
                    "end": 582
                },
                {
                    "start": 582,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 948
                },
                {
                    "start": 948,
                    "end": 1089
                },
                {
                    "start": 1091,
                    "end": 1257
                },
                {
                    "start": 1257,
                    "end": 1356
                },
                {
                    "start": 1356,
                    "end": 1446
                },
                {
                    "start": 1446,
                    "end": 1580
                },
                {
                    "start": 1580,
                    "end": 1732
                },
                {
                    "start": 1732,
                    "end": 1915
                },
                {
                    "start": 1915,
                    "end": 2134
                },
                {
                    "start": 2134,
                    "end": 2213
                },
                {
                    "start": 2215,
                    "end": 2362
                },
                {
                    "start": 2362,
                    "end": 2455
                }
            ],
            "ref_mentions": [
                {
                    "start": 441,
                    "end": 444,
                    "matchedPaperCorpusId": "237204511"
                },
                {
                    "start": 444,
                    "end": 447,
                    "matchedPaperCorpusId": "246822452"
                },
                {
                    "start": 447,
                    "end": 450,
                    "matchedPaperCorpusId": "208139345"
                },
                {
                    "start": 727,
                    "end": 730,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1860,
                    "end": 1864,
                    "matchedPaperCorpusId": "256697414"
                },
                {
                    "start": 1864,
                    "end": 1867,
                    "matchedPaperCorpusId": "3488815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.310546875
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "The remarkable generation and data fitting capabilities of large models like diffusion models have garnered significant attention, and also raised concerns regarding image copyright.Recently, machine unlearning has emerged as a potential solution to mitigate the infringement risk by unlearning the corresponding copyright memory.This work introduces a new large-scale dataset and benchmark, coupled with standardized metrics, for evaluating machine unlearning methods, specifically designed to assess the efficacy of machine unlearning methods in safeguarding copyright.Notably, this dataset represents the inaugural compilation in this domain that is predicated on diffusion models.The dataset's caliber is substantiated by rigorous human and artist evaluations, ensuring that the metrics resonate with human perception and are aligned with the sensibilities of creators.\n\nEach instance within the dataset comprises an anchor image, a prompt, and nine corresponding generated images.\n\nIs there a label or target associated with each instance?If so, please provide a description.\n\nEach instance represents an original image, along with its corresponding prompt and nine images generated by the stable diffusion model that potentially exhibit copyright infringement.\n\nIs any information missing from individual instances?If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable).This does not include intentionally removed information, but might include, e.g., redacted text.\n\nNo.\n\nAre relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?If so, please describe how these relationships are made explicit.\n\nThere is no explicit correlation between individual instances.\n\nAre there recommended data splits (e.g., training, development/validation, testing)?If so, please provide a description of these splits, explaining the rationale behind them.",
            "score": 0.4731532714828293,
            "section_title": "Conclusion",
            "char_start_offset": 24843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 182,
                    "end": 330
                },
                {
                    "start": 330,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 684
                },
                {
                    "start": 684,
                    "end": 873
                },
                {
                    "start": 875,
                    "end": 985
                },
                {
                    "start": 987,
                    "end": 1044
                },
                {
                    "start": 1044,
                    "end": 1080
                },
                {
                    "start": 1082,
                    "end": 1266
                },
                {
                    "start": 1268,
                    "end": 1321
                },
                {
                    "start": 1321,
                    "end": 1436
                },
                {
                    "start": 1436,
                    "end": 1532
                },
                {
                    "start": 1534,
                    "end": 1537
                },
                {
                    "start": 1539,
                    "end": 1651
                },
                {
                    "start": 1651,
                    "end": 1716
                },
                {
                    "start": 1718,
                    "end": 1780
                },
                {
                    "start": 1782,
                    "end": 1866
                },
                {
                    "start": 1866,
                    "end": 1956
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.149658203125
        },
        {
            "corpus_id": "268049158",
            "title": "Break It 'Til You Make It: An Exploration of the Ramifications of Copyright Liability Under a Pre-training Paradigm of AI Development",
            "text": "Namely, the act of downloading a popular song from the Billboard Top 10 does not necessarily implicate other songs.On the other hand, when model deployers and fine-tuners want to build a model to generate work in the style of an artist, unexpected works in the training data may be found to contribute to the end result.Furthermore, at the scale of billions of data points, any one piece of data (or, one copyrighted work) will likely do little to change the functionality-including the generative functionalityof the model.This makes a technical focus on guaranteeing nonexpressive or non-similarity properties of model outputs while nonconsensually training on copyrighted works potentially neglectful of the foundational role of copyrighted works as part of datasets.In particular, an author's expression may not only be present in her work but also in aspects of the works of others-and those aspects could be specifically technically tagged to draw from in the production of similar works.To further lay out duties for data collection and use for model training, it may be helpful to look to recent proposals that recognize the social and collective value of data, such as [Gordon-Tapiero et al. 2022;Parsons and Viljoen 2023] that build on and reform privacy and data protection, which have both historically focused on the process surrounding data collection and use.Deployment Duties.At deployment time, duties could prevent model developers from profiting from the deployment of systems that interfere with the market for the original work without the associated copyright liability.Menell and Nimmer [2007] discuss how the legislative history of copyright law points toward tort principles and suggests that the tort framework could better balance innovation and copyright protection: if a technology can be designed in a different way to prevent infringements, it should be.This could involve the deployment of technological guardrails: system designers may have a duty to identify and deploy technological guardrails to prevent users from creating infringing outputs (e.g., preventing users from using prompts that ask for work to be output in the style of a particular artist).\n\nThe potential for infringing uses and outputs of deployed ML systems are particularly clear for pre-trained generative art and text models where barriers to entry are low.",
            "score": 0.4731312090486599,
            "section_title": "DUTIES",
            "char_start_offset": 32376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 115,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 524
                },
                {
                    "start": 524,
                    "end": 770
                },
                {
                    "start": 770,
                    "end": 994
                },
                {
                    "start": 994,
                    "end": 1374
                },
                {
                    "start": 1374,
                    "end": 1392
                },
                {
                    "start": 1392,
                    "end": 1592
                },
                {
                    "start": 1592,
                    "end": 1885
                },
                {
                    "start": 1885,
                    "end": 2190
                },
                {
                    "start": 2192,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 1178,
                    "end": 1206,
                    "matchedPaperCorpusId": "253238983"
                },
                {
                    "start": 1592,
                    "end": 1616,
                    "matchedPaperCorpusId": "154805941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2047119140625
        },
        {
            "corpus_id": "261279983",
            "title": "AI Art and its Impact on Artists",
            "text": "Given the speed at which image generators have been adopted and their impact, countries around the world are grappling with what policies to enact in response. In particular, there is a lot of uncertainty about whether using copyrighted materials to train image generators is copyright infringement. Some governmental bodies, like the EU, will require companies to \"document and make publicly available a summary of the use of training data protected under copyright law\" 27  [44], which could trigger copyright lawsuits if it becomes possible to identify specific instances of copyright infringement [72]. However, it is not clear what the scope of this law is and if it requires an itemized list of what is included in the training data, or only a summary of other key information. \n\nWhile a number of artists have filed class action lawsuits in the US against companies providing commercial image generation tools [129], image generators represent a dynamic between artists and large-scale companies appropriating their work that has previously not been examined in US copyright law [56]. This is due to the unprecedented scale at which artists' works are being used to create image generators, the recent proliferation of publicly available image generators trained on that content, and the level to which the output of the image generators threatens to displace artists. Furthermore, this dynamic is distinct because of the data collection practices by which image generators are developed in the first place [67]. \n\nWhile some of the harms discussed in Section 4 overlap with the rights protected by US copyright law, others are not. There are also a number of unanswered legal questions when it comes to determining the ways in which copyright law applies to image generators and both the inputs and outputs that go into creating these tools. Hence, US copyright law is largely unequipped to tackle many of the types of harms posed by these systems to content creators. This lack of certainty about whether copyright applies means that the companies producing these tools can do so largely without accountability, unless they are sued for specific violations of copyright law. And waiting for court determinations on their lawsuits means that artists may not be able to get recourse until the cases are resolved. In this section, we highlight specific parts of US copyright law that may be a source of uncertainty and tension for artists and companies using their work.",
            "score": 0.4726350557966542,
            "section_title": "AI ART AND US COPYRIGHT LAW",
            "char_start_offset": 35476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2319
                },
                {
                    "start": 2320,
                    "end": 2476
                }
            ],
            "ref_mentions": [
                {
                    "start": 1086,
                    "end": 1090,
                    "matchedPaperCorpusId": "233748873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2802734375
        },
        {
            "corpus_id": "274436785",
            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
            "text": "Diffusion models [59] have been widely applied in various generative tasks, including high-quality image synthesis, image style transfer, image-to-image translation, and text-to-image synthesis [11,52,53,65,75]. These models emulate the diffusion process observed in non-equilibrium Figure 1. Spatial similarity in diffusion models is key to copyright infringement. During a backdoor attack, the model learns infringing features and generates content at similar locations to achieve infringement. Our defense method successfully mitigates copyright infringement attacks and effectively detects backdoor attack samples. \n\nthermodynamics by incrementally introducing noise to the data, which approximates a Gaussian distribution. Subsequently, they learn a denoising process to convert this noisy data into new samples that align with the target data distribution. Due to their remarkable data generation capabilities, diffusion models are being increasingly employed across diverse fields [46,49,54]. \n\nHowever, as commercial text-to-image diffusion models become increasingly prevalent [57,61], copyright issues have emerged as a significant concern. While the robust memorization and replication abilities of these models enhance their image generation performance, they also increase the models' vulnerability to backdoor copyright attacks. By injecting concealed poisoned data into the training set, diffusion models can be compromised without the need for fine-tuning [62]. Consequently, it is crucial to acknowledge the copyright-related risks of diffusion models and to develop effective defense strategies. \n\nCurrent solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,61,76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74,77], which can result in copyright violations, have yet to be developed. \n\nIn this paper, we investigated the strong correlation between the content replication and prompts of diffusion models (exemplified by Stable Diffusion [1]). We analyzed the tight coupling of image and prompts features through cross-attention, revealing the spatial similarity of imageprompt associative features within cross-attention feature maps. Through experiments involving spatial transformations of samples, we observed that when identical spatial transformations were applied to the samples, the duplication also exhibited the same spatial transformations.",
            "score": 0.472330298785854,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1613
                },
                {
                    "start": 1616,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2024
                },
                {
                    "start": 2027,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2375
                },
                {
                    "start": 2376,
                    "end": 2591
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "244714856"
                },
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 201,
                    "end": 204,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "260900064"
                },
                {
                    "start": 207,
                    "end": 210,
                    "matchedPaperCorpusId": "257427673"
                },
                {
                    "start": 995,
                    "end": 998,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1086,
                    "end": 1090,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1090,
                    "end": 1093,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 1849,
                    "end": 1852,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.464111328125
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "In this paper, we review the current access criterion of containing copyrighted material in the training set (direct access) in copyright infringement of generative models and point out its insufficiency by introducing disguised copyright infringement (indirect access).Specifically, such an infringement is realized by injecting disguised samples into the training set, which urges LDMs to produce copyrighted content.Such disguises are generated with a simple algorithm and demonstrated to share the same concept with their target copyrighted images using the textual inversion tool.To alleviate the concern on the disguises, we expand the current visual auditing (browsing the training set) with additional tools, i.e., feature similarity search and encoderdecoder examination to better identify these disguises.Furthermore, we propose a broader definition of acknowledgment to cover this new type of copyright violation.\n\nLimitations and future work: One interesting future work is to quantify the number of disguises needed for reproducing in large-scale training, which can be further linked to the quantification of memorization of such models (Carlini et al. 2022;Somepalli et al. 2023a;Somepalli et al. 2023b;Carlini et al. 2023;Ippolito et al. 2023;Zhang et al. 2021)).\n\nAdditionally, although our algorithms can generate descent disguises, we believe there is still room for improvement for optimization.Finally, one extension we didn't touch is the possibility of \"chopping\" copyrighted data and hiding it in several images.It is intriguing to explore whether it is possible to generate such a smuggler's dataset and detection towards it.\n\nLearning from noisy data A simultaneous and independent work (Daras et al. 2024) considers learning LDMs from noisy data.Although the techniques are very different, our works share a similar implication: the training dataset may not immediately resemble the generations produced, thus al-lowing copyright issues to be disguised from an auditor who manually inspects the dataset.In the work of Daras et al. (2024), the training data is the original data with Gaussian noise applied to it.In our case, the training data has the original data hidden in the latent space.",
            "score": 0.47231521563626383,
            "section_title": "Conclusion",
            "char_start_offset": 23965,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 815
                },
                {
                    "start": 815,
                    "end": 924
                },
                {
                    "start": 926,
                    "end": 1279
                },
                {
                    "start": 1281,
                    "end": 1415
                },
                {
                    "start": 1415,
                    "end": 1536
                },
                {
                    "start": 1536,
                    "end": 1650
                },
                {
                    "start": 1652,
                    "end": 1773
                },
                {
                    "start": 1773,
                    "end": 2030
                },
                {
                    "start": 2030,
                    "end": 2139
                },
                {
                    "start": 2139,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 1151,
                    "end": 1172,
                    "matchedPaperCorpusId": "246863735"
                },
                {
                    "start": 1172,
                    "end": 1195,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1218,
                    "end": 1238,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1238,
                    "end": 1259,
                    "matchedPaperCorpusId": "263610040"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.541015625
        },
        {
            "corpus_id": "268049158",
            "title": "Break It 'Til You Make It: An Exploration of the Ramifications of Copyright Liability Under a Pre-training Paradigm of AI Development",
            "text": "CONCEPTUALIZATION AND DEVELOPMENTAL REALITY CREATES REGULATORY UNCERTAINTY.\n\nThe technical utility of pre-training and then fine-tuning is well established [Devlin et al. 2018;Russakovsky et al. 2015].From the perspective of proposed direct copyright liability tests, this paradigm of development could also be favorable to corporations with the resources required to pre-train high-performing models.Moreoever, copyright's constitutional roots in a consideration of technological progress and welfare could support the claim that the production of pre-trained models is a non-infringing use of copyrighted works.For example, Quang [2021] discusses how a pre-training paradigm can support innovation through increased access to AI system development for smaller developers that build upon pre-trained models.\n\nThe combination of multiple actors and training processes could complicate the attribution of copyright liability, and rightsholders may additionally turn to indirect liability and indirect infringement as a way to protect their work.Indirect liability applies to technology that facilitates downstream copyright infringements.From a practical perspective, indirect liability suits are important for enforcement because it can be infeasible or impractical to target a diverse and distributed population of direct infringers [Menell 2013].However, I identify ways that the application of indirect copyright liability to the modern ML supply chain may be evaded.First, in certain instances, a primary or direct infringer in the sense of copyright law may simply not exist.Second, the Sony Safe Harbor may afford developers of pre-trained models with a strong defense against claims of indirect infringement.\n\n3.1 Identifying direct infringement could be complicated at multiple points.\n\nApplying indirect copyright liability starts with determining direct infringement-without direct infringement, there is no indirect infringement [Menell et al. 2023].In the case of pre-trained models, it may be difficult to establish direct infringement from model fine-tuning or use3 .The complexity of the interaction between model pre-training, fine-tuning, and use is difficult to disentangle.Furthermore, downstream users and developers often have limited knowledge about training data or mitigation techniques.\n\nFine-tuning.One possible location for liability in the generation of infringing output is with the model fine-tuner.",
            "score": 0.47126126119969336,
            "section_title": "THE GULF BETWEEN TECHNICAL",
            "char_start_offset": 12390,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 77,
                    "end": 201
                },
                {
                    "start": 201,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 613
                },
                {
                    "start": 613,
                    "end": 808
                },
                {
                    "start": 810,
                    "end": 1044
                },
                {
                    "start": 1044,
                    "end": 1137
                },
                {
                    "start": 1137,
                    "end": 1348
                },
                {
                    "start": 1348,
                    "end": 1470
                },
                {
                    "start": 1470,
                    "end": 1580
                },
                {
                    "start": 1580,
                    "end": 1715
                },
                {
                    "start": 1717,
                    "end": 1793
                },
                {
                    "start": 1795,
                    "end": 1961
                },
                {
                    "start": 1961,
                    "end": 2081
                },
                {
                    "start": 2081,
                    "end": 2192
                },
                {
                    "start": 2192,
                    "end": 2311
                },
                {
                    "start": 2313,
                    "end": 2325
                },
                {
                    "start": 2325,
                    "end": 2429
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 200,
                    "matchedPaperCorpusId": "2930547"
                },
                {
                    "start": 1334,
                    "end": 1346,
                    "matchedPaperCorpusId": "154690660"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.128662109375
        },
        {
            "corpus_id": "270703065",
            "title": "U Can't Gen This? A Survey of Intellectual Property Protection Methods for Data in Generative AI",
            "text": "The tool relies on Contrastive Language-Image Pre-Training (CLIP) retrieval to enable artists to search the LAION-5B and LAION-400M datasets, which are used to train Stable Diffusion and Midjourney models.In the Spawning framework, this detection serves as an initial step, which can be followed by an opt-out mechanism for artists to retract their art from the datasets.\n\nData attribution in generative models involves identifying the contribution of specific data samples to the generated outputs.From an ethical point of view, this process is crucial for understanding how models generate content, addressing not only concerns about copyright but also data privacy and transparency.Techniques for data attribution may include proactive detection methods such as embedding watermarks in the training data which can later be detected in the generated content, i.e. forensic watermarks discussed in Section VI-B4.On the other hand, different analytical methods may be used to trace the influence of training samples on the behaviour of the model.\n\nGiven a generated sample, finding the contributing matches across the large training dataset is often infeasible, however, in some special cases, the approximate matches can be extracted.This special case requires generated content to be highly, almost 100% influenced by a single training sample and is shown to be a feasible scenario [62], [61].In Section V we discussed this as a data replication problem that may be leveraged for copyright infringements.In this context, it can serve as evidence to the copyright owner that their data has been used for training.\n\nWhen it comes to attribution in the more general but complex scenario where the generated sample is influenced by multiple training samples, Wang et al. [128] take a leap towards understanding the influences of an individual image in the generative process.",
            "score": 0.47082383391299115,
            "section_title": "B. Categorisation of IP protection methods",
            "char_start_offset": 70668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 205,
                    "end": 371
                },
                {
                    "start": 373,
                    "end": 499
                },
                {
                    "start": 499,
                    "end": 685
                },
                {
                    "start": 685,
                    "end": 913
                },
                {
                    "start": 913,
                    "end": 1046
                },
                {
                    "start": 1048,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1395
                },
                {
                    "start": 1395,
                    "end": 1506
                },
                {
                    "start": 1506,
                    "end": 1614
                },
                {
                    "start": 1616,
                    "end": 1873
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26416015625
        },
        {
            "corpus_id": "278481437",
            "title": "Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog Generation using LLMs",
            "text": "As LLMs continue to scale in size and training datasets expand, the ability of LLMs to learn, memorize, and respond with texts that are copyright-protected is becoming an increasing risk to state-of-the-art models. Questions regarding the legal implications of the definition of \"fair use\" in model training and generation are actively debated [23], [24]. This has given rise to a series of lawsuits facing prominent generative-AI entities, including Stability AI in which plaintiffs allege improper use of their image content in training [25], along with claims by the New York Times that GPT-4 infringes copyright by reproducing articles in its output [26]. This has also impacted LLMs for code generation, in which Microsoft and OpenAI are claimed to have improperly utilized and monetized open-source programs [21]. These developments have motivated the need to develop frameworks to identify if copyrighted content in utilized in LLM training [27] and prevent the generation of copyrighted text [28]. This concern of improper use of code in training is relevant to the hardware domain regarding IP piracy as LLMs begin to be fine-tuned on hardware datasets. Beyond utilizing opensource code and preliminary license checks [17], minimal protections have been established in ensuring copyright-protected hardware is not utilized in LLM training.",
            "score": 0.4705940903137414,
            "section_title": "A. LLM Risks in Copyright Infringement",
            "char_start_offset": 5028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1348
                }
            ],
            "ref_mentions": [
                {
                    "start": 654,
                    "end": 658,
                    "matchedPaperCorpusId": "274597687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0849609375
        },
        {
            "corpus_id": "257771630",
            "title": "Foundation Models and Fair Use",
            "text": "As demonstrated throughout this work, the risk of copyright violation and litigation, even with fair use protection, is a real concern. To mitigate these risks, we recommend that foundation model practitioners consider implementing the mitigation strategies outlined here and pursuing other novel research in this area. There is significant, exciting, technical research required to make technical mitigation strategies robust and aligned with fair use doctrine. We reinforce that machine learning researchers must play a role in providing viable mitigation mechanisms to demonstrate that models are truly covered by fair use.\n\nPreventing extreme outcomes in the evolution of fair use law by advancing mitigation strategies. Legal scholars have noted that there might be two extreme outcomes for fair use and machine learning (Sobel, 2017). On one hand, there is a possibility that courts may rule that foundation models are widely acceptable under fair use regardless of the likelihood of infringement or efforts at mitigation, which could have adverse effects on the income of data creators and disregard the ethical and moral rights attached to their work. On the other hand, there is a possibility that courts may declare that generative foundation models cannot be trained on unlicensed copyrighted data in most cases. This scenario could lead to a concentration of power for companies that have retained licenses to large amounts of data; companies like YouTube or Facebook might be able to leverage large amounts of user-contributed data where others would be shut out of model training. Neither of these two outcomes is ideal. As litigation progresses, identifying mechanisms to prevent extreme outcomes will be critical. For example, it is important to demonstrate that not all forms of foundation models are inherently infringing and that some of their potential risks can be effectively managed through technical means.\n\nWith better demonstrations of co-evolving technical mitigation strategies, the law might find a middle ground that allows model training and deployment with sufficient effort to implement objectively strong mitigation strategies. Courts may consider the reasonable efforts of model builders and deployers to mitigate copyright risk, both in deciding fair use and in determining whether they can face indirect infringement liability. Trademark courts have taken a similar approach, for example in Tiffany v. eBay (2d Cir. 2010). As such, advancing research in this area (with methods such as improved similarity metrics) may help in preventing extreme outcomes in legal settings. 41 We should not over-zealously filter.",
            "score": 0.46968713549942986,
            "section_title": "Forward-looking Agenda",
            "char_start_offset": 107753,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 826,
                    "end": 839,
                    "matchedPaperCorpusId": "115500744"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.154541015625
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "The target model is a text-to-image diffusion model that has not been pretrained on copyrighted images, and II. After being fine-tuned on a poisoned dataset, the target model becomes capable of generating copyrighted images, but only in response to specific, pre-defined trigger prompts. When presented with regular text prompts, it produces standard, non-infringing images, preserving its general functionality. \n\nIt is crucial to emphasize the importance of ensuring that the model is not pre-trained on copyrighted materials. Recent findings by Li et al. (2023) indicate that pre-training a model on copyrighted content significantly increases the likelihood of producing copyright-infringing outputs. Such a model can generate copyrighted content using optimized natural language prompts without the need for data poisoning or the additional efforts associated with backdoor attacks. For clarification, we list the constraints of the copyright infringement attack as follows: \n\n\u2022 Inconspicuous Prompt Trigger: In our setting, the trigger refers to natural language prompts that are indistinguishable from other clean prompts, ensuring they do not arouse suspicion or be cleaned by text cleaning techniques. \n\n\u2022 Trigger Specificity: The model only produces copyrighted images when activated by certain triggers while retaining its ability to generate normal images with standard prompts. \n\n\u2022 Poisoning Data without Copyright Issues: The poisoning data used to compromise the model should not bear significant resemblance to the original copyrighted works, aligning with legal standards to avoid detection (Osterberg & Osterberg, 2003). \n\n\u2022 Stealthiness of Poisoning Data: The text-image pairs introduced as poison must be matching pairs to avoid being filtered out during data preprocessing. Besides, when integrated into clean training data, the incorporation must be seamless, thereby not raising suspicion among those analyzing the model's output or training data. \n\nComparison with Standard Backdoor Attacks. Our attack method is different from traditional backdoor attacks. \n\nIn our approach, the trigger is a specific prompt, while the backdoors are poisoned image-caption pairs. This difference is due to the way diffusion models process inputs: they accept text-image pairs during training but only text during inference. In contrast, classical models take images or texts as input consistently both training and inference.",
            "score": 0.46956542251731537,
            "section_title": "Copyright Infringement Attack",
            "char_start_offset": 9054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1390
                },
                {
                    "start": 1393,
                    "end": 1638
                },
                {
                    "start": 1641,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2081
                },
                {
                    "start": 2084,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2332
                },
                {
                    "start": 2333,
                    "end": 2434
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6826171875
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "Copyright law confers upon creators the exclusive rights to reproduce, distribute, and monetize their creative works. However, recent progress in text-to-image generation has introduced formidable challenges to copyright enforcement. These technologies enable the unauthorized learning and replication of copyrighted content, artistic creations, and likenesses, leading to the proliferation of unregulated content. Notably, models like stable diffusion, which excel in text-to-image synthesis, heighten the risk of copyright infringement and unauthorized distribution.Machine unlearning, which seeks to eradicate the influence of specific data or concepts from machine learning models, emerges as a promising solution by eliminating the \\enquote{copyright memories} ingrained in diffusion models. Yet, the absence of comprehensive large-scale datasets and standardized benchmarks for evaluating the efficacy of unlearning techniques in the copyright protection scenarios impedes the development of more effective unlearning methods. To address this gap, we introduce a novel pipeline that harmonizes CLIP, ChatGPT, and diffusion models to curate a dataset. This dataset encompasses anchor images, associated prompts, and images synthesized by text-to-image models. Additionally, we have developed a mixed metric based on semantic and style information, validated through both human and artist assessments, to gauge the effectiveness of unlearning approaches. Our dataset, benchmark library, and evaluation metrics will be made publicly available to foster future research and practical applications (https://rmpku.github.io/CPDM-page/, website / http://149.104.22.83/unlearning.tar.gz, dataset).",
            "score": 0.4694035734002602,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.266357421875
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.",
            "score": 0.4693161651788127,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.385986328125
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "We briefly review related work about text-to-image diffusion models, copyright protection and Reinforcement Learning from Human Feedback. \n\nText-to-Image Diffusion Models: Recently, text-toimage diffusion models have garnered significant attention in research. These advanced methods (Balaji et al. 2023;Nichol et al. 2022;Rombach et al. 2022a;Saharia et al. 2022) have demonstrated exceptional capabilities in converting textual descriptions into visually coherent and realistic images with high accuracy. The advancements in these techniques have unlocked numerous possibilities for various downstream tasks, including image editing (Avrahami, Lischinski, and Fried 2022;Ho, Jain, and Abbeel 2020a;Kawar et al. 2023), image denoising (Ho, Jain, and 1 https://law.justia.com/cases/federal/appellate-courts/F2/562/ 1157/293262/ Abbeel 2020a; Xie et al. 2023), andsuper-resolution (Sohl-Dickstein et al. 2015;Ho, Jain, and Abbeel 2020b). \n\nCopyright Protection: Several studies in the legal literature have examined copyright issues in machine learning and data mining, focusing primarily on potential infringements during the training phase.: (1)Watermarking (Dogoulis et al. 2023;Epstein et al. 2023), which inserts specific, unnoticeable patterns into protected images to detect copyright infringement, has been explored, but further research is needed to improve its robustness. (2)Concept Removal: To remove explicit artwork from large models, (Gandikota et al. 2023) presents a fine-tuning method for concept removal from diffusion models. Additionally, (Zhang et al. 2023) presents the \"Forget-Me-Not\" method, which enables the targeted removal of specific objects and content from large models within 30 seconds while minimizing the impact on other content. (3) Dataset Deduplication: (Somepalli et al. 2022) explores whether diffusion models create unique artworks or directly replicate certain content from the training dataset during image generation.",
            "score": 0.467564000830991,
            "section_title": "Related Work",
            "char_start_offset": 4920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 140,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 323,
                    "end": 344,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 344,
                    "end": 363,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 635,
                    "end": 673,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 700,
                    "end": 718,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 1159,
                    "end": 1181,
                    "matchedPaperCorpusId": "258297834"
                },
                {
                    "start": 1181,
                    "end": 1201,
                    "matchedPaperCorpusId": "264436550"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2359619140625
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "RLCP aims to achieve the dual objectives of maintaining high-quality image generation and ensuring compliance with copyright laws, thereby addressing the pressing challenge of copyright infringement in text-to-image diffusion models. It consists of several key components: \n\n\u2022 Reward-Based Learning Framework: Our approach involves using a discriminator in the reward model to score generated samples. Images that are less similar to copyrighted data receive higher rewards. This reward system helps guide the model towards generating compliant images. \u2022 Fine-Tuning with Reinforcement Learning: Starting from a pre-trained text-to-image diffusion model, we fine-tune it using RL techniques to optimize the generation process according to the defined reward functions. This involves framing the denoising process as a multistep decision-making problem and applying policy gradient algorithms to maximize the reward signal. \u2022 KL Regularization: To prevent the model from overfitting to the reward function, we introduce KL divergence as a regularization term. This helps maintain the generative capabilities of the original diffusion model while steering it towards producing non-infringing content. \n\nSpecific training process we describe how we fine-tune a pre-trained text-to-image diffusion model using reinforcement learning (RL) to minimize the risk of generating copyright-infringing content. Our approach is guided by a reward function that leverages the copyright loss (CL) metric defined in Eq. ( 5), and incorporates KL regularization to ensure the model maintains high-quality image generation. \n\nModel Initialization: We begin with a pre-trained diffusion model M \u03b8 , where \u03b8 denotes the model parameters. The model is initialized based on a large-scale text-toimage dataset that includes both copyrighted (D c ) and noncopyrighted (D nc ) data. \n\nReward Function Design: The reward function r(x 0 , c) is the key component guiding the model's training. It is defined as a weighted sum of the semantic similarity loss L sem and the perceptual similarity loss L perc : \n\nwhere: \n\n\u2022 CL sem (x 0 , c) represents the semantic similarity loss, calculated as the Mean Squared Error (MSE) between the generated image embeddings and the embeddings of the original copyright images.",
            "score": 0.46718116649603725,
            "section_title": "The proposed method RLCP",
            "char_start_offset": 13464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 272
                },
                {
                    "start": 275,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1605
                },
                {
                    "start": 1608,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1857
                },
                {
                    "start": 1860,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2079
                },
                {
                    "start": 2082,
                    "end": 2088
                },
                {
                    "start": 2091,
                    "end": 2285
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3984375
        },
        {
            "corpus_id": "269635525",
            "title": "An Inversion-based Measure of Memorization for Diffusion Models",
            "text": "Diffusion Models (DMs) [22,49] have shown impressive capability in the generation of images [40,41], videos [6], 3D point cloud [34], etc. These techniques lay the foundation for commercial systems or communities such as Stable Diffusion [41], Midjourney [1], DALL\u2022E [3,40] and Imagen [42], which have attracted millions of active users. The popularity of diffusion models can be attributed to the hierarchical denoising procedure, which offers high stability for training on billions of data [45] and scalability for multimodal conditional generation [40,42]. \n\nHowever, large-scale data sets used to train the prevalent DMs, e.g., the open-source image-caption dataset LAION [45], are widely acknowledged to contain content that will raise concerns regarding copyright infringe- ment, privacy invasion, etc. For example, recent reports revealed that LAION could refer to photographers' work without authorization [18] and contain private medical photographs [2]. With the uncurated data for training, diffusion models are likely to generate content that infringes the creators' rights or exposes private information [9,46]. In this work, we focus on memorization in DMs, one of the most critical issues of training data misuse. \n\nMemorization usually manifests itself at sampling time, when a model generates images closely resembling training images. It can turn into a problem both technically and practically. Technically, memorization indicates failure of generalization to some degree because a probabilistic generative model is supposed to produce novel and diversified images rather than simply replicate those in the training set. Practically, memorization might also raise ethical and legal concerns. First, even the model developers are authorized to train their model with protected images, the image owners will never expect their works to be exposed to arbitrary users due to memorization as this would cause indisciplinable dissemination, violating the contextual integrity [36]. Second, in past years, text-to-image models have been facing lawsuits for generating derivative images that mimic the style of artists. Compared to derivative generations whose legality is still in pending [44], exact replication of copy-righted images is undisputedly intolerable.",
            "score": 0.46669222483374945,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 560
                },
                {
                    "start": 563,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2277
                }
            ],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 30,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 128,
                    "end": 132,
                    "matchedPaperCorpusId": "232092778"
                },
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 493,
                    "end": 497,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 556,
                    "end": 559,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 1118,
                    "end": 1121,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1121,
                    "end": 1124,
                    "matchedPaperCorpusId": "256662278"
                },
                {
                    "start": 1990,
                    "end": 1994,
                    "matchedPaperCorpusId": "150528892"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.293701171875
        },
        {
            "corpus_id": "277452130",
            "title": "TraceMark-LDM: Authenticatable Watermarking for Latent Diffusion Models via Binary-Guided Rearrangement",
            "text": "As shown in Fig. 1, the image generation and watermarking mechanisms involve the service provider, regular users, and malicious users. The service provider serves as the central entity, responsible for developing and maintaining the image generation model. It offers the API for target users and embeds watermarks to safeguard the copyright of generated content. Regular users are legitimate individuals who adhere to the service agreement and utilize the image generation model by providing text descriptions or other forms of input. They can generate images using the API provided by the service provider. Malicious users are those who attempt to bypass watermark protection mechanisms to generate illicit content or tamper with others' generated content to commit infringement. These users employ various methods, such as noise addition, noise removal, or image compression, to remove or disguise the watermark and evade detection. Their primary objective is to generate unauthorized images or use generated images for infringing purposes. \n\nTo effectively safeguard the copyright of generated images and attribute illicit content to specific users, the service provider must implement watermarking mechanisms, which can be divided into two categories based on functionality: detection watermarking and attribution watermarking.",
            "score": 0.4659427547905963,
            "section_title": "III. PROBLEM STATEMENT AND BACKGROUND A. Scenario Description",
            "char_start_offset": 12238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1331
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27294921875
        },
        {
            "corpus_id": "270764347",
            "title": "Evaluating Copyright Takedown Methods for Language Models",
            "text": "Copyright and LMs. Language models are trained on massive amounts of data sourced from the internet, which may include copyrighted material due to imperfect curation processes. This has led to a wave of litigation in the United States and other countries, as content creators challenge the use of their copyrighted works in the training and deployment of foundation models (Tremblay v. OpenAI, Inc.,, 2023; Kadrey v. Meta Platforms, Inc., 2023; Chabon v. OpenAI, Inc.,, 2023; DOE 1 v. GitHub, Inc., N.D. Cal. 2022). Studies have demonstrated that these models can generate verbatim chunks from copyrighted books and code, effectively reproducing protected works without authorization (Henderson et al., 2023;Liang et al., 2023;Chang et al., 2023;Lee et al., 2024;Karamolegkou et al., 2023;Chu et al., 2024). These findings have raised concerns about the ethical use of language models and have led to a growing call for increased transparency and accountability in their development and deployment (Bommasani et al., 2023;Longpre et al., 2023). Recent research has also shown that image and video generation models can reproduce copyrighted characters (He et al., 2024;Kim et al., 2024); however, these are beyond the scope of this work, as we focus on textual materials. Besides our work, Chen et al. (2024) propose CopyBench concurrently, which also aims to measure copyright issues in the model generation. Different from ours, they categorize copyright issues into literal and non-literal copying and includes fluency as one of the utility metrics. Future work should incorporate both CoTaEval and CopyBench for a more comprehensive evaluation. \n\nMitigations for Copyright Concerns. Few solutions have been proposed to technically address the copyright and transparency issues associated with LMs. Min et al. (2023) suggest training a parametric language model on an open-source corpus and augmenting it with a non-parametric datastore containing copyrighted materials, which would be queried only during inference.",
            "score": 0.4653793250677819,
            "section_title": "Related Work",
            "char_start_offset": 26700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1648
                },
                {
                    "start": 1651,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 2019
                }
            ],
            "ref_mentions": [
                {
                    "start": 727,
                    "end": 746,
                    "matchedPaperCorpusId": "258426273"
                },
                {
                    "start": 746,
                    "end": 763,
                    "matchedPaperCorpusId": "260416806"
                },
                {
                    "start": 763,
                    "end": 789,
                    "matchedPaperCorpusId": "264426289"
                },
                {
                    "start": 789,
                    "end": 806,
                    "matchedPaperCorpusId": "261076348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1409912109375
        },
        {
            "corpus_id": "278338968",
            "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
            "text": "Model could also be made to unlearn copy righted data but that comes with a huge computational expense [31,7,8]. \n\nPost-deployment strategies are usually cheaper. Model users could be educated on using AI responsibly. Furthermore, before a user inputs a prompt, a \"system message\" could be added that aims to reduce the IP-infringement risks [13]. This is a kind of prompt engineering techniques [28,5,17] that have been popularly used to enhance GenAI model performance on diverse tasks. Another possibility is to check each generated output against a corpus of copyrighted images. Since the size of such a corpus is huge, this approach soon becomes infeasible. \n\nWe focus on prompt engineering and evaluate its effectiveness for IP risk mitigation-Figure 1 presents a snapshot of our results indicating that prompt engineering could substantially alter training data replication for vision models. Since GenAI outputs are sensitive to input prompts, we expect that via a carefully engineered prompt, IP infringement related risk can be reduced. Prompt engineering has already been explored for reducing hallucination in large language models [26,3]. For vision models however, it is largely unexplored [10]. \n\nPrompt engineering for image generation and user accessibility has primarily developed through informal, trial-and-error approaches. In artistic communities, a common trend has emerged where prompts frequently take the form of \"X in the style of Y,\" with Y referring to a particular artist or artistic movement. Based on this observation, [11] explored how modifications to this prompt pattern influence the behavior of the image generation model. Their experiments focused on the influence of different phrasings of the prompt, different random initializations, the number of iterations, and the style as well as the subject parameter. \n\nIn this paper, we want to contribute to ongoing research on prompting in visual diffusion models by applying various prompt engineering strategies for generating non-copyright-violating content and evaluating their performances. To the best of our knowledge, for IP risk mitigation involving vision models, ours is the first work of its kind that develops an extensive framework for exploring prompt engineering.",
            "score": 0.4649415689723713,
            "section_title": "Introduction",
            "char_start_offset": 2167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 115,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1848
                },
                {
                    "start": 1851,
                    "end": 2079
                },
                {
                    "start": 2080,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 107,
                    "matchedPaperCorpusId": "257833863"
                },
                {
                    "start": 107,
                    "end": 109,
                    "matchedPaperCorpusId": "258740988"
                },
                {
                    "start": 109,
                    "end": 111,
                    "matchedPaperCorpusId": "266374816"
                },
                {
                    "start": 1204,
                    "end": 1208,
                    "matchedPaperCorpusId": "237513697"
                },
                {
                    "start": 1551,
                    "end": 1555,
                    "matchedPaperCorpusId": "237513697"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.430908203125
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "The increasing sophistication of text-to-image generative models has led to complex challenges in defining and enforcing copyright infringement criteria and protection. Existing methods, such as watermarking and dataset deduplication, fail to provide comprehensive solutions due to the lack of standardized metrics and the inherent complexity of addressing copyright infringement in diffusion models. To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. Experiments conducted on 3 mixed datasets of copyright and non-copyright images demonstrate that our approach significantly reduces copyright infringement risk while maintaining image quality.",
            "score": 0.46389217053018206,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.449462890625
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "In addition to the discussion in \u00a7 1 on the unique challenges posed by copyrighted characters and the value of a quantitative study In this matter, we further expand on the legal background and broader societal impacts of our study below. \n\nLegal Motivations Past work has studied the setting of verbatim regurgitation of images (car, 2023), and some lawsuits focus on this particular legal issue (Vincent, 2023;Andersen et al. v. Stability AI et al., N.D. Cal. 2023). Some recent work builds datasets for benchmarking copyright infringement unlearning methods (Ma et al., 2024) and attempts to jailbreak proprietary systems to output copyrighted images (Kim et al., 2024). Among the copyrighted subjects of interest, copyrighted characters, such as popular Intellectual Property (IP) from Disney, Nintendo, and Dreamworks, pose a unique legal challenge (Sag, 2023;Henderson et al., 2023;Lee et al., 2024). At least one lawsuit in China has already resulted in liability for an image generation service that generated the copyrighted character, Ultraman (Shimbun, 2024). Unlike in the verbatim memorization setting, copyrighted characters are computationally more like general concepts that can appear in many poses, sizes, and variations in the training data. Typical deduplication, or even near access free learning approaches (Vyas et al., 2023), will not work for this task (Henderson et al., 2023). \n\nCopyrighted characters are also in a certain area of copyright law with distinct rules to determine infringement (Schreyer, 2015;Hennessey, 2020). To simplify the legal rules, characters are defined by key distinctive features that as a whole comprise the character. not (Liu, 2013;Lee, 2019). \n\nAs discussed earlier, to simplify the legal rules, characters are often defined by key distinctive features that as a whole compromise the character. This can lead to interesting situations. For example, in 2023 the copyright for the original version of Mickey Mouse character (Steamboat Willie) entered the public domain. But this version of the character did not wear white gloves.",
            "score": 0.4638905225713983,
            "section_title": "A LEGAL BACKGROUND AND BROADER SOCIETAL IMPACTS",
            "char_start_offset": 36283,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 241,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1403
                },
                {
                    "start": 1406,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1699
                },
                {
                    "start": 1702,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2085
                }
            ],
            "ref_mentions": [
                {
                    "start": 1329,
                    "end": 1348,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 1519,
                    "end": 1535,
                    "matchedPaperCorpusId": "153105953"
                },
                {
                    "start": 1535,
                    "end": 1551,
                    "matchedPaperCorpusId": "226546743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.029815673828125
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "While no methods explicitly claim to be used for authenticating copyright on few-shot generation, both existing image generation [1,31] and inpainting [33] pipelines exhibit the potential to do so.Particularly, we compare our method with three types of pipelines that could be employed for identifying infringements: Text-to-Image Generation.Using the official Text-to-Image pipeline in diffusers 4 , we generate 100 \u00d7 Num images for each fine-tuned model using the prompt employed during training.Here, 'Num' represents the total number of images in the membership and holdout datasets.This process is applied individually to each image in both datasets.Image-to-Image Generation.Leveraging the official Image-to-Image pipeline in diffusers 5 , we use the training prompt to generate 100 \u00d7 Num images for each fine-tuned model.The inference step is set to 50 and the img2img strength is set to 0.7 for each image in the membership or holdout dataset.Inpainting.Employing the state-of-the-art inpainting pipeline DDNM [33], we generate 100 \u00d7 Num images by masking the right half of each image.We use the prompt applied during training for inpainting, and the inference step is fixed at 50.\n\nWe set the number of generated images per input image to 100.This ensures that the time cost for both the baseline method and our method remains similar, facilitating a fair comparison (see Appendix H for more details).We define the similarity between the output images and a given image as the highest similarity achieved among all the generated images corresponding to one target image.\n\nThe comparison between CGI-DM and other pipelines is presented in Tab. 1.It is evident that CGI-DM outperforms others significantly in various few-shot generation scenarios 4 https://huggingface.co/docs/diffusers/api/pipelines/stable diffusion/text2img  across different datasets.",
            "score": 0.4631839749548756,
            "section_title": "Comparison with Existing Methods",
            "char_start_offset": 16755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 342
                },
                {
                    "start": 342,
                    "end": 498
                },
                {
                    "start": 498,
                    "end": 587
                },
                {
                    "start": 587,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 951
                },
                {
                    "start": 951,
                    "end": 962
                },
                {
                    "start": 962,
                    "end": 1093
                },
                {
                    "start": 1093,
                    "end": 1189
                },
                {
                    "start": 1191,
                    "end": 1252
                },
                {
                    "start": 1252,
                    "end": 1410
                },
                {
                    "start": 1410,
                    "end": 1579
                },
                {
                    "start": 1581,
                    "end": 1654
                },
                {
                    "start": 1654,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 151,
                    "end": 155,
                    "matchedPaperCorpusId": "254125609"
                },
                {
                    "start": 1018,
                    "end": 1022,
                    "matchedPaperCorpusId": "254125609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.023101806640625
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "In this paper, we study the important subquestion of copyrighted protection focusing on copyrighted characters. We address two main research questions: 1) Which textual prompts can trigger generation of copyrighted characters; and 2) How effective are current runtime mitigation strategies and how we can improve them? To systematically study these questions, we develop an evaluation framework that considers both elimination of similar output to copyrighted characters and generated image's consistency with user input. We show how to leverage embedding space distance and common training corpora to extract useful indirect anchors-descriptions and keywords not explicitly mentioning the characters' names but can be effective in triggering copyrighted character generation. Existing mitigations, namely prompt rewriting, are not fully effective and we suggest new runtime methods to improve them. Our work calls for more attention to the indirect anchoring challenge and the effectiveness of deployed mitigation strategies for copyrighted character protection. The insights we provide here can be operationalized by model deployers for copyright-aware image and video generation systems in the future. Huang is supported by the Wallace Memorial Fellowship. This research is partially supported by a Princeton SEAS Innovation Grant. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors.",
            "score": 0.46249106375526144,
            "section_title": "CONCLUSION",
            "char_start_offset": 31696,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1502
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "For diffusion models, the output is influenced not only by the prompt but also by the latent noise. Latent noise represents encoded representations of the input that capture essential features in a lower-dimensional space. These latent variables guide the generation process, affecting the finer details of the resulting image. In this section, we propose a reinforcement learning (RL)-based latent control method to mitigate copyright infringement in diffusion-based generative models. Our method involves training an agent to search the input latent variables that yield lower infringement scores, ensuring that the generated outputs do not violate copyright. \n\nSpecifically, for latent variable z, we define a policy \u03c0 \u03c9 parameterized by \u03c9, allowing us to sample latent noise \u03f5 \u223c \u03c0 \u03c9 (z), which follows a Gaussian distribution. The sampled noise \u03f5 is then passed through the pre-trained diffusion decoder f to produce the image x = f (z, \u03f5). \n\nTo assess the copyright infringement potential of the generated image, we employ our CopyJudge to obtain the infringement score s f . Based on this score, we define a reward function: \n\nThis reward is designed to penalize outputs with higher infringement scores, thus encouraging the generation of non-infringing content. We optimize the parameters \u03c9 by maximizing the expected reward, L(\u03c9), defined as: \n\nThe gradient of this objective is computed using the REIN-FORCE rule (Williams, 1992), which is given by: \n\nDuring the training process, the latent variable z is updated according to the following rule: \n\nwhere \u03b2 is the step size. We further conduct normalization for the latent variables to maintain stability and prevent extreme deviations. This RL-based approach allows the agent to explore variations in the latent space, thereby improving its ability to generate non-infringing content. The detailed algorithm can be found in appendix B.1.",
            "score": 0.46197030369168335,
            "section_title": "Mitigation via RL-based Latent Control",
            "char_start_offset": 13699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 944
                },
                {
                    "start": 947,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1555
                },
                {
                    "start": 1558,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1897
                }
            ],
            "ref_mentions": [
                {
                    "start": 1422,
                    "end": 1438,
                    "matchedPaperCorpusId": "2332513"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.415771484375
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "This essay is an attempt to work systematically through the copyright infringement analysis of the generative AI supply chain. Our goal is not to provide a definitive answer as to whether and when training or using a generative AI is infringing conduct. Rather, we aim to map the surprisingly large number of live copyright issues that generative AI raises, and to identify the key decision points at which the analysis forks in interesting ways. \n\nWe assume basic familiarity with generative AI systems and terminology. We will focus primarily on transformer-based large language models (LLMs) and on diffusion-based image models, but most of the analysis should apply to other architectures and other types of generated content. \n\nPart I provides a brief breakdown of the stages in what we call the \"generative AI supply chain. \" Part II, the main body of the essay, works through the copyright analysis.",
            "score": 0.4609555609725797,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 446
                },
                {
                    "start": 449,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 906
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.318603515625
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "All generative-AI models are trained on some type of data: examples of the type of material that they are to generate. Text-generation systems are trained on text, image-generation systems are trained on images, music-generation systems on music, and so on. All of this data must come from somewhere. The most prominent current generative AI models have been trained on data that already existed and was not created specifically for the purpose of AI training. And for AIs that generate potentially copyrightable (or copyrightinfringing) material, the training data itself will often include copyrightable expression. GitHub Copilot is trained on copyrighted code, ChatGPT is trained on textual data scraped from the web, Stable Diffusion is trained on images, and so on. \n\nFor the most part, it is the copyright owners of these individual training works who are the potential plaintiffs in any copyright infringement suit against actors at other stages of the supply chain. These are the relevant copyrights. \n\nIndividual pieces of training data are useless by themselves. 1 The AI training process requires vast quantities of data to create cutting-edge models, and those vast quantities of data must be arranged into datasets that have recurring, standard structure. 2 Sometimes this process is carried out by the same entities that train the AI models. More commonly, however, the process is split across different actors. Stable Diffusion, for example, is trained on images from datasets curated by the non-profit organization LAION. It is necessary, therefore, to consider the potential liability of dataset curators separately from the potential liability of model trainers. \n\nNote that dataset curation, as just described, will frequently involve \"the collection and assembling of preexisting materials or of data that are selected, coordinated, or arranged in such a way that the resulting work as a whole constitutes an original work of authorship. \" 3 As such, training datasets can themselves be copyrighted, such that the copying without permission of the dataset as a whole could constitute infringement, separate and apart from infringement on the underlying works the dataset comprises. In practice, however, it appears that most uses of training datasets are licensed -either through a bilateral negotiation or by means of an open-source license offered to the world by the dataset compiler.",
            "score": 0.46061111119426906,
            "section_title": "A. Data Creation",
            "char_start_offset": 1080,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2408
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.465087890625
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "Figure 2: Overview of RLCP. \n\naiming to produce a feature vector fi that is close to f i . \n\nTo ensure the model does not generate images that infringe on copyrights, we define a copyright loss CL, which penalizes the generation of images xi that are overly similar to those in D c . Additionally, we employ the Fr\u00e9chet Inception Distance (FID) to evaluate the quality of the generated images, ensuring that they are both visually coherent and diverse. \n\nThe objective is to train the model in such a way that it minimizes the copyright loss L c while maintaining a high FID score, effectively balancing the trade-off between reducing copyright infringement and preserving image quality.",
            "score": 0.46032967152169646,
            "section_title": "RL Fine-tune",
            "char_start_offset": 9106,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 30,
                    "end": 90
                },
                {
                    "start": 93,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 687
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1873779296875
        },
        {
            "corpus_id": "269294137",
            "title": "An Economic Solution to Copyright Challenges of Generative AI",
            "text": "Recent advancements in generative artificial intelligence (AI) have profoundly impacted the creative industries, ushering in an era of AI-generated content in literature, visual arts, and music. Trained on vast datasets of human-generated material, generative AI models such as large language models and diffusion models can now produce content with a sophistication that rivals-and may potentially displace-the works of human artists [30,2,16]. This burgeoning capability raises crucial questions about the legal and ethical boundaries of creative authorship, particularly concerning copyright infringement by generative models [32,34]. Consequently, several AI companies are currently involved in lawsuits over allegations of producing content that potentially infringes on copyrights [34,13]. \n\nEfforts to mitigate the tension between owners of copyright in the training data and AI developers have emerged, mostly involving modifications to generative model training or inference to reduce the likelihood of generating infringing outputs [37,5,35,15,4]. However, these modifications may compromise model performance due to either the exclusion of high-quality, copyrighted training data from training or restrictions on content generation [22]. The complexity and ambiguity of copyright law add another layer of difficulty, blurring the line between infringing and non-infringing outputs. The resulting uncertainty could lead to a significant waste of resources on both sides while these issues are debated in courts [34]. \n\nRather than restricting AI developers' use of copyrighted data, we propose establishing a mutually beneficial revenue-sharing agreement between AI developers and copyright owners. This proposal echoes an argument recently advocated in economics [1]. However, a major challenge in developing a revenue-sharing model for generative AI, in contrast to conventional cases of sharing between digital platforms and independent content creators [7], lies in the complexity of training generative models on diverse data sources. This results in the \"black-box\" nature of model training and content generation, making the traditional, straightforward pro rata methods unsuitable [24]. \n\nIn this paper, we introduce a simple framework that appropriately compensates copyright owners for using their copyrighted data in training generative AI systems based on the cooperative game theory, thereby directly addressing the intricacies of copyright challenges.",
            "score": 0.45952234921141055,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2204
                },
                {
                    "start": 2207,
                    "end": 2475
                }
            ],
            "ref_mentions": [
                {
                    "start": 435,
                    "end": 439,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 629,
                    "end": 633,
                    "matchedPaperCorpusId": "258515384"
                },
                {
                    "start": 633,
                    "end": 636,
                    "matchedPaperCorpusId": "259844568"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "259844568"
                },
                {
                    "start": 1521,
                    "end": 1525,
                    "matchedPaperCorpusId": "259844568"
                },
                {
                    "start": 2199,
                    "end": 2203,
                    "matchedPaperCorpusId": "257149150"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.226806640625
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "Our experiments have demonstrated that by generating disguises, one can indirectly acquire access to copyrighted material as part of the training data to fuel LDM-based models, which could reproduce the copyrighted samples during inference or deployment.We expand the definition of direct access to acknowledgment to cover such scenarios as possible copyright infringement: any training data x that contains similar latent information as that of copyrighted image x c , measured by their similarity in the latent feature representation, even visually different from x c , shall be considered to have an acknowledgment of x c .The notion of acknowledgment may also augment existing regulatory frameworks (e.g., the White House Executive Order on AI (Biden 2023)) on AI governance in terms of data quality evaluation.Additionally, our quantification of acknowledgment (the detection method) provide a timely tool for auditing beyond black-box access, which was pointed out to be insufficient in (Casper et al. 2024).",
            "score": 0.4593896952000544,
            "section_title": "A broader definition of access",
            "char_start_offset": 22937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 815
                },
                {
                    "start": 815,
                    "end": 1014
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33154296875
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "Instead, they would involve copyright owners identifying copyrighted works and demanding that the model operator prevent generations that are substantially similar to those works. Some of those works might be identified based on known outputs that are recognizably similar to suspected inputs. But others might simply involve copyright owners handing over to model operators large catalogs of works to block, much as they currently do with ContentID on YouTube. Matching new generations against a catalog of copyrighted works is not a trivial problem, but it is one that has been very approximately solved by major social networks, which use perceptual hashing to prevent the upload of various kinds of identified content. Generative AI companies could at least add similar perceptualhash-driven filtering to the outputs of their models. Alternatively, they could retrain the models without the infringing works in the training dataset. This approach may only work on a prospective basis, however, due to the expense and time required to train a full model. It is often infeasible to retrain a model simply to remove infringing works, and it would be completely unworkable to retrain on each new notice. \n\nIn practice, there would likely be a gravitational force pulling the operator's duties towards the duties of a service provider under section 512(c) or (d): block infringing generations on notice, block infringing generations on actual knowledge, block infringing generations on red-flag knowledge, avoid having a business model that directly ties income to infringement, and terminate the abilities of repeat infringers to continue making generations. It is not the case that these specific requirements arise out of the tests for indirect infringement -the Napster rule is only that a provider must block the infringement of specific works as to which it has been provided notice. Instead, we suspect that the section 512 doctrines will be a convergence point because a model operator that does not implement them might seem to a court to look like an increasingly unappealing candidate for fair use. The reason for this is that courts have now had two decades of experience -which means two decades of precedents -with the section 512 safe harbors.",
            "score": 0.4591460746354108,
            "section_title": "Id. \u00a7 512(i)(1)(A",
            "char_start_offset": 52451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2257
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1959228515625
        },
        {
            "corpus_id": "268691556",
            "title": "Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes",
            "text": "The emergence of Generative Artificial Intelligence (GenAI) models is fostering a radical transformation in the creative domain.By using GenAI mod- * Equal.els, non-professional users can now generate highquality content such as text, images, music, or code.At the same time, however, GenAI is also disrupting copyright law.By reducing the costs and talent barriers associated with creative production, GenAI has flooded markets with a massive yield of syntactic content, leaving scholars and copyright regulators to ponder whether to merit copyright protection to such works (Burk, 2023;Grimmelmann, 2016).The approach proposed in this paper can help the stakeholders grappling with this question by informing them of the scope of unprotected elements in copyrighted works.In addition, numerous lawsuits are emerging, claiming that GenAI models are \"21st-century collage tools\" that infringe the copyrighted works used to train these models v. Stability AI Ltd. (2023); J. Doe 1 v. GitHub (2022); Dogan and Liu (2005).These pending class action lawsuits will require courts to decide, first, whether unauthorized use of copyrighted works to train GenAI models constitutes copyright infringement and second, when does the output generated by GenAI models infringes the copyright of works included in the training dataset.Our paper only addresses the latter question.\n\nThese challenges put increasing pressure on the legal system to discern which portions of copyrighted works are protected from unauthorized use and which portions are not protected and are, therefore, available for others to use.Copyright law's ultimate goal is to foster the creation and dissemination of expressive works by granting authors only limited rights to their respective expressions (Leval, 1990).That is because creation never occurs in a vacuum, and the author always draws upon preexisting materials by adapting them into an original expression.Consequently, some elements of copyrighted works that are building blocks of further creation, such as style, ideas, functions, and methods, are not protected against unauthorized uses (Elkin-Koren et al., 2024).In fact, copyright law encourages users to draw upon these unprotected materials to create new original works of authorship (Litman, 1990).",
            "score": 0.4588442668064644,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 128,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 258
                },
                {
                    "start": 258,
                    "end": 324
                },
                {
                    "start": 324,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 774
                },
                {
                    "start": 774,
                    "end": 1019
                },
                {
                    "start": 1019,
                    "end": 1321
                },
                {
                    "start": 1321,
                    "end": 1366
                },
                {
                    "start": 1368,
                    "end": 1597
                },
                {
                    "start": 1597,
                    "end": 1777
                },
                {
                    "start": 1777,
                    "end": 1928
                },
                {
                    "start": 1928,
                    "end": 2140
                },
                {
                    "start": 2140,
                    "end": 2279
                }
            ],
            "ref_mentions": [
                {
                    "start": 576,
                    "end": 588,
                    "matchedPaperCorpusId": "257940624"
                },
                {
                    "start": 2113,
                    "end": 2139,
                    "matchedPaperCorpusId": "258865475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11419677734375
        },
        {
            "corpus_id": "277711160",
            "title": "Legal regulation of AI-assisted academic writing: challenges, frameworks, and pathways",
            "text": "Copyright infringement risks in AI-assisted academic writing stem primarily from the legality of AI system training data and the infringement risks of generated content. Regarding training data, large language models use large amounts of potentially copyright-protected textual materials during training, and the legal nature of this usage remains controversial (European-Union, 2019). The Authors Guild v. OpenAI case centrally reflects this issue, and its judgment will significantly impact the legitimate scope of AI training data usage (Niu, 2023). \n\nRegarding generated content, AI systems may tend to reproduce expression patterns from training data, increasing the potential risks of copyright infringement (Xiao, 2024;European-Union, 2019). Research indicates that large language models may unconsciously reproduce (copy or adapt) expressions from training data during generation, and this \"latent derivation,\" where AI systems subtly incorporate preexisting expressions without explicit attribution, presents new challenges for copyright protection. \n\nIn response to these risks, we propose systematic prevention mechanisms. First, we recommend improving AI usage registration systems that require researchers to maintain detailed records of AI tool usage, including key information such as purpose, scope, and extent. Second, we should develop specialized content similarity checks and source tracing systems to identify potential infringement issues quickly through technical means. Finally, we should establish clear review standards to regulate the limits of AI-assisted creation (Gulumbe et al., 2024).",
            "score": 0.4582689444013641,
            "section_title": "Infringement risk prevention",
            "char_start_offset": 9338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1616
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17578125
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "We evaluate with a model p behaves with an equal chance of sampling from either model q 1 or q 2 . We consider such model p as a strict protection model that even with the original caption as prompt, there is a 50% chance of generating from the safe model. In contrast, a model p trained on the entire dataset could frequently produce highly similar generations to the target copyrighted image, making it extremely inefficient for the selection of a valid threshold k. Next, we will separately discuss the data, the metrics, and the threat prompts for evaluation. \n\nEvaluation Data. We use samples generated by p for evaluation. To determine whether they infringe the copyright of the target image, we rely on the similarity between samples and the target image as there is no widely acknowledged computable standard for infringement to our knowledge. We employ the current SOTA similarity metric SSCD [31] for copy detection. As SSCD scores of infringed samples for different target images differ significantly by obser-   vation (shown in Fig. 5), we do not indicate a fixed score threshold for all target images. Instead, we use relative thresholds determined as percentiles of the similarity scores among samples generated by p with the original caption of the target image as the prompt, e.g. SSCD-50%. Recognizing the variable criteria of infringement, we report results with other choices of thresholds in Sec. 11.3. \n\nEvaluation Metrics. The CP-k method achieves copyright protection by selectively accepting generated samples using the threshold k x , which can be indicated by the Acceptance Rate (AR). For a given AR, a good protection system is expected to rarely accept infringing content, i.e., have a low False Accept Rate (FAR), as defined in Eq. ( 8). It is worth noting that the choice of k x dictates the trade-off between model safety and efficiency. Furthermore, to our knowledge, there is no principled way to determine k x . As a result, we evaluate the success of attack by reporting the FAR at different AR, e.g., FAR@5%AR. Additionally, the copyright infringement rate (CIR) is also presented for scenarios without copyright protection, i.e., AR=100%.",
            "score": 0.4580976498652406,
            "section_title": "Evaluation Settings",
            "char_start_offset": 22478,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1423
                },
                {
                    "start": 1426,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "247011159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2132568359375
        },
        {
            "corpus_id": "257771630",
            "title": "Foundation Models and Fair Use",
            "text": "the model user is the one that uploads copyrighted content and the model transforms this content (e.g., adding a filter) before giving it back to the user, liability is more likely to rest with the user. This is more like photo editing software or text editing software. A potential example of this is style transfer for diffusion models. If the user uploads a copyrighted work and asks the model to transform it into a different style, it is more likely that the liability falls to the user who uploaded the image if they later try to resell it. This is like Andy Warhol taking a photograph and transforming it into a piece of art. However, if the user simply asks the website to generate an image from a prompt, and the model generates a copyrighted image, then the liability might fall more on the model deployer if they are profiting off of the distribution of the material. The extent to which the model takes the input image and turns it into something non-transformative might also be taken into account. For example, if the model takes a novel image of a dog and transforms it into a known copyrighted image of a dog, this might create more liablity for the model deployer.\n\nCreator. Throughout this work we will not generally cover liablity to the model creator for the model weights themselves. It is unresolved as to whether the model parameters themselves are infringing, and thus whether the model trainers are liable. Lemley & Casey (2020) have argued that the training process is generally fair use since the model weights themselves are transformative, and thus generally liability would not pass on to model creators. Sobel (2017) argued that if the training process does not result in an expressive model, training is fair use. Others have argued that the model functions as a compressed database of the training data (Andersen et al. v. Stability AI et al., N.D. Cal. 2023), thus making the model creators liable. However, this is not necessarily true of all the training data and the likelihood of verbatim (or significantly similar) extraction can reduce with the amount of training data. For example, Somepalli et al. (2022) found that extraction of training data from diffusion models is less likely if there is more diverse training data.\n\nWhen the model is",
            "score": 0.45797073248986553,
            "section_title": "Implied Licenses and Common Crawl.",
            "char_start_offset": 68864,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1432,
                    "end": 1453,
                    "matchedPaperCorpusId": "219342558"
                },
                {
                    "start": 1635,
                    "end": 1647,
                    "matchedPaperCorpusId": "115500744"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30322265625
        },
        {
            "corpus_id": "266798835",
            "title": "The Study of Copyright Infringement Liability of Generative Artificial Intelligence",
            "text": "When generative AI uses prior works for pre-training and infringes the right of reproduction and adaptation of earlier works, it signifies that the AI system utilizes existing works as a foundation for generating new content without obtaining the consent of the original creators. This behavior directly violates the rights of the original creators who hold the copyright to those works. Consequently, generative AI service providers who knowingly engage in this practice can be held liable for engaging in direct copyright infringement. \n\nIt's important to note that generative AI service providers have the autonomy to make independent decisions regarding whether or not to infringe on others' works. They are not obligated or compelled to use copyrighted materials in their pre-training process, which means it is a conscious choice that they make. Furthermore, these service providers possess the necessary technical capabilities to avoid copyright infringement. They have access to various tools and algorithms that can scan and analyze the collected data to identify any unauthorized works that may be present and eliminate or filter them out. After gathering the data, generative AI service providers typically go through a meticulous cleaning process. This involves the removal of any duplicate, noisy, or irrelevant data to ensure the accuracy and quality of the training data. During this step, automated tools and algorithms play a crucial role in detecting any copyrighted materials that should not be included in the pre-training process. Through the deployment of these automated filters, service providers have the means to warrant that the pre-training data they employ does not incorporate any resources that would infringe upon copyright. \n\nHowever, it is essential to emphasize that these actions do not absolve generative AI service providers of liability in cases where copyright infringement does occur during the pre-training phase. Even with meticulous filtering and cleansing, it remains their responsibility to confirm that the data used for pre-training does not contain copyrighted materials without authorization. Failure to adhere to these responsibilities and to knowingly use copyrighted content without permission may expose generative AI service providers to legal consequences for their involvement in direct copyright infringement. Thus, the availability of technical tools and algorithms is not a safeguard against potential infringement, but rather a resource to facilitate compliance with copyright laws.",
            "score": 0.4576303656506393,
            "section_title": "Liability for infringement of generative AI using prior works for pre-training",
            "char_start_offset": 12349,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 537
                },
                {
                    "start": 540,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1756
                },
                {
                    "start": 1759,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2367
                },
                {
                    "start": 2368,
                    "end": 2543
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32861328125
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "We find then when provided with generic prompts for images in these categories, diffusion models tend to generate images that contain the Superman logo or character (in the former case) or the Starbucks logo (in the latter). To efficiently generate candidates for target topics for both these avenues, we leverage the abilities of Language Models to provide prompts originating from famous game and movie titles that either contain polysemic terms or can be easily overrepresented. \n\nIt is notable to mention that we exclude topics related to artwork and individual artists from our designated target topics. Within the scope of this study, our primary emphasis lies in assessing partial copyright infringement. Specifically, this involves finding the presence of copyrighted content that is visually discernible within image segments. We find that while diffusion models can accurately replicate the style of artists [Casper et al., 2023], this may be a form of derivative work [Cornell, 2022] which is less straightforward to ascertain copyright infringement. Consequently, we refrain from delving into copyright matters pertaining to artistic style and creations by specific artists. The identification of style replication within artworks demands a more intricate approach, involving deeper consideration of how the style is employed, which exceeds the boundaries of the scope of this work.",
            "score": 0.4572725182609485,
            "section_title": "B Details on Collecting Potentially",
            "char_start_offset": 35390,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1394
                }
            ],
            "ref_mentions": [
                {
                    "start": 979,
                    "end": 994,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07879638671875
        },
        {
            "corpus_id": "270258236",
            "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
            "text": "In this paper, a method for genericizing outputs by quantifying originality was introduced, which significantly improves the performance of the existing prompt rewriting method. 6][37][38][39][40] As an alternative approach for mitigating the risks of infringing copyright in training data, methods for computing fair shares of royalty and compensating the creators of training data have been proposed. 41,42 8][49] In this paper, we proposed an approach to engineering prompts within AI systems, specifically aimed at genericizing outputs to address copyright concerns in text-to-image generative models.",
            "score": 0.4566145854216071,
            "section_title": "Related works",
            "char_start_offset": 30878,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 605
                }
            ],
            "ref_mentions": [
                {
                    "start": 411,
                    "end": 415,
                    "matchedPaperCorpusId": "271866031"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25244140625
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "We reuse the CM metric to indicate the similarities between the original copyright images and their unlearned counterparts after processed by unlearning methods.Additionally, we evaluate changes of CLIP scores, denoted as \u2206CLIP, for text-image similarity.This indicates the extent to which the prompt that generates potential infringement is nullified.\n\nExtent of Model Degradation during Unlearning.The unlearning process inherently degrades the model by eliminating certain infringement-suspected concepts.Nevertheless, it is vital to preserve the Stable Diffusion model's generation capacities for copyright-irrelevant contents.We assess the degree of model degradation using the widely-recognized FID (Fr\u00e9chet Inception Distance) metric [20].\n\nOur benchmark facilitates a straightforward evaluation of potential copyright infringement, while facilitating comparison among various unlearning methods.Moreover, we perform comprehensive benchmark tests on our proposed CPDM dataset.In our experiments, we utilize gradient ascent-based and response-based pruning methods for unlearning, as comparison baselines for other unlearning approaches, specifically targeting the Stable Diffusion models.This evaluation provides valuable insights into assessing copyright infringement and the efficacy of unlearning methods in reducing infringement risks, while preserving the ability to generate non-infringing contents.",
            "score": 0.45607668107092736,
            "section_title": "Effectiveness of Unlearning.",
            "char_start_offset": 3524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 352
                },
                {
                    "start": 354,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 508
                },
                {
                    "start": 508,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 746
                },
                {
                    "start": 748,
                    "end": 903
                },
                {
                    "start": 903,
                    "end": 983
                },
                {
                    "start": 983,
                    "end": 1195
                },
                {
                    "start": 1195,
                    "end": 1412
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27490234375
        },
        {
            "corpus_id": "259501495",
            "title": "Measuring the Success of Diffusion Models at Imitating Human Artists",
            "text": "Accepted to the 1 st Workshop on Generative AI and Law, colocated with the International Conference on Machine Learning, Honolulu, Hawaii, USA. 2023. Copyright 2023 by the author(s). For each artist, we generate an imitation image from Stable Diffusion with the prompt \"Artwork from < artist name >.\" \n\nNext, we encode the image with a CLIP image encoder (Radford et al., 2021). We also encode labels corresponding to n total artists plus one or more 'default' labels with a CLIP text encoder. Finally, we classify the image among all labels using a geometric similarity measure between the encodings. If the label reliably corresponds to the correct artist, we consider the model to have the capability to imitate that artist. \n\nalso show that a sample of the artist's work can be matched to these imitation images with a high degree of statistical reliability. (Rombach et al., 2022) and Midjourney (Midjourney, 2022), are capable of generating images from arbitrary, user-specified prompts. Their success has largely been due to training on large amounts of text/image data, often including copyrighted works (Schuhmann et al., 2021). Modern image-generation diffusion models are trained using CLIP-style encoders. When given an encoding of a caption, a diffusion model is trained to generate an image corresponding to the caption (Ramesh et al., 2022). Accordingly, a diffusion model that generates images from these embeddings is trained to be the inverse of a CLIP image encoder. \n\nLegal Motivation: In the United States, Newton v. Diamond, 388 F.3d 1189, 1195(9th Cir. 2004) established that copyright infringement \"is measured by considering the qualitative and quantitative significance of the copied portion in relation to the plaintiff's work as a whole\". However, the subjective nature of these determinations makes practical enforcement complicated. (Balganesh et al., 2014;Kaminski & Rub, 2017;Balagopalan et al., 2023).",
            "score": 0.4554424676788291,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 377,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 863,
                    "end": 885,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1402587890625
        },
        {
            "corpus_id": "266798835",
            "title": "The Study of Copyright Infringement Liability of Generative Artificial Intelligence",
            "text": "In recent years, the field of Artificial Intelligence (AI) has witnessed remarkable advancements, particularly in the domains of deep learning, large-scale language models, and content generation. These breakthroughs have greatly enhanced the capacity of machines to simulate and accomplish human cognitive tasks. Based on this capability, AI can be broadly categorized into three types: narrow or weak AI, general or strong AI, and artificial superintelligence or super AI [1]. Weak AI has already proven its efficacy in various fields, such as speech recognition, image recognition, and recommendation systems. The industry perceives generative artificial intelligence, exemplified by ChatGPT, as a crucial milestone in the journey towards attaining strong AI. This progress in AI technology has also led to in-depth legal discussions and refinements in understanding generative artificial intelligence. \n\nGenerative AI refers to a type of artificial intelligence with the capacity to acquire knowledge and skills independently, achieving a level of comprehension akin to that of humans. Moreover, generative AI has a certain degree of creativity, which allows it to create original content upon demand. This poses a significant challenge to the current copyright legal system. The existing framework primarily focuses on human-generated content, and it is not well-equipped to handle the unique characteristics and implications of AI-generated works. The growing prevalence of generative AI technology in the field of content creation means that there is a foreseeable increase in disputes related to the infringement of others' copyrights by generative AI. Given the rapid pace at which AI generates content, it is imperative to establish a robust legal framework for protecting creators' rights and appropriately assigning liability in cases of copyright infringement. \n\nThis article aims to contribute to the discourse on liability in the context of copyright infringement by generative AI within the current legal landscape. It is divided into three parts to comprehensively address the different facets of this complex issue. Part 1 provides an overview of generative AI technology and identify the various civil subjects involved, encompassing users of generative AI and the service providers offering generative AI platforms. Part 2 delves into the infringement and liability related to generative AI that undergoes pre-training using prior works, exploring the legal implications of using copyrighted works as a foundation for training generative AI models and analyzes the potential liability of different actors in this process.",
            "score": 0.4552304117309862,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1873
                },
                {
                    "start": 1876,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2335
                },
                {
                    "start": 2336,
                    "end": 2641
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 477,
                    "matchedPaperCorpusId": "219710554"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2286376953125
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "To mitigate IP infringement, two primary approaches have emerged: \n\n\u2022 Reducing Memorization During Training: Implementing differential privacy (Dwork et al., 2014) techniques during the training of generative models can help minimize the retention of specific data points, thereby reducing the risk of reproducing protected content (Abadi et al., 2016;Chen et al., 2022;Dockhorn et al., 2022). \n\n\u2022 Prompt Engineering: Employing strategies such as negative prompts during the inference phase can exclude undesired concepts or elements from the generated output (Wang et al., 2024;He et al., 2024), or optimizing unsafe prompts (Chin et al., 2023;Rando et al., 2022), thereby avoiding the inclusion of IP-protected material. \n\nDespite the widespread copyright concerns surrounding generative AI and the numerous IP mitigation approaches recently proposed, the issue of benchmarking VLM IP infringement detection remains largely underexplored. As a result, a primary focus of our paper is to address the capabilities of VLMs in detecting and mitigating IP infringement.",
            "score": 0.4531015087471823,
            "section_title": "Vision Language Models",
            "char_start_offset": 7990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 68,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1066
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 163,
                    "matchedPaperCorpusId": "207178262"
                },
                {
                    "start": 332,
                    "end": 352,
                    "matchedPaperCorpusId": "207241585"
                },
                {
                    "start": 352,
                    "end": 370,
                    "matchedPaperCorpusId": "250210875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.625
        },
        {
            "corpus_id": "270559583",
            "title": "Data Shapley in One Training Run",
            "text": "This finding supports that training data profoundly influences the capabilities of generative AI models and should be compensated accordingly [8,54], even when the output does not closely resemble the original copyrighted material or when the model applies output filters to avoid generating verbatim replicates of the training data.This discovery expands the conventional focus on copyright violations, which typically addresses instances of near-verbatim replication, as seen in the dispute between New York Times and OpenAI, to also include cases where the generated content is significantly influenced by copyrighted material without directly replicating it.",
            "score": 0.45283379289724596,
            "section_title": "Does Contribution Require Memorization?",
            "char_start_offset": 26500,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 662
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2135009765625
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "Generative models, especially the recent advanced Latent Diffusion Models (LDM) (Rombach et al. 2022), have shown tremendous ability to generate new images, even of creative or artistic form according to text prompts.Such models are trained on a large corpus of data, which may consist of copyrighted material.Additionally, prior works have established that such generative models are prone to regurgitating content from their training data (Ippolito et al. 2023;Zhang et al. 2021;Carlini et al. 2022;Vyas et al.In this paper, we will focus on copyright law within the jurisdiction of the United States.To establish a copyright violation, two factors must be present.First, the accused must have had access to the copyrighted material.Second, the accused must produce content that bears \"substantial similarity\" to the copyrighted material (reproducing).Note that the definition of substantial similarity can be ambiguous.Within the context of images, its definition appears to be relatively broad (Steinberg v. Columbia Pictures Industries, Inc. 1987), and in particular encompasses near-exact copies.\n\nTurning our attention to the former \"access\" criterion: the natural way to establish that a model had access to a particular piece of copyrighted material is to inspect its training data.For example, in the case of Andersen v. Stability AI Ltd. (Dist. Court 2023), the case was allowed to proceed based on the fact that copyrighted images were found in LAION-5B (Schuhmann et al. 2022) (the training data used for Stable Diffusion) using haveibeentrained.com.\n\nWe challenge the perspective that such visual auditing is sufficient to establish access to copyrighted material.Our results show that it is possible to conceal copyrighted images within the training dataset for LDMs.Specifically, LDMs are equipped with a fixed encoder for dimension reduction such that the diffusion learning process occurs in the latent space.This structure can be maliciously exploited to generate disguised copyrighted samples: given a copyrighted image, we show how to generate a disguise such that it is visually different from the copyrighted sample but shares similar latent information.",
            "score": 0.45283379289724596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 310
                },
                {
                    "start": 310,
                    "end": 512
                },
                {
                    "start": 512,
                    "end": 603
                },
                {
                    "start": 603,
                    "end": 667
                },
                {
                    "start": 667,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 854
                },
                {
                    "start": 854,
                    "end": 922
                },
                {
                    "start": 922,
                    "end": 1102
                },
                {
                    "start": 1104,
                    "end": 1291
                },
                {
                    "start": 1291,
                    "end": 1563
                },
                {
                    "start": 1565,
                    "end": 1678
                },
                {
                    "start": 1678,
                    "end": 1782
                },
                {
                    "start": 1782,
                    "end": 1927
                },
                {
                    "start": 1927,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 101,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 481,
                    "end": 501,
                    "matchedPaperCorpusId": "246863735"
                },
                {
                    "start": 501,
                    "end": 512,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 1466,
                    "end": 1488,
                    "matchedPaperCorpusId": "252917726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4541015625
        },
        {
            "corpus_id": "269761311",
            "title": "Risks and Opportunities of Open-Source Generative AI",
            "text": "Generative AI models are at the heart of a growing legal conundrum due to their training on copyrighted material without explicit consent (Saveri and Butterick, 2023; The New York Times, 2024).This issue is exacerbated by the tendency of these models to precisely replicate text from their training data, a phenomenon known as \"memorization\" (Karamolegkou et al., 2023;Carlini et al., 2022).This practice has led to significant copyright disputes, as evidenced by high-profile lawsuits such as Sarah Silverman vs. Meta and New York Times vs. OpenAI and Microsoft (Saveri and Butterick, 2023; The New York Times, 2024), which revolve around the unauthorized use of copyrighted content in training datasets and the direct reproduction of such texts upon prompting.\n\nThe opacity surrounding the composition of training data and the challenges in curtailing exact text generation from these datasets underscore a crucial barrier to addressing copyright concerns within the generative AI space.\n\nOpen-source models, however, offer a promising avenue for mitigating these copyright issues in ways closed models cannot.By disclosing or detailing their training data (see \u00a73 for the pipeline components), open models can provide much-needed transparency, offering insights into data attribution and potentially clarifying the bounds of \"fair use\" regarding copyrighted materials.\n\nFurthermore, the open nature of these models invites community involvement in refining techniques to reduce memorization, such as through advanced decoding strategies or post-processing methods Karamolegkou et al. (2023).Initiatives like the Aya Initiative also demonstrate how opensource approaches can lead to the proactive curation of non-proprietary datasets Hartmann et al. (2023), potentially decreasing reliance on copyrighted material and, by extension, the risk of future copyright disputes.This collaborative and transparent model stands in contrast to closed systems, presenting a unique solution to the complex issue of copyright in generative AI.",
            "score": 0.45244170997404864,
            "section_title": "+ Open Models Can Help Reduce Copyright Disputes",
            "char_start_offset": 54460,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 391
                },
                {
                    "start": 391,
                    "end": 762
                },
                {
                    "start": 764,
                    "end": 989
                },
                {
                    "start": 991,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1371
                },
                {
                    "start": 1373,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1873
                },
                {
                    "start": 1873,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 342,
                    "end": 369,
                    "matchedPaperCorpusId": "264426289"
                },
                {
                    "start": 1567,
                    "end": 1593,
                    "matchedPaperCorpusId": "264426289"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.284423828125
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "The strength of these grounds will vary from case to case. \n\nIt is not possible to make blanket statements about the remainder of factor one, or about factors two and three. \n\n\u2022 Factor one: Some output uses will be commercial, some will not. (We are thinking here of the uses made by the users of the generations; there is a colorable argument that the commerciality of generation as a service may be relevant to the fair use claims as to the generations, but we think our analysis is slightly cleaner and more useful.) \n\n\u2022 Factor two: Most of the training data will typically have been \"published\" within the meaning of copyright law; it would not otherwise be available within the training data at all. (A fraction of works may have been held confidentially and released into training datasets without their authors' permission; the case for removing such works is comparatively stronger.) Some training data will be primarily informational; some will be primarily expressive. \n\n\u2022 Factor three: This is a replay of substantial similarity. Some generations will closely resemble the works they were copied from; others will copy comparatively smaller portions of the works, both qualitatively and quantitatively. Even when a work is transformative under the first factor, courts will still also inquire into whether the generation copies more than necessary for that transformation -a \"painting of a car driving in a snowstorm in the style of Frida Kahlo\" might copy just her color palette, brushwork, and floral motifs, or it might also put the entire composition of Self-Portrait with Thorn Necklace and Hummingbird inside the resulting image. We conclude that some generations will be fair uses and others will nota conclusion that forces a reconsideration of whether the underlying models are fair uses. The nonexpressive-use argument no longer goes through, because some uses of these models are expressive -and indeed, some of them are infringing. \n\nA full four-factor analysis has been given elsewhere, so we will belabor the details. Instead, we emphasize a few points. First, the models qua models are arguably highly transformative -both because they represent the works internally in new and very different ways, and also because they are capable of generating highly transformative works as outputs.",
            "score": 0.4523038363930513,
            "section_title": "Id. \u00a7 512(i)(1)(A",
            "char_start_offset": 48183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 61,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1954
                },
                {
                    "start": 1957,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2312
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06951904296875
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "During the unlearning experiments conducted on these two foundational models, for each image to be forgotten, the learning rate for gradient ascent is set at 3e-4, detailed hyperparameters are listed in Table 1. \n\nThe result are listed on Table 2, Table 3, Table 4. While RLCP shows relatively weaker performance on the FID, it still performs exceptionally well on the L2-norm metric. This discrepancy can be explained by the nature of FID, which measures the distribution similarity between generated and target images. Since RLCP focuses on avoiding the generation of copyrighted data, it naturally alters the distribution, leading to a higher FED score. Despite this, the L2-norm results indicate that RLCP maintains strong performance in generating non-infringing content that aligns with the original image features. \n\nWe also give examples of fine-tuned results for copyright protection on three datasets (See in Figure 5, Figure 6, Figure 7). \n\nImpact of Copyright Data Proportion. We further explore how the proportion of copyrighted images in the training dataset affects the performance of the RLCP method. As illustrated in Figure 8, increasing the proportion of copyrighted data leads to an increase in copyright loss, while FID decreases slightly, reflecting a trade-off between protection and quality. One potential concern is whether RLCP can scale effectively when a large proportion of the training set consists of copyrighted images. The results address this concern by showing that RLCP continues to perform strongly, even with a high percentage of copyrighted images, outperforming all alternative methods. \n\nWe measured copyright loss and FID across vari-ous datasets with differing ratios of copyrighted to noncopyrighted images. As shown in Figure 8, the copyright loss also rises with more copyrighted data, yet the FID shows only a slight decrease. \n\nWhile RLCP demonstrates a clear ability to reduce copyright infringement, there is an inherent trade-off between minimizing copyright loss and maintaining image quality. The results suggest that RLCP provides a favorable balance, where the reduction in copyright loss is achieved with only a minimal impact on FID. This balance is particularly evident in the results for portrait images, where RLCP achieves both low copyright loss and high FID.",
            "score": 0.452231044746888,
            "section_title": "Experiment Results",
            "char_start_offset": 20793,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 214,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1873
                },
                {
                    "start": 1876,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16064453125
        },
        {
            "corpus_id": "271874323",
            "title": "Not Every Image is Worth a Thousand Words: Quantifying Originality in Stable Diffusion",
            "text": "Privacy and Copyright Infringements The intersection of privacy and copyright infringements in generative models has garnered significant attention. This approach assumes that to avoid copyright infringement, the output of a model shouldn't be too sensitive to any of its individual training samples. Bousquet et al. (Bousquet et al., 2020) suggest the use of differential privacy (Dwork et al., 2006) 2023), however, investigated the gap between privacy and copyright infringement from the perspective of the law, and showed that requiring such notions of stability may be too strong, and are not always aligned with the original intention of the law. Closer to our approach, (Scheffler et al., 2022) suggests a framework to quantify originality by measuring the description length of a content with and without access to the allegedly copyrighted material. Our approach of textual inversion also looks for a succinct description of the content but, dis-tinctively, our definition depends on the distribution of the data, and measures originality with respect to the whole data to be trained. This may lead to different outcomes, for example, when the allegedly copyrighted material contains a distinctive trait that is not necessarily original. \n\nAttribution in Generative Models Attribution in generative models is a crucial area of research, focusing on identifying the sources of data that contribute to the generation of specific outputs. Park et al. introduced the TRAK method to address data attribution in large-scale models (Park, 2022), and recently, Wang et al. (Wang et al., 2023b) proposed a method for evaluating the attribution in Stable Diffusion models of data points in the generation process, which is closely related to assessing the originality of generated images. However, such a method requires full access and knowledge of the training set on which the model was trained.",
            "score": 0.4519390621192928,
            "section_title": "Related Works",
            "char_start_offset": 20867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1246
                },
                {
                    "start": 1249,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1897
                }
            ],
            "ref_mentions": [
                {
                    "start": 317,
                    "end": 340,
                    "matchedPaperCorpusId": "225067265"
                },
                {
                    "start": 381,
                    "end": 401,
                    "matchedPaperCorpusId": "2468323"
                },
                {
                    "start": 677,
                    "end": 701,
                    "matchedPaperCorpusId": "249375708"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11859130859375
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "In the swiftly progressing realm of generative artificial intelligence (AI), the pressing concern of copyright infringement emerges prominently.As AI technologies continue to autonomously generate content from copyrighted data, inquiries about ownership and safeguarding rights surface, reverberating across diverse professional domains.This escalating trend raises critical discussions surrounding ethical, legal, and socio-economic implications, necessitating nuanced exploration and strategic interventions to navigate this evolving landscape effectively.For instance, in July 2023 a group of novelists collectively sued OpenAI for alleged usage of their books to train their models and output similar content to the novelists' prose [117].Moreover, in December 2023 The New York Times filed a lawsuit against OpenAI and Microsoft, alleging copyright infringement by having its articles scraped without permission to train their generative models [118].More recently, Marcus and Southen revealed how generative models such as Midjourney and OpenAI's Chat GPT-4 produced outputs strongly reminiscent of scenes from copyrighted films and shows [82,124].As a concrete example, Figure 1 illustrates how a prompt from Southen resulted in an output resembling a shot from the trailer of Dune (2021).Notably, Midjourney's terms of service [87] highlight that users assume liability when requesting the model to generate content featuring copyrighted trademarks.This delegation of responsibility not only places the burden of infringement on users, but also diverts accountability from Midjourney's developers, who have openly admitted to using copyrighted trademarks without authorization [103].\n\nIn light of these developments, this survey aims to delve into the complex interplay between generative AI and protecting intellectual property (IP).Through synthesizing existing methods and legal analyses, we provide a comprehensive overview of the current landscape surrounding copyright in generative AI.To the best of our knowledge, this work presents the first thorough study on robust and applicable solutions to copyright issues in generative AI, which also combines contextual legal analysis for future consideration.The challenges and opportunities inherent in this burgeoning field offer insights that can inform policymakers, practitioners, and researchers alike when developing generative AI.",
            "score": 0.4514084388563879,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 337
                },
                {
                    "start": 337,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 743
                },
                {
                    "start": 743,
                    "end": 956
                },
                {
                    "start": 956,
                    "end": 1154
                },
                {
                    "start": 1154,
                    "end": 1296
                },
                {
                    "start": 1296,
                    "end": 1457
                },
                {
                    "start": 1457,
                    "end": 1691
                },
                {
                    "start": 1693,
                    "end": 1842
                },
                {
                    "start": 1842,
                    "end": 2000
                },
                {
                    "start": 2000,
                    "end": 2218
                },
                {
                    "start": 2218,
                    "end": 2397
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.197265625
        },
        {
            "corpus_id": "264451840",
            "title": "The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI",
            "text": "Ultimately, it can be challenging to determine which laws should apply to a given machine learning project when the relevant rules vary between the locations where the data was scraped and annotated, where it was downloaded, where the model was trained, and where the model was deployed. \n\nWhile geographical disparities in regulatory frameworks present one set of challenges, the subjectivity inherent in determining whether copyright infringement has occurred makes it even more challenging to design technical safeguards. For example, in the U.S. part of the copyright infringement analysis depends on whether two works are subjectively similar from the perspective of an ordinary person (Mohler, 1999;Cohen, 1986;Balganesh et al., 2014). This is a subjective standard and existing case law may be challenging to extend to generative AI outputs. As a result, while there are technical strategies that can reduce the risk of infringement (Henderson et al., 2023;Sag, 2023;Vyas et al., 2023), it will be difficult for developers to create technical safeguards that eliminate this risk entirely. \n\nOpen legal question regarding copyright and model training. Apart from these jurisdictional and interpretive ambiguities, the process of training a model raises specific copyright questions (Epstein et al., 2023). \n\nTraining a model poses several interesting legal questions with respect to copyright and infringement may occur in several ways even before any outputs are generated. \n\nFirst, the act of creating a training dataset by scraping existing works involves making a digital copy of the underlying data. As the name implies, copyright gives the author of a protected work the exclusive right to make copies of that work. If the scraped data is protected by copyright, then creating training data corpora may raise copyright issues (Quang, 2021). Second, copyright holders generally have an exclusive right to create derivative works (e.g., translations of a work) but it is not clear whether a trained machine learning model should be considered a derivative of the training data (Lee et al., 2023). If models are considered to be derivative works, then training a model would be more likely to violate the rights of the training data's copyright holders (Gervais, 2021).",
            "score": 0.45058101750836044,
            "section_title": "Surpervised Dataset Example: SQuAD",
            "char_start_offset": 33689,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1311
                },
                {
                    "start": 1314,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 705,
                    "end": 717,
                    "matchedPaperCorpusId": "55215303"
                },
                {
                    "start": 717,
                    "end": 740,
                    "matchedPaperCorpusId": "150380376"
                },
                {
                    "start": 1288,
                    "end": 1310,
                    "matchedPaperCorpusId": "259095707"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302001953125
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "Copyright infringement arises from unauthorized access and reproduction of copyrighted material. Copyright protection methods (Vyas et al., 2023;Wiggers, 2023) for text-to-image diffusion models, grounded in copyright law, mainly focus on the curation of training data, which is different from the prevention of Not-Safe-For-Work (NSFW) content generation (Poppi et al., 2023). To prevent the production of inappropriate content, a classifier can be trained over historical NSFW contents to identify future NSFW outputs (Poppi et al., 2023). However, copyrighted contents do not follow a clear pattern, making it challenging to learn. Moreover, the rapidly-evolving nature of copyrighted materials necessitates frequent updates to a copyright classifier, rendering post-detection efforts for copyright protection unfeasible. In response, Vyas et al. (2023) introduced a theoretical framework for copyright protection through access restriction. Additionally, model editing methods have been developed for better control over image generation (Gandikota et al., 2023;Kumari et al., 2023;Zhang et al., 2023a). Moreover, strategies like applying perturbations or watermarks to images are also being explored to protect copyrighted materials (Cui et al., 2023;Ray & Roy, 2020;Zhao et al., 2023;Li et al., 2022;Tang et al., 2023;Guo et al., 2023). \n\nBackdoor Attacks on Diffusion Models. Backdoor attacks involve embedding triggers during the training of neural networks, causing the model to behave normally until the trigger is activated (Gu et al., 2017). With the growing prominence of diffusion models (Ho et al., 2020;Song et al., 2021), there's increased focus on their vulnerability to such attacks, highlighted in research by Chen et al. (2023) and Chou et al. (2023). Studies have explored compromising diffusion models by targeting their components like text encoders (Struppek et al., 2022) and altering the diffusion process (Chou et al., 2023).",
            "score": 0.4500335355241645,
            "section_title": "Related Work",
            "char_start_offset": 4413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1342
                },
                {
                    "start": 1345,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1953
                }
            ],
            "ref_mentions": [
                {
                    "start": 1042,
                    "end": 1066,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 1066,
                    "end": 1086,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 1256,
                    "end": 1272,
                    "matchedPaperCorpusId": "227129816"
                },
                {
                    "start": 1290,
                    "end": 1306,
                    "matchedPaperCorpusId": "252683790"
                },
                {
                    "start": 1306,
                    "end": 1324,
                    "matchedPaperCorpusId": "257636598"
                },
                {
                    "start": 1619,
                    "end": 1637,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 1730,
                    "end": 1748,
                    "matchedPaperCorpusId": "257482560"
                },
                {
                    "start": 1753,
                    "end": 1771,
                    "matchedPaperCorpusId": "254564071"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360595703125
        },
        {
            "corpus_id": "274436785",
            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
            "text": "In order to assess the defense, we employed the Copyright Infringement Rate (CIR) and First Attack Epoch (FAE) as measures. CIR represents the probability of generating copyright infringement samples using the poisoned model, CIR(DM, I) = P (SSCD(DM (t poison ), I) > 0.5). FAE, on the other hand, represents the number of epochs required for the model to complete the attack for the first time during the training process, F AE(DM ) = min {epoch | SSCD(DM (t poison ), I) > 0.5, epoch}. For the computation of CIR, we iteratively generate 100 images for each poisoned caption and count the number of instances where SSCD > 0.5, then used to calculate the probability. \n\nImplementation Details: To determine the optimal threshold for the poison-score, we conducted experiments with values of 0.2, 0.25, and 0.3. After evaluating precision, recall, and F1 score, we selected 0.25 as the most suitable threshold. For the parameter \u03bb in Eq. ( 7), we tested values of 0.05, 0.10, and 0.15, ultimately selecting 0.05 based on the experimental results. During the defense fine-tuning process, we utilized the Adam optimization algorithm with a learning rate of 1e-5, a batch size of 8, and trained for 100",
            "score": 0.4490767614119085,
            "section_title": "Experimental Setup",
            "char_start_offset": 20670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1199
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.076416015625
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "https://openai.com/policies/terms-of-use information; (b) \"Watermark techniques\" can be used by the data owner to trace and distinguish whether a generated work is produced based on their original creation; (c) \"Machine Unlearning\" means that the data owner can request a deletion of their data from the model or its output, once they identify the copyright infringement; \n\n(d) \"Dataset De-duplication\" removes duplicated data to mitigate the memorization effect to prevent the training data from being generated; (e) \"Alignment\" which uses a reward signal to reduce the memorization in LLMs; and \n\n(f) others including improved training and generation algorithms for better behaviors of LLMs. \u2022 For DGM users who create new works assisted by DGMs, the protected object is the generated contents from DGMs. Thus, the techniques for this type of copyright is barely related to the generation process of DGMs, as traditional copyright protection strategies can be also applied for protection of the DGM generated contents. \n\n\u2022 For DGM providers, there are representative \"watermarking strategies\" to inject the watermarks into the generated content or model parameters, such that we can track the ownership of the model. \n\nGiven the diversity in protection objectives, as well as DGM applications, we are motivated to have an overview on existing computational methods in this direction. Essentially, in Section 2, we will majorly discuss the copyright protection techniques for DGMs in the image domain. In Section 3, we discuss the strategies for text generation. Finally, we discuss the related problems in other domains, such as graphs, codes and audio generation in Section 4. In each section, we will introduce the background knowledge of existing DGMs, as well as the existing methodologies for data protection under different scenarios.",
            "score": 0.44905115566117115,
            "section_title": "Introduction",
            "char_start_offset": 4143,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 371
                },
                {
                    "start": 374,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1842
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1414794921875
        },
        {
            "corpus_id": "269757496",
            "title": "Disrupting Style Mimicry Attacks on Video Imagery",
            "text": "While many debate the ethical and legal issues around the training of generative image models, all agree that their arrival has dramatically disrupted a range of visual art industries, from fine art to illustrations, concept art and graphics arts.Recent works has studied the harms experienced by professional artists due to these large-scale image generators, including reputational damage, economic loss, plagiarism and copyright infringement [4,24].One of the more harmful uses of these image models is \"style mimicry,\" where someone \"finetunes\" a model on small samples of a specific artist's art, then uses the result to produce images in the artist's individual style without their knowledge [2,5,29,51].These mimicry models (usually lightweight models known as LORAs) are hosted on sites including Civitai, Tensor.art,PromptHero and HuggingFace.\n\nRecently, the security and machine learning communities have developed a number of tools to disrupt unauthorized style mimicry, including Glaze [43], Mist [27], and Anti-Dreambooth [49].These tools disrupt the mimicry process, by modifying images so that they misrepresent themselves in a target model's style feature space during finetuning, while constraining changes to minimize visual impact to human eyes.Since their introduction, they have been adopted widely by artists across the globe, e.g.Glaze reports more than 2.3 million downloads in a year [20].\n\nThere are signs, however, that mimicry attacks are shifting away from 2D art images and towards video content (see Figure 1).Online videos such as animations, game cut-scenes, music videos and TV shows provide attractive sources for training mimicry models for several reasons.First, a single video can provide thousands of frames, each convertible to a standalone image for training.For example, YouTube videos range from 30 to 60 frames per second, and even a short 5 minute video can yield 18,000 frames for potential training.Second, extracting frames from videos provides far more flexibility to choose a specific scene, character or perspective.This offers far richer training content compared to static images like movie posters or promotional art.",
            "score": 0.44886852242737557,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 247,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 710
                },
                {
                    "start": 710,
                    "end": 825
                },
                {
                    "start": 825,
                    "end": 852
                },
                {
                    "start": 854,
                    "end": 1040
                },
                {
                    "start": 1040,
                    "end": 1264
                },
                {
                    "start": 1264,
                    "end": 1353
                },
                {
                    "start": 1353,
                    "end": 1414
                },
                {
                    "start": 1416,
                    "end": 1541
                },
                {
                    "start": 1541,
                    "end": 1693
                },
                {
                    "start": 1693,
                    "end": 1800
                },
                {
                    "start": 1800,
                    "end": 1946
                },
                {
                    "start": 1946,
                    "end": 2067
                },
                {
                    "start": 2067,
                    "end": 2171
                }
            ],
            "ref_mentions": [
                {
                    "start": 445,
                    "end": 448,
                    "matchedPaperCorpusId": "259464282"
                },
                {
                    "start": 448,
                    "end": 451,
                    "matchedPaperCorpusId": "261279983"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "256662278"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "256697414"
                },
                {
                    "start": 1035,
                    "end": 1039,
                    "matchedPaperCorpusId": "257766375"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21240234375
        },
        {
            "corpus_id": "269294137",
            "title": "An Economic Solution to Copyright Challenges of Generative AI",
            "text": "Recent efforts in machine learning have primarily focused on minimizing the likelihood of creating copyright-infringing content by generative AI models. One approach involves training an auxiliary generative model on non-copyrighted data and utilizing rejection sampling to reduce the likelihood of reproducing copyrighted material [37]. However, this method is susceptible to adversarial attacks [25]. Alternatively, [5] suggested modifying generative models' training objectives to avoid generating outputs that closely resemble copyrighted data. Yet another technique focuses on protecting unique artistic styles by incorporating adversarial perturbations into copyrighted images for model fine-tuning [35]. More recently, techniques such as rewriting the input prompt [15] and modifying outputs to be more generic [4] have also been proposed. \n\nThe Shapley value has been suggested as a means to fairly distribute revenue in traditional sectors such as royalty agreements between music copyright holders and radio broadcasters [42]. More recently, it has been used for data valuation where the utility function is the prediction accuracy of the machine learning model [11,19]. This differs from our SRS framework, which uses the log-likelihood as the utility since there is no such thing as prediction accuracy for generative models. \n\nThere are a few techniques that have been extended to the generative models. A simple approach utilizes similarity scores between training data and generated content as a valuation metric [43]. Another commonly used approach is the leave-one-out (LOO) score or its approximations. For example, [10] extends the TRAK framework [29] to generative models, and [44] further introduced empirical approaches to improving the performance of [10]. Notably, [7] proposed a revenue-sharing mechanism for AI-generated music based on TRAK, which is closely related to our work. However, the LOO scores neglect the high-order training data interactions, which may result in undesirable attribution scores (see Section B in the supplementary materials for detailed discussion).",
            "score": 0.44885400822980476,
            "section_title": "Related Work",
            "char_start_offset": 18290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 846
                },
                {
                    "start": 849,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2103
                }
            ],
            "ref_mentions": [
                {
                    "start": 1172,
                    "end": 1176,
                    "matchedPaperCorpusId": "102350503"
                },
                {
                    "start": 1176,
                    "end": 1179,
                    "matchedPaperCorpusId": "67855573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31884765625
        },
        {
            "corpus_id": "269214015",
            "title": "\\copyright Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model",
            "text": "The rapid advancement of generative AI models has sparked significant concern over their potential to produce copyright-infringing content.As these models elevate the quality of their outputs, the lack of proper attribution to human creators becomes increasingly problematic.To mitigate these societal concerns, we introduce the \"Copyright Plug-in Authorization\" framework, a system designed to align with the principles of copyright law and ensure that the use of copyrighted material in AI-generated content is appropriately managed and rewarded.One instantiation of our framework leverages the Low-Rank Adaptor (LoRA) to convert copyrighted data into plug-ins, facilitating the tracking of content usage and the fair distribution of rewards to original creators.This approach not only respects the rights of content creators but also provides a clear and accessible mechanism for users to engage with copyrighted material in a legally compliant manner.\n\nNonetheless, the current framework may still face challenges managing a vast array of plug-ins.As the number of plug-ins grows, maintaining a user-friendly interface and ensuring seamless access to specific generations becomes critical.This requires innovative solutions to organize and retrieve plug-ins effectively, without compromising the user experience.Another challenge is ensuring the backward compatibility of plug-ins when the base model is updated.Retraining the entire collection of plug-ins following a model update can be prohibitively expensive and time-consuming.Ensuring backward compatibility, so that previously created plug-ins continue to function with new model iterations, is essential to avoid obsolescence and maintain user trust.\n\nOur Copyright Plug-in Authorization framework represents an initial step towards balancing the innovative potential of generative AI with the imperative of copyright protection.By addressing the challenges of plug-in management, model updating, and performance maintenance, we aim to foster an ecosystem where creativity and legal compliance can coexist and thrive.\n\nin Context (2015)",
            "score": 0.4488361196778261,
            "section_title": "Discussion",
            "char_start_offset": 20418,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 275
                },
                {
                    "start": 275,
                    "end": 548
                },
                {
                    "start": 548,
                    "end": 765
                },
                {
                    "start": 765,
                    "end": 955
                },
                {
                    "start": 957,
                    "end": 1052
                },
                {
                    "start": 1052,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1316
                },
                {
                    "start": 1316,
                    "end": 1416
                },
                {
                    "start": 1416,
                    "end": 1536
                },
                {
                    "start": 1536,
                    "end": 1712
                },
                {
                    "start": 1714,
                    "end": 1891
                },
                {
                    "start": 1891,
                    "end": 2079
                },
                {
                    "start": 2081,
                    "end": 2098
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.118408203125
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "Vision-language models (VLMs), which integrate both textual and visual data to enable crossmodal reasoning, have demonstrated remarkable capabilities in tasks such as image classification, visual question answering (VQA) (Antol et al., 2015), and multimodal understanding. Notable VLMs such as CLIP (Radford et al., 2021), large vision-language models (LVLMs) such as GPT-4o (GPT4V), Claude 3.5 (Claude3.5), VILA-2.7b (Lin et al., 2023), and Qwen-VL (Bai et al., 2023) have been trained to interpret and generate content based on textual and visual inputs, making them prime candidates for assessing IP infringement detection. However, despite their extensive deployment in various applications, the effectiveness of LVLMs in identifying copyright-protected content remains largely untested. Given the increasing reliance on these models in content moderation, digital rights management, and automated compliance monitoring, it is crucial to evaluate their ability to detect copyright infringement. \n\nTo address this gap, our work presents a systematic evaluation of LVLMs for copyright detection by constructing a dedicated benchmark dataset. Recognizing the absence of a comprehensive dataset that includes both clear cases of IP infringement and ambiguous non-infringing samples, we create a dataset comprising: \n\n\u2022 Positive samples that contain well-known IP characters generated using different AI models with direct and descriptive prompts that replicate their distinctive features. \n\n\u2022 Negative samples that resemble IP characters in certain aspects but do not fully qualify as copyright violations. These images are generated using modified negative prompt engineering techniques. \n\nThese images are selected through rigorous human annotation after the generation. \n\nOur dataset focuses on five widely recognized fictional characters: Iron Man, Batman, Spider-Man, Superman, and Super Mario, ensuring a balanced representation of both positive and negative samples. To evaluate the effectiveness of VLMs, we conduct experiments using in-context learning (ICL) (Mann et al., 2020;Dong et al., 2022) and zero-shot learning (ZSL) (Wang et al., 2019) approaches, where models are tested on their ability to classify image samples accurately.",
            "score": 0.4484230710300586,
            "section_title": "Introduction",
            "char_start_offset": 2013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1688
                },
                {
                    "start": 1691,
                    "end": 1772
                },
                {
                    "start": 1775,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 241,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 299,
                    "end": 321,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08447265625
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "In the rapidly evolving landscape of generative artificial intelligence (AI), the increasingly pertinent issue of copyright infringement arises as AI advances to generate content from scraped copyrighted data, prompting questions about ownership and protection that impact professionals across various careers. With this in mind, this survey provides an extensive examination of copyright infringement as it pertains to generative AI, aiming to stay abreast of the latest developments and open problems. Specifically, it will first outline methods of detecting copyright infringement in mediums such as text, image, and video. Next, it will delve an exploration of existing techniques aimed at safeguarding copyrighted works from generative models. Furthermore, this survey will discuss resources and tools for users to evaluate copyright violations. Finally, insights into ongoing regulations and proposals for AI will be explored and compared. Through combining these disciplines, the implications of AI-driven content and copyright are thoroughly illustrated and brought into question.",
            "score": 0.4483924175418692,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.247314453125
        },
        {
            "corpus_id": "267565879",
            "title": "Balancing Innovation and Regulation: A Comprehensive Analysis and Neural Network Approach to AI Copyright Challenges",
            "text": "This article explores emerging issues surrounding artificial intelligence (AI) and copyright through a two-pronged approach. First, it provides an extensive literature review analyzing government and industry strategies for addressing AI copyright concerns and evaluates their rationality. Second, it details experiments conducted using neural networks to examine relevant information and investigate image copyright challenges, assessing mainstream large language models\u2019 efficacy in handling copyright matters. \nThe literature review explores AI copyright perspectives of the United Kingdom, China, the European Union, and the United States. It finds that countries emphasize balanced regulation and innovation (UK), ethical content creation (China), regulating high-risk applications (EU), or principles like non-discrimination and privacy (US). However, comprehensive governance frameworks are needed to navigate AI\u2019s ethical, social, and legal intricacies. \nThe experimental portion trains a convolutional neural network on a dataset of 41 infringing and non-infringing image sets to identify copyright infringement. While achieving over 80% accuracy, enhancements through expanded training data, segmentation, and multi-domain detection could improve generalization. The paper concludes with an analysis advocating copyright adaptation for AI creations, measured protections for standalone AI works, and constructive policies from interdisciplinary dialogue.",
            "score": 0.4473390507361126,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.130126953125
        },
        {
            "corpus_id": "278338968",
            "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
            "text": "Copyright issues in visual diffusion models arise when models replicate their training data within the image generation process. Therefore, to detect IP infringement, we compare the generated image with the images in the training data. Since training data is huge, a comparison to the entire dataset is prohibitively expensive. We thereforesimilar to [25]-limit our comparison to a subset of the training data. \n\nImage embeddings: To compare two images, we first encode them. Image embedding can happen on a content and style level. Some embeddings correspond rather to the object-level content of the generated images, whereas other perform better in capturing the artistic style of the generated images [19]. We focus on rather content based embedding because similarity based upon style might not be considered copyrighted [14]. \n\nTo embed an image, we consider CLIP embedding [18]. CLIP embeddings are useful when dealing with text-to-image models because they map both images and text into a shared semantic space. This also allows for meaningful comparisons beyond pixel-level similarity. By using CLIP embeddings, we evaluate similarity not only at a visual level but also in terms of content and semantic meaning. \n\nSimilarity score: Once we have the image embedding, we compare images using a similarity score between the embeddings. Two images are considered to be similar when the similarity score between their embeddings is larger than certain threshold [4]. We con-sider the cosine similarity between the CLIP embeddings as our similarity score, and a threshold of 0.85 as high IP infringement risk. Notice that [4] used the same similarity threshold to identify near duplicates in the training data for deduplication. \n\nWe briefly recall other possible choices for a similarity score: authors in [24] use split dot product of Self Supervised Copy Detection (SSCD) scores as similarity measure, where scores are predicted based on differential entropy regularization, see [16]. In [27], authors utilize the Euclidean L2 norm on pixel space and SSCD scores, while [4] employ cosine similarity between CLIP embeddings to identify near-duplicates. \n\n3 Experimental Results",
            "score": 0.44677379446662446,
            "section_title": "Detecting IP Infringement",
            "char_start_offset": 7363,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1221
                },
                {
                    "start": 1224,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2158
                },
                {
                    "start": 2161,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 1986,
                    "end": 1990,
                    "matchedPaperCorpusId": "247011159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55078125
        },
        {
            "corpus_id": "271874323",
            "title": "Not Every Image is Worth a Thousand Words: Quantifying Originality in Stable Diffusion",
            "text": "In this work, we introduced a novel approach to assess the originality of images with Text-to-Image (T2I) Generative Diffusion models, and have investigated its behavior in this aspect under a controlled environment. Our methodology leverages the concept of familiarity within the model's training data to quantify the originality of tested images. By employing textual inversion techniques, we demonstrated that the number of tokens required to represent and reconstruct an image serves as a measure of its originality, without requiring access to the training data, nor a specific prompt that potentially poses copyright complications. \n\nOur analysis confirmed that T2I models are capable of producing new original content, highlighting the importance of training models on diverse and comprehensive datasets. These findings also challenge the traditional view of avoiding memorization in models. Instead, we propose that models should familiarize themselves with a broad spectrum of data, respecting copyright constraints, to enhance their ability to generate new content. \n\nIn summary, our study offers a fresh perspective on evaluating originality in the context of generative models, which can inform copyright analysis and assist in delineating the legal protection afforded to such images more efficiently and accurately. By quantifying the familiarity of concepts to the model, we provide insights that align with legal definitions and can aid in addressing copyright eligibility, infringement, and licensing issues. In addition to law-related applications, our approach opens up new avenues for research in the intersection of generative models, originality assessment, and generative quality. \n\nLimitations One of the primary constraints for the method is the reliance on textual inversion, which may not capture all aspects of originality in complex images. Additionally, our method's effectiveness is contingent on the quality and diversity of the training data, which might not always be optimal. Furthermore, the correlation between token count and originality, although significant, may not be universally applicable across different model architectures or datasets. Future research should explore alternative measures of originality and test the robustness of our approach across a broader range of models and data, making it readily available for deployment. Finally, our work demonstrates that T2I models can be utilized to discriminate original and nonoriginal work. That being said, an important motivation of our work is to assess originality of T2I content.",
            "score": 0.44634504347084036,
            "section_title": "Discussion",
            "char_start_offset": 23712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2376
                },
                {
                    "start": 2377,
                    "end": 2486
                },
                {
                    "start": 2487,
                    "end": 2580
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.224365234375
        },
        {
            "corpus_id": "267400526",
            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
            "text": "In generating images, the artificial intelligence generation platform led by Stable Diffusion often directly plagiarizes the images used for training or moves the elements in them. In Getty v. Stable Diffusion (James V, 2023), Getty showed ample evidence that Stable Diffusion recreated its watermark in images generated by Stable Diffusion, which Getty argued appeared on models' \"bizarre or grotesque images that would be blurred or defaced.\" and impair the quality of the Getty Images markup\" [8]. , such incidents occur frequently, violating the interests of the copyright owner and quickly confusing the public, making it difficult to distinguish the actual owner of the copyright. Additionally, Companies using AI-generated technology profit from it. As Lucas R. (2023) projected, \"After OpenAI freely exploited everybody's web content, it then proceeded to use that data to build commercial products that it is now attempting to sell back to the public for exorbitant sums of money.\" [9]",
            "score": 0.44495929523248967,
            "section_title": "Copying or modifying elements of a copyrighted work",
            "char_start_offset": 17779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 994
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.199951171875
        },
        {
            "corpus_id": "277502009",
            "title": "Who Owns the Output? Bridging Law and Technology in LLMs Attribution",
            "text": "Several cases already exemplify how technical attribution evidence can decisively influence the outcome of copyright disputes involving AI. The first, famously discussed in the Getty Images cases against Stability AI [84], is watermark image detection. \n\nIn these cases, Getty argues that the presence of their digital watermark in Stable Diffusion's output provides clear, almost irrefutable evidence of unauthorized training in Getty's library. This argument implies that no amount of abstraction by the AI model can erase the fact that specific protected images were used; the watermark's appearance is a forensic 'smoking gun'. Moreover, if it's shown that Stability AI developers attempted to remove or obscure watermarks from training images, Getty could argue that there was a knowing removal of copyright management information. Thus, watermarking technology can be used both to detect the infringement and form the basis of a legal violation itself.",
            "score": 0.44442554028773235,
            "section_title": "The Getty Case",
            "char_start_offset": 67080,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 252
                },
                {
                    "start": 255,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 958
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 221,
                    "matchedPaperCorpusId": "275082199"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.376953125
        },
        {
            "corpus_id": "270703065",
            "title": "U Can't Gen This? A Survey of Intellectual Property Protection Methods for Data in Generative AI",
            "text": "One major aspect of the underlying problem and an aspect that distinguishes these current cases from historic cases of plagiarism and other forms of copyright infringement is the sheer scale.While the artists take years of education and practice to develop their distinct artistic style and ultimately profit from commissioning their art, tools that use their art without consent and create a competitive alternative in an instant, seemingly without a quantitative limit, present a viable threat to their work and careers.\n\nThis situation sparked research on mitigation methods for potential intellectual property rights violations in GAI.The protection encompasses a diverse array of technical strategies, each tailored to address specific vulnerabilities.For instance, some techniques aim to support the artist in opting out of large scraped datasets, while other works focus on disabling style mimicry behaviours.Many of these techniques build upon prior research on other ethical issues in GAI, such as privacy [6], or inappropriate content generation [7].\n\nWe have now arrived at a point where a multitude of technical solutions are available, but a systematisation of the threats and vulnerabilities they address, their costs (e.g., in terms of distorting original artwork), and their shortcomings for protection is lacking.This paper addresses this gap.It systematically reviews the current state-of-the-art technical approaches for safeguarding training data against IP violations.We outline the potential violations of intellectual property considered in the literature and provide a discussion about the underlying properties of generative models that enable such violations.Following this, we present a variety of technical strategies, ranging from cleaning training data, model modifications, adversarial injections and policy-driven frameworks, that have been proposed to address these concerns.\n\nOur contributions include:\n\n\u2022 a review of potential IP violations of training data for GAI models;\n\n\u2022 a systematic review of technical solutions for protecting the IP of content available for training the large GAI models; \u2022 a taxonomy and classification of IP protection methods; and \u2022 discussion on policy and practice revolving IP issues in the context of GAI.The remainder of this paper is organised as follows: Section II discusses other survey papers on the intersection of intellectual property rights and GAI.In Section III, we describe our research methodology.",
            "score": 0.44435738035422795,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 522
                },
                {
                    "start": 524,
                    "end": 639
                },
                {
                    "start": 639,
                    "end": 757
                },
                {
                    "start": 757,
                    "end": 916
                },
                {
                    "start": 916,
                    "end": 1060
                },
                {
                    "start": 1062,
                    "end": 1330
                },
                {
                    "start": 1330,
                    "end": 1360
                },
                {
                    "start": 1360,
                    "end": 1489
                },
                {
                    "start": 1489,
                    "end": 1685
                },
                {
                    "start": 1685,
                    "end": 1908
                },
                {
                    "start": 1910,
                    "end": 1936
                },
                {
                    "start": 1938,
                    "end": 2008
                },
                {
                    "start": 2010,
                    "end": 2273
                },
                {
                    "start": 2273,
                    "end": 2427
                },
                {
                    "start": 2427,
                    "end": 2480
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26123046875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "In this work, we propose a data generation pipeline to generate realistic copyright-infringing examples on diffusion models. Our proposed pipeline generated prompts that are seemingly unrelated to the target copyrighted topic, but can still be utilized to produce copyrighted content. Additionally, our pipeline tackles partial copyright infringement. Through our proposal, we present a toolkit that includes potentially copyrighted topics, target images of copyright topics with annotations of copyrighted content, and a dataset generation pipeline. The toolkit can be used as a whole on diffusion models for testing copyright-related performance and generating copyright-violating samples. We demonstrate with this toolkit that contemporary diffusion models are highly susceptible to generating copyrighted content. The findings emphasize the immediate necessity for appropriate measures to prevent models from generating copyrighted materials. This is especially crucial since our study shows even common phrases can prompt models to create images containing copyrighted content. This work can serve copyright research for diffusion models. For instance, the research community can leverage this toolkit to assess diffusion models based on copyright-related criteria. Furthermore, copyright protection algorithms can employ it for effectiveness evaluation. \n\nAll Query Templates for Prompt Generation Form N phrases using all of the exact words in the exact order: KEYWORD0, KEYWORD1, . . . , KEYWORDN. The phrases should be similar to CATEGORY TOPIC. Form N sentences that start with the phrase START-PHRASE. Do not make reference to the CATEGORY TOPIC. Use words that are challenging to represent visually. Form N sentences that uses all of the exact words in the exact order: KEYWORD0, KEYWORD1, . . . , KEY-WORDN. Use the words continuously wherever possible. Ensure grammatical correctness. Start the sentence with the exact STARTPHRASE. Do not make reference to the CATEGORY TOPIC. Use words that are challenging to represent visually. \n\nTable 3: The list of all templates used for prompt generation on GPT3.5. Capitalized words are variables that can be changed. We query GPT models to generate candidate prompts that contain extracted keywords but have different semantic meanings from our target topics. We employ different queries to ensure diversity in the prompt generation.",
            "score": 0.4423252600956913,
            "section_title": "Conclusion",
            "char_start_offset": 30099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2044
                },
                {
                    "start": 2047,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2315
                },
                {
                    "start": 2316,
                    "end": 2389
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.304443359375
        },
        {
            "corpus_id": "270620522",
            "title": "EnTruth: Enhancing the Traceability of Unauthorized Dataset Usage in Text-to-image Diffusion Models with Minimal and Robust Alterations",
            "text": "Generative models, especially text-to-image diffusion models, have significantly advanced in their ability to generate images, benefiting from enhanced architectures, increased computational power, and large-scale datasets. While the datasets play an important role, their protection has remained as an unsolved issue. Current protection strategies, such as watermarks and membership inference, are either in high poison rate which is detrimental to image quality or suffer from low accuracy and robustness. In this work, we introduce a novel approach, EnTruth, which Enhances Traceability of unauthorized dataset usage utilizing template memorization. By strategically incorporating the template memorization, EnTruth can trigger the specific behavior in unauthorized models as the evidence of infringement. Our method is the first to investigate the positive application of memorization and use it for copyright protection, which turns a curse into a blessing and offers a pioneering perspective for unauthorized usage detection in generative models. Comprehensive experiments are provided to demonstrate its effectiveness in terms of data-alteration rate, accuracy, robustness and generation quality.",
            "score": 0.4422491916077811,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.361572265625
        },
        {
            "corpus_id": "265659495",
            "title": "Watermarking for Neural Radiation Fields by Invertible Neural Network",
            "text": "The usage scenarios of the algorithm described in this paper are as follows: \n\nAlice has acquired some pictures of a 3D scene by taking photographs, etc; Alice embeds watermarks in the images and renders 3D scenes by training NeRF models; \n\nAlice shares NeRF models and 3D scenes online for others to enjoy; \n\nBob acquired the NeRF model without Alice's permission and posted it on the web under his name; \n\nAlice sees the NeRF model posted by Bob and uses his model to render a 2D image for watermark extraction, thereby verifying that Alice is the copyright holder of the NeRF model; \n\nBob is infringing on copyright and needs to withdraw the release;",
            "score": 0.44156473618555847,
            "section_title": "A. Application Scenarios",
            "char_start_offset": 5876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 79,
                    "end": 238
                },
                {
                    "start": 241,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 405
                },
                {
                    "start": 408,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 653
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0225372314453125
        },
        {
            "corpus_id": "273812068",
            "title": "Do LLMs Know to Respect Copyright Notice?",
            "text": "Researchers and industry leaders have placed significant emphasis on protecting the intellectual property rights associated with the weight parameters of LLMs (Peng et al., 2023;Xue et al., 2021). This focus is driven by the resource-intensive nature of large model training and the need to prevent the unauthorized use of these powerful tools (Zhang et al., 2018;He et al., 2022;Dale, 2021). \n\nLLMs possess the capability to memorize and reproduce extensive segments of their training data, which may include copyrighted text and sensitive information (Karamolegkou et al., 2023;Carlini et al., 2019;Lee et al., 2023;Carlini et al., 2022;Kandpal et al., 2022). This memorization potential raises critical copyright infringement concerns, especially as models scale up in size and data capacity (Carlini et al., 2021;Ozdayi et al., 2023;Chao et al., 2023), or under extraction attack (Ishihara, 2023). Some innovative methodologies like \"copyright traps\" (Shilov et al., 2024;Shi et al., 2023;Meeus et al., 2024) have been developed to detect copyrighted content in LLMs' training data. More experiments have examined the extent to which LLMs can generate verbatim copyrighted content (Liang et al., 2022) and quantified the associated legal risks using metrics that capture exact and near-exact reproduction (Carlini et al., 2021;Lee et al., 2022). In addition, the pervasiveness of copyright concerns extends beyond a single modality, encompassing multiple forms of digital content creation, such as text, scripts, images and videos, (Moayeri et al., 2024;Kim et al., 2024), code (Yu et al., 2023), and others. This widespread impact underscores the urgency of addressing these issues (Lucchi, 2023).",
            "score": 0.44147282671724697,
            "section_title": "Related Work on Copyright vs LLM",
            "char_start_offset": 2961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1702
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 195,
                    "matchedPaperCorpusId": "235914115"
                },
                {
                    "start": 344,
                    "end": 364,
                    "matchedPaperCorpusId": "44085059"
                },
                {
                    "start": 364,
                    "end": 380,
                    "matchedPaperCorpusId": "244909149"
                },
                {
                    "start": 380,
                    "end": 391,
                    "matchedPaperCorpusId": "229170661"
                },
                {
                    "start": 553,
                    "end": 580,
                    "matchedPaperCorpusId": "264426289"
                },
                {
                    "start": 580,
                    "end": 601,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 601,
                    "end": 618,
                    "matchedPaperCorpusId": "247450984"
                },
                {
                    "start": 639,
                    "end": 660,
                    "matchedPaperCorpusId": "246823128"
                },
                {
                    "start": 795,
                    "end": 817,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 884,
                    "end": 900,
                    "matchedPaperCorpusId": "258888114"
                },
                {
                    "start": 976,
                    "end": 993,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 1185,
                    "end": 1205,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 1309,
                    "end": 1331,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1331,
                    "end": 1348,
                    "matchedPaperCorpusId": "235829052"
                },
                {
                    "start": 1582,
                    "end": 1599,
                    "matchedPaperCorpusId": "260140077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040924072265625
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Copyrighted Topics \n\nWe discuss how we select target topics to serve as inputs for our data generation pipeline. Our objective is to identify topics associated with copyrighted images that contain highly specific features. As such, generating these features would not be considered as transformative works, thereby resulting in explicit copyright infringement [Milner Library, 2023]. \n\nAs such, we concentrate on three distinct domains: movies, video games, and logos (trademarks). These domains are particularly well-aligned with potentially copyrighted subjects, as movies, video games, and logos are products meant for commercial usage. Hence, creators of these products have the incentive to protect their intellectual property. Further, copyright infringement in these domains would be in the form of explicit replication of the subjects rather than a form of style transfer. Images from these domains are also very popular, increasing the likelihood of their inclusion within diffusion model training sets. Additionally, we prioritize recently released movies and video games, to ensure that our samples are of high quality. Images from recent years are also more likely to be protected by copyright as they have not yet entered the public domain [Office, 2023]. Nevertheless, it is important to emphasize that our approach is a form of academic research and we thus refrain from asserting that the topics we have gathered in this study definitively qualify as copyrighted subjects. \n\nOne direction involves finding titles containing polysemic words or phrases. Polysemic refers to the capability of an object to have several possible meanings that vary depending on the context. An illustrative example of such a term is \"Halo\", which can refer to either a glowing ring above an angel's head or a ring around a planet or the video game. However, even when used in the former context, the generated image (Figure 1) is still from the video game. Another avenue is to identify broad categories that are over-represented. For instance, the superhero category is over-represented by the word \"Superman\" and the coffee brand is over-represented by \"Starbucks\". We find then when provided with generic prompts for images in these categories, diffusion models tend to generate images that contain the Superman logo or character (in the former case) or the Starbucks logo (in the latter).",
            "score": 0.4406757860377437,
            "section_title": "B Details on Collecting Potentially",
            "char_start_offset": 33227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 21,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2387
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10577392578125
        },
        {
            "corpus_id": "266798835",
            "title": "The Study of Copyright Infringement Liability of Generative Artificial Intelligence",
            "text": "As AI continues to produce content at an unprecedented rate, it is essential to establish a robust legal framework to protect the rights of creators and appropriately assign liability in cases of copyright infringement. Generative AI technology has the characteristics of high data demand, strong human-computer interaction, weak interpretability, and weak stability. The main civil subjects in generative AI services are generative AI service providers and users. In the pre-training stage, if generative AI uses copyrighted works without authorization, it should be recognized as copyright infringement, and the relevant infringement liability should be borne by generative AI service providers. In the content generation stage, the high similarity between AI-generated content and prior works can be attributed to various factors, including flaws in the generative model, intentional design choices, and user input and guidance. Both the generative AI service provider and the user bear direct tort liability for the infringement. However, the question of whether generative AI service providers also bear indirect tort liability should be explored, considering their role and similarities to traditional ISPs. Generative AI service providers should fulfill their obligations, take appropriate actions to prevent infringement, and report any illegal activities to the relevant authorities.",
            "score": 0.44033632873920275,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2744140625
        },
        {
            "corpus_id": "257495777",
            "title": "Erasing Concepts from Diffusion Models",
            "text": "The high cost of the process makes it challenging to establish a causal connection between specific changes in the data and the capabilities that emerge, but users report that removing explicit images and other subjects from the training data may have had a negative impact on the output quality [30]. And despite the effort, explicit content remains prevalent in the model's output: when we evaluate generation of images using prompts from the 4,703 prompts of the Inappropriate Image Prompts (I2P) benchmark [38], we find that the popular SD 1.4 model produces 796 images with exposed body parts identified by a nudity detector, while the new trainingset-restricted SD 2.0 model produces 417 (Figure 7). \n\nAnother major concern regarding the text-to-image models is their ability to imitate potentially copyrighted content. Not only is the quality of the AI-generated art on par with the human-generated art [34], it can also faithfully replicate an artistic style of real artists. Users of Stable Diffusion [31] and other large-scale text-to-image synthesis systems have discovered that prompts such as \"art in the style of [artist]\" can mimic styles of specific artists, potentially devaluing original work. Copyright concerns of several artists has led to a lawsuit against the makers of Stable Diffusion [1], raising new legal issues [41]; the courts have yet to rule on these cases. Recent work [42] aims to protect the artist by applying an adversarial perturbation to artwork before posting it online to prevent the model from imitating it. That approach, however, cannot remove a learned artistic style from a pretrained model. \n\nIn response to safety and copyright infringement concerns, we propose a method for erasing a concept from a text-to-image model. Our method, Erased Stable Diffusion (ESD), fine-tunes the model's parameters using only undesired concept descriptions and no additional training data. Unlike training-set censorship approaches, our method is fast and does not require training the whole system from scratch. Furthermore, our method can be applied to existing models without the need to modify input images [42].",
            "score": 0.4400863365636181,
            "section_title": "Introduction",
            "char_start_offset": 1667,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 1010,
                    "end": 1014,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2474365234375
        },
        {
            "corpus_id": "272060563",
            "title": "Managing Copyright Infringement Risks in Generative Artificial Intelligence Data Mining",
            "text": "The infringement of works by generative AI becomes explicit through the generation of content. Based on the previous discussion that non-expressive use does not amount to the use of works and is not subject to tort liability, further infringement analysis will not be conducted here. However, in the case of expressive use, the situation differs. When the generated content from generative artificial intelligence exhibits original expression during the output stage, it should be categorized as a \"content conversion\", qualifying as fair use and therefore exempt from tort liability. On the other hand, if the generated content merely replicates or rearranges the learned work without adding significant originality, resulting in substantial similarity to the original work, it should not be considered fair use but rather as copyright infringement [12]. In practice, when assessing infringement considering the input of previous works, the determination may follow the standard of \"contact + substantial similarity\". In other words, if the generated content is substantially similar to the original work and lack prior permission from the copyright owner, the infringement of the right to copy can be established directly without the need to evaluate the degree of similarity in terms of quantity and quality. The consideration of similarity in quantity and quality comes into play when the defendant subsequently claims \"fair use\" as a defense. \n\nThe resulting tort liability should be clearly defined in legislation to protect the rights of the original owners. This paper argues that the tort liability should be categorized and discussed based on different development models. \n\nWhen the service provider purchases the model developed by the technology developer and provides it to the user directly, it should be established that the technology developer bears the corresponding copyright infringement liability. Because it understands and can control the data input of generative AI. Unless the developer can prove that the model was not exposed to the original work during the data training process. As a result, the problem of \"algorithmic black box\" can be effectively solved by comprehensively checking the algorithms, training data, and output content, so as to better improve the interpretability of artificial intelligence. \n\nWhen the service provider fine-tunes the model to meet the requirements of specific application scenarios, both the pre-training in the technology development stage and the fine-tuning before the later market application will essentially shape the generative AI algorithm model.",
            "score": 0.43977027429971455,
            "section_title": "Clarify the Subject of Tort Liability",
            "char_start_offset": 15475,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2338
                },
                {
                    "start": 2341,
                    "end": 2619
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25732421875
        },
        {
            "corpus_id": "270357944",
            "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI",
            "text": "Existing works [23][24][25][26][27] find that the visual generative AI models have the memorizations on the training data.The potential reason for such IP infringement phenomenon is that the visual generative model have the memorizations on the training data [23][24][25], and the training data (e.g., LAION dataset [28] and WebVid dataset [29]) of the visual generative artificial intelligence might contain a large amount of publicly available copyrighted data.",
            "score": 0.4396506628741437,
            "section_title": "Related Work",
            "char_start_offset": 7745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 463
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 23,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 340,
                    "end": 344,
                    "matchedPaperCorpusId": "232478955"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.206298828125
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "Image-based copyright infringement arises when visual content is copied, shared, or disseminated without the authorization of the copyright owner.This challenge has led to the creation of advanced technologies aimed at detecting unauthorized reproduction and use of copyrighted images.\n\nFor instance, Trappey et al. present a robust trademark similarity assessment system, in which they train a convolutional neural network (CNN) on 100,000 positive image pairs and 150,000 negative image pairs, setting aside 20,000 image pairs for validation [126].As shown in Figure 3, they train a Siamese neural network (SNN) [61] on these similarities and extract feature vectors using a CNN pre-trained on the VGGnet architecture [115].Upon evaluation using 3 single-scale CNNs, 3 multi-scale CNNs, and 1 multi-scale CNN with random filters, 2 of the multi-scale CNNs with 3 layers + 7 layers and 3 layers + 5 layers was shown to be the most accurate, with both having 76% training set accuracy and 89% validation set accuracy.Similarly, Kim et al. also train an SNN, combining regions of interest (RoI) and image hashing to measure an input's similarity to other existing images and subsequently detect copyright infringement [58].By combining methods of Image RoI, hashing, and verification, their proposed method not only correctly identifies 88.7% of copyright infringement in images, but also reduces the false positive rate from 91.8% to 2.8% when compared to methods without image verification.\n\nIn comparing trademark similarity assessment by Trappey et al., which utilizes multi-scale CNNs, against Kim et al.'s photo identification framework combining ROI, hashing, and verification, it becomes evident that Kim et al.'s approach offers a more refined solution.Not only does it ensure precise identification of copyright breaches, but it also notably decreases the false positive rate, demonstrating superior adaptability in pinpointing AI-generated copyright infringement and holding a distinct advantage over Trappey et al. 's method.By weaving the insights from these comparative analyses into the fabric of future methodologies, we can significantly bolster our capacity to combat copyright violations effectively.",
            "score": 0.4395849363983861,
            "section_title": "Image-based Infringement",
            "char_start_offset": 5489,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 285
                },
                {
                    "start": 287,
                    "end": 550
                },
                {
                    "start": 550,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 1017
                },
                {
                    "start": 1017,
                    "end": 1222
                },
                {
                    "start": 1222,
                    "end": 1491
                },
                {
                    "start": 1493,
                    "end": 1761
                },
                {
                    "start": 1761,
                    "end": 2036
                },
                {
                    "start": 2036,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 720,
                    "end": 725,
                    "matchedPaperCorpusId": "14124313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30126953125
        },
        {
            "corpus_id": "269294137",
            "title": "An Economic Solution to Copyright Challenges of Generative AI",
            "text": "Generative artificial intelligence (AI) systems are trained on large data corpora to generate new pieces of text, images, videos, and other media. There is growing concern that such systems may infringe on the copyright interests of training data contributors. To address the copyright challenges of generative AI, we propose a framework that compensates copyright owners proportionally to their contributions to the creation of AI-generated content. The metric for contributions is quantitatively determined by leveraging the probabilistic nature of modern generative AI models and using techniques from cooperative game theory in economics. This framework enables a platform where AI developers benefit from access to high-quality training data, thus improving model performance. Meanwhile, copyright owners receive fair compensation, driving the continued provision of relevant data for generative model training. Experiments demonstrate that our framework successfully identifies the most relevant data sources used in artwork generation, ensuring a fair and interpretable distribution of revenues among copyright owners.",
            "score": 0.43958241520631935,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328125
        },
        {
            "corpus_id": "260333979",
            "title": "Model Synthesis for Zero-Shot Model Attribution",
            "text": "I N recent years, advanced generative (vision) models have revolutionized various fields such as art creation, design, and human-computer interaction [1]- [4]. Despite their positive impact, these models have also given rise to new concerns, such as copyright infringement issues and content supervision. To address these concerns, model attribution, the process of identifying the source model of generated content, has gained increasing attention [5]- [10]. It helps deter unauthorized copying and distribution, enabling content creators and rights holders to prove ownership and take legal action against infringements. Furthermore, model attribution allows regulators to identify and act against entities using generative models for harmful, illegal, or unethical purposes. \n\nExisting research seeks to identify the unique fingerprints on the images they generate, which can be leveraged to attribute a generated image to its source model. A commonly adopted model attribution paradigm frames the task as a multi-class classification problem [6], [8]- [10]. In this setup, images generated by a limited and static set of models are used to train a classifier, where each image is labeled with a unique model Fig. 1: Illustration of existing model attribution method (above) and our method (below). Existing methods rely on training with a limited set of real-world models, allows them to attribute seen classical models included in the training data. However, they struggle with generalizing to emerging unseen models like Stable Diffusion and DALL-E due to a gap in fingerprint distribution. Our method, in contrast, trains on numerous synthetic models that replicate a broader range of real-world generative model fingerprints, enabling it to attribute unseen models effectively in a zero-shot manner. \n\nID. During testing, the test image comes from a seen model predefined in the training set, and the classifier would identify the model ID of the image based on the model fingerprint it has learned [6], [8], [9]. However, there arises a scenario where a testing image originates from an unseen model not present during training. To tackle this, some methodologies [10], [11] adopt an open-set setup, assigning them an \"unknown\" label.",
            "score": 0.43918744060278136,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1807
                },
                {
                    "start": 1810,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2137
                },
                {
                    "start": 2138,
                    "end": 2243
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 153,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "257496280"
                },
                {
                    "start": 1046,
                    "end": 1049,
                    "matchedPaperCorpusId": "201058738"
                },
                {
                    "start": 1051,
                    "end": 1054,
                    "matchedPaperCorpusId": "247158092"
                },
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "257496280"
                },
                {
                    "start": 2007,
                    "end": 2010,
                    "matchedPaperCorpusId": "201058738"
                },
                {
                    "start": 2012,
                    "end": 2015,
                    "matchedPaperCorpusId": "247158092"
                },
                {
                    "start": 2017,
                    "end": 2020,
                    "matchedPaperCorpusId": "250280166"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.313720703125
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "As an increasing number of companies incorporate textto-image diffusion models (DMs) (Rombach et al., 2022;Nichol et al., 2021;Saharia et al., 2022;Ramesh et al., 2022) into their products, the issue of copyright becomes increasingly prominent (Vincent, 2023). To responsibly harness The proposed method does not require access to or control over the diffusion model's training or fine-tuning processes. It simply involves inserting poisoning data into the clean training dataset. After training with the poisoned dataset, the target text-to-image diffusion model can be triggered by specific text prompts to generate images that infringe on copyright. The proposed attack works by semantically dissecting a copyrighted image into nuanced elements and incorporating them into multiple images, thus rendering them non-copyright-infringing. Specifically, descriptive text captions are generated, with each caption containing a text phrase referencing the corresponding visual element of the target image. Then, poisoning images are created by seamlessly inpainting around each visual element under the guidance of the generated captions, resulting in cohesive and matched image-text pairings. Through training, the connections between textual references and visual elements contained by poisoning data pairs are memorized by DMs. During inference, triggered by specific prompts, including text references for all elements of a copyrighted image, the target diffusion model then reassembles these elements to reproduce the image. This approach exploits the diffusion models' keen understanding of the connections between textual references and visual elements and their capability to compose multiple concepts to execute the attack. Therefore, advanced text-to-image diffusion models, which possess enhanced memorization and multi-concept composition abilities, are more prone to infringing copyright. \n\nWe empirically demonstrate the efficacy of SilentBadDiffusion in inducing various versions of stable diffusion versions (SDs) to generate infringing images when triggered by specific prompts. Besides, our experiments show the stealth of the poisoning data. Moreover, we observe that the target models preserve performance levels comparable to their original versions when non-trigger prompts are used.",
            "score": 0.4386113459657853,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1898
                },
                {
                    "start": 1901,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 107,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55859375
        },
        {
            "corpus_id": "269033466",
            "title": "RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion",
            "text": "While we use pretrained models for all components of our pipeline, it is important to acknowledge biases and ethical issues that stem from the training of these large-scale image generative models [50]. As these models are often trained on vast collections of internet data, they can reflect negative biases and stereotypes against certain populations, as well as infringe on the copyright of artists and other creatives. It is essential to consider these factors when using these models and our technique broadly.",
            "score": 0.43838270687429215,
            "section_title": "Ethical Considerations",
            "char_start_offset": 24481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 514
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12451171875
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "For the process of identifying substantial similarity, we refer to the abstraction-filtering-comparison test method (Abramson, 2002), which has been widely adopted in practical court rulings on infringement cases, and propose an automated infringement identification framework using large visionlanguage models, as seen in Figure 1. In the copyright expression extraction stage, we break down images into different elements (such as composition and color patterns), and filter out non-copyrightable parts, leaving copyrighted portions to assess substantial similarity. In the next copyright infringement determination stage, multiple LVLMs debate and score the similarity of images given the copyrighted elements, with a final decision made by a meta-judge LVLM based on their consensus. Human priors are injected into the models through few-shot demonstrations to better align with human preferences. \n\nCopyright expression extraction via image-to-text abstraction and filtration. The process of distinguishing between the fundamental ideas and the specific expressions of an image is a crucial step in determining copyright protection. The core idea of our method is to break down the image into different layers or components, in order to examine the true copyright elements. \n\nFirst, during the abstraction phase, the image is analyzed and decomposed into its fundamental building blocks. This involves identifying the core elements that contribute to the overall meaning or aesthetic of the image, such as composition, themes, color palette, or other unique visual elements. We can implement this using an LVLM \u03c0 abs , defined as: \n\nwhere z and z cr represent the expressions of x and x cr in text after decoupling, respectively. The goal is to abstract away the superficial features of the image that do not hold significant creative value and instead focus on the underlying concepts that convey the essence of the work. \n\nThe next step is filtering. At this stage, elements of the image that are not eligible for copyright protection are removed from consideration. These can include generic concepts, common patterns, functional aspects, or elements derived from public domain sources. For example, standard design patterns or commonly used motifs in artwork may not be deemed original enough to warrant protection under copyright law. This process could be defined as: \n\nwhere z c and z c cr are the filtered copyright expressions, and \u03c0 f il is another independent LVLM. Filtering helps ensure that only the truly creative, original aspects of the image are preserved for comparison.",
            "score": 0.4382822818792246,
            "section_title": "Abstraction-Filtering-Comparison Framework",
            "char_start_offset": 6659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1635
                },
                {
                    "start": 1638,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2344
                },
                {
                    "start": 2345,
                    "end": 2378
                },
                {
                    "start": 2381,
                    "end": 2481
                },
                {
                    "start": 2482,
                    "end": 2594
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 132,
                    "matchedPaperCorpusId": "6322845"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1143798828125
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "This structure can be maliciously exploited to generate disguised copyrighted samples: given a copyrighted image, we show how to generate a disguise such that it is visually different from the copyrighted sample but shares similar latent information.The closeness of the two samples in the latent space can be quantitatively measured by a distance function, or qualitatively revealed by a concept extraction tool called textual inversion (Gal et al. 2022), both of which we will demonstrate in our empirical study.\n\nOur study reveals the possibility of creating a new training dataset that does not appear to directly or blatantly contain any copyrighted data.Nonetheless, if a model is trained on this derivative training dataset, it would behave similarly as if the copyrighted data were present.Such disguises may still exhibit copyright infringement, although only accessing proprietary data indirectly.In Figure 1, we display a comparison between the previous copyright infringement Figure 1: An overview of conventional (with direct access to copyrighted material) and disguised (with indirect access) copyright infringement for latent diffusion models.For direct access, training an LDM-based model on copyrighted material x c and reproducing x c is subjected to copyright infringement.For indirect access, one trains the same model on disguised samples x d , which are drastically different from x c , but is still able to reproduce x c during inference.phenomenon (direct access) with the disguised copyright infringement (indirect access).Clearly, there was still access to the copyrighted material in the latter training pipeline, which raises the following question:\n\nWhat constitutes access?How to quantify it?\n\nWe answer the first question by introducing a notion of acknowledgment, which refers to a criterion that any sample that contains similar latent information as that of a copyrighted sample should be considered acknowledging it, despite possible visual dissimilarity.To quantify acknowledgment in practice, a deeper inspection than visually auditing the training set is required.Thus we further propose a twostep detection method: (1) a feature similarity search for screening suspects; (2) an encoder-decoder examination to confirm disguises, which augments the existing criterion.In summary, we make the following contributions:",
            "score": 0.4381229100250422,
            "section_title": "Introduction",
            "char_start_offset": 1942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 250,
                    "end": 514
                },
                {
                    "start": 516,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 907
                },
                {
                    "start": 907,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1462
                },
                {
                    "start": 1462,
                    "end": 1549
                },
                {
                    "start": 1549,
                    "end": 1678
                },
                {
                    "start": 1680,
                    "end": 1704
                },
                {
                    "start": 1704,
                    "end": 1723
                },
                {
                    "start": 1725,
                    "end": 1991
                },
                {
                    "start": 1991,
                    "end": 2103
                },
                {
                    "start": 2103,
                    "end": 2306
                },
                {
                    "start": 2306,
                    "end": 2354
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3203125
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "We propose a pipeline to coordinate CLIP and diffusion models to generate a dataset that contains anchor images, corresponding prompts, and images generated by text-to-image models, reflecting the potential abuses of copyright, as illustrated in Fig. 3. Initially, we collect a set of images that potentially contain copyrighted content, which serves as anchor images.Subsequently, these images are fed into the clip-interrogator6 , allowing us to obtain prompts that correspond to each anchor image.We also provided experimental results using tools other than clip-interrogator in the supplementary materials.Finally, the prompts are used as inputs for the stable diffusion model, resulting in the generation of images by the stable diffusion model.The final outcomes indicate that even such a rudimentary pipeline can effectively generate a substantial volume of works pertinent to infringement issues.We conducted a statistical of the styles in prompts obtained using the pipeline in Fig. 3.The various styles at the bottom are provided by artists.",
            "score": 0.4380452061481285,
            "section_title": "Dataset Creation Pipeline",
            "char_start_offset": 9355,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 500
                },
                {
                    "start": 500,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 750
                },
                {
                    "start": 750,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 994
                },
                {
                    "start": 994,
                    "end": 1051
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0943603515625
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "In Fig. 2, we show example outputs under four scenarios. It is evident that with amplification, the CP-k protected generative model does indeed output copyright-infringed content with high probability, especially when using the Anti-NAF prompt. These examples suggest that probabilistic safeguards against copyright infringement are vulnerable to the amplification attack. Additional visualizations of outputs on various copyrighted images are demonstrated in Fig. 3. In Tab. 1, we report CIRs and FARs at various ARs on two datasets. Notably, there is a significant growth in both metrics when amplification attack is employed, highlighting its effectiveness in amplifying the probability of infringing generations. The superior performance of Anti-NAF under- CLIP-Int. w/ Amp. PEZ w/ Amp. Anti-NAF w/ Amp. \n\n-greedy-max Bandit Amp. \n\n-greedy-cdf Bandit Amp. \n\n(a) FAR-AR on POKEMON. CLIP-Int. w/ Amp. PEZ w/ Amp. Anti-NAF w/ Amp. \n\n-greedy-max Bandit Amp. \n\n-greedy-cdf Bandit Amp. \n\n(b) FAR-AR on LAION-mi. scores its efficacy in rendering infringing generations with a substantial probability. The overall FAR-AR curves are illustrated in Fig. 4. We can observe that bandit variants of amplification lead to a smaller variance across different target copyrighted images, especially at lower acceptance rates, indicating that our bandit strategies achieve a more steady attack. In Figs. 4c and 4d, we plot the overall FAR-AR curves over different amplification steps. There is a clear trend of rapidly improved performance with increased amplification steps, due to the cumulative probability of generating infringing samples. This finding underlines the potential risk in practical applications of probabilistic copyright protections, given the high frequency of daily interactions with text-to-image generative models. We provide additional results and human evaluations in Sec. 11.",
            "score": 0.4379476245142824,
            "section_title": "Results",
            "char_start_offset": 25578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 807
                },
                {
                    "start": 810,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 1887
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19482421875
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "No viewer would say that the model has the same \"total concept and feel\" as a painting; no reader would say that it is substantially similar to a blog post; and so on. There is therefore a colorable argument that a model is not a copy of any of the works it was trained on. Although the model could be prompted to generate a substantially similar work, it would be the generation that infringes, and not the model. This is an interesting argument, and we will return to it below. But there is a persuasive counter-argument that a model is a copy, because the Copyright Act does not require that copies be directly human-intelligible. After all, a Blu-Ray is not directly intelligible by humans, either, but it counts as a \"copy\" of the movie on it. Indeed, all digital copies are unintelligible. Instead, they are objects \"from which the work can be perceived, reproduced, or otherwise communicated \u2026 with the aid of a machine or device. \" 20 Thus, even if a model is uninterpretable, it might still be possible to \"perceive[]\" or \"reproduce[]\" a copyrighted work embedded in its weights through suitable prompting. 21 n this view, the necessary condition for a model to count as a substantially similar copy of a work is that the model is capable of generating that work as an output. Note that this is direct infringement, not secondary. The theory is not that the generation is an infringing copy and the model is a tool in causing that infringement in the way that a tape-duplicating machine might be a tool in making infringing cassettes. 22 The theory is that the model itself is an infringing copy, regardless of whether that particular generation is ever made. (Alert readers will note the similarity to the debate over whether the mere act of making it available without a download infringes the distribution right. 23 ) \n\nAnother perspective is that a model is a compilation of (at least some of the) works that it was trained on. If so, then it might be a copy irrespective of whether the model can generate a specific work when used in the normal way. That is because there might be other ways of inspecting the model that are capable of recovering training data.",
            "score": 0.4377919844178266,
            "section_title": "B. Substantial Similarity",
            "char_start_offset": 21967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1829
                },
                {
                    "start": 1832,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2175
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12445068359375
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "2) Training stage protection Different from directly employing DGMs to generate new images, DGMs are also usually trained or fine-tuned on some source images  to effectively learn useful information from  for future generations. Therefore, \"training-stage protection\" aims to add imperceptible noise to the copyrighted images, to break the training process of potential DGMs for data copyright protection. This type of method is first explored on GAN-based models. For example, GAN-based Deepfake models [81] are representative tools that can be leveraged to swap the faces from the source images to the faces of a target person, which severely abuses the copyrights of both source image holders and target people. Similar tools like EditGAN [75] and Introspective Adversarial Network [10] are developed to edit images that pose more threats to the copyrights of creative and artistic works. To handle this problem, Yang et al. [146] proposed to utilize the idea of adversarial examples [41], to break the balance in the min-max game in GAN-based DeepFake models. Specifically, they focus on Deepfake models which are trained on the target person's face images  to generate other face images belonging to the target person. \n\nTo protect the targeted face  being exploited by Deepfake, they directly adopt the fast gradient sign method (FGSM) method to generate adversarial examples for GAN-based models, \n\nwhere L  refers to the loss of the discriminator in GAN. During the generation process, the discriminator  (\u2022) will have a large loss value on the target samples  target , which consequently breaks the balance of the min-max game during the training of GAN. As a result, the generated images based on the protected target images will have a degraded quality (see Figure . 2). Notably, in Eq.(3), a transformation operator Tr(\u2022) is introduced to improve the robustness of the perturbation under various image transformations, including resizing, affine transformation and image remapping.",
            "score": 0.43747939128666863,
            "section_title": "Unrecognizable Examples.",
            "char_start_offset": 21027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1403
                },
                {
                    "start": 1406,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 742,
                    "end": 746,
                    "matchedPaperCorpusId": "243832879"
                },
                {
                    "start": 928,
                    "end": 933,
                    "matchedPaperCorpusId": "219687654"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5166015625
        },
        {
            "corpus_id": "261049723",
            "title": "DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization",
            "text": "\u2022 We introduce a data-free solution that utilizes LLM and SD models to generate training data, eliminating the necessity of directly accessing highly confidential copyrighted images. \n\n\u2022 The experimental results demonstrate that the proposed DUAW can cause obvious distortion to the outputs of fine-tuned SD models, with 96.43% of output images successfully identified as infringing images by a simple classifier.",
            "score": 0.43727418324672646,
            "section_title": "Introduction",
            "char_start_offset": 3853,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 413
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08538818359375
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "In Fig. 8, we show example outputs of three target copyrighted images under four attack and defense scenarios. Similar to Fig. 2, using a benign prompt (such as the original caption), in the first column, we can observe that outputs without copyright protection infringe the copyright of target images with high probability; in the second column, after  copyright protection, all samples are non-infringing content as CP-k rejects all infringing samples. In the third column, we find that an amplification attack with a benign prompt can be unsuccessful, because such a prompt may not provide a strictly positive probability of producing infringing generations from models protected by CP-k. However, in the last column, with an adversarial prompt obtained from our proposed Anti-NAF algorithm, we can see that most of the outputs are copyright-infringed, which means that the probability of infringing samples is largely amplified. In Fig. 9, we give detailed FAR-AR curves on each target copyrighted image in LAION-mi dataset. We can find that our proposed bandit amplification method performs more steadily in the worst cases. For example, in Figs. 9a and 9d, when acceptance rate is lower than 20%, the FAR of Anti-NAF with amplification is nearly 0%; while \u03b5-greedy-max/cdf bandit amplification can adapt to follow the best choice of prompts (e.g., PEZ or CLIP-Interrogator) and keep a competitive FAR score.",
            "score": 0.4367129493036608,
            "section_title": "Detailed Analysis on Results",
            "char_start_offset": 35988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1413
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2127685546875
        },
        {
            "corpus_id": "259274742",
            "title": "Next Steps for Human-Centered Generative AI: A Technical Perspective",
            "text": "As a biased Generative AI model's output might permeate into the real world (e.g., people using ChatGPT to write various sorts of documents), it will soon become imperative to stop such biased AI-generated data from being used to train future models. According to Veselovsky et al., 33-46% of crowd workers are estimated to use LLMs in completing their tasks [139]. One next-step is to incorporate ongoing efforts that detect AI-generated contents (e.g., [74,103]) into the screening of training data, which still remains as one highly challenging topic. \n\n3.1.2 Preventing creators' data from being used for training Generative AI. In a recent discussion, Pamela Samuelson described latest legal cases and the challenge of determining what constitutes infringement in the context of AI-generated contents [42]. Similar to how the General Data Protection Regulation (GDPR) influenced a wide range of changes in how technology handles user data (e.g., cookies on websites), we can anticipate changes in Generative AI systems as new advances on the legal front take place, such as adding Adversarial noises [123] or \"certified\" watermarks [13,17,57] to prevent unobstructed usage of artists' work. Next-steps for HGAI are as follows. \n\nOne grand challenge is enabling creators to protect their works in public domains from being used for training Generative AI, which requires the entire industry and research community to establish new protocols of data collection. For example, as of Dec 2023, the Adobe Firefly generative AI model is claimed to be trained on a dataset of licensed content, such as Adobe Stock, and public domain content where copyright has expired 4 . Similar to how open source licenses enable and regulate how one's code can be used, we need to develop similar mechanisms for writers, artists, and designers to specify permitted usage of their work. For example, one possible mechanism is that artists who opt-in to contribute their work for training a model can have access to that model's generated contents for their future projects. \n\nAlternatively, Generative AI developers should allow creators to audit existing training data with tools to help them identify whether their work has been inadvertently included.",
            "score": 0.4360374764142495,
            "section_title": "3.1.1",
            "char_start_offset": 17064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2056
                },
                {
                    "start": 2059,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "218596111"
                },
                {
                    "start": 1141,
                    "end": 1144,
                    "matchedPaperCorpusId": "250340993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10211181640625
        },
        {
            "corpus_id": "268201483",
            "title": "On the Challenges and Opportunities in Generative AI",
            "text": "Interestingly, recent works show that publicly available LLMs and large-scale text-to-image models can implicitly \"memorize\" training data, in the sense that points from the dataset can be (almost exactly) reconstructed, which potentially infringes on data privacy [23,22,188,136]. This connects to the open question of whether it is possible to train generative models while preserving data privacy. To alleviate this problem, approaches like training generative models with differential privacy (DP) constraints offer an attractive theoretical framework to ensure privacy [111,41]. However, DPbased approaches suffer from a tradeoff between privacy and utility. Moreover, in the context of image generation, scaling such approaches to high-resolution datasets remains elusive. Therefore, building privacy constraints in large-scale training of generative models can be a promising direction for further research. \n\nIn addition to privacy, recent advances in large-scale generative modeling can lead to unauthorized distribution or replication of training data resulting in copyright infringement liabilities 6 . While some recent work tries to alleviate this issue [113,248], there are several outstanding technical challenges ranging from mitigating copyright infringements during dataset curation [24] to reliable detection of copyright violations in generated samples.",
            "score": 0.4354046103179531,
            "section_title": "Privacy and Copyright Infringement",
            "char_start_offset": 30601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 914
                },
                {
                    "start": 917,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1373
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 269,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 269,
                    "end": 272,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 574,
                    "end": 579,
                    "matchedPaperCorpusId": "238634219"
                },
                {
                    "start": 579,
                    "end": 582,
                    "matchedPaperCorpusId": "252968205"
                },
                {
                    "start": 1167,
                    "end": 1172,
                    "matchedPaperCorpusId": "256697414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50341796875
        },
        {
            "corpus_id": "271212748",
            "title": "A Survey of Defenses against AI-generated Visual Media: Detection, Disruption, and Authentication",
            "text": "Robust authentication embeds watermarks resistant to the generation process and achieves copyright protection by comparing the similarity of the decoded watermarks to the original watermarks held by the copyright owner. These methods are summarized in Table IV. \n\n1) Copyright Protection: Copyright protection protects the source training data or the used generative model of generative media. Specifically, a copyright owner embeds a robust watermark w before, during, or after generation. Then the output O w with the transferable watermark is generated by generative models and a decoded watermark \u0175 can be obtained from it. By comparing \u0175 with w, the copyright owner can protect their copyright if the matching score is larger than a threshold \u03c4 . \n\nSource data protection typically embeds watermarks into source training data (inputs) of generative models. Early works focus on GAN watermarking. Yu et al. [49] trains an encoder and decoder independent of GAN training to embed and verify bit string watermarks. FakeTagger [225] designs an encoder-decoder watermarking framework to inject bit string watermarks into the channel codings of input representations and then reconstruct the inputs. Recent works also concentrate on DM watermarking, in which some works tailored for training-from-scratch setting [232] while others for finetuning setting [233], [236]. DiffusionShield [232] introduces watermarks with pattern uniformity and joint optimization to improve the reproduction of watermarks in generated images. GenWatermark [233] adds spatial watermarks into training data and incorporates a detector fine-turned along with the image synthesis process to decode watermarks. FT-Shield [236] embeds watermarks at the early stage of fine-tuning and devises a detector mixture effective for various fine-tuning models. \n\nModel protection signs the generative model used by generative media, where the traceable watermarks can be embedded into all discussed watermarking positions. Input watermarking requires retraining generative models to produce watermarks for generators [49], [226], [227]. For encoder watermarking, Zhang et al. [222] propose a training-free plug-and-play bit string watermarking method performing in latent space.",
            "score": 0.4346890354337632,
            "section_title": "B. Robust Authentication",
            "char_start_offset": 51718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2243
                }
            ],
            "ref_mentions": [
                {
                    "start": 911,
                    "end": 915,
                    "matchedPaperCorpusId": "238419552"
                },
                {
                    "start": 1028,
                    "end": 1033,
                    "matchedPaperCorpusId": "235795820"
                },
                {
                    "start": 2082,
                    "end": 2086,
                    "matchedPaperCorpusId": "238419552"
                },
                {
                    "start": 2095,
                    "end": 2100,
                    "matchedPaperCorpusId": "258180539"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27734375
        },
        {
            "corpus_id": "257050406",
            "title": "On Provable Copyright Protection for Generative Models",
            "text": "There have been several studies of copyright issues in machine learning and data mining in the law literature, though most of them focus on potential infringements in the training phase. Sag [2018] surveys the question of whether data mining and machine learning on copyrighted text falls under \"fair use\" and states that \"allowing [text data mining] and other similar non-expressive uses of copyrighted works without authorization is entirely consistent with the fundamental structure of copyright law.\". Sag [2018] also states that under U.S. law \"extracting a short phrase or snippet of text from one work and using it in another does not amount to a reproduction of the work if the localized similarity is not substantial, is not quantitatively or qualitatively significant, or is otherwise de minimis.\" (However, European courts have a stricter threshold for the amount of similarity.) Sobel [2018] also discusses the issue of \"fair use\" in training. While he mentions the issue of output generation, the article does not focus on it since (at the time) \"works generated by Al are fascinating and entertaining, but today they remain novelties rather than mainstream sources of entertainment or compelling substitutes for human expression.\" Gillotte [2020] studies copyright infringement in AI-generated artworks and concludes that regarding the training phase \"an engineer may use copyrighted works to train an Al program to generate artwork without incurring infringement liability.\" Hristov [2016] considers a separate issue regarding AI and copyright: whether it should be possible to grant copyright to AI-authored works. Current rulings (Board [2022]) by the U.S. copyright review board state that wholly AI generated works cannot be considered for copyright. \n\nMemorization of training samples is considered undesirable for many reasons apart from copyright. Lee et al. [2022b] show that deduplication can significantly reduce memorization, but not eliminate it (see also bottom row of Table 1 in [Kandpal et al., 2022]). Ippolito et al. [2022] state that \"deduplication does not guarantee that a model will not still memorize individual (deduplicated) examples, necessitating defenses that operate at inference-time\".",
            "score": 0.4343393050609853,
            "section_title": "Related works",
            "char_start_offset": 49394,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1769
                },
                {
                    "start": 1772,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 187,
                    "end": 197,
                    "matchedPaperCorpusId": "86424284"
                },
                {
                    "start": 506,
                    "end": 516,
                    "matchedPaperCorpusId": "86424284"
                },
                {
                    "start": 891,
                    "end": 903,
                    "matchedPaperCorpusId": "115500744"
                },
                {
                    "start": 1245,
                    "end": 1260,
                    "matchedPaperCorpusId": "233748873"
                },
                {
                    "start": 1870,
                    "end": 1888,
                    "matchedPaperCorpusId": "235829052"
                },
                {
                    "start": 2008,
                    "end": 2030,
                    "matchedPaperCorpusId": "246823128"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1947021484375
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "An increasingly adopted method for safeguarding copyrighted data in generative AI involves machine unlearning, which focuses on erasing specific samples from a training dataset so as to \"forget\" the data [138].Figure 4 visualizes the process of machine unlearning within a trained model, extracting the target data to forget while maintaining similar capabilities between the original and unlearned model.This process is especially crucial in scenarios where the presence of certain data points can significantly impact the model's accuracy or integrity.\n\nAs an example, Zhang et al. present a suite of machine unlearning techniques designed to shield images from copyright infringement within generative AI systems [167].These techniques, including Elastic Weight Adjustment (EWC) [60], Decreasing Moment Matching (DMM) [108], Synaptic Consolidation [106], Catastrophic Unlearning [148], and Incremental Moment Matching (IMM) [67], target the removal of specific data from AI models.With a focus on image datasets labeled with copyright information, the approaches vary in their mechanisms and outcomes.EWC, for instance, fine-tunes AI models by adjusting critical weights, thus selectively erasing copyrighted materials without affecting other learned data [167].DMM, on the other hand, employs Gaussian distributions to align model parameters' moments, targeting the excision of copyrighted images from training sets [167].Both Synaptic Consolidation and Catastrophic Unlearning address the challenge of catastrophic forgetting, ensuring the deletion of copyrighted content doesn't compromise the model's overall performance [106].Lastly, IMM enhances the system's resilience by aligning posterior distribution moments across tasks, solidifying the framework's capability to protect copyrights [67].Together, these strategies form a comprehensive technical foundation for safeguarding intellectual property rights in the evolving landscape of generative AI.\n\n3.1.4Data Deduplication.Similar in vein to machine unlearning, data deduplication involves the removal of redundant data within a model's training dataset, leaving only unique data remaining [21].Data deduplication algorithms can efficiently identify duplicate files or chunks of data within a dataset [137].",
            "score": 0.4341108762106212,
            "section_title": "Methods of Protecting Copyrighted",
            "char_start_offset": 17519,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 210,
                    "end": 405
                },
                {
                    "start": 405,
                    "end": 554
                },
                {
                    "start": 556,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 984
                },
                {
                    "start": 984,
                    "end": 1104
                },
                {
                    "start": 1104,
                    "end": 1265
                },
                {
                    "start": 1265,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1634
                },
                {
                    "start": 1634,
                    "end": 1802
                },
                {
                    "start": 1802,
                    "end": 1960
                },
                {
                    "start": 1962,
                    "end": 1967
                },
                {
                    "start": 1967,
                    "end": 1986
                },
                {
                    "start": 1986,
                    "end": 2158
                },
                {
                    "start": 2158,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 716,
                    "end": 721,
                    "matchedPaperCorpusId": "260979744"
                },
                {
                    "start": 782,
                    "end": 786,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 821,
                    "end": 826,
                    "matchedPaperCorpusId": "7257374"
                },
                {
                    "start": 851,
                    "end": 856,
                    "matchedPaperCorpusId": "249461673"
                },
                {
                    "start": 882,
                    "end": 887,
                    "matchedPaperCorpusId": "237386255"
                },
                {
                    "start": 927,
                    "end": 931,
                    "matchedPaperCorpusId": "5160005"
                },
                {
                    "start": 1259,
                    "end": 1264,
                    "matchedPaperCorpusId": "260979744"
                },
                {
                    "start": 1420,
                    "end": 1425,
                    "matchedPaperCorpusId": "260979744"
                },
                {
                    "start": 1628,
                    "end": 1633,
                    "matchedPaperCorpusId": "249461673"
                },
                {
                    "start": 1797,
                    "end": 1801,
                    "matchedPaperCorpusId": "5160005"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.404541015625
        },
        {
            "corpus_id": "277468540",
            "title": "Unfair Learning: GenAI Exceptionalism and Copyright Law",
            "text": "If the copyrighted work did not improve the model's capabilities, GenAI companies would not train the model on the work. 67 It is unclear why the 66 As Brauneis puts it, \"If the only thing copyright protected in a work were the aesthetic or hedonic reactions that it produced, or if works used to train generative AI were created for the sole purpose of producing such reactions, then that factual difference could make a legal difference\u2026Adopting a view that anhedonic learning, and only anhedonic learning, is a fair use would unfairly disadvantage human authors for whom aesthetic experience and learning are inextricably intertwined.\" Copyright and the Training of Human Authors and Generative Machines at 20. 67 This is why GenAI developers want as many expressive training materials as they can collect, longer and more detailed descriptions of items in images, and so on. It appears that the latest breakthroughs in computer vision hinge not just on the quantity of data, but on the quality, including \"highly detailed image captions,\" Matt Deitke et al., Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models, https://arxiv.org/abs/2409.17146. Moreover, all artistic works inherently include the expressions of the artists, such as how they chose to do shading, value of capability should not matter more than the value of subjective enjoyment, given that the model capabilities are more likely to undermine, disrupt, and compete with the copyright owners' markets than someone's personal subjective enjoyment. \n\nRegarding the transformation of content, such as words, into numbers as GenAI does, the changes are arguably less significant than the transformation occurring in the human brain, where words are converted into complex electrochemical signals-a process that is no less irreversible. Notably, copyright law does not distinguish between different forms of transformation when assessing infringement. For example, a concert recording converted into an mp3 file is still subject to copyright restrictions, just as GenAI's transformation of words into numbers should be. 68 The encoded words, sounds, and images in a model are merely a derivative work, or, perhaps, a compilation. 69 xamining how GenAI and humans learn highlights further distinctions.",
            "score": 0.433826047894945,
            "section_title": "Counterargument",
            "char_start_offset": 63440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2233
                },
                {
                    "start": 2234,
                    "end": 2302
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.063720703125
        },
        {
            "corpus_id": "268691556",
            "title": "Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes",
            "text": "A growing number of researchers in recent years explore how to address legal problems by applying theories and methods of computer science.This literature seeks to narrow the gap between the vague and abstract concepts used by law by applying mathematical models to offer more rigor, coherent and scalable definitions into issues such as privacy (Dwork et al., 2006), or fairness and discrimination (Dwork et al., 2012).The study of copyright by computer science methods has only emerged recently.Scheffler et al. (2022), for instance, proposed a framework to test substantial similarity by comparing Kolmogorov-Levin complexity with and without access to the original copyright work.\n\nIn the context of generative models Carlini et al. (2023); Haim et al. (2022), explore whether generative diffusion models memorize protected works that appeared in the models' training set.This can be considered as a preliminary issue to the problem of establishing copyright infringement.However, as we discuss in section 4, memorization of input content does not necessarily equate with copyright infringement.There is also active and thoughtprovoking discussion on how machine learning technologies are reshaping our understanding of copyright within the realm of law.Asay (2020) explores the question of whether AI system outputs should be subject to copyright.Additionally, Grimmelmann (2015); Lemley and Casey (2020) explore the implications of copyright law for literary machines that extract content and manage databases of information.Other works, like Bousquet et al. (2020); Vyas et al. (2023) attempt to evaluate copyright infringement in GenAI models using privacy-like notions.The contribution of these works lies in proposing a procedure to inform copyright analysis in line with the thesis of this paper.However, this approach may not scale easily, and it also falls short of providing nuanced information regarding the level of genericity, which could be crucial for resolving copyright legal disputes (Elkin-Koren et al., 2024).",
            "score": 0.43329821801022805,
            "section_title": "Related Work",
            "char_start_offset": 6728,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 684
                },
                {
                    "start": 686,
                    "end": 876
                },
                {
                    "start": 876,
                    "end": 976
                },
                {
                    "start": 976,
                    "end": 1099
                },
                {
                    "start": 1099,
                    "end": 1258
                },
                {
                    "start": 1258,
                    "end": 1352
                },
                {
                    "start": 1352,
                    "end": 1531
                },
                {
                    "start": 1531,
                    "end": 1678
                },
                {
                    "start": 1678,
                    "end": 1807
                },
                {
                    "start": 1807,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 366,
                    "matchedPaperCorpusId": "2468323"
                },
                {
                    "start": 399,
                    "end": 419,
                    "matchedPaperCorpusId": "13496699"
                },
                {
                    "start": 722,
                    "end": 743,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 745,
                    "end": 763,
                    "matchedPaperCorpusId": "249712435"
                },
                {
                    "start": 1258,
                    "end": 1269,
                    "matchedPaperCorpusId": "264375893"
                },
                {
                    "start": 1366,
                    "end": 1384,
                    "matchedPaperCorpusId": "152665774"
                },
                {
                    "start": 1386,
                    "end": 1409,
                    "matchedPaperCorpusId": "219342558"
                },
                {
                    "start": 1549,
                    "end": 1571,
                    "matchedPaperCorpusId": "225067265"
                },
                {
                    "start": 1573,
                    "end": 1591,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056976318359375
        },
        {
            "corpus_id": "273662226",
            "title": "Exploring Local Memorization in Diffusion Models via Bright Ending Attention",
            "text": "Text-to-image diffusion models like Stable Diffusion (Rombach et al., 2022) have achieved unparalleled proficiency in creating images that not only showcase exceptional fidelity and diversity but also closely correspond with the user's input textual prompts. This advancement has garnered attention from a broad spectrum of users, leading to the extensive dissemination and commercial utilization of models trained on comprehensive web-scale datasets, such as LAION (Schuhmann et al., 2022), alongside their produced images. However, this widespread usage introduces legal complexities for both the proprietors and users of these models, particularly when the training datasets encompass copyrighted content. The inherent ability of these models to memorize and replicate training data during inference raises significant concerns, potentially infringing on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The challenge is further exacerbated by the training datasets' vast size, which makes thorough human scrutiny unfeasible. Illustratively, several high-profile lawsuits (Saveri & Matthew, 2023) have been initiated against entities like Stability AI, DeviantArt, Midjourney, and Runway AI by distinguished artists. These lawsuits argue that Stable Diffusion acts as a '21st-century collage tool', remixing copyrighted works of countless artists used in its training data. \n\nIn response to these legal challenges, Carlini et al. (2023) and Somepalli et al. (2023a) proposed similarity metrics to evaluate memorization, and recent efforts (Somepalli et al., 2023b;Wen et al., 2024;Chen et al., 2024) have focused on developing strategies to detect and mitigate memorization, achieving notable success. However, these metrics and strategies adopt a global perspective, comparing entire generated images to training images. We identify a significant gap in these approaches when dealing with cases where only parts of the training image are memorized, which we refer to as local memorization, as opposed to cases where the entire training image is memorized, termed global memorization.",
            "score": 0.4331571945689951,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 1489,
                    "end": 1510,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1515,
                    "end": 1539,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1613,
                    "end": 1638,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1655,
                    "end": 1673,
                    "matchedPaperCorpusId": "258967610"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46337890625
        },
        {
            "corpus_id": "278534729",
            "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art",
            "text": "\u2022 We propose a supervised contrastive learning framework, DFA-CON, for training visual encoders to detect copyright infringement in DeepFake art. To the best of our knowledge, this is the first work that introduces a dedicated model tailored specifically for infringement detection in AI-generated artworks. \n\n\u2022 We conduct a comprehensive comparison between DFA-CON and recent general-purpose vision foundation models to assess the need for domain-specific training in the context of generative art forensics. \n\n\u2022 We release our model checkpoints and code to facilitate future research. The codebase is modular and supports arXiv:2505.08552v1 [cs.CV] 13 May 2025 \n\nintegration of any embedding model via a standardized wrapper interface, enabling rapid experimentation and reproducibility.",
            "score": 0.4331469298572676,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2401,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 789
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.223388671875
        },
        {
            "corpus_id": "259165411",
            "title": "Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?",
            "text": "(remarking on the increasing concerns over AI systems' replication of human data). See Taylor, supra note 39 (\"Whether the AI companies are infringing on the artists' copyrights . . . to not only train the AI programs but also to generate new art . . . is a complicated question that will need to be settled in the courts.\"). support the improvement of generative AI platforms and systems in the context of innovation, but not at the expense of replicating copyrighted work. Intent aside, creating trustworthy or advanced systems should not come at the cost of infringing on an individual's protected material or creating distrust in sharing innovative material. 104 ciding whether particular AI behavior should be classified as output (transformative) or training (non-transformative) is burdensome and will likely hinge on the Supreme Court's discretion. 105 enerative AI companies are aware of this and are mindful of how to manipulate this to their advantage. 106 other variable in judging [the first factor in the] fair use [analysis] is whether or not the training data and model have been created by academic researchers and nonprofits. This generally strengthens fair use defenses and startups know this. So, for example, Stability AI, the company that distributes Stable Diffusion, didn't directly collect the model's training data or train the models behind the software. Instead, it funded and coordinated this work by academics and the Stable Diffusion model is licensed by a German university. This lets Stability AI turn the model into a commercial service (DreamStudio) while keeping legal distance from its creation. 107 ability AI (\"Stability\") and Midjourney are currently in legal crosshairs as both are accused of training their respective AI imaging software programs by downloading billions of copyrighted images without licensors' permissions. 108 The class action complaint contends that Stability commercially benefited and profited from its use of the copyrighted images, and as a result, harmed the plaintiffs through the loss of potential commissions. 109 In their complaint, the plaintiffs allege that Stability's software, Stable Diffusion, creates derivative works via a mathematical software process based on the copyrighted images used in the training process.",
            "score": 0.4319437715538506,
            "section_title": "C. \"Is any of this actually legal?\" The Fair Use Defense 96",
            "char_start_offset": 35182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2079
                },
                {
                    "start": 2080,
                    "end": 2293
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1334228515625
        },
        {
            "corpus_id": "269137659",
            "title": "Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models",
            "text": "As image generative models have rapidly improved in scale and sophistication, the possibility of them mimicking artists' personal styles has been an important topic of discussion in the literature [18].Many previous works describe ways to either detect potential direct image copying in generated images, or to foil any future copying attempts by imperceptibly altering the artists' works to prevent effective training by the generative models.These include techniques like adding imperceptible watermarks to copyrighted artworks [7,8,28], and crafting \"un-learnable\" examples on which models struggle to learn the style-relevant information [24,29,30].These methods are typically computationally expensive and incur a loss in image quality, which may render these techniques impractical for many artists.Also, they do not protect artworks which have been previously uploaded to the internet without any safeguards.Others have suggested methods to mitigate this issue from the model owner's perspective -to either de-duplicate the dataset before training [4,25,26], or to remove concepts from the model after training (\"unlearning\") [3,9,13].These are also technically challenging, and require the model owner to invest significant resources which may again inhibit their practicality.Methods like [4,25,26] are also more focused on analyzing direct image copying from the training data, and thus may not be applicable to preventing style copying.None of these works tackle the problem of detecting potentially copied art styles in generated art, especially in a manner which may be relevant to legal standards of copyright infringement.According to current US legal standards [2], an artwork has to meet the \"substantial similarity\" test for it to be infringing on copyright.This similarity has to be established on analytic and holistic terms Fig. 2: Example generations from Stable Diffusion 2 when prompted to produce specific paintings by Vincent Van Gogh, along with the histogram of similarities between the generated image and corresponding real image.Even for a famous artist like Vincent Van Gogh, generative models rarely produce near-exact duplicates.However, Van Gogh's style appears consistently, even when similarity is low.[11,14].",
            "score": 0.4318331002003194,
            "section_title": "Related Works",
            "char_start_offset": 8141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 202,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 653
                },
                {
                    "start": 653,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1142
                },
                {
                    "start": 1142,
                    "end": 1285
                },
                {
                    "start": 1285,
                    "end": 1447
                },
                {
                    "start": 1447,
                    "end": 1637
                },
                {
                    "start": 1637,
                    "end": 1776
                },
                {
                    "start": 1776,
                    "end": 2060
                },
                {
                    "start": 2060,
                    "end": 2163
                },
                {
                    "start": 2163,
                    "end": 2239
                },
                {
                    "start": 2239,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 1061,
                    "end": 1064,
                    "matchedPaperCorpusId": "258987384"
                },
                {
                    "start": 1304,
                    "end": 1307,
                    "matchedPaperCorpusId": "258987384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427978515625
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "By leveraging the improbability of safe models generating infringing content, the probability of generative models doing the same is thereby substantially reduced. The copyright protection algorithm of NAF, CP-k, can filter out infringing content generated by the models with high probability, even when the input prompts are adversarially designed. \n\nIn this paper, we propose Virtually Assured Amplification Attack (VA3), a novel online attack framework, to show the vulnerability of probabilistic copyright protection. This framework induces text-to-image generative models with probabilistic copyright protection to generate infringing content. Our approach is grounded in the realization that in real-world scenarios, a malicious attacker intending to induce copyright infringement could engage in multiple interactions with the generative model via prompts. This persistent engagement poses a significant challenge to probabilistic protection methods, as it amplified the probability of producing infringing content of each generation. \n\nIn our proposed framework, the attacker functions as a conditional prompt generator, creating adversarial prompts iteratively based on previous interactions with the generative model. Our primary theoretical result, Theorem 1, suggests that the amplification attack is guaranteed to succeed with high probability, given a sufficient number of interactions with the generative model and a strictly positive lowerbound on success probability of each single engagement. \n\nRegarding practical algorithms, our work encompasses two technical innovations. Firstly, we present effective strategies to manage the exploration-exploitation dilemma in online prompt selection, thereby enhancing the stability of the attack. Secondly, we propose Anti-NAF, a theoretically motivated adversarial prompt optimization algorithm tailored for NAF copyright protection, to generate prompts fulfilling the conditions of Theorem 1. \n\nOur experimental results validate the efficacy of our proposed online attack approach under diverse scenarios. These findings underline the potential copyright infringement risk of applying probabilistic protection in practical applications of text-to-image generative models, for both providers and users.",
            "score": 0.43170792112344897,
            "section_title": "Introduction",
            "char_start_offset": 1911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 349
                },
                {
                    "start": 352,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1953
                },
                {
                    "start": 1956,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2262
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.411865234375
        },
        {
            "corpus_id": "276435095",
            "title": "Digital Watermarking Technology for AI-Generated Images: A Survey",
            "text": "The rapid advancement of AI-generated content (AIGC) has significantly improved the realism and accessibility of synthetic images. While large image generation models offer immense potential in creative industries, they also introduce serious challenges, including copyright infringement, content authentication, and the traceability of generated images. Digital watermarking has emerged as a promising approach to address these concerns by embedding imperceptible yet detectable signatures into generated images. This survey provides a comprehensive review of three core areas: (1) the evolution of image generation technologies, highlighting key milestones such as the transition from GANs to diffusion models; (2) traditional and state-of-the-art digital image watermarking algorithms, encompassing spatial domain, transform domain, and deep learning-based approaches; (3) watermarking methods specific to AIGC, including ownership authentication of AI model and diffusion model, and watermarking of AI-generated images. Additionally, we examine common performance evaluation metrics used in this field, such as watermark capacity, watermark detection accuracy, fidelity, and robustness. Finally, we discuss the unresolved issues and propose several potential directions for future research. We look forward to this paper offering valuable reference for academics in the field of AIGC watermarking and related fields.",
            "score": 0.43150757381849175,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1983642578125
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dream-booth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio.",
            "score": 0.4313345328951686,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.451171875
        },
        {
            "corpus_id": "277502009",
            "title": "Who Owns the Output? Bridging Law and Technology in LLMs Attribution",
            "text": "The DMCA provides essential mechanisms to protect the CMI of digital works, ensuring that unauthorized alterations or removal do not obscure the ownership or creation details of copyrighted content. This framework is particularly relevant for generative AI models trained on extensive datasets that may contain copyrighted works. However, the absence of uniform international copyright norms leads to varied interpretations across jurisdictions, making compliance challenging for AI developers working with large, heterogeneous datasets [32,33]. \n\nCopyright Challenges in Generative AI Generative AI models, which create content by deriving from large datasets that often include copyrighted texts, images, and other media, face significant copyright challenges. The use of this material without explicit authorization has led to numerous legal disputes. Notable cases, including Anderson v. Stability AI Ltd and Getty Images v. Stability AI, underscore the increased scrutiny of generative AI models. Courts frequently examine whether generative outputs constitute derivative works, applying fair use doctrine principles, especially when these outputs closely resemble the original content used for training [29,31]. \n\nFair use and fair dealing In the United States, the fair use doctrine serves as a primary defence for many generative AI copyright cases. Courts assess factors such as the transformative nature of use, the amount of original material used, and the potential impact on the market for the original work. Generative AI systems often argue that their outputs are transformative; however, this argument is carefully scrutinized, particularly if the outputs closely mimic or directly reflect the copyrighted works in the training data. For instance, OpenAI's ChatGPT has encountered defamation claims, raising questions about liability when AI-generated information contains false or defamatory statements about real individuals [30]. \n\nIn Europe, the concept of \"fair use\" as it exists in U.S. copyright law does not have a direct equivalent. Instead, European Union (EU) copyright law follows a \"fair dealing\" approach, which is more restrictive and allows specific exceptions and limitations on copyright use. These exceptions are narrowly defined and generally only apply in particular contexts such as research, criticism, news reporting, parody, and educational purposes. \n\nIn the EU, the InfoSoc and DSM Directivesshape the framework for copyright limitations.",
            "score": 0.4310919816026011,
            "section_title": "Legal Framework",
            "char_start_offset": 35600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 545
                },
                {
                    "start": 548,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1948
                },
                {
                    "start": 1951,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2226
                },
                {
                    "start": 2227,
                    "end": 2391
                },
                {
                    "start": 2394,
                    "end": 2481
                }
            ],
            "ref_mentions": [
                {
                    "start": 537,
                    "end": 541,
                    "matchedPaperCorpusId": "261375096"
                },
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "258515384"
                },
                {
                    "start": 1209,
                    "end": 1213,
                    "matchedPaperCorpusId": "269588252"
                },
                {
                    "start": 1213,
                    "end": 1216,
                    "matchedPaperCorpusId": "257771630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2362060546875
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "Recently, generative AI models have been extensively developed to produce a wide range of synthesized content, including text, images, audio, and code, among others. For example, advanced image generative models, such as Diffusion Models (DMs) [45], can produce highly realistic and detailed photographs and paintings. Similarly, large language models (LLMs) like ChatGPT [2] can be leveraged to compose coherent and creative text articles with arbitrary genres and storylines. We refer to these advanced models as \"Deep Generative Models\" (DGMs). However, because of the remarkable fidelity and authenticity of the generated contents from DGMs, concerns have been raised regarding the associated copyright issues. For example, the New York Times sued OpenAI and Microsoft for using copyrighted work for training chatGPT 1 . Midjourney was accused to output images copied from commercial films 2 . These copyright issues may pertain to various parties involved in the generation process. In specifics: \n\n(1) Source Data Owners. To generate high-quality contents, DGMs require training on a large amount of data, collected from various resources such as the Internet, even without the permission of the original data owner. As demonstrated in recent studies [15,16], it is likely that both DMs and LLMs can produce contents with a high coincidence to parts of the training data samples. Besides, DGMs can also be utilized to directly edit the contents or imitate the artistic styles from source images and texts. These facts raise concerns for the data owners, as DGMs can generate data that closely resembles or replicates their original data without authorization. \n\n(2) DGM Users. DGMs are also frequently utilized by DGM users and assist DGM users for creative composing. \n\nHowever, whether the DGMs users should receive copyright for their generated contents is still a complex and evolving legal and ethical issue. For example, in 2023 3 , US Copyright Office refused to register a graphic novel for an artist facilitated by Midjourney 4 (a popular AI image generation model). However, another giant generative-AI company, OpenAI, claims that the model users own the created data via the models from OpenAI, including the right to reprint, sell, and merchandise 5 . \n\n(3) DGM Providers.",
            "score": 0.4310919816026011,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1665
                },
                {
                    "start": 1668,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2270
                },
                {
                    "start": 2273,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 248,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.364990234375
        },
        {
            "corpus_id": "277667210",
            "title": "We Are All Creators: Generative AI, Collective Knowledge, and the Path Towards Human-AI Synergy",
            "text": "However, rights holders counter that if the AI output serves the same market as the original (e.g., AI-generated images competing with stock photos, AI text competing with news articles), the use is substitutive, not transformative, potentially causing market harm (the fourth fair use factor) [Browne and Shapiro, 2024]. Recent court decisions offer mixed signals. The Supreme Court's ruling in Andy Warhol Foundation v. Goldsmith emphasized that transformative purpose must be weighed against commercial use and market substitution [Ginsburg and Goldstein, 2024]. In Thomson Reuters v. Ross, a district court rejected fair use for an AI trained on legal headnotes, finding the AI's purpose (legal research) was too similar to the original work's purpose and potentially harmed the market for licensing data for AI training, though this case involved non-generative AI [Samuelson and Lemley, 2024]. The applicability of fair use to generative AI training remains highly contested and fact-dependent. \n\nThe EU AI Act includes transparency requirements, such as demanding providers publish summaries of training data, partly to help rightsholders enforce the opt-out right [Rosati and Senftleben, 2024]. However, the practical effectiveness of the opt-out and the feasibility of creating truly informative yet non-infringing summaries remain significant challenges [Samuelson and Lemley, 2024]. \n\nThe problem of attribution and compensation presents a near-insurmountable hurdle. Tracing a specific AI output (e.g., an image, a paragraph) back to the individual training data points that influenced its generation is computationally complex, likely impossible in most cases [Ginsburg and Budiardjo, 2023]. Furthermore, copyright law generally does not protect artistic style. While AI can mimic styles, this mimicry itself may not constitute infringement, just as human artists can be inspired by and adopt others' styles. \n\nLawsuits like Getty Images v. Stability AI, Andersen v. Stability AI, and Authors Guild v. OpenAI are grappling with these issues, including claims related to the removal or alteration of Copyright Management Information (CMI) under the DMCA \u00a71202 [Ginsburg and Budiardjo, 2023].",
            "score": 0.4306505120877086,
            "section_title": "Navigating the Copyright Labyrinth: Collective",
            "char_start_offset": 13762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1921
                },
                {
                    "start": 1924,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 1172,
                    "end": 1201,
                    "matchedPaperCorpusId": "274721338"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08929443359375
        },
        {
            "corpus_id": "274436785",
            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
            "text": "The diffusion model has gained significant attention due to its remarkable data generation ability in fields such as image synthesis. However, its strong memorization and replication abilities with respect to the training data also make it a prime target for copyright infringement attacks. This paper provides an in-depth analysis of the spatial similarity of replication in diffusion model and leverages this key characteristic to design a method for detecting poisoning data. By employing a joint assessment of spatial-level and feature-level information from the detected segments, we effectively identify covertly dispersed poisoned samples. Building upon detected poisoning data, we propose a novel defense method specifically targeting copyright infringement attacks by introducing a protection constraint term into the loss function to mitigate the impact of poisoning. Extensive experimental results demonstrate that our approach achieves an average F1 score of 0.709 in detecting copyright infringement backdoors, resulting in an average increase of 68.1% in First-Attack Epoch (FAE) and an average decrease of 51.4% in Copyright Infringement Rate (CIR) of the poisoned model, effectively defending against copyright infringement. Additionally, we introduce the concept of copyright feature inversion, which aids in determining copyright responsibility and expands the application scenarios of defense strategies.",
            "score": 0.42999068626136666,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.462890625
        },
        {
            "corpus_id": "257771630",
            "title": "Foundation Models and Fair Use",
            "text": "The third commonly produced category of generative AI is image generation.\n\nComplexities of fair use with images. As with code or text data, it is unlikely that verbatim generation of images would yield a successful fair use defense. And others have found that it is possible in some circumstances to extract training data from image generation foundation models (Somepalli et al., 2022;Carlini et al., 2023). As Somepalli et al. (2022) and others note, however, as foundation models for image generation train on more data, they are less likely to output content similar to the training data on average. These cases are more likely to be fair use.\n\nBut generated images, and generated art in particular, have their own complexities when it comes to fair use, with sometimes conflicting outcomes. For example, in a recent case, a video game company used the likeness of a WWE wrestler in a video game. The wrestler had tattoos that the company faithfully replicated in the game. The tattoo artist sued for infringement and a jury determined that this was not covered by fair use (Alexander v. Take small portions of an image, like the tattoo on a player's arm, to trigger copyright problems that are not guaranteed a fair use defense. Consider the following hypothetical.\n\nHypothetical 2.5: Generate Me Video-Game Assets.\n\nOne direction for generative art is creating video game assets. There are already mechanisms to generate 3D models from text (Poole et al., 2022). Consider a situation where a video game company builds a machine learning model into their system that generates art on the fly within the game to populate a virtual world dynamically. The game is a hit, but artists begin to notice that their artwork shows up in the game with only slight modifications, for example on tattoos for video game characters. Is this fair use? While their lawsuit is not guaranteed to succeed, there is still some risk for the video game company if the outcome follows Alexander v. Take-Two Interactive Software, Inc. (S.D. Ill. 2020).\n\nStyle Transfer. What about more abstract scenarios, where art is generated in different styles? There are two components to this. First, let us consider the rights of the original image that is being transformed into a different style. Relevant is a case that was recently argued before the Supreme",
            "score": 0.4293735117920764,
            "section_title": "Generated Images",
            "char_start_offset": 48067,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2220458984375
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "This subsection summarizes the strategies for protecting the copyright of DGMs for image generation. This protection is important for two major reasons. First, a powerful DGM, which necessitates extensive computational resources and well-annotated data for its creation, needs to be safeguarded from copyright infringements (such as being stolen by malicious users to offer unauthorized paid services). Second, the unregulated distribution of those models may lead to ethical concerns, including their potential misuse for generating misinformation, which necessitates techniques to identify the origin of an image. \n\nDeep Generative Model Watermarking is a common solution for model copyright protection. It involves incorporating distinct information, known as a watermark, into the models before their deployment. The embedded watermark can be retrieved from a potentially infringing model or its generated data to confirm any suspected copyright violations. \n\nTo achieve good performance in protecting the model while preserving the original generation performance, the watermarking technique should incorporate the following key properties: (a) Fidelity: the ability of the watermarking method to not significantly impact the general performance of the model (the diversity and visual quality of the generated images); (b) Integrity: the accuracy with which the watermark can be extracted; (c) Capacity: the length of message that can be effectively encoded to and extracted from the watermark; (d) Robustness: the ability of the watermark to withstand alterations to the model or perturbation on the watermarked generated images; and (e) Efficiency: the computational cost of the watermark embedding and extraction process. \n\nThe proposed DGM watermarking methods can be summarized into three categories according to the specific way they embed the watermark: \n\n\u2022 Parameter-based watermarking encodes the watermark message into the model's parameters or structural configurations; \u2022 Image-based watermarking embeds the watermark message into every image generated by the model; \n\n\u2022 Triggered-based watermarking secretly incorporates a trigger to the protected model such that an image with copyright information will be generated once the trigger is activated.",
            "score": 0.4293546657348744,
            "section_title": "Model Copyright Protection",
            "char_start_offset": 43411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1729
                },
                {
                    "start": 1732,
                    "end": 1865
                },
                {
                    "start": 1868,
                    "end": 2083
                },
                {
                    "start": 2086,
                    "end": 2266
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5205078125
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "First, the models qua models are arguably highly transformative -both because they represent the works internally in new and very different ways, and also because they are capable of generating highly transformative works as outputs. Second, the business models of the model trainer and deployer become relevant here: models that are sold or provided as paid services are clearly commercial, whereas opensource releases are not. Third, at least some models will compete with at least some of the authors represented in their training datasets on at least some generations. \n\nConsidered as a straight up-or-down question, then, the fair use status of models is a difficult one. One attractive answer for courts may be to hold that the models themselves are fair uses while holding their creators and deployers liable for infringing generations. If generation leads to direct liability, then generation is an at-your-own risk activity. This would make the operation of generation services infeasible unless the operator were able to filter the outputs for all copyrighted works. Doing so would probably require training only on vetted, licensed datasets (as Adobe's generative AI model purportedly has been). In this world, however, it would probably not be infringing to distribute a trained model as a set of weights (as Stability AI's releases have been), because Sony. The fact that this tool (which is fair use in itself) is used by others for infringing purposes would be counterbalanced by the substantial noninfringing uses, leading to immunity under Sony. This might not be an attractive business model, because it might be hard for buyers to monetize these models and because of the ease of copying and further redistributing the models, but it could at least exist. \n\nAnother response, however, might be to treat generation as creating only indirect liability for model operators. In this world, operators would be allowed to provide models as services -but they would need to respond to notices of specific infringements under a Napster-like rule. These would probably not be notices directed to specific generations by named users, which would be difficult to detect and track. Instead, they would involve copyright owners identifying copyrighted works and demanding that the model operator prevent generations that are substantially similar to those works.",
            "score": 0.42930116868634666,
            "section_title": "Id. \u00a7 512(i)(1)(A",
            "char_start_offset": 50262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2368
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.248046875
        },
        {
            "corpus_id": "270764497",
            "title": "Navigating LLM Ethics: Advancements, Challenges, and Future Directions",
            "text": "Academic work in this area addresses the complex challenges posed by these AI systems in creating content that may infringe on intellectual property rights. Scholars are examining the ethical implications of using LLMs for generating text, images, or code that closely resemble copyrighted material, raising questions about originality and ownership [170]. The focus is also on developing frameworks and guidelines to navigate copyright laws in the context of AIgenerated content, exploring how existing legal structures can accommodate the unique nature of LLM outputs [171]. Several authors discuss methods of watermarking models and outputs to protect copyright and intellectual property [172]- [174]. One paper also provides an innovative take on the subject, arguing that LLMs can be used in copyright compliance checking [175]. \n\nBoth academic research and mainstream news has recently discussed ownership and copyright surrounding LLMs and other forms of generative AI. The size and make-up of training datasets presents challenges in several legal areas, including infringement, fair use, and liability. The debate surrounding data scraping is both technically and legally complex, while content filtering and moderation at the scale at which many LLMs operate is highly resource intensive. The future of LLM copyright will likely be in constant evolution and debate, as many copyright laws were not designed with AI-generated content in mind.",
            "score": 0.428871613373869,
            "section_title": "Intellectual Property and Plagiarism in LLM",
            "char_start_offset": 41125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1451
                }
            ],
            "ref_mentions": [
                {
                    "start": 827,
                    "end": 832,
                    "matchedPaperCorpusId": "262127971"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.087890625
        },
        {
            "corpus_id": "234777751",
            "title": "Copyright in generative deep learning",
            "text": "We should also remember from Section 2 that GDL involves the creation of a probabilistic model describing data of interest, from which we obtain new works through sampling. This peculiarity leads to other critical questions. In fact, the model creation and storage, which contains the extracted probabilistic features, may infringe the copyright of works used for training. Its storage cannot be considered as transient or incidental; therefore, it is allowed only if it does not constitute a (partial) reproduction of protected works, since, in that case, there is no copyright relevant activity (Margoni, 2018).\n\nDeep learning models are usually stored as sets of numerical weights; usually, they do not fall in the category of those that are considered as a partial reproduction of a work. However, if the model is built to mimic in output the input (with some nonsubstantial changes) and it is trained over a protected work, the model would represent that work, at least partially. Moreover, even if the model is not intentionally built to mimic a protected work, it could still end up doing so to an infringing degree: it might reconstruct idiosyncrasies of input data instead of reflecting underlying trends about them (Sobel, 2017). If just trained with the goal of learning how to reproduce works, an overfitted generative model may actually be considered as a direct reproduction of the works.\n\nThese issues are not only related to the model itself, but also to the output it can generate. Protected input data are commonly used to train models to generate similar output. Then that output may infringe copyright in the preexisting work or works to which it is similar to (Sobel, 2017). In this context, the importance of adversarial training from GANs and searching for diverging from existing works as done by Elgammal et al. (2017) may tip the balance toward legality. With respect to other classic techniques, the generative part of a GAN never uses protected data; therefore, it is harder to obtain an output representing an expression of protected data. In addition, new techniques with a novelty objective, which tries to increase the distance between outputs and training data, will tend to be more transformative. This may be crucial, since prior appropriation art cases suggest that,",
            "score": 0.4285959734205257,
            "section_title": "Additional issues",
            "char_start_offset": 27080,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1225,
                    "end": 1238,
                    "matchedPaperCorpusId": "115500744"
                },
                {
                    "start": 1681,
                    "end": 1694,
                    "matchedPaperCorpusId": "115500744"
                },
                {
                    "start": 1821,
                    "end": 1843,
                    "matchedPaperCorpusId": "24986117"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3544921875
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "Under existing copyright doctrine, this should probably end the inquiry: the output is not a \"copy\" of any of those works, and therefore does not infringe on the copyright in any of them, because infringement is assessed on a work-by-work basis. \n\nIn between, there will be difficult cases where similarity requires a more case-by-case and factually intensive analysis. For example, an output might resemble a particular input only in a few recognizable aspects -e.g., a text generation copies a few phrases from an article, or an image generation uses the composition and color palette of a painting but with a different subject, or where a musical generation mashes up multiple songs. In other cases, an output will draw broadly on common elements of a particular artist's stylean illustrator's linework, or a photographer's use of light and shadow -but not be a close copy of any specific work of that artist. All of these cases raise two common doctrinal questions: the quantitative threshold of substantial similarity, and the filtration of unprotectable ideas from the expression in those works. \n\nFrom our zoomed-out perspective, however, the very factual intensity and complexity of these issues paradoxically makes them legally uninteresting. To quote Learned Hand on the idea-expression dichotomy,\"Nobody has ever been able to fix that boundary, and nobody ever can. \" 19 Whether a particular generation is substantially similar or not is ultimately a jury question requiring assessment of audiences' subjective responses to the works. Generative AI will produce cases requiring this lay assessment, and beyond that it is impossible to anticipate in advance all of the possible variations that will arise. So we assume that some generated outputs will infringe from the viewpoint of lay audiences, but that it will not be possible to perfectly predict which ones will. \n\nSubstantial similarity of models, on the other hand, is a legally interesting question. A model, as a collection of weights, is different in kind from the types of copyrightable works it was trained on. Models are not themselves human-intelligible. No viewer would say that the model has the same \"total concept and feel\" as a painting; no reader would say that it is substantially similar to a blog post; and so on.",
            "score": 0.42849749331972614,
            "section_title": "B. Substantial Similarity",
            "char_start_offset": 19837,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1878
                },
                {
                    "start": 1881,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2297
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.080078125
        },
        {
            "corpus_id": "273263196",
            "title": "An Overview of Trustworthy AI: Advances in IP Protection, Privacy-Preserving Federated Learning, Security Verification, and GAI Safety Alignment",
            "text": "As for training data IP protection, adversarial example-based methods [91], [92], [93] have emerged as major strategies to prevent copyrighted training data from being used in GAI training. These methods aim to render copyrighted data \"unrecognizable\" to GAI models, thereby preserving the IP of the original content. Additionally, responsible model builder can actively filter out copyrighted contents through machine unlearning [94], [95] at the post-GAI model training phase or by removing duplicate data [96], [97] to mitigate memorization issues during the GAI model training phase. Moreover, watermarkbased methods can also be devised to identify the source of the model-generated data. For example, DiffusionShield [98] is a watermarking method tailored for diffusion models by embedding an imperceptible, ownership-encoded watermark into copyrighted training data. The watermark, along with a decoder, is jointly optimized to ensure that the watermark can be easily extracted from the generated images, thereby facilitating the detection of any potential infringements. It is important to note that only several key works and directions in IP protection for GAI are highlighted here. Interested readers are encouraged to consult the reference [86] for a more comprehensive and in-depth coverage of this topic. Finally, due to the diversity of GAI applications and the fast-evolving and varying model architectures, both the attacks and defenses are still very nascent. Continuous adaptation and updating of the IP threat protection strategies of GAI are envisaged.",
            "score": 0.42832109532578355,
            "section_title": "B. Model IP Protections",
            "char_start_offset": 42628,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1572
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 74,
                    "matchedPaperCorpusId": "252683525"
                },
                {
                    "start": 430,
                    "end": 434,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 508,
                    "end": 512,
                    "matchedPaperCorpusId": "258987384"
                },
                {
                    "start": 722,
                    "end": 726,
                    "matchedPaperCorpusId": "269741415"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.480224609375
        },
        {
            "corpus_id": "270199864",
            "title": "Towards Black-Box Membership Inference Attack for Diffusion Models",
            "text": "Recently, there has been a surge in the popularity of generative models, with diffusion models in particular, gaining huge attention within the AI community [42,45,46]. These models have demonstrated remarkable capabilities across various tasks, including unconditional image generation [14,44], text-to-image generation [31,36,53] and image-to-image generation [37]. This surge has given rise to powerful AI art models such as DALL-E 2 [35], Stable Diffusion [36], and Imagen [38]. AI-generated art holds a promising future and is expected to have a widespread impact. \n\nEffective training of diffusion models requires high-quality data. It is thus crucial to design an algorithm that can identify whether a specific artwork has been used during the training of a model, thereby providing protection for these artworks and detecting misuse of data. This is especially important due to the rapid growth of generative models, which has raised concerns over intellectual property (IP) rights, data privacy, and the ethical implications of training on copyrighted or proprietary content without consent. As these models are increasingly deployed across industries, detecting whether a specific piece of content was used in training can help prevent unauthorized use of artistic works, protecting creators' copyrights and ownership rights. This is a classic problem in the field of machine learning, first introduced by [41] and named \"membership inference attack\". \n\nIts objective is to ascertain whether a particular data sample participated in the training phase of a target model. This approach is instrumental in probing privacy breaches and identifying illicit data utilization. Researchers primarily focus on membership inference attacks for classification models [24,26,39,52], embedding models [7,28,43], and generative models [1,10,12]. \n\nIn the domain of membership inference attacks against diffusion models, [16,51] use a whitebox approach, which assumes access to the entire diffusion model and utilizes loss and likelihood to determine whether a sample is in the training set. [6,20,47] have relaxed these requirements, eliminating the need for the entire model. They leverage the insight that samples within the training set yield more accurate noise predictions, thereby achieving high accuracy in membership inference attack tasks.",
            "score": 0.4282130335121679,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2345
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 477,
                    "end": 481,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1416,
                    "end": 1420,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1767,
                    "end": 1771,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1777,
                    "end": 1780,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1799,
                    "end": 1802,
                    "matchedPaperCorpusId": "222125006"
                },
                {
                    "start": 1805,
                    "end": 1808,
                    "matchedPaperCorpusId": "214743021"
                },
                {
                    "start": 1832,
                    "end": 1835,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 1838,
                    "end": 1841,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 2088,
                    "end": 2091,
                    "matchedPaperCorpusId": "256503774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.458984375
        },
        {
            "corpus_id": "267636709",
            "title": "Mapping the Ethics of Generative AI: A Comprehensive Scoping Review",
            "text": "Frequently discussed in the literature are violations of copyright and intellectual property rights stemming from the unauthorized collection of text or image training data 45,53,77 . Another concern relates to generative models memorizing or plagiarizing copyrighted content 103,130,147 . Additionally, there are open questions and debates around the copyright or ownership of model outputs 93 , the protection of creative prompts 53 , and the general blurring of traditional concepts of authorship 139 .",
            "score": 0.4268795056642635,
            "section_title": "Governance -Regulation.",
            "char_start_offset": 22557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 505
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 176,
                    "matchedPaperCorpusId": "259837320"
                },
                {
                    "start": 176,
                    "end": 179,
                    "matchedPaperCorpusId": "259095707"
                },
                {
                    "start": 280,
                    "end": 284,
                    "matchedPaperCorpusId": "259836898"
                },
                {
                    "start": 432,
                    "end": 434,
                    "matchedPaperCorpusId": "259095707"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2183837890625
        },
        {
            "corpus_id": "270095412",
            "title": "Going beyond Compositions, DDPMs Can Produce Zero-Shot Interpolations",
            "text": "Our research on the interpolation abilities of diffusion models holds promise for improving fairness in machine learning.By showcasing the model's capacity to generate diverse attributes covering the range of a latent factor from a subset of examples, our work can contribute to mitigating biases in training data.However, it's essential to recognize potential misuse.While our work is on low-dimensional images of insufficient quality for deception, the method could potentially be improved and exploited for deepfake generation in higher resolutions.Such concerns are inherent to the development of most novel generative modeling techniques.Importantly, our focus is not technically advancing the state of the art in image generation.Instead, we merely study the properties of diffusion models and show what is already possible with current models.\n\nThis demonstration also has concrete implications on the use of generative models as tools in creating new content.At time of writing there are ongoing lawsuits concerning the generation of copyrighted content (Marcus & Southen, 2024) and while the examples discussed in our citation are clearly infringing even if they were interpolated in the manner we discuss (which we find unlikely), the ability to interpolate and potentially extrapolate across latent factors hints at the possibility at truly novel synthesized content -which then itself would complicate discussions around the level of derivative vs. original \"work\" present in the generated content.\n\nIn no way should our work be cited as \"proof\" that any models involved in such lawsuits are not infringing.It requires further development in dataset provenance attestation and legal frameworks to make any such claims.Presumably, our findings will play a rule in developing these legal frameworks.Until then, we think it is important to err on the side of caution and not train production models on any content for which the artists or copyright holders have not explicitly consented to be included as part of generative model training.\n\nFurther, while we are excited about the idea of using interpolated factors to de-bias classifiers, e.g. by developing methods for holding all factors except the interpolated one constant in order to remove spurious correlations with e.g.gender or skin color, the model used for such debiasing might itself be flawed and/or introduce new biases.",
            "score": 0.4264765183545848,
            "section_title": "Impact Statement",
            "char_start_offset": 26407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 121,
                    "end": 314
                },
                {
                    "start": 314,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 552
                },
                {
                    "start": 552,
                    "end": 643
                },
                {
                    "start": 643,
                    "end": 736
                },
                {
                    "start": 736,
                    "end": 850
                },
                {
                    "start": 852,
                    "end": 967
                },
                {
                    "start": 967,
                    "end": 1510
                },
                {
                    "start": 1512,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1730
                },
                {
                    "start": 1730,
                    "end": 1809
                },
                {
                    "start": 1809,
                    "end": 2048
                },
                {
                    "start": 2050,
                    "end": 2287
                },
                {
                    "start": 2287,
                    "end": 2394
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2386474609375
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "This method complements precaution measures and provides robust legal proof of infringements.\u2022 We propose a new framework, CGI-DM, for authentication.Utilizing Monte Carlo sampling and PGD optimization, we employ gradient inversion based on the partial representation of a given image.The similarity between the recovered samples and original samples can serve as a robust and visible indicator for infringements.Notably, while most gradient inversion methods focus on classification models, we pioneer its application in the domain of generative models, emphasizing a new approach.\u2022 We conduct extensive experiments on the WikiArt and Dreambooth datasets to substantiate the efficacy and robustness of our approach in distinguishing training samples from those not used during training.These demonstrate our method's effectiveness in authenticating digital copyrights and thus validating infringements for both style mimicry and fabricated images.",
            "score": 0.42632605323177014,
            "section_title": "Introduction",
            "char_start_offset": 4681,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 150
                },
                {
                    "start": 150,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 413
                },
                {
                    "start": 413,
                    "end": 582
                },
                {
                    "start": 582,
                    "end": 787
                },
                {
                    "start": 787,
                    "end": 948
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2080078125
        },
        {
            "corpus_id": "270063652",
            "title": "Automatic Jailbreaking of the Text-to-Image Generative AI Systems",
            "text": "Surprisingly, we observe that current commercial T2I systems, including Midjourney [19], Copilot [18], and Gemini [29], result in copyright violations with a low block rate, 13.3%, even with such naive prompts.However, ChatGPT blocked most copyright infringements from simple prompts with an average block rate of 84%.\n\nTo see whether this censorship mechanism by ChatGPT is sufficient enough, we further propose a simple yet effective Automated Prompt Generation Pipeline (APGP) which automatically generates jailbreaking prompts by optimizing a large language model (LLM) using the self-generated QA score and keyword penalty.To bypass the word-based detection, we give a penalty when prompts contain specific keywords, such as \"Mickey Mouse,\" when describing the copyrighted content.Simultaneously, to prevent overly generic descriptions without these keywords, we introduce a self-generated QA score.This score assesses how well the answers that are generated solely from the prompt match the questions, where questions are derived from the target image.Our scoring function effectively optimizes LLM to refine prompts that are at high risk of inducing copyright infringement in T2I systems.\n\nSpecifically, given a target image, the first step is optimizing the instruction with LLM [34] for vision-language models [1,16] to generate a seed prompt that describes the target image (Figure 2, Blue).Then, a revision optimization step uses the LLM to refine the prompt to accurately depict the image that achieves a higher score (Figure 2, Green) according to the proposed scoring function (Figure 2, Yellow).In the post-processing step, we append suffix prompts, e.g., keyword-suppressing suffix, and intention added suffix, that compel the generation to rigorously evaluate the copyright infringement of T2I systems.The overall pipeline does not require any weight updates or gradient computations; it only needs inference with LLMs and T2I models, which is fast and computationally inexpensive.Furthermore, our pipeline allows non-AI specialists to easily check their IP rights on commercial T2I systems by simply providing a single IP content.",
            "score": 0.4261475308341915,
            "section_title": "Introduction",
            "char_start_offset": 1871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 210,
                    "end": 318
                },
                {
                    "start": 320,
                    "end": 628
                },
                {
                    "start": 628,
                    "end": 786
                },
                {
                    "start": 786,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 1058
                },
                {
                    "start": 1058,
                    "end": 1195
                },
                {
                    "start": 1197,
                    "end": 1401
                },
                {
                    "start": 1401,
                    "end": 1610
                },
                {
                    "start": 1610,
                    "end": 1819
                },
                {
                    "start": 1819,
                    "end": 1998
                },
                {
                    "start": 1998,
                    "end": 2148
                }
            ],
            "ref_mentions": [
                {
                    "start": 1322,
                    "end": 1325,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1014404296875
        },
        {
            "corpus_id": "271600782",
            "title": "Unlocking Fair Use in the Generative AI Supply Chain: A Systematized Literature Review",
            "text": "However, their contribution when they ensure flexibility in the model accuracy and agility remains unprotected. \n\nRecommendations: Given this gap between the values protected under copyright laws and the attributes valued by stakeholders, this paper, therefore, calls for the need to reinvent copyright law with computational solutions that go beyond the letter of the law but encapsulate the spirit of the law. In particular, given the analysis of these studied papers, as future work, researchers may consider developing tools that offer a communal environment for fostering interpersonal communication and a sense of community with other stakeholders in the ecosystem. Researchers may also consider creating tools that enhance the GenAI infrastructure by supporting creative autonomy and providing nuanced technical means of self-expression while taking into account stakeholder needs. \n\n4.2. Does the advocacy of fair use doctrine enable the progress of copyright law's objectives? \n\nThe analyzed papers also show that ML practitioners believe that data processes such as training and fine-tuning models are the most challenging aspects of their workflow. With GenAI companies like OpenAI advocating for training models as fair use and reports suggesting that researchers may run out of data to train, it is pertinent to ask whether using the fair use doctrine to train GenAI models is, in fact, producing transformative content that is enhancing or even promoting the enhancement of public knowledge of science and arts. \n\nOpenAI, in its submission to the Copyright Office fied that AI systems learn patterns from the training corpus and use those patterns to generate 'novel' expressive works which 'share some commonality with works in the corpus subject to the model learning from 'enormous number of works' . Furthermore, OpenAI argues that such infringement \"is an unlikely accidental outcome.\" However, a report by the US Congressional Research Service12 brings forth a contradicting narrative by the Getty Images lawsuit13 that alleges \"Stable Diffusion at times produces images that are highly similar to and derivative of the Getty Images.\" This report also brings attention to a paper (Somepalli et al., 2022) that found \"a significant amount of copying\" in less than 2% of the images created by Stable Diffusion where the authors claimed that their methodology \"likely underestimates the true rate\" of copying .",
            "score": 0.4256774265419048,
            "section_title": "Value derived by users from GenAI outputs:",
            "char_start_offset": 24806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 114,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2154
                },
                {
                    "start": 2155,
                    "end": 2427
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08648681640625
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "Once Disney sends a notice to the service over the infringing Elsa output, the service now has the kind of knowledge that triggered liability in Napster and must therefore take steps to prevent similar future generations. 37 So the bottom line would appear to be that a generation service is liable if and only if it fails to act after receiving notice about specific works. \n\nBut this is not the only possible analysis of generation services. Another take on the situation would be that because the model is based on and indeed incorporates copyrighted works as to which the model deployer has the necessary volition, the service is directly liable when it is further used to make infringing outputs. On this view, both the user and the service could be direct infringers. The model is like a very large archive of copyrighted works, so prompting it for a specific generation is like using SciHub to download a specific article. \n\nThere is a third possible story. Suppose a user types in \"heroic princesses\" and the model generates a picture of Elsa and Anna. Here, the user has innocently requested a generation, and it is the model that has narrowed down the enormous space of possible outputs to one that happens to be infringing. There is a colorable argument that the service is a direct infringer, like a bookstore whose shelves are stocked with a mixture of legitimate and pirated editions, but that the user is not. If so, then the user cannot do very much with the output -it is objectively an infringing reproduction and adaptation, so further reproduction and distribution will infringe, regardless of the user's knowledge -but the generation itself is not directly infringing as to the user. Nor is it indirectly infringing: the user has no financial interest (vicarious) in an infringing output, nor is there knowledge of (contributory) or intent to infringe (inducing). \n\nThe two-by-two matrix is not complete. It does not seem likely that a court would treat both service and user as indirect infringers. Doing so would violate the doctrinal requirement that there be a direct infringer for indirect liability to attach, leaving both potentially responsible parties free of liability, and allowing the act of generation to drop out of the copyright system entirely.",
            "score": 0.4243511165598238,
            "section_title": "D. Direct and Indirect Infringement",
            "char_start_offset": 34383,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2281
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10174560546875
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "In this paper, we consider a text-to-image generative model p equipped with probabilistic copyright protection. The probability that p generates infringing content with prompt x is upper-bounded. Following the typical adversarial prompt attack setting, we consider a malicious attacker who seeks to manipulate p to produce content that violates the copyright of a specific piece of target copyrighted data y C , of which the infringing contents form Y C \u2286 Y. Different from the standard paradigm, we consider an online attack scenario where the attacker is allowed to interact with p for T times within each attack trial. During the t = 1, \u2022 \u2022 \u2022 , T -th interaction, the attacker inputs a prompt x t and receives a generated sample y t \u223c p(\u2022|x t ). Besides furnishing a prompt, the attacker is prohibited from intervening directly in the generation process. This constraint is consistent with real-world scenarios where users interact with generative models as black boxes through APIs. At the end of each attack trial, the attacker will select y * \u2208 {y t } T t=1 as the final output, and the attack is regarded successful if y * \u2208 Y C . The formal objective of the attacker is to maximize the success rate of attack, i.e.,",
            "score": 0.4239944507687218,
            "section_title": "Problem Formulation",
            "char_start_offset": 9206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1223
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13720703125
        },
        {
            "corpus_id": "277955485",
            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
            "text": "While some work [15] have pointed out extensive source copying for generative models, for practical purposes, something that does not exactly replicate the source should be enough [17]. In particular, we observe that if an image has the same semantic segmentation mask as the original source data, they have a higher likelihood of being identified as copyright infringement. Conversely, a simple distortion like reflection or resizing is often enough to circumvent copyright claims [18]. \n\nFrom this observation and taking cue from conditional diffusion model [16], we propose a twostep image generation model. The first step is to create an image segmentation mask from a text prompt using a diffusion model. This model would be trained on image segmentation masks instead of complete images. There would be a greater sampling rate, so that the model can deviate from the training segmentation masks. Using the generated segmentation mask as a conditional control, we would next create the full image, and here we would use a diffusion model as used by [16] with a smaller sampling rate. As we would have a lesser likelihood of generating a similar segmentation mask, we would have a lesser likelihood of generating a copy as well. We suspect this is closer to how an animal or human brain conceptualizes an image in the mind, first by identifying the parts and then filling in the details. \n\nTo the best of our knowledge, this is the first work to address copyright concerns in diffusion model-based image generation by utilizing a two-step process involving image segmentation masks.",
            "score": 0.42363425789791104,
            "section_title": "Introduction",
            "char_start_offset": 3937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1586
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32958984375
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "However, this method is highly dependent on the specific image and model, and it lacks general reliability. Watermarking (Dogoulis et al. 2023;Epstein et al. 2023) inserts specific, unnoticeable patterns into protected images to detect copyright infringement, but further research is needed to improve its robustness. Machine unlearning (Bourtoule et al. 2020;Ginart et al. 2019;Huang et al. 2021;Gao et al. 2023;Nguyen et al. 2022) removes contributions of copyright data, aligning with the right to be forgotten, while dataset deduplication (Somepalli et al. 2022) helps reduce the risk of training sample memorization. \n\nDespite these efforts, existing copyright protection methods still have the following limitations: (1) They lack a standardized copyright metric that aligns with copyright laws and regulations, making it difficult to determine if generated images constitute copyright infringement; (2) These methods often prioritize performance on specific downstream tasks rather than focusing on general applicability, resulting in approaches that work well only on certain datasets but lack the versatility needed for broader use across diverse datasets. \n\nTo tackle these challenges, we propose a Reinforcement Learning-based Copyright Protection method (RLCP) for text-to-image diffusion models to reduce the possibility of generating copyright-infringing content. Specifically, inspired by Courts 1 in the US which employs two-part test to determine copyright violation, which contains an extrinsic test examining objective similarity in specific expressive elements, and an intrinsic test assessing subjective similarity from the perspective of a reasonable audience, we first propose copyright metric that mirror these legal standards by combining semantic and perceptual similarity. Then, we propose a novel framework that combines reinforcement learning with copyright infringement metrics. We leverage the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decisionmaking process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. \n\nOur main contributions are as follows:",
            "score": 0.4234911697023893,
            "section_title": "Introduction",
            "char_start_offset": 1955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2256
                },
                {
                    "start": 2259,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 143,
                    "matchedPaperCorpusId": "258297834"
                },
                {
                    "start": 143,
                    "end": 162,
                    "matchedPaperCorpusId": "264436550"
                },
                {
                    "start": 337,
                    "end": 360,
                    "matchedPaperCorpusId": "5613334"
                },
                {
                    "start": 360,
                    "end": 379,
                    "matchedPaperCorpusId": "257804739"
                },
                {
                    "start": 397,
                    "end": 413,
                    "matchedPaperCorpusId": "257804739"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2998046875
        },
        {
            "corpus_id": "276576186",
            "title": "Interrogating LLM design under a fair learning doctrine",
            "text": "It is not necessary because models could still infringe even if their outputs are never similar to copyrighted materials, so long as copying of copyrighted materials occurred when preprocessing training data (which is universally agreed) or potentially if the models themselves implicitly contain copies of copyrighted materials. Behavioral analysis is also not sufficient because transformer-based language models are expressive generation models which assign non-zero probability to every piece of text [37]. This makes them susceptible to adversarial prompting [61,73], and with the right prompt, they can generate any piece of text; thus the ability to extract copyrighted text from a LLM cannot be taken as per se copyright infringement on its own, to which OpenAI has claimed in their lawsuits [7]. The key legal question is not whether the outputs are substantially similar, but whether the fair use exception applies. \n\nStructuralism. The second approach focuses on the structure, or design, of a generative model, to ensure that the model is learning and memorizing only non-expressive facts and ideas from its training data. Normatively, it may be desirable for model developers to deploy models that are copyright safe by design [20], which requires developers to make careful decisions during training. These decisions can include which datasets are selected, how they were filtered, whether they were upweighted, and also the use of memorization-reduction techniques such as differential privacy [9]. \n\nThe study of memorization is an active research area [33] which has begun to understand the effects of basic design decisions on memorization [39,69]. These design decisions matter, as generative models can differ significantly in their capacity to memorize training data, presenting varying levels of copyright risk [44,58]. Training decisions can be systematically evaluated and optimized, and the machine learning literature presents many examples of rigorously studying LLM design decisions for the purposes of improving performance [45,52]. How then could we formalize a Manuscript submitted to ACM notion of \"fair learning\" [48]? This is the heart of our inquiry, where we provide an operationalization of \"fair learning\" and relate it to judicial adjudication.",
            "score": 0.4234125231973126,
            "section_title": "Behavioral and structural perspectives",
            "char_start_offset": 11133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2283
                }
            ],
            "ref_mentions": [
                {
                    "start": 568,
                    "end": 571,
                    "matchedPaperCorpusId": "201698258"
                },
                {
                    "start": 1240,
                    "end": 1244,
                    "matchedPaperCorpusId": "149765855"
                },
                {
                    "start": 1509,
                    "end": 1512,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 1658,
                    "end": 1662,
                    "matchedPaperCorpusId": "246823128"
                },
                {
                    "start": 1662,
                    "end": 1665,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 1837,
                    "end": 1840,
                    "matchedPaperCorpusId": "258515384"
                },
                {
                    "start": 2053,
                    "end": 2057,
                    "matchedPaperCorpusId": "235829052"
                },
                {
                    "start": 2057,
                    "end": 2060,
                    "matchedPaperCorpusId": "258832491"
                },
                {
                    "start": 2146,
                    "end": 2150,
                    "matchedPaperCorpusId": "219342558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2176513671875
        },
        {
            "corpus_id": "265551926",
            "title": "Tokenized Model: A Blockchain-Empowered Decentralized Model Ownership Verification Platform",
            "text": "In recent years, Artificial Intelligence Generated Content (AIGC) has experienced rapid development and demonstrated strong performance in various domains such as text, speech, and image generation. Applications like ChatGPT and Stable Diffusion have attracted considerable attention since released. Given the powerful capabilities of AIGC models, they can be employed to generate diverse types of content and generate revenue. Therefore, AIGC models hold significant commercial value in many application areas such as advertising, gaming, news, and artistic creation. Model owners who legitimately operate AIGC models stand to gain substantial financial returns. Correspondingly, the training of generative models typically requires substantial computational resources, often accessible only to large enterprises or institutions capable of bearing the costs. This circumstance increases the likelihood of model stealing. Malicious attackers can employ various techniques to Yihao Li, Yanyi Lai and Chuan Chen are with the Department of School of Computer Science, Sun-Yat sen University, Guangdong, 510275, China. Tianchi Liao and Zibin Zheng are with the Department of School of Software Engineering, Sun-Yat sen University, Guangdong, 519000, China. \n\nCorresponding author E-mail: chenchuan@mail.sysu.edu.cn steal a well-trained generative model at a low cost, exploiting it for their own profit, thereby infringing upon the rights of model owners and giving rise to copyright disputes. Hu et al. [1] have explored the feasibility of stealing generative models, providing evidence that the aforementioned concerns are not unfounded. Without a reliable model authentication mechanism, the model cannot serve as an asset for financial institutions or investors, thereby restricting the circulation and transaction of the model, making it difficult to conduct financial applications. \n\nIn order to address the issue of model copyright, model watermarking technology has become increasingly important. Model watermarking technology is a technique that embeds specific identifying information into the model. By incorporating watermarks into the model, it can be uniquely identified, and the generated content can be tracked to determine its source and the model ownership. As described in the background, the copyright issues faced by AIGC have made the need for model authentication increasingly urgent. With the widespread application of generative models and the substantial amount of content they create, the protection of the model itself as a creative entity becomes crucial.",
            "score": 0.42206425755629495,
            "section_title": "A. Background",
            "char_start_offset": 33,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1883
                },
                {
                    "start": 1886,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2271
                },
                {
                    "start": 2272,
                    "end": 2403
                },
                {
                    "start": 2404,
                    "end": 2580
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1748046875
        },
        {
            "corpus_id": "274437628",
            "title": "Negative Token Merging: Image-based Adversarial Feature Guidance",
            "text": "Adversarial feature guidance using a negative prompt has been widely explored for a range of applications [2,4,17,21,47]. While remarkable, as discussed in Sec. 1 we find that the use of a negative-prompt alone might not always be sufficient (Fig. 1, 2) or even sometimes feasible [6]. Our work thus explores a complementary modality by performing adversarial guidance directly using a reference image. \n\nToken merging [7,8] proposes to increase the throughput of existing ViT [11] models by gradually merging redundant tokens in the transformer blocks. Recent works [27,48] apply the idea of token merging for video editing in order to better maintain temporal coherence of the edited video. \n\nIn contrast, we explore cross-frame negative token merging as a mechanism for providing adversarial feature guidance. \n\nIncreasing output diversity has been explored to address mode-collapse with diffusion models [5,29,34,50]. \n\nThe reduced diversity occurs due a several factors including training-data imbalance, classifier-free guidance [21], preference-optimization finetuning [26,36] etc., and is hard to eliminate at pretraining stage. Prior works on addressing this often require costly retraining / finetuning [29]. In contrast, we propose a simple training-free approach which inherently improves semantic diversity of the output features. \n\nCopyright mitigation. The growing concern over copyright infringement by generative models, has attracted significant attention in recent literature [18,25,31,39,40,44]. \n\nA particularly pressing issue is generation of copyrighted characters by diffusion models [15,17,18,24,46]. Prior works for addressing these risks typically require expensive finetuning and unlearning [9,16,51] to remove copyrighted information from model weights. Our work (NegToMe) thus provides a simple approach for reducing visual similarity to copyrighted content in a training-free manner.",
            "score": 0.4204117715762455,
            "section_title": "Related work",
            "char_start_offset": 4142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 692
                },
                {
                    "start": 695,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 921
                },
                {
                    "start": 924,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1343
                },
                {
                    "start": 1346,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 567,
                    "end": 571,
                    "matchedPaperCorpusId": "266348383"
                },
                {
                    "start": 911,
                    "end": 914,
                    "matchedPaperCorpusId": "271704960"
                },
                {
                    "start": 914,
                    "end": 917,
                    "matchedPaperCorpusId": "258427128"
                },
                {
                    "start": 1080,
                    "end": 1083,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1213,
                    "end": 1217,
                    "matchedPaperCorpusId": "271704960"
                },
                {
                    "start": 1505,
                    "end": 1508,
                    "matchedPaperCorpusId": "86424284"
                },
                {
                    "start": 1511,
                    "end": 1514,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "268732879"
                },
                {
                    "start": 1719,
                    "end": 1722,
                    "matchedPaperCorpusId": "256416326"
                },
                {
                    "start": 1725,
                    "end": 1728,
                    "matchedPaperCorpusId": "257833863"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.293212890625
        },
        {
            "corpus_id": "266798835",
            "title": "The Study of Copyright Infringement Liability of Generative Artificial Intelligence",
            "text": "Generative AI, with its unique operating mechanism, undergoes a pre-training process where it thoroughly \"chews\" and \"digests\" the provided corpus. This process involves analyzing and understanding the structure, patterns, and nuances of the corpus, allowing the AI model to gain a deep knowledge and representation of the language present in the corpus. It learns to recognize and comprehend the various writing styles, sentence structures, and vocabulary used in the provided texts. Consequently, when generating content, the AI model does not simply replicate a specific work from the corpus. Instead, it leverages the acquired knowledge and generates original content by drawing upon the learned patterns and structures. It weaves together ideas and language in a manner that can resemble the style and characteristics of the corpus but presents a unique and novel composition. \n\nCopyright law, aligning with the traditional copyright tradition, primarily safeguards the \"external expression\" of ideas rather than the ideas themselves. This means that while the AIgenerated content may draw inspiration from the corpus, it does not infringe on the copyright of the original work as long as it does not reproduce its external expression verbatim. \n\nHowever, if the AI-generated content bears significant resemblance to a prior work, it may be considered copyright infringement according to the \"contact + substantial similarity\" rule. This rule requires that there is both direct contact with the copyrighted work and a substantial similarity between the generated content and the earlier work. In cases where the copyright holder struggles to prove direct contact with their work, the presence of obvious similarities between the generated content and the earlier work can indicate unauthorized access and utilization of the work. These similarities may include shared characteristics, themes, plot structures, or even specific phrasings that are distinctive to the prior work. The style and manner in which the content is presented can also play a role in establishing the absence of originality and unauthorized usage. \n\nWithin the context of generative AI, copyright holders often encounter challenges in proving \"contact + substantial similarity\" due to the extensive use of prior works in machine learning processes. This is because the AI system, during its pre-training phase, digests a vast amount of existing copyrighted material, blurring the direct connection between the generated content and any individual prior work [8]. To address this difficulty, judicial authorities can adopt a two-step approach.",
            "score": 0.41968122540928,
            "section_title": "Infringement of content generated by artificial intelligence that is highly similar to the prior work",
            "char_start_offset": 15126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2124
                },
                {
                    "start": 2127,
                    "end": 2325
                },
                {
                    "start": 2326,
                    "end": 2539
                },
                {
                    "start": 2540,
                    "end": 2619
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2037353515625
        },
        {
            "corpus_id": "273791614",
            "title": "A Dual-Module System for Copyright-Free Image Recommendation and Infringement Detection in Educational Materials",
            "text": "Images are extensively utilized in educational materials due to their efficacy in conveying complex concepts. However, unauthorized use of images frequently results in legal issues related to copyright infringement. To mitigate this problem, we introduce a dual-module system specifically designed for educators. The first module, a copyright infringement detection system, employs deep learning techniques to verify the copyright status of images. It utilizes a Convolutional Variational Autoencoder (CVAE) model to extract significant features from copyrighted images and compares them against user-provided images. If infringement is detected, the second module, an image retrieval system, recommends alternative copyright-free images using a Vision Transformer (ViT)-based hashing model. Evaluation on benchmark datasets demonstrates the system\u2019s effectiveness, achieving a mean Average Precision (mAP) of 0.812 on the Flickr25k dataset. Additionally, a user study involving 65 teachers indicates high satisfaction levels, particularly in addressing copyright concerns and ease of use. Our system significantly aids educators in creating educational materials that comply with copyright regulations.",
            "score": 0.41878351526657287,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.097900390625
        },
        {
            "corpus_id": "259501495",
            "title": "Measuring the Success of Diffusion Models at Imitating Human Artists",
            "text": "Modern diffusion models have set the state-of-the-art in AI image generation. Their success is due, in part, to training on Internet-scale data which often includes copyrighted work. This prompts questions about the extent to which these models learn from, imitate, or copy the work of human artists. This work suggests that tying copyright liability to the capabilities of the model may be useful given the evolving ecosystem of generative models. Specifically, much of the legal analysis of copyright and generative systems focuses on the use of protected data for training. As a result, the connections between data, training, and the system are often obscured. In our approach, we consider simple image classification techniques to measure a model's ability to imitate specific artists. Specifically, we use Contrastive Language-Image Pretrained (CLIP) encoders to classify images in a zero-shot fashion. Our process first prompts a model to imitate a specific artist. Then, we test whether CLIP can be used to reclassify the artist (or the artist's work) from the imitation. If these tests match the imitation back to the original artist, this suggests the model can imitate that artist's expression. Our approach is simple and quantitative. Furthermore, it uses standard techniques and does not require additional training. We demonstrate our approach with an audit of Stable Diffusion's capacity to imitate 70 professional digital artists with copyrighted work online. When Stable Diffusion is prompted to imitate an artist from this set, we find that the artist can be identified from the imitation with an average accuracy of 81.0%. Finally, we also show that a sample of the artist's work can be matched to these imitation images with a high degree of statistical reliability. Overall, these results suggest that Stable Diffusion is broadly successful at imitating individual human artists.",
            "score": 0.4185848365485682,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41162109375
        },
        {
            "corpus_id": "257038157",
            "title": "Prompt Stealing Attacks Against Text-to-Image Generation Models",
            "text": "Legal Debates / Copyright Laws Concerning Prompts. The legal environment for AI-generated content (AIGC) is quickly changing, with various legal authorities worldwide working to adapt copyright laws for the digital era. In 2023, the US Copyright Office opened a public comment period from August to October to address complex issues related to AI and copyright [41]. Currently, copyright ownership in AIGC remains ambiguous due to unclear data collection regulation, the need for equitable benefit distribution, inconsistent global legal views on AI copyright, and challenges in tracing all original works used in AI training [38]. Compared to the complex copyright landscape of generated images, user-curated prompts involve significant human efforts and thereby align more closely with traditional copyright concepts that copyright laws protect works created and fixed by humans [37,42]. Prompt marketplaces like Prompt-Base and PromptSea also explicitly state that the prompts on their platforms are owned by the individuals who created them [49,50]. Therefore, prompt stealing attacks infringe on intellectual property rights. Societal Impacts. This work has three important implications for the AIGC participants, prompt engineers, and marketplace owners. First, our work demonstrates the feasibility of prompt stealing attacks. Given the image showcased in the prompt marketplace, an adversary is capable of inferring the selling prompt without payment quickly. The extremely short attack time makes the threat severity of prompt stealing attacks more than just stealing a certain prompt, but represents a new data breach channel to prompt marketplaces. For example, an adversary can perform prompt stealing attacks to quickly steal thousands of prompts on marketplaces and sell them on underground forums or a competitive marketplace. This data breach incident causes substantial financial losses to the victim marketplaces and jeopardizes their business models, which has also been seriously discussed in PromptSea's white paper [50]. Second, we bring insights on how to mitigate prompt stealing attacks, i.e., by introducing unperceived perturbations on the showcased images. We argue that there is an urgency to propose effective and flexible defenses against prompt stealing attacks. The proposed defense method PromptShield can serve as a foundational baseline for this direction. Third, we contribute by collecting Lexica-Dataset, a dataset with 61,467 prompt-image pairs and categorized modifiers.",
            "score": 0.41768283152534535,
            "section_title": "Discussion",
            "char_start_offset": 54049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2184
                },
                {
                    "start": 2185,
                    "end": 2294
                },
                {
                    "start": 2295,
                    "end": 2392
                },
                {
                    "start": 2393,
                    "end": 2511
                }
            ],
            "ref_mentions": [
                {
                    "start": 626,
                    "end": 630,
                    "matchedPaperCorpusId": "257280234"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0975341796875
        },
        {
            "corpus_id": "261076348",
            "title": "How to Protect Copyright Data in Optimization of Large Language Models?",
            "text": "Now we provide a boundary that illustrates the efficacy of Copyright Regression in safeguarding copyrighted data, while also addressing the criteria outlined in Definition 1, which serves as our definition of copyright protection in this paper. \n\nWe set `(x) in Definition 4 as a `2 metric for measuring parameter x on learning data A. Now we present our result to confirm that training using our Copyright Regression method can ensure that the model's outputs do not infringe copyright. Specifically, we can assert that the trained model L is protected against copyright infringement with a threshold of \u2327 c based on Theorem 16 below. Theorem 16. Let x \u21e4 be denoted the optimal parameter on Copyright Regression. We define `(x) as Definition 4, denote `(x) as the original train objective of Softmax Regression. \n\nDenote \u270f 2 2 (0, 0.1) a scalar. Denote \u2327 \n\nNow we have provided evidence of the copyright protection achieved through training under the Copyright Regression objective. This method has been rigorously proven and offers complete control over copyright infringement. However, concerns may arise regarding the potential impact of the Copyright Regression approach on the model's overall performance, particularly when copyright data includes high-quality novels and images that contribute significantly to the model's performance. In fact, language models cannot remember all their training data. Their training loss has a considered range instead of equal to 0. Based on this, we only need to let model's performance on copyrighted data be different from model's performance on other data, even if this difference is very small, then we can ascertain whether the model has access to these copyright data during output generation and intentionally avoids outputting them. The difference, namely \u2327 , can be easily controlled by adjusting the value of c and n 1 /n, we will continue to explain that why we say this in Section 7.",
            "score": 0.4175882354790474,
            "section_title": "L is \u2327 c -Copyright-Protected",
            "char_start_offset": 24624,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 247,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1938
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07147216796875
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "In the context of copyright, a backdoor attack against diffusion models is a type of security attack designed to induce diffusion models to reproduce copyright-infringing outputs by manipulating the clean training dataset. We denote the the clean dataset as D train , the poisoning data as D, and the poisoned dataset as D train = D train \u222a D. Subsequently, a compromised model, M , is obtained after training on D train . The attacker's objective is to make the diffusion model to generate artwork M (y t ), with substantial similarity to the copyrighted image x t when given a specific prompt y t . The substantial similarity is measured using a evaluator denoted as F(\u2022, \u2022) : X \u00d7 X \u2192 R. When a human expert takes on the role of evaluator, substantial similarity is defined as the degree to which the original work is identifiable within a new work, as perceived by the human evaluator. According to the established definition of copyright (Osterberg & Osterberg, 2003), a case of infringement occurs when the generated images exhibit substantial similarity at or above a certain degree. Thus, the copyright infringement is formulated as a satisfiability problem, as depicted in Equation ( 1): \n\nHere, \u03b4 and \u03c4 denote the constraints that identify generated images as violating copyright laws and ensure the separation of poisoning data from being copyright-infringing. The goal of the attacker is to identify one feasible set in the space of possible poisoning data, which is denoted as D \u22c6 . \n\nTo ensure the stealth of poisoning data, the similarity of the closest image in the poisoning data to the copyrighted image should be lower than that of the closest image in the clean dataset. The disparity in similarity is quantified by a margin, denoted as \u03b3. Then, we have \u03c4 = max x\u2208Dtrain F(x, x t ) \u2212 \u03b3, where \u03b3 \u2265 0. \n\nSubstantial Similarity Metric. Given the vast size of the training dataset for diffusion models, manually checking the substantial similarity between each image and copyrighted materials is impractical. This necessitates the implementation of an automated detector.",
            "score": 0.41724075768712116,
            "section_title": "Problem Formulation",
            "char_start_offset": 12940,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1494
                },
                {
                    "start": 1497,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2086
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56298828125
        },
        {
            "corpus_id": "259836898",
            "title": "The Ethical Implications of Generative Audio Models: A Systematic Literature Review",
            "text": "weight music generation model to generate instrumental music. In analyzing their output they found that their model was sensitive to Western music theory in that it \"it maintains the configuration of the circle of fifths; distinguishes major and minor keys from interval vectors, and manifests meaningful structures between music phases\" [68]  \u2020 . Machine learning models often perpetuate biases in the training data, and generative models are no different. It is important to be aware of the composition of the training data to understand what biases could be perpetuated. 4.2.4 Copyright Infringement. Perhaps one of the most important considerations of generative music models-both ethically and potentially legally-was only discussed by two papers in the entire corpus: copyright infringement. There are many legality questions surrounding the copyright of AI generated content. At least three lawsuits in early 2023 are currently discussing whether models trained on publicly available works constitute copyright infringement [31]. Research in the text and vision domain is even geared toward specifically identifying to what degree generative models are memorizing training data [11,39,56] or are producing outputs with \"substantial similarity\" to items in the training set [62]. However, in this corpus of generative audio models, only two papers discussed the potential for copyright infringement [2, 20]  \u2020 . Esling et al. focused their research on maximizing novelty in the music generation system in order to subvert the potential for copyright issues and increase creativity in their generation [20]  \u2020 . Agostinelli et al. \"conducted a thorough study of memorization, adapting and extending a methodology used in the context of text-based LLMs\" in order to determine the degree to which their model memorized the training dataset and understand the potential for copyright infringement [2]  \u2020 . Of the remaining 75 papers discussing generative music models (97%), not one discussed the potential for copyright infringement or training data memorization. 4.2.5 Cultural Appropriation. Generative audio models sometimes train on incomprehensible amounts of training data, and it follows that some of this training data comes from cultures outside the creator of the algorithm or users of the model.",
            "score": 0.41677257400234935,
            "section_title": "Predominance of Western Bias. Zhao et al. proposed a light-",
            "char_start_offset": 28668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2309
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05780029296875
        },
        {
            "corpus_id": "258865475",
            "title": "Can Copyright be Reduced to Privacy?",
            "text": "There is a growing concern that generative AI models will generate outputs closely resembling the copyrighted materials for which they are trained. This worry has intensified as the quality and complexity of generative models have immensely improved, and the availability of extensive datasets containing copyrighted material has expanded. Researchers are actively exploring strategies to mitigate the risk of generating infringing samples, with a recent line of work suggesting to employ techniques such as differential privacy and other forms of algorithmic stability to provide guarantees on the lack of infringing copying. In this work, we examine whether such algorithmic stability techniques are suitable to ensure the responsible use of generative models without inadvertently violating copyright laws. We argue that while these techniques aim to verify the presence of identifiable information in datasets, thus being privacy-oriented, copyright law aims to promote the use of original works for the benefit of society as a whole, provided that no unlicensed use of protected expression occurred. These fundamental differences between privacy and copyright must not be overlooked. In particular, we demonstrate that while algorithmic stability may be perceived as a practical tool to detect copying, such copying does not necessarily constitute copyright infringement. Therefore, if adopted as a standard for detecting an establishing copyright infringement, algorithmic stability may undermine the intended objectives of copyright law.",
            "score": 0.4165591982721836,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44140625
        },
        {
            "corpus_id": "264591425",
            "title": "Did the Neurons Read your Book? Document-level Membership Inference for Large Language Models",
            "text": "Content creators, such as authors or artists, have raised concerns about the inclusion of their work in the training data of generative AI, including LLMs but also multi-modal models such as DALLE-2 [55] and Stable Diffusion [57].These concerns have led to multiple lawsuits against technology companies who have acknowledged the use of copyrighted material for training, without the consent of the creators.\n\nFor instance, Stability AI is currently defending against two lawsuits, one filed by Getty Images [23] and one filed as a class action lawsuit by multiple creators [3], both of which argue the use of copyright-protected content to train Stable Diffusion.Further, US comedian Sarah Silverman and other authors have filed lawsuits against Meta [32], claiming Meta has infringed their copyrights by training LLaMA on pirated copies of their content.The US Authors Guild also published an open letter, signed by more than 15,000 authors, calling upon AI industry leaders to protect their content [25].Since then, other content creators have filed lawsuits against LLM developers claiming copyright infringement, including the New York Times [46] and Universal Music [20].\n\nNotably, since these lawsuits and public concerns have emerged, the original data source for the Books3 dataset has been removed3 and technology companies tend to not disclose details on the dataset used to train the latest language models any longer [31,48,69].\n\nRecognizing these ongoing court cases, Samuelson [59] articulates the challenges between copyright laws and generative AI.In particular, they mention that the court will need to decide whether the inclusion of in-copyright works in the training data of AI models falls under fair use, in which case it would not be copyright-protected.While it is still uncertain how copyright applies to generative AI, it is clear that content creators are concerned [25] while the technology will continue to evolve rapidly without necessarily taking these concerns into account [8,9].",
            "score": 0.4165073124005278,
            "section_title": "Copyright and generative AI",
            "char_start_offset": 11163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 408
                },
                {
                    "start": 410,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 856
                },
                {
                    "start": 856,
                    "end": 1007
                },
                {
                    "start": 1007,
                    "end": 1177
                },
                {
                    "start": 1179,
                    "end": 1441
                },
                {
                    "start": 1443,
                    "end": 1565
                },
                {
                    "start": 1565,
                    "end": 1778
                },
                {
                    "start": 1778,
                    "end": 2013
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0638427734375
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "To evaluate the effectiveness of SilentBadDiffusion, we measured the average Copyright Infringement Rates and the average First-Attack Epoch for attacks on Stable Diffusion v1.4 at different poisoning ratios (= #Poisoning data #Poisoning data + #Clean data ), specifically at 5%, 10%, and 15%. The CIR were measured post-fine-tuning, and both the average CIR and average FAE were computed over T = 20 independent attacks. Each attack targeted a copyright image randomly selected from the Pokemon or Midjourney datasets. For the attack on Midjourney images, the LAION data is used as the clean dataset in this experiment. The results are presented in Table 2. With the increase in poisoning ratios, the CIR increases, and the FAE decreases in both cases. Additionally, we present the visualization of images produced by model training over the cleaning and poisoned dataset under the trigger prompt in Figure 3.",
            "score": 0.4164234611115468,
            "section_title": "Backdoor Attack Effectiveness",
            "char_start_offset": 25414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 910
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.060302734375
        },
        {
            "corpus_id": "260155285",
            "title": "Not with my name! Inferring artists' names of input strings employed by Diffusion Models",
            "text": "In this paper, a novel approach to estimate the probability of an artist's name being used in the input prompt of an image generated by a diffusion model, specifically DALL-E 2, was presented. Our approach aimed to address the concern of potential intellectual property infringement in the context of image generation, as the usage of an artist's name in the input string might imply that the diffusion model has learned from some or all of the artist's work, potentially violating their copyright. \n\nThis work employed metric learning for classification of an extremely limited number of authors but in future more sophisticated similarity measures, larger and more diverse datasets, and additional techniques to refine the estimation of the input string's content could be explored. Additionally, investigating the ethical implications and possible solutions to the challenges posed by AI-generated content in relation to copyright and intellectual property protection should be a priority for the research community.",
            "score": 0.41529440714986204,
            "section_title": "Conclusion and future works",
            "char_start_offset": 18875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1019
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.228271484375
        },
        {
            "corpus_id": "265066820",
            "title": "Robust Retraining-free GAN Fingerprinting via Personalized Normalization",
            "text": "Synthetic image generation has made significant progress in recent years and generative models are now widely used in commercial applications. These models are provided to commercial users as production tools or for selling services. Protecting the Intellectual Property Rights (IPR) of model owners has become a pressing issue to avoid potential copyright infringements, such as unauthorized duplication or model theft, when these models are delivered to malicious users. Deep Neural Network (DNN) watermarking has been proposed as a solution to protect the IPR associated with DNN models [1]. Most DNN watermarking methods focus on the protection of discriminative models, namely, networks developed for classification tasks, and less attention is paid to generative models. Yet, some methods for the watermarking of generative models have started appearing recently. Given the large entropy of the output of generative models, the watermark can be directly extracted from the output produced by the model, thus permitting to verification of the watermark in a so-called box-free setting. In this way, it is possible to determine the source of images produced by generative models and associate any image to the generative model that produced it [2]. \n\nWith the exception of a few scattered works [3], the existing approaches for the watermarking of generative models, notably GANs, are designed for ownership verification, aiming at making it possible to retrieve the model authorship information from the generated images. These methods embed a fixed watermark, linking the model Zhihua Xia is the corresponding author. to the owner, and require retraining or finetuning if different a watermark has to be embedded in the model. \n\nIn this paper, we focus on a different scenario, hereafter referred to as GAN fingerprinting, illustrated in Fig. 1. In this scenario, the model distributor, simply referred to as the model owner, releases distinct watermarked model instances to different users, in such a way that the user-specific fingerprint can be recovered from the images produced by these models for copyright authentication and to trace back to the guilty user in case of a violation of the license agreements (traitor tracing).",
            "score": 0.4152337650137709,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 590,
                    "end": 593,
                    "matchedPaperCorpusId": "235494966"
                },
                {
                    "start": 1248,
                    "end": 1251,
                    "matchedPaperCorpusId": "235692919"
                },
                {
                    "start": 1299,
                    "end": 1302,
                    "matchedPaperCorpusId": "229212743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.288818359375
        },
        {
            "corpus_id": "269982010",
            "title": "How to Trace Latent Generative Model Generated Images without Artificial Watermark?",
            "text": "Recently, latent generative models (Rombach et al., 2022) have attracted significant attention and showcased outstanding capabilities in generating a wide range of high-resolution images with surprising quality.Many state-of-the-art image generation models belong to latent generative models, such as DALL-E 3 (Betker et al., 2023) from OpenAI, Parti (Yu et al., 2022a) from Google, and Stable Diffusion (Rombach et al., 2022) from Stability AI.These models allow for achieving a near-optimal point between reducing computing complexity and preserving visual details, greatly boosting the efficiency in both training and generation phase.Among them, Stable Diffusion is the most widely-used, which has already gained more than 10 million users1 .\n\nAs latent generative models become more prevalent, the issues surrounding their potential for misuse are becoming increasingly important (Schramowski et al., 2023;Wang et al., 2023c;Pan et al., 2024;Liu et al., 2024;Wen et al., 2023b;Chen et al., 2023).For example, malicious users may use the latent generative models to generate and distribute images containing inappropriate concepts such as \"sexual\", \"drug use\", \"weapons\", and \"child abuse\" (Schramowski et al., 2023).AI-powered plagiarism (Francke & Bennett, 2019) and IP (intellectual property) infringement problem surrounding the latent generative models are also important issues.For instance, users may synthesize high-quality images using one company's or open-sourced latent generative models and then dishonestly present them as their own original artwork (e.g., photographs and paintings) to gain recognition and reputation, which is harmful to society and may cause a series of IP problems.Consequently, it's crucial to be able to trace the source of images generated by latent generative models, i.e., determining if a certain image was produced by a specific model.\n\nThere are several existing methods for tracing the source of the images.",
            "score": 0.4146505635841167,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 746
                },
                {
                    "start": 748,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1221
                },
                {
                    "start": 1221,
                    "end": 1388
                },
                {
                    "start": 1388,
                    "end": 1704
                },
                {
                    "start": 1704,
                    "end": 1881
                },
                {
                    "start": 1883,
                    "end": 1955
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 57,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 310,
                    "end": 331,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 404,
                    "end": 426,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 885,
                    "end": 911,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 982,
                    "end": 1000,
                    "matchedPaperCorpusId": "253254809"
                },
                {
                    "start": 1194,
                    "end": 1220,
                    "matchedPaperCorpusId": "253420366"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.165771484375
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "We have seen that numerous stages of the generative-AI supply chain involve prima facie copyright infringement. This means that copyright's all-purpose defense, fair use, plays a major role in making generative AI possible at all. 50 ther authors have discussed the fair use issues in greater detail, so we will focus on only a few salient points. 51 irst, without generation, there is a strong argument that both assembling training datasets and training AI models is fair use for most applications. The best explanation of this conclusion is Matthew Sag's concept of nonexpressive uses-bulk uses of copyrighted works that do not involve the consumption of expression. 52 Examples include digital stylometry, sentiment analysis, and plagiarism detection. These uses do not involve human experience of expression that lies at the heart of the copyright system, and they do not compete with authors. Training a model for these purposes may implicate other important societal interests, but they are not typically described as copyright interests. The reasoning here is essentially backwards-looking.",
            "score": 0.4146254156541228,
            "section_title": "F. Fair Use",
            "char_start_offset": 42544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1098
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09124755859375
        },
        {
            "corpus_id": "277955485",
            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
            "text": "When comparing generated images to original photos, we have used a statistic called the Frechet Inception Distance (FID) score [23] to evaluate the accuracy and realism of the generated images. Calculate the difference, as extracted by a trained inception network, between the feature distributions of the generated and actual images. A quantitative indicator of how well a generative model generates realistic and accurate images can be determined with FID. A lower FID score indicates that the distribution of features in the generated images is more similar to the distribution of features in the real images. The FID score ranges are interpreted as follows: \n\n\u2022 FID \u2248 0: Generated images are extremely similar to the reference image, strongly suggesting potential copyright infringement. \n\n\u2022 FID \u2248 10-30: Moderate similarity; the generated images might be influenced by the original images but are not identical. \n\n\u2022 FID > 30: Low similarity, suggesting that the generated images are sufficiently different from the reference images. \n\n\u2022 FID > 50: Very low similarity; the generated images are highly distinct from the reference images, making copyright infringement concerns less likely.",
            "score": 0.4145757301987947,
            "section_title": "FID:",
            "char_start_offset": 15730,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1037
                },
                {
                    "start": 1040,
                    "end": 1192
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1417236328125
        },
        {
            "corpus_id": "275757882",
            "title": "Governance of Generative AI in Creative Work: Consent, Credit, Compensation, and Beyond",
            "text": "As a result of generative AI relying on large amounts of data to train its models and generate creative content, many copyright issues have emerged, such as those surrounding authorship, fair use, licensing, and liability, among others [49,86]. This is mainly because generative AI can mimic both the content and style of creative output; content referring to \"the objects and concepts in a piece of art\", and style referring to how the art is expressed [90]. \n\nCreative workers around the world have spoken up about the harms of generative AI on their work, mentioning issues such as damage to their professional reputation, economic losses, plagiarism, copyright issues, and an overall decrease in creative jobs [44]. \n\nThe creative community recognizes the potential harms generative AI can have on their work and has started taking action. For example, the SAG-AFTRA and Writers Guild of America went on strike in 2023 to protest against issues relating to intellectual property and usage of AI [81,87]. \n\nGenerative AI can only mimic the technical aspects of producing creative content, but these re-creations are not based on artistic training or traditions, which are developed over many years of practice to create a style unique to each creative professional [68]. This mimicry is especially concerning given that when AI-generated works are framed as being anthropomorphic, users perceive the AI as being more \"human\", which changes how users allocate responsibility [26]. Increasingly, anthropomorphic descriptions are used to describe AI [6,44]. \n\nCurrently, there are no formal protections for creative workers, and therefore creative workers must take action by themselves if they want to prevent their work from being potentially trained on. Watermarking, which is where content is marked to convey information, such as its provenance and authenticity [55], has been suggested as another way of protecting creative work. However, empirical research has shown that watermarking is not very effective, and researchers suggest new methods be used [45]. Shan et al. [73] created Glaze, a system that, when applied on training data such as artwork shared online, misleads generative AI models and prevents the models from mimicking the artists' style. The researchers found that Glaze was effective at preventing style mimicry [73].",
            "score": 0.41450130713776434,
            "section_title": "Impacts of Generative AI on Creative Workers",
            "char_start_offset": 9063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1557
                },
                {
                    "start": 1560,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2261
                },
                {
                    "start": 2262,
                    "end": 2342
                }
            ],
            "ref_mentions": [
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "258236199"
                },
                {
                    "start": 714,
                    "end": 718,
                    "matchedPaperCorpusId": "261279983"
                },
                {
                    "start": 1477,
                    "end": 1481,
                    "matchedPaperCorpusId": "221671894"
                },
                {
                    "start": 1553,
                    "end": 1556,
                    "matchedPaperCorpusId": "261279983"
                },
                {
                    "start": 2059,
                    "end": 2063,
                    "matchedPaperCorpusId": "258557682"
                },
                {
                    "start": 2077,
                    "end": 2081,
                    "matchedPaperCorpusId": "256662278"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14892578125
        },
        {
            "corpus_id": "270357944",
            "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI",
            "text": "For instance, we find that the contents produced by these AI models, such as images and videos, can inadvertently include characters that bear a striking resemblance to IP-protected characters owned by other companies.In Figure 1 and Figure 2, we demonstrate examples of the IP infringement of the generated content of the state-of-the-art visual generative AI models, i.e., DALL-E 3 and Midjourney.As can be clearly seen, all models generates an image which is highly similar to the character \"Spider-Man\" when using the prompt \"Generate an image of the Spider-Man\".Furthermore, the model can even generate the \"Spider-Man\" images without directly mentioning the character's name in the prompt.This is particularly problematic when the visual generation involves well-known characters belonging to large companies in the movie, gaming, and entertainment industries, such as Sony, Marvel, and Nintendo.The increasing sophistication of these visual generative AI systems might raise complex legal and ethical questions around the boundaries of fair use, derivative works, and the appropriate ownership of the generated content.To investigate the IP infringement issues of the state-of-the-art visual generative AIs on the IP protected characters owned by large companies in the entertainment industries, we design a straightforward method to generate prompts that can effectively trigger these models to activate models' IP infringement issues on specific target character, even without directly stating the character's name.It works in a black-box setting where the weight parameters and the internal outputs of the models are not available.We employed both prompts that explicitly included the name of the IP-protected character as well as prompts that described the target character without naming them, and study the IP infringement behaviors of the models under these prompts.To evaluate the extent of IP infringement issues in visual generative AI models, we create a benchmark consisting of six representative IP protected characters owned by large companies (e.g., Sony, Marvel, Nintendo, and DC Entertainment).We then evaluate the extent of intellectual property infringement by these AI models on this benchmark.Our experiments demonstrate that the IP infringement issues are widely existing in both open-source and commercial closed source models.",
            "score": 0.41450130713776434,
            "section_title": "Introduction",
            "char_start_offset": 2163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 218,
                    "end": 399
                },
                {
                    "start": 399,
                    "end": 567
                },
                {
                    "start": 567,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 1126
                },
                {
                    "start": 1126,
                    "end": 1524
                },
                {
                    "start": 1524,
                    "end": 1641
                },
                {
                    "start": 1641,
                    "end": 1880
                },
                {
                    "start": 1880,
                    "end": 2118
                },
                {
                    "start": 2118,
                    "end": 2221
                },
                {
                    "start": 2221,
                    "end": 2357
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2109375
        },
        {
            "corpus_id": "273228819",
            "title": "TeaserGen: Generating Teasers for Long Documentaries",
            "text": "We note that, as a generative model trained on copyrighted material, our proposed system has the potential to generate samples that could lead to copyright infringement. Moreover, as discussed in Section 6, the proposed system can make mistakes in matching names and faces, and may pair irrelevant visuals to the generated narrations, resulting in risks of factual inaccuracy.",
            "score": 0.41450130713776434,
            "section_title": "ETHICS STATEMENT",
            "char_start_offset": 30856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 376
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1258544921875
        },
        {
            "corpus_id": "257280234",
            "title": "A Pathway Towards Responsible AI Generated Content",
            "text": "There is a risk of copyright infringement with the generated content if it copies existing works, whether intentionally or not, raising legal questions about IP infringement. \n\nIn November 2022, Matthew Butterick filed a class action lawsuit against Microsoft's subsidiary GitHub, accusing that their product Copilot, a code-generating service, violated copyright law [Butterick, 2022]. The lawsuit centers around Copilot's illegal use of licensed code sections from the internet without attribution. Texas A&M professor Tim Davis also provided examples of his code being copied verbatim by Copilot [Jennings, 2022]. Although Microsoft and OpenAI have acknowledged that Copilot is trained on open-source software in public GitHub repositories, Microsoft claims that the output of Copilot is merely a series of code \"suggestions\" and does not claim any rights in these suggestions. Microsoft also does not make any guarantees regarding the correctness, security, or copyright of the generated code. \n\nIn addition to code generation, text-to-image generative models like Stable Diffusion also faced accusations of infringing on the creative work of artists, as they are trained on billions of images from the Internet without the approval of the IP holders, which some argue is a violation of their rights. This is evident in Stable Diffusion, which has generated images with the Getty Images' watermark on them [Vincent, 2023]. Somepalli et al. [Somepalli et al., 2022] also presented evidence suggesting that Stable Diffusion copies from the data on which it was trained on. While Stable Diffusion disclaims any ownership of generated images and allows users to use them freely as long as the image content is legal and non-harmful, this freedom raises questions about ownership ethics.",
            "score": 0.41387170840669163,
            "section_title": "IP infringement examples",
            "char_start_offset": 17846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 177,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1786
                }
            ],
            "ref_mentions": [
                {
                    "start": 368,
                    "end": 385,
                    "matchedPaperCorpusId": "252780163"
                },
                {
                    "start": 1427,
                    "end": 1468,
                    "matchedPaperCorpusId": "262217524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.306884765625
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "In this section, we outline two methods for generating positive samples that infringe on copyright protection laws. For this purpose, we selected three widely-used generative AI models: Stable Diffusion XL (generated 40% of the image samples) (Podell et al., 2023), Ideogram (generated 40% of the image samples) (ideogramAI), and DALL-E (generated 20% of the image samples) (Betker et al., 2023). \n\nGenerate IP Characters with Direct Prompt. \n\nThe simplest approach to generating positive samples involves using direct prompts with generative models, such as \"Generate an image of <a char-acter>.\" This method typically produces images that closely resemble the IP-protected characters, as illustrated in Fig. 2. Using this technique, we generated 40 images for each character class. \n\nGenerate with Descriptive Prompt. Rewriting direct prompts that reference copyright-protected content into longer, more descriptive prompts, as explored by Wang et al. (2024) and He et al. (2024), can sometimes reduce the risk of IP infringement. However, this approach is not entirely effective in preventing outputs that closely resemble copyrighted characters (He et al., 2024), as rewritten prompts often retain a high degree of similarity to the original IP-associated names. \n\nTo enhance the diversity of our dataset, we first generate images using descriptive prompts and then apply a human evaluation process to filter out most positive samples, as detailed in Section 3.2. We use GPT-4o (GPT4V) here as its exceptional text generation capabilities. We construct descript prompt with the following guidance to GPT-4o: \n\n\u2022 Creating a prompt that describes a character similar to <Target Character>. This prompt should enable text-to-image AI models to generate images without directly mentioning the name of the <Target Character>. \n\nFinally, we curate the selected positive images, as illustrated in Fig. 3.",
            "score": 0.4138158601847801,
            "section_title": "Collecting Positive Samples",
            "char_start_offset": 12340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1824
                },
                {
                    "start": 1827,
                    "end": 1901
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 395,
                    "matchedPaperCorpusId": "264403242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.154296875
        },
        {
            "corpus_id": "258999304",
            "title": "Challenges and Remedies to Privacy and Security in AIGC: Exploring the Potential of Privacy Computing, Blockchain, and Beyond",
            "text": "Unlike photographers, users who create works using AI still cannot know the final effect of the work after completing the input [19]. This is because AI models perform complex processing on the input, and the final result inevitably includes certain features in the training data, which also involves infringement of human works [30]. \n\nTo address these issues, one possible approach is to perform special processing on AI generated works, such as clearly marking the source and generation method of the generated works, or using special licensing or authorization mechanisms to manage the use and distribution of the generated works. In addition, it is necessary to modify the existing copyright legal framework to adapt to the unique nature of AI generated works. Overall, the issue of copyright ownership in AIGC generated works is complex and requires comprehensive consideration of technical, legal, and ethical factors, and currently remains a major challenge faced by AIGC.",
            "score": 0.4133728325739847,
            "section_title": "3.2.4",
            "char_start_offset": 43427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 980
                }
            ],
            "ref_mentions": [
                {
                    "start": 329,
                    "end": 333,
                    "matchedPaperCorpusId": "231951381"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11029052734375
        },
        {
            "corpus_id": "271135364",
            "title": "Mind the Ownership Gap? Copyright in AI-generated Language Data",
            "text": "A group of 17 authors, including John Grisham and George R. R. Martin, went as far as to sue Open AI for \"systematic theft on a mass scale\" (Italie, 2023).As a matter of fact, there are a number of other lawsuits brought by authors against AI companies (see, Setty, 2023) or content producers accused of using AI techniques (Khalid, 2024).According to recent media reports, a Chinese court found a provider of an AI text-toimage tool guilty of copyright infringement; the tool (when prompted accordingly) generated images of Ultraman, a popular cartoon character, that were substantially similar to the original artwork (Costigan, 2024).\n\nThe opinion according to which AI-generated outputs are in fact infringing copyright in the data used to train the underlying model remains to be tested by European and US courts, and rightholders have so far struggled to consistently identify outputs which bear a resemblance to items of training data without wilfully contriving circumstances intended to create such resemblance 16 .\n\nFinally, it has also happened that, for fear of a successful copyright infringement lawsuit, platforms removed AI-generated content when pressured by rightholders (Snapes, 2023).Such content was, therefore, assumed to infringe copyright.",
            "score": 0.4131759512437114,
            "section_title": "Grey areas related to copyright protection of AI outputs",
            "char_start_offset": 18513,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 155,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 637
                },
                {
                    "start": 639,
                    "end": 1024
                },
                {
                    "start": 1026,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1263
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054595947265625
        },
        {
            "corpus_id": "267750879",
            "title": "Copyleft for Alleviating AIGC Copyright Dilemma: What-if Analysis, Public Perception and Implications",
            "text": "Immediately after the question about AI-generated images, we asked our participants a yes-no question about whether they think they can use the generated picture freely. No statistically significant difference between the number of people who selected \"yes\" and \"no\" (\u03c7 2 = 1.37, df = 1, p = 0.2419), which further demonstrated the ambivalence rooted in the AIGC copyright dilemma. \n\nCopyleft is a feasible option People inclined to use authorized AIGC under restrictions. Three questions were presented to the participants about their expected level of authorization and freedom when using AIGC, the results were captured in Figure 7. Clearly, we observed two negatively skewed distributions of bar, implying that more people a) supported obtaining authorization of AIGC before use (n = 96 + 55, 71.56%), and b) preferred to be imposed on loose restrictions or have no restriction at all while using AIGC (n = 86 + 42, 60.66%). More specifically, the option we considered as most reasonable, using authorized AIGC under loose restrictions (response = 4 in Figure 7), received the strongest endorsement from our participants. \n\nPeople generally agreed that copyleft could resolve AIGC copyright infringement. Meanwhile, more than half of our participants (n = 123, 58.29%) thought copyleft is capable of preventing copyright infringement of AIGC if properly used by both model owner and model user. Interestingly, the largest difference between the number of participants who chose \"no\" (n = 42) and \"yes\" (n = 13) was observed in the bar at the upper-right corner of Figure 7. That is, among the people who strongly agreed in obtaining authorization of AIGC (n = 55), 79.25% of them thought copylefting AIGC could prevent copyright infringement. \n\nPeople were generally positive about applying copyleft to AIGC. We asked participants what will happen if AIGC is copylefted. Six negative predictions and five positive predictions were provided; they were free to choose any number of these upsides and downsides.",
            "score": 0.4129398081786965,
            "section_title": "Results",
            "char_start_offset": 20189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1746
                },
                {
                    "start": 1749,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2012
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034423828125
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "Main results: In Section 4, we reveal the latent (copyrighted) information contained in the acquired samples x d using textual inversion, where the diffusion model (i.e., the U-Net in LDM) is not retrained.To further demonstrate the effectiveness of the disguises x d and examine disguised copyright infringement in other scenarios, we further perform evaluation on DreamBooth (Ruiz et al. 2023), an LDM-based fine-tuning method on a small set of images designed for subject-driven generation.Specifically, we choose five images from the DreamBooth dataset12 as the (designated) copyrighted image x c , and generate their corresponding disguises x d using Algorithm 1 by choosing the noisy version of x c as the base images x b , training 10000 epochs and setting the weight parameter \u03b1 = 1000.After generating the disguises x d , we apply DreamBooth for fine-tuning (on x d only) and inference following the general recipe13 .In Figure 24, we show the copyrighted images x c in the first row, their corresponding disguises x d in the second row, and images generated by DreamBooth by fine-tuning with the above x d in the third row.Our results qualitatively confirm that copyrighted information in x c can be reproduced by fine-tuning on x d with DreamBooth.\n\nEvaluation on model utility: To further examine the practicality of the disguises, we want to demonstrate that the model utility is not deprecated after training (or fine-tuning) on x d .To accomplish this task, we perform the same text-to-image generation task with the prompt \"a photo of an astronaut riding a horse on Mars\" before/after training on x d with LDM14 .Specifically, the \"before\" model is Stable Diffusion v1-4, and the \"after\" model is the acquired model by fine-tuning the \"before\" model on the dog disguise (second row, first column in Figure 24).We show our results in Figure 25 for 6",
            "score": 0.4126304917920611,
            "section_title": "F.1. Evaluation on DreamBooth",
            "char_start_offset": 37186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 493
                },
                {
                    "start": 493,
                    "end": 794
                },
                {
                    "start": 794,
                    "end": 927
                },
                {
                    "start": 927,
                    "end": 1133
                },
                {
                    "start": 1133,
                    "end": 1259
                },
                {
                    "start": 1261,
                    "end": 1448
                },
                {
                    "start": 1448,
                    "end": 1629
                },
                {
                    "start": 1629,
                    "end": 1826
                },
                {
                    "start": 1826,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 377,
                    "end": 394,
                    "matchedPaperCorpusId": "251800180"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1063232421875
        },
        {
            "corpus_id": "270063121",
            "title": "Protect-Your-IP: Scalable Source-Tracing and Attribution against Personalized Generation",
            "text": "The AI-generated content (AIGC) model, especially the personalized generation model [55], has adverse implications on the copyright and intellectual property (IP) of various visual content such as artworks by artists, portraits of individuals, and photographs by photographers.Images generated through personalized methods may propagate misinformation and infringe upon copyrights, thereby engendering negative societal repercussions.In response to these concerns, significant research efforts have been directed towards AIGC copyright watermarking for images [37,27,54].Several methodologies, including those employing box-free watermarking techniques [50,43,51,19,35,44,28,38], have been developed to trace back the fine-tuning of generative models like Generative Adversarial Network (GAN) [12,20,13,4] and Diffusion models [17,33,31,7,34], thereby serving a post-hoc protective function.Cool photo!I will input them into personalized generation models and generate images, then upload online.\n\nI see these images on Internet, and I doubt that they are generated via my portraits.\n\nIt is AIGC-generated.I detect your copyright in this image, and it is generated using InstantID.\n\nThere is a new kind of personalized generation model, and I can generate new images via this model.\n\nI can scale my attributor for new methods, and it is generated using PhotoMaker.\n\nCopyright Info",
            "score": 0.41199692522958226,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 434
                },
                {
                    "start": 434,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 996
                },
                {
                    "start": 998,
                    "end": 1083
                },
                {
                    "start": 1085,
                    "end": 1106
                },
                {
                    "start": 1106,
                    "end": 1181
                },
                {
                    "start": 1183,
                    "end": 1282
                },
                {
                    "start": 1284,
                    "end": 1364
                },
                {
                    "start": 1366,
                    "end": 1380
                }
            ],
            "ref_mentions": [
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "257766375"
                },
                {
                    "start": 657,
                    "end": 660,
                    "matchedPaperCorpusId": "229718542"
                },
                {
                    "start": 660,
                    "end": 663,
                    "matchedPaperCorpusId": "232148007"
                },
                {
                    "start": 663,
                    "end": 666,
                    "matchedPaperCorpusId": "260334882"
                },
                {
                    "start": 666,
                    "end": 669,
                    "matchedPaperCorpusId": "264492075"
                },
                {
                    "start": 669,
                    "end": 672,
                    "matchedPaperCorpusId": "258588430"
                },
                {
                    "start": 675,
                    "end": 678,
                    "matchedPaperCorpusId": "268692714"
                },
                {
                    "start": 793,
                    "end": 797,
                    "matchedPaperCorpusId": "1033682"
                },
                {
                    "start": 800,
                    "end": 803,
                    "matchedPaperCorpusId": "10894094"
                },
                {
                    "start": 827,
                    "end": 831,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 831,
                    "end": 834,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 834,
                    "end": 837,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 837,
                    "end": 839,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10302734375
        },
        {
            "corpus_id": "265506098",
            "title": "Situating the social issues of image generation models in the model life cycle: a sociotechnical approach",
            "text": "Once these images are outputted, bias risks being further reproduced through the use of these problematic images. \n\nPrivacy-sensitive images may be used in data acquisition and training, heightening the relevance of privacy concerns in this phase. Additional issues arise in the deployment phase if users utilize the text-to-image models to attempt to generate privacy-sensitive images, such as explicit images or images depicting specific individuals. Moreover, as with intellectual property issues above, there is a risk that platform owners skim prompts inputted by users. The spread of these generated images, and their potential re-use in further models pose additional privacy risks. \n\nIssues in the informational environment can arise through data acquisition as there is no way to assess the accuracy or factual nature of the image-text pairs acquired. These issues then come to the fore in deployment, where the models can be used to intentionally or inadvertently generate images which misrepresent reality. Without watermarking or other indications through which to clarify the provenance of these images, there are key concerns about their detrimental impact on the informational environment at large once they are used and are in circulation. \n\nAs with intellectual property issues, cultural issues occur when culturally important images are scraped for use by the model. The issues similarly re-emerge in deployment and use of the images, such as when the generated images misrepresent culturally important events or works. They also pose broader cultural issues when the platforms are utilized to replace the work of human artists. \n\nAnd finally, issues related to the natural environment are unique in this analysis as they emerge across all phases of the model life cycle. Each step in the model life cycle implicitly involves an environmental cost, as each piece of hardware and all infrastructure that is used in the development and deployment of text-to-image models create burdens and pressures on the natural environment. Though there remains a dearth of relevant research in this area, these are nonetheless critical considerations. \n\nAs is clear through the discussion above, the social issues of image generation models appear across the model life cycle. For this reason, it is essential that pre-emptive consideration of these issues be embedded throughout all phases of the model life cycle. In Table 2 below, we propose a matrix which could be used in risk assessments of present and future image generation models at the intersection of the social and technical axes.",
            "score": 0.4119011291191825,
            "section_title": "Social issues in the model life cycle",
            "char_start_offset": 43111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 116,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1255
                },
                {
                    "start": 1258,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2155
                },
                {
                    "start": 2158,
                    "end": 2280
                },
                {
                    "start": 2281,
                    "end": 2419
                },
                {
                    "start": 2420,
                    "end": 2597
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.045684814453125
        },
        {
            "corpus_id": "269484238",
            "title": "MMTryon: Multi-Modal Multi-Reference Control for High-Quality Fashion Generation",
            "text": "As most generative approaches, MMTryon can be used for malicious purposes by generating images that infringe upon copyrights and/or privacy. Given these considerations, responsible use of the model is advocated. \n\nA.4 Limitation and future work. \n\nWhile our method demonstrates strong performance, it still has certain limitations. In the data generation process, our method is influenced by the limitations of pretrained models, making it challenging to produce data that meets the requirements for very fine parts, such as cuffs and collars. This restricts our ability to generate detailed components. Moving forward, we may focus on finetuning large models to construct a more freely detailed and fine-grained dataset, aiming to enhance the upper limit of our model.",
            "score": 0.4116907572793035,
            "section_title": "A.3 Societal impact.",
            "char_start_offset": 26306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 211
                },
                {
                    "start": 214,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 769
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046539306640625
        },
        {
            "corpus_id": "276250558",
            "title": "Training-Free Constrained Generation With Stable Diffusion Models",
            "text": "Stress-strain curves MSE [\u2193] 179.5 175.6 12.5 1.2 risk of generating outputs which closely resemble copyrighted material. For this setting, a pretrained proxy model is fine-tuned to determine whether the generation infringes upon existing copyrighted material. This model has been calibrated so that the output logits can be directly used to evaluate the likelihood that the samples resemble existing protected material. Hence, by minimizing this surrogate constraint function, we directly minimize the likelihood that the output image includes copyrighted material. \n\nTo implement this, we define a permissible threshold for the likelihood function captured by the classifier. A balanced dataset of 8,000 images is constructed to fine-tune the classifier and diffusion models. Here, we use cartoon mouse characters 'Jerry,' from Tom and Jerry, and copyrightprotected character 'Mickey Mouse'. When fine-tuning the diffusion model, we do not discriminate between these two characters, but the classifier is tuned to identify 'Mickey Mouse' as a copyrighted example. \n\nInner minimizer. Our correction step begins by performing Principal Component Analysis (PCA) on the 512 features input to the last layer and selecting the two principal components. This analysis yields two well-defined clusters corresponding to the class labels. Provided this, we formulate a correction by iteratively driving the noisy samples toward the centroid of the target cluster, as illustrated in Figure 4 (right). During the early stages of the denoising process, if the classifier assigns a high probability to the sample being 'Mickey Mouse,' we correct the sample toward the 'Jerry' cluster in the feature space. Specifically, we iteratively adjust the sample until its distance from the 'Jerry' cluster falls below a predefined threshold. This correction is achieved by minimizing the distance between the sample's feature representation and the centroid of the 'Jerry' cluster, effectively guiding the generation process away from the copyrighted class label. \n\nAfter this correction, the denoising process is allowed to evolve naturally without further intervention. This method ensures that the generated images are guided away from resembling copyrighted material while still allowing the model to produce high-quality outputs.",
            "score": 0.4115432389239016,
            "section_title": "Structural analysis",
            "char_start_offset": 28763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2042
                },
                {
                    "start": 2045,
                    "end": 2150
                },
                {
                    "start": 2151,
                    "end": 2313
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.264892578125
        },
        {
            "corpus_id": "271404273",
            "title": "Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?",
            "text": "In recent years, Large Language Models (LLMs) have pushed the frontiers of natural language processing by facilitating sophisticated tasks like text generation, translation, and summarization. With their impressive performance, LLMs are increasingly integrated into various applications, including virtual assistants, chatbots, content generation, violators can face legal consequences, including lawsuits and financial penalties. For LLMs, copyright infringement can occur through (1) generation of copyrighted content during deployment and (2) illegal usage of copyrighted works during training. Ensuring the absence of copyrighted content in the vast training datasets of LLMs is challenging. Moreover, legal debates around generative AI copyright infringement vary by region, complicating compliance further. \n\nCurrent lawsuits against AI companies for unauthorized use of copyrighted content (e.g., Andersen v. Stability AI Ltd, NYT v. OpenAI) highlight the urgent need for methods to address these challenges. In this paper, we focus on studying the effects of watermarking LLMs on two critical issues: (1) preventing the generation of copyrighted content, and (2) detecting copyrighted content in training data. We show that watermarking can significantly impact both the generation of copyrighted text and the detection of copyrighted content in training data. \n\nFirstly, we observe that current LLM output watermarking techniques can significantly reduce the probability of LLMs generating copyrighted content, by tens of orders of magnitude. Our empirical results focus on two recent watermarking methods: UMD (Kirchenbauer et al. 2023) and Unigram-Watermark (Zhao et al. 2023). Both methods split the vocabulary into two sets (green and red) and bias the model towards selecting tokens from the green set by altering the logits distribution, thereby embedding a detectable signal. We provide both empirical and theoretical results to support our findings. \n\nSecondly, we demonstrate that watermarking techniques can decrease the success rate of Membership Inference Attacks (MIAs), which aim to detect whether a piece of copyrighted text was part of the training dataset. Since MIAs exploit the model's output, their performance can suffer under watermarking due to changes in the probability distribution of output tokens.",
            "score": 0.41150982509986567,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1368
                },
                {
                    "start": 1371,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 1966
                },
                {
                    "start": 1969,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2334
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2432861328125
        },
        {
            "corpus_id": "273654195",
            "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
            "text": "Due to the continuous data collection process, there would be multiple iterative versions of the generative model [36]. \n\nIn this context, we encounter several challenges when designing incentive schemes: (1) The absence of a standardized copyright metric that complies with existing copyright laws and regulations complicates the determination of whether generated images infringe on copyrights; (2) Allocating the total budget across various versions of the generated model poses difficulties. Investing excessively in earlier versions may leave new data holders, who join at a later stage, without adequate compensation. \n\nTo tackle these challenges, we introduce a Copyright-aware Incentive Scheme tailored for generative art models. Drawing inspiration from U.S. court practices [21], which utilize a two-part test to assess copyright violations, we develop a copyright metric that aligns with these legal standards. This metric encompasses an extrinsic component that evaluates objective similarity in specific expressive elements, as well as an intrinsic component that considers subjective similarity from the perspective of a reasonable audience. By integrating both semantic and perceptual similarity, our proposed metric effectively reflects judicial criteria. Additionally, we have designed a hierarchical budget allocation method based on reinforcement learning, which first establishes the budget for each round and subsequently determines the remuneration for data holders based on their contributions and the associated copyright loss in each round. \n\nOur contribution are as follows: \n\n(1) We propose the first metric for copyright loss that closely follows the procedures of copyright law. (2) We designed a hierarchical budget allocation method based on reinforcement learning, which first determines the budget for each round and then determines the remuneration of the data holder based on the data holder's contribution and copyright loss in each round. This is the first incentive scheme that compensates data contributors based on copyright loss and contributions. (3) Extensive experimental comparisons between existing incentive schemes and our method demonstrate that our approach is more attractive to data holders who possess high-quality and original images.",
            "score": 0.41137171202938516,
            "section_title": "Introduction",
            "char_start_offset": 4466,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 122,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1600
                },
                {
                    "start": 1603,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2288
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.298828125
        },
        {
            "corpus_id": "272060563",
            "title": "Managing Copyright Infringement Risks in Generative Artificial Intelligence Data Mining",
            "text": "Generative artificial intelligence can independently generate pictures, text, animation, audio and other content by learning a large amount of training data to meet user needs based on user requirements. The specific operation mode includes four stages. The first stage involves inputting training data, which includes the acquisition and storage of training data, which includes the acquisition and storage of training data that may fall within the copyright protection period and be used without the consent of the original copyright owner. The second stage involves data analysis. Generative artificial intelligence extracts patterns and forms a common expression template by analyzing a large amount of training data. The third stage is input instruction, which guides generative artificial intelligence in creating content that fulfills the user's needs. The fourth stage is content generation, which optimizes the expression model through learning from training data and constant adjustments by users, ultimately generating content indistinguishable from human works. \n\nAs mentioned earlier, it can be seen that generative AI cannot be developed without the use of data. High-quality and rich data is the basis of artificial intelligence training, and the size and quality of data directly affects the quality of the content generated. Artificial intelligence can form a creation model highly similar to human expression through the analysis and learning of a large amount of data, so that the artificial intelligence model has the ability to generate content autonomously in the face of different needs. However, the development of generative artificial intelligence is currently hindered by the lack of high quality data in Chinese. If we can solve the problem of insufficient data usage and make full use of China's high-quality works, it will effectively meet the data demand for the development of artificial intelligence [2]. However, at the same time, it may also seriously infringe on the rights of the original copyright owners. \n\nIn this regard, we should combine the legislative purpose of copyright law and the needs of social public interests, balance the interests of copyright owners and the interests of artificial intelligence development. Copyright law not only protects the generation of original content, but also promotes the use of works after their creation, in order to balance the exclusive rights of copyright owners with the interests of the public in using and enjoying works.",
            "score": 0.4112541657554752,
            "section_title": "Generative Artificial Intelligence Operation Mode",
            "char_start_offset": 1659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2043
                },
                {
                    "start": 2046,
                    "end": 2262
                },
                {
                    "start": 2263,
                    "end": 2510
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30126953125
        },
        {
            "corpus_id": "276558199",
            "title": "Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models",
            "text": "We focus on copyright protection in non-adversarial settings, aiming to prevent models from unintentionally generating or being directly prompted to generate copyrighted content during normal usage. While a sufficiently motivated attacker might still be able to coerce the model into producing such content, our primary objective is to prevent routine copyright infringements.",
            "score": 0.4109106243239653,
            "section_title": "Threat Model",
            "char_start_offset": 5773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 376
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12939453125
        },
        {
            "corpus_id": "269148917",
            "title": "Detecting AI-Generated Images via CLIP",
            "text": "The past few years have seen a meteoric increase in the quality of AI-generated images (AIGI).The pairing of internet-scale datasets with new models based on techniques such as Diffusion [1]; Diffusion-based models (see Table 1) are overtaking the previous state-of-the-art Generative Adversarial Network (GAN) models, which are difficult to train due to mode collapse, lack of convergence, vanishing gradients, and overall training instability [2].Large-scale diffusion models can create a wide variety of images from a single model, even allowing the user to specify the artistic style of the output image.\n\nThe impact of AIGI has been amplified by the relative accessibility of AIGI models to the public.With cloud-hosted, web-based natural language models accessed via popular platforms such as Discord, no programming knowledge or expensive GPUenabled hardware is required to use generative models.Many AIGI model hosts offer free trial accounts, and paid versions cost only a handful of dollars per month [3,4].The combination of speed, quality, and availability have led to a deluge of AIGI content posted and shared across the internet.\n\nThe scale and quality of AI-generated content exacerbates old problems and introduces entirely new ones.Conflicts often arise because AIGI are now difficult to distinguish from real images, and affected parties may only discover an image is synthetic after damage has been done [5].Copyright of AI-generated content is thorny, as traditionally only humans are allowed to hold copyright [6].It is not clear if copyright is even applicable in the situation, as trained AI models do not store content after training or copy content during generation [7].While edge cases have always existed in copyright law such as copyright of animal-created works [8], the sheer number of training images used and the increase in AIGI use now requires clear solutions to these issues.\n\nA further problem is the ability of AIGI to generate enormous amounts of harmful content, either offensive or misleading.AIGI tools allow easy, targeted generation or modification of images on a massive scale with no prerequisites for artistic or computational skill.This creates potential societal-level impacts; AIGI can mislead the public via social media or advertising.",
            "score": 0.41045944639441756,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 94,
                    "end": 449
                },
                {
                    "start": 449,
                    "end": 608
                },
                {
                    "start": 610,
                    "end": 707
                },
                {
                    "start": 707,
                    "end": 903
                },
                {
                    "start": 903,
                    "end": 1017
                },
                {
                    "start": 1017,
                    "end": 1144
                },
                {
                    "start": 1146,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1428
                },
                {
                    "start": 1428,
                    "end": 1536
                },
                {
                    "start": 1536,
                    "end": 1697
                },
                {
                    "start": 1697,
                    "end": 1913
                },
                {
                    "start": 1915,
                    "end": 2036
                },
                {
                    "start": 2036,
                    "end": 2182
                },
                {
                    "start": 2182,
                    "end": 2289
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0809326171875
        },
        {
            "corpus_id": "276250558",
            "title": "Training-Free Constrained Generation With Stable Diffusion Models",
            "text": "This method ensures that the generated images are guided away from resembling copyrighted material while still allowing the model to produce high-quality outputs. By selectively modifying the generated content during the initial stages of denoising, we can effectively prevent the model from producing images that infringe on copyrights without significantly affecting the overall image quality. \n\nResults. Figure 4 (right) illustrates the correction path that occurs during the initial stages of denoising. Once the correction is completed, the denoising process proceeds freely, as shown in Figure 4 (left), where we compare the evolution of the original sample and that of the corrected sample. \n\nWe implement a Conditional Diffusion Model baselines using and unconstrained stable diffusion model identical to the one used for our method. The conditional baseline generates the protected cartoon character (Mickey Mouse) 33% of the time, despite conditioning it against these generations. \n\nConversely, our Latent Constrained Model only generates the protected cartoon character 10% of the time, aligning with the expected bounds of the classifier's predictive accuracy. Our method has proven to be highly effective because it preserves the generative capabilities of the model while imposing the defined constraints. Notably, the difference between the original image and the corrected one primarily affects the areas near the figure that violate the constraint, while the rest of the image remains largely unchanged. The FID scores of the generated images, increasing only slightly from 61.2 to 65.1, remain largely unaltered by the gradientbased correction. This demonstrates that our approach can selectively modify generated content to avoid copyrighted material without compromising overall image quality.",
            "score": 0.41012329576720746,
            "section_title": "Structural analysis",
            "char_start_offset": 30914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1814
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.380126953125
        },
        {
            "corpus_id": "258833125",
            "title": "The \u201cCode\u201d of Ethics: A Holistic Audit of AI Code Generators",
            "text": "Unauthorized usage of source code in PLG models can raise legal and ethical issues, especially if the generated content is not transformative (i.e., the model outputs similar content to copyrighted training data) [39]. For example, GitHub Copilot has been found to generate large segments of copied code without attribution to licensed source code [27], [73], leading to a lawsuit against it for alleged copyright infringement [2]. OpenAI, the owner of ChatGPT, is also facing potential fines and a ban in Europe if it fails to convince the authorities its data use practices are legal [38]. \n\nTo mitigate this infringement harm, the U.S. Copyright Office recently launched an initiative to examine copyright law and policy issues over the use of copyrighted materials in generative AI training [4]. We notice that copyright protection for code is more complex and challenging than that for creative works such as text or music, which have had extensive legal and technical measures in place to protect them [39], [75]. Although text and code generation have some similarities in how their models are trained, they have each spawned distinctive case law with different assessments when it comes to fair use of copyrighted material [39]. As a result, it is difficult for any arbitration committee or court to apply standard copyright principles, such as idea and/or expression distinctions in text-and music-related cases, into code-related cases due to the technical sophistication and functionality of software program [17]. Therefore, it remains a complex and open challenge for collecting digital forensics to detect and prevent copyright violations in code domain. \n\nThe potential misuse of PLG models also raises ethical concerns that require attention, particularly for model deployment [19]. One of the major issues is the generation of false information [20], [80] and artificial hallucinations [9], which can 1545-5971 \u00a9 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. \n\nSee https://www.ieee.org/publications/rights/index.html for more information. \n\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. lead to the spread of misinformation.",
            "score": 0.40997361990478853,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 3801,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 591
                },
                {
                    "start": 594,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1668
                },
                {
                    "start": 1671,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2026
                },
                {
                    "start": 2029,
                    "end": 2106
                },
                {
                    "start": 2109,
                    "end": 2200
                },
                {
                    "start": 2201,
                    "end": 2220
                },
                {
                    "start": 2221,
                    "end": 2258
                }
            ],
            "ref_mentions": [
                {
                    "start": 1014,
                    "end": 1018,
                    "matchedPaperCorpusId": "158783914"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "253238911"
                },
                {
                    "start": 1903,
                    "end": 1906,
                    "matchedPaperCorpusId": "257037938"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.172607421875
        },
        {
            "corpus_id": "269449051",
            "title": "AI and the Dynamic Supply of Training Data",
            "text": "More recently, however, intellectual property law has emerged as a critical area of the policy discourse because input data may be subject to copyright protection (Eshraghian, 2020;Samuelson, 2023).While some jurisdictions allow exemptions for research and development in copyright law, there is general legal uncertainty (Fiil-Flynn et al., 2022;Henderson et al., 2023).Several high-profile lawsuits against AI developers allege, among other claims, direct copyright infringement by creating unauthorized copies of their works and using such copies as training data.1 As a result, policymakers around the world are tasked with balancing innovation in AI and the interests of rightsholders.So far, however, there is not sufficient empirical evidence to guide policy (Peukert and Windisch, 2024).\n\nIn this paper, we shine a light on how strategic behavior shapes the flow of data, studying the response of individual creators to their works being made available for the training of commercial AI.\n\nOur empirical setting is one of the largest stock photography websites in the world, Unsplash.In the summer of 2020, Unsplash released metadata of a subset of 25,000 photos explicitly for the training of commercial AI applications (hereafter the \"LITE\" dataset).This natural experiment allows us to identify the causal effects of being included in AI training data and characterize strategic responses of those affected.We compare the upload behavior of contributors whose works were included in this dataset to contributors whose works were not included.In addition, we investigate changes in the variety and novelty of user contributions due to the release of the LITE dataset.We measure variety and novelty by calculating the similarity of each new upload to all existing images one year prior to the treatment based on their associated keywords using natural language processing methods.This allows us to track how variety and novelty change over time for both treated and control users, before and after the release of the LITE dataset.\n\nWe find that treated users left the platform at a higher-than-usual rate.Conditional on remaining active, treated users substantially slowed down the rate of new uploads by about 40% per month.\n\nOur content analysis shows that within-users, uploads decrease in variety but not in novelty compared to the existing stock of images.",
            "score": 0.40995172269468694,
            "section_title": "Introduction",
            "char_start_offset": 1792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 690
                },
                {
                    "start": 690,
                    "end": 795
                },
                {
                    "start": 797,
                    "end": 995
                },
                {
                    "start": 997,
                    "end": 1091
                },
                {
                    "start": 1091,
                    "end": 1259
                },
                {
                    "start": 1259,
                    "end": 1417
                },
                {
                    "start": 1417,
                    "end": 1552
                },
                {
                    "start": 1552,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1888
                },
                {
                    "start": 1888,
                    "end": 2038
                },
                {
                    "start": 2040,
                    "end": 2113
                },
                {
                    "start": 2113,
                    "end": 2233
                },
                {
                    "start": 2235,
                    "end": 2369
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 181,
                    "matchedPaperCorpusId": "215991449"
                },
                {
                    "start": 181,
                    "end": 197,
                    "matchedPaperCorpusId": "259844568"
                },
                {
                    "start": 322,
                    "end": 347,
                    "matchedPaperCorpusId": "254149911"
                },
                {
                    "start": 766,
                    "end": 794,
                    "matchedPaperCorpusId": "264474179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1026611328125
        },
        {
            "paperId": "2e28407a767f388713fbd372248989c2e28473a7",
            "corpusId": 268048573,
            "title": "WIP: Auditing Artist Style Pirate in Text-to-image Generation Models",
            "venue": "Proceedings 2024 Workshop on AI Systems with Confidential COmputing",
            "year": 2024,
            "referenceCount": 46,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14722/aiscc.2024.23009?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14722/aiscc.2024.23009, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "151483943",
                    "name": "L. Du"
                },
                {
                    "authorId": "2288042024",
                    "name": "Zheng Zhu"
                },
                {
                    "authorId": "2238153157",
                    "name": "Min Chen"
                },
                {
                    "authorId": "2237990407",
                    "name": "Shouling Ji"
                },
                {
                    "authorId": "2147335888",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "2138800088",
                    "name": "Jiming Chen"
                },
                {
                    "authorId": "2238124154",
                    "name": "Zhikun Zhang"
                }
            ],
            "abstract": "\u2014The text-to-image models based on diffusion processes, capable of transforming text descriptions into detailed images, have widespread applications in art, design, and beyond, such as DALL-E, Stable Diffusion, and Midjourney. However, they enable users without artistic training to create artwork comparable to professional quality, leading to concerns about copyright infringement. To tackle these issues, previous works have proposed strategies such as adversarial perturbation-based and watermarking-based methods. The former involves introducing subtle changes to disrupt the image generation process, while the latter involves embedding detectable marks in the artwork. The existing methods face limitations such as requiring modifications of the original image, being vulnerable to image pre-processing, and facing difficulties in applying them to the published artwork. To this end, we propose a new paradigm, called StyleAuditor , for artistic style auditing. StyleAuditor identifies if a suspect model has been fine-tuned using a specific artist\u2019s artwork by analyzing style-related features. Specifically, StyleAuditor employs a style extractor to obtain the multi-granularity style representations and treats artwork as samples of an artist\u2019s style. Then, StyleAuditor queries a trained discriminator to gain the auditing decisions. The results of the experiment on the artwork of thirty artists demonstrate the high accuracy of StyleAuditor , with an auditing accuracy of over 90% and a false positive rate of less than 1.3%.",
            "corpus_id": "268048573",
            "text": "\u2014The text-to-image models based on diffusion processes, capable of transforming text descriptions into detailed images, have widespread applications in art, design, and beyond, such as DALL-E, Stable Diffusion, and Midjourney. However, they enable users without artistic training to create artwork comparable to professional quality, leading to concerns about copyright infringement. To tackle these issues, previous works have proposed strategies such as adversarial perturbation-based and watermarking-based methods. The former involves introducing subtle changes to disrupt the image generation process, while the latter involves embedding detectable marks in the artwork. The existing methods face limitations such as requiring modifications of the original image, being vulnerable to image pre-processing, and facing difficulties in applying them to the published artwork. To this end, we propose a new paradigm, called StyleAuditor , for artistic style auditing. StyleAuditor identifies if a suspect model has been fine-tuned using a specific artist\u2019s artwork by analyzing style-related features. Specifically, StyleAuditor employs a style extractor to obtain the multi-granularity style representations and treats artwork as samples of an artist\u2019s style. Then, StyleAuditor queries a trained discriminator to gain the auditing decisions. The results of the experiment on the artwork of thirty artists demonstrate the high accuracy of StyleAuditor , with an auditing accuracy of over 90% and a false positive rate of less than 1.3%.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1396484375
        },
        {
            "paperId": "f240825e618621eff36510d916b23f6c06af680d",
            "corpusId": 271662609,
            "title": "Auditing and instructing text-to-image generation models on fairness",
            "venue": "AI and Ethics",
            "year": 2024,
            "referenceCount": 67,
            "citationCount": 14,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s43681-024-00531-5.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12103484, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2055616945",
                    "name": "Felix Friedrich"
                },
                {
                    "authorId": "2166299958",
                    "name": "Manuel Brack"
                },
                {
                    "authorId": "2140415179",
                    "name": "Lukas Struppek"
                },
                {
                    "authorId": "2140396795",
                    "name": "Dominik Hintersdorf"
                },
                {
                    "authorId": "40896023",
                    "name": "P. Schramowski"
                },
                {
                    "authorId": "2165225321",
                    "name": "Sasha Luccioni"
                },
                {
                    "authorId": "2066493115",
                    "name": "K. Kersting"
                }
            ],
            "abstract": "Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases during the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias in any direction based on human instructions yielding arbitrary proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, requiring no data filtering nor additional training.",
            "corpus_id": "271662609",
            "text": "Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases during the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias in any direction based on human instructions yielding arbitrary proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, requiring no data filtering nor additional training.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.01490020751953125
        },
        {
            "paperId": "6bdb1e327c4bd661173e94871648f442626e451c",
            "corpusId": 277856857,
            "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models",
            "venue": "The Web Conference",
            "year": 2025,
            "referenceCount": 75,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.13061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "151483943",
                    "name": "L. Du"
                },
                {
                    "authorId": "2288042024",
                    "name": "Zheng Zhu"
                },
                {
                    "authorId": "2238153157",
                    "name": "Min Chen"
                },
                {
                    "authorId": "2328027909",
                    "name": "Zhou Su"
                },
                {
                    "authorId": "2237990407",
                    "name": "Shouling Ji"
                },
                {
                    "authorId": "2147335888",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "2238129188",
                    "name": "Jiming Chen"
                },
                {
                    "authorId": "2238124154",
                    "name": "Zhikun Zhang"
                }
            ],
            "abstract": "Text-to-image models based on diffusion processes, such as DALL-E, Stable Diffusion, and Midjourney, are capable of transforming texts into detailed images and have widespread applications in art and design. As such, amateur users can easily imitate professional-level paintings by collecting an artist's work and fine-tuning the model, leading to concerns about artworks' copyright infringement. To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable. To this end, we propose a novel method for data-use auditing in the text-to-image generation model. The general idea of ArtistAuditor is to identify if a suspicious model has been finetuned using the artworks of specific artists by analyzing the features related to the style. Concretely, ArtistAuditor employs a style extractor to obtain the multi-granularity style representations and treats artworks as samplings of an artist's style. Then, ArtistAuditor queries a trained discriminator to gain the auditing decisions. The experimental results on six combinations of models and datasets show that ArtistAuditor can achieve high AUC values (> 0.937). By studying ArtistAuditor's transferability and core modules, we provide valuable insights into the practical implementation. Finally, we demonstrate the effectiveness of ArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor is open-sourced at https://github.com/Jozenn/ArtistAuditor.",
            "corpus_id": "277856857",
            "text": "Text-to-image models based on diffusion processes, such as DALL-E, Stable Diffusion, and Midjourney, are capable of transforming texts into detailed images and have widespread applications in art and design. As such, amateur users can easily imitate professional-level paintings by collecting an artist's work and fine-tuning the model, leading to concerns about artworks' copyright infringement. To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable. To this end, we propose a novel method for data-use auditing in the text-to-image generation model. The general idea of ArtistAuditor is to identify if a suspicious model has been finetuned using the artworks of specific artists by analyzing the features related to the style. Concretely, ArtistAuditor employs a style extractor to obtain the multi-granularity style representations and treats artworks as samplings of an artist's style. Then, ArtistAuditor queries a trained discriminator to gain the auditing decisions. The experimental results on six combinations of models and datasets show that ArtistAuditor can achieve high AUC values (> 0.937). By studying ArtistAuditor's transferability and core modules, we provide valuable insights into the practical implementation. Finally, we demonstrate the effectiveness of ArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor is open-sourced at https://github.com/Jozenn/ArtistAuditor.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1650390625
        },
        {
            "paperId": "90a1843a2c1259708a6b11df7f6701ec0643e6ae",
            "corpusId": 257079049,
            "title": "Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 51,
            "citationCount": 125,
            "influentialCitationCount": 24,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2302.10893",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.10893, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2055616945",
                    "name": "Felix Friedrich"
                },
                {
                    "authorId": "40896023",
                    "name": "P. Schramowski"
                },
                {
                    "authorId": "2166299958",
                    "name": "Manuel Brack"
                },
                {
                    "authorId": "2140415179",
                    "name": "Lukas Struppek"
                },
                {
                    "authorId": "2140396795",
                    "name": "Dominik Hintersdorf"
                },
                {
                    "authorId": "2165225321",
                    "name": "Sasha Luccioni"
                },
                {
                    "authorId": "2066493115",
                    "name": "K. Kersting"
                }
            ],
            "abstract": "Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required.",
            "corpus_id": "257079049",
            "text": "Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.00997161865234375
        },
        {
            "paperId": "6621ce99f7bdfb0e1b9bac13e76d14ed287f71c8",
            "corpusId": 264146074,
            "title": "Unified High-binding Watermark for Unconditional Image Generation Models",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 32,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.09479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2258181379",
                    "name": "Ruinan Ma"
                },
                {
                    "authorId": "2258526955",
                    "name": "Yu-an Tan"
                },
                {
                    "authorId": "2152536985",
                    "name": "Shangbo Wu"
                },
                {
                    "authorId": "2117183021",
                    "name": "Tian Chen"
                },
                {
                    "authorId": "2115647361",
                    "name": "Yajie Wang"
                },
                {
                    "authorId": "50025128",
                    "name": "Yuanzhang Li"
                }
            ],
            "abstract": "Deep learning techniques have implemented many unconditional image generation (UIG) models, such as GAN, Diffusion model, etc. The extremely realistic images (also known as AI-Generated Content, AIGC for short) produced by these models bring urgent needs for intellectual property protection such as data traceability and copyright certification. An attacker can steal the output images of the target model and use them as part of the training data to train a private surrogate UIG model. The implementation mechanisms of UIG models are diverse and complex, and there is no unified and effective protection and verification method at present. To address these issues, we propose a two-stage unified watermark verification mechanism with high-binding effects for such models. In the first stage, we use an encoder to invisibly write the watermark image into the output images of the original AIGC tool, and reversely extract the watermark image through the corresponding decoder. In the second stage, we design the decoder fine-tuning process, and the fine-tuned decoder can make correct judgments on whether the suspicious model steals the original AIGC tool data. Experiments demonstrate our method can complete the verification work with almost zero false positive rate under the condition of only using the model output images. Moreover, the proposed method can achieve data steal verification across different types of UIG models, which further increases the practicality of the method.",
            "corpus_id": "264146074",
            "text": "Deep learning techniques have implemented many unconditional image generation (UIG) models, such as GAN, Diffusion model, etc. The extremely realistic images (also known as AI-Generated Content, AIGC for short) produced by these models bring urgent needs for intellectual property protection such as data traceability and copyright certification. An attacker can steal the output images of the target model and use them as part of the training data to train a private surrogate UIG model. The implementation mechanisms of UIG models are diverse and complex, and there is no unified and effective protection and verification method at present. To address these issues, we propose a two-stage unified watermark verification mechanism with high-binding effects for such models. In the first stage, we use an encoder to invisibly write the watermark image into the output images of the original AIGC tool, and reversely extract the watermark image through the corresponding decoder. In the second stage, we design the decoder fine-tuning process, and the fine-tuned decoder can make correct judgments on whether the suspicious model steals the original AIGC tool data. Experiments demonstrate our method can complete the verification work with almost zero false positive rate under the condition of only using the model output images. Moreover, the proposed method can achieve data steal verification across different types of UIG models, which further increases the practicality of the method.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.4306640625
        },
        {
            "paperId": "7a034b3a3115ca2167e676c5d72af9039005c0f3",
            "corpusId": 268542715,
            "title": "Parameter-Guided Image Generation with Denoising Diffusion Probabilistic Models",
            "venue": "ACM Cloud and Autonomic Computing Conference",
            "year": 2023,
            "referenceCount": 23,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CAC59555.2023.10450463?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CAC59555.2023.10450463, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292347275",
                    "name": "Yi Kang"
                },
                {
                    "authorId": "2275741566",
                    "name": "Ziqi Xie"
                },
                {
                    "authorId": "2275576621",
                    "name": "Lulu Liu"
                },
                {
                    "authorId": "2111200066",
                    "name": "Xiaodong Zhao"
                },
                {
                    "authorId": "2275102092",
                    "name": "Yufei Chen"
                }
            ],
            "abstract": "Text-to-image generation models aim to generate visual images by leveraging the rich semantic information inherent in the input text. However, existing models encounter challenges in generating satisfactory images when the input text contains a substantial amount of domain-specific parametric information. To tackle this challenge, we propose a high-detail image generation method for parameterized text inputs, named PIG-DDPM. This method builds upon the Denoising Diffusion Probabilistic Model(DDPM) and utilizes a weighted bag of words model to create a concise and efficient representation of paramet-ric text features. Moreover, the existing DDPM model requires powerful hardware capabilities to generate High-Definition (HD) images, which limits its practical deployment on a large scale. To address this issue, our model incorporates positional information encoding in the textual features and generates HD images by training and stitching together multiple small-sized local images. Through a series of experiments, we substantiate that our method demonstrates noteworthy advantages in terms of both image quality and clarity when compared to the state-of-the-art methods.",
            "corpus_id": "268542715",
            "text": "Text-to-image generation models aim to generate visual images by leveraging the rich semantic information inherent in the input text. However, existing models encounter challenges in generating satisfactory images when the input text contains a substantial amount of domain-specific parametric information. To tackle this challenge, we propose a high-detail image generation method for parameterized text inputs, named PIG-DDPM. This method builds upon the Denoising Diffusion Probabilistic Model(DDPM) and utilizes a weighted bag of words model to create a concise and efficient representation of paramet-ric text features. Moreover, the existing DDPM model requires powerful hardware capabilities to generate High-Definition (HD) images, which limits its practical deployment on a large scale. To address this issue, our model incorporates positional information encoding in the textual features and generates HD images by training and stitching together multiple small-sized local images. Through a series of experiments, we substantiate that our method demonstrates noteworthy advantages in terms of both image quality and clarity when compared to the state-of-the-art methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0026111602783203125
        },
        {
            "paperId": "1a3d6119d9513ad27fa4fc3262e517ec6a6d2261",
            "corpusId": 258740710,
            "title": "FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention",
            "venue": "International Journal of Computer Vision",
            "year": 2023,
            "referenceCount": 44,
            "citationCount": 254,
            "influentialCitationCount": 44,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.10431",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.10431, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2046958974",
                    "name": "Guangxuan Xiao"
                },
                {
                    "authorId": "2068568214",
                    "name": "Tianwei Yin"
                },
                {
                    "authorId": "1768236",
                    "name": "W. Freeman"
                },
                {
                    "authorId": "145403226",
                    "name": "F. Durand"
                },
                {
                    "authorId": "2143833459",
                    "name": "Song Han"
                }
            ],
            "abstract": "Diffusion models excel at text-to-image generation, especially in subject-driven generation for personalized images. However, existing methods are inefficient due to the subject-specific fine-tuning, which is computationally intensive and hampers efficient deployment. Moreover, existing methods struggle with multi-subject generation as they often blend identity among subjects. We present FastComposer which enables efficient, personalized, multi-subject text-to-image generation without fine-tuning. FastComposer uses subject embeddings extracted by an image encoder to augment the generic text conditioning in diffusion models, enabling personalized image generation based on subject images and textual instructions with only forward passes. To address the identity blending problem in the multi-subject generation, FastComposer proposes cross-attention localization supervision during training, enforcing the attention of reference subjects localized to the correct regions in the target images. Naively conditioning on subject embeddings results in subject overfitting. FastComposer proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation. FastComposer generates images of multiple unseen individuals with different styles, actions, and contexts. It achieves 300$$\\times $$\n \u00d7\n \u20132500$$\\times $$\n \u00d7\n speedup compared to fine-tuning-based methods and requires zero extra storage for new subjects. FastComposer paves the way for efficient, personalized, and high-quality multi-subject image creation. Code, model, and dataset are available here (https://github.com/mit-han-lab/fastcomposer).",
            "corpus_id": "258740710",
            "text": "Diffusion models excel at text-to-image generation, especially in subject-driven generation for personalized images. However, existing methods are inefficient due to the subject-specific fine-tuning, which is computationally intensive and hampers efficient deployment. Moreover, existing methods struggle with multi-subject generation as they often blend identity among subjects. We present FastComposer which enables efficient, personalized, multi-subject text-to-image generation without fine-tuning. FastComposer uses subject embeddings extracted by an image encoder to augment the generic text conditioning in diffusion models, enabling personalized image generation based on subject images and textual instructions with only forward passes. To address the identity blending problem in the multi-subject generation, FastComposer proposes cross-attention localization supervision during training, enforcing the attention of reference subjects localized to the correct regions in the target images. Naively conditioning on subject embeddings results in subject overfitting. FastComposer proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation. FastComposer generates images of multiple unseen individuals with different styles, actions, and contexts. It achieves 300$$\\times $$\n \u00d7\n \u20132500$$\\times $$\n \u00d7\n speedup compared to fine-tuning-based methods and requires zero extra storage for new subjects. FastComposer paves the way for efficient, personalized, and high-quality multi-subject image creation. Code, model, and dataset are available here (https://github.com/mit-han-lab/fastcomposer).",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0041351318359375
        },
        {
            "paperId": "840ecbd1b61287ac2c9563b9506d98f4e7f117ad",
            "corpusId": 268732879,
            "title": "CPR: Retrieval Augmented Generation for Copyright Protection",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2024,
            "referenceCount": 71,
            "citationCount": 17,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2403.18920",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.18920, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35838711",
                    "name": "Aditya Golatkar"
                },
                {
                    "authorId": "16163297",
                    "name": "A. Achille"
                },
                {
                    "authorId": "1914913470",
                    "name": "L. Zancato"
                },
                {
                    "authorId": "2293780337",
                    "name": "Yu-Xiang Wang"
                },
                {
                    "authorId": "2288200185",
                    "name": "Ashwin Swaminathan"
                },
                {
                    "authorId": "2075295257",
                    "name": "S. Soatto"
                }
            ],
            "abstract": "Retrieval Augmented Generation (RAG) is emerging as a flexible and robust technique to adapt models to private users data without training, to handle credit attribution, and to allow efficient machine unlearning at scale. However, RAG techniques for image generation may lead to parts of the retrieved samples being copied in the model's output. To reduce risks of leaking private information contained in the retrieved set, we introduce Copy-Protected generation with Retrieval (CPR), a new method for RAG with strong copyright protection guarantees in a mixed-private setting for diffusion models. CPR allows to condition the output of diffusion models on a set of retrieved images, while also guaranteeing that unique identifiable information about those example is not exposed in the generated outputs. In particular, it does so by sampling from a mixture of public (safe) distribution and private (user) distribution by merging their diffusion scores at inference. We prove that CPR satisfies Near Access Freeness (NAF) which bounds the amount of information an attacker may be able to extract from the generated images. We provide two algorithms for copyright protection, CPR-KL and CPR-Choose. Unlike previously proposed rejection-sampling-based NAF methods, our methods enable efficient copyright-protected sampling with a single run of backward diffusion. We show that our method can be applied to any pre-trained conditional diffusion model, such as Stable Diffusion or unCLIP. In particular, we empirically show that applying CPR on top of unCLIP improves quality and text-to-image alignment of the generated results (81.4 to 83.17 on TIFA benchmark), while enabling credit attribution, copy-right protection, and deterministic, constant time, unlearning.",
            "corpus_id": "268732879",
            "text": "Retrieval Augmented Generation (RAG) is emerging as a flexible and robust technique to adapt models to private users data without training, to handle credit attribution, and to allow efficient machine unlearning at scale. However, RAG techniques for image generation may lead to parts of the retrieved samples being copied in the model's output. To reduce risks of leaking private information contained in the retrieved set, we introduce Copy-Protected generation with Retrieval (CPR), a new method for RAG with strong copyright protection guarantees in a mixed-private setting for diffusion models. CPR allows to condition the output of diffusion models on a set of retrieved images, while also guaranteeing that unique identifiable information about those example is not exposed in the generated outputs. In particular, it does so by sampling from a mixture of public (safe) distribution and private (user) distribution by merging their diffusion scores at inference. We prove that CPR satisfies Near Access Freeness (NAF) which bounds the amount of information an attacker may be able to extract from the generated images. We provide two algorithms for copyright protection, CPR-KL and CPR-Choose. Unlike previously proposed rejection-sampling-based NAF methods, our methods enable efficient copyright-protected sampling with a single run of backward diffusion. We show that our method can be applied to any pre-trained conditional diffusion model, such as Stable Diffusion or unCLIP. In particular, we empirically show that applying CPR on top of unCLIP improves quality and text-to-image alignment of the generated results (81.4 to 83.17 on TIFA benchmark), while enabling credit attribution, copy-right protection, and deterministic, constant time, unlearning.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.146484375
        },
        {
            "paperId": "d7e421ffbbac776aa385999cd0b97108364066f6",
            "corpusId": 269148397,
            "title": "Watermark-embedded Adversarial Examples for Copyright Protection against Diffusion Models",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2024,
            "referenceCount": 66,
            "citationCount": 16,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2404.09401",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.09401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2215214510",
                    "name": "Peifei Zhu"
                },
                {
                    "authorId": "2279770177",
                    "name": "Tsubasa Takahashi"
                },
                {
                    "authorId": "2261286386",
                    "name": "Hirokatsu Kataoka"
                }
            ],
            "abstract": "Diffusion Models (DMs) have shown remarkable capa-bilities in various image-generation tasks. However, there are growing concerns that DMs could be used to imitate unauthorized creations and thus raise copyright issues. To address this issue, we propose a novel framework that em-beds personal watermarks in the generation of adversarial examples. Such examples can force DMs to generate images with visible watermarks and prevent DMs from imitating unauthorized images. We construct a generator based on conditional adversarial networks and design three losses (adversarial loss, GAN loss, and perturbation loss) to gen-erate adversarial examples that have subtle perturbation but can effectively attack DMs to prevent copyright violations. Training a generator for a personal watermark by our method only requires 5\u201310 samples within 2\u20133 minutes, and once the generator is trained, it can generate adver-sarial examples with that watermark significantly fast (0.2s per image). We conduct extensive experiments in various conditional image-generation scenarios. Compared to ex-isting methods that generate images with chaotic textures, our method adds visible watermarks on the generated images, which is a more straightforward way to indicate copy-right violations. We also observe that our adversarial exam-ples exhibit good transferability across unknown generative models. Therefore, this work provides a simple yet powerful way to protect copyright from DM-based imitation.",
            "corpus_id": "269148397",
            "text": "Diffusion Models (DMs) have shown remarkable capa-bilities in various image-generation tasks. However, there are growing concerns that DMs could be used to imitate unauthorized creations and thus raise copyright issues. To address this issue, we propose a novel framework that em-beds personal watermarks in the generation of adversarial examples. Such examples can force DMs to generate images with visible watermarks and prevent DMs from imitating unauthorized images. We construct a generator based on conditional adversarial networks and design three losses (adversarial loss, GAN loss, and perturbation loss) to gen-erate adversarial examples that have subtle perturbation but can effectively attack DMs to prevent copyright violations. Training a generator for a personal watermark by our method only requires 5\u201310 samples within 2\u20133 minutes, and once the generator is trained, it can generate adver-sarial examples with that watermark significantly fast (0.2s per image). We conduct extensive experiments in various conditional image-generation scenarios. Compared to ex-isting methods that generate images with chaotic textures, our method adds visible watermarks on the generated images, which is a more straightforward way to indicate copy-right violations. We also observe that our adversarial exam-ples exhibit good transferability across unknown generative models. Therefore, this work provides a simple yet powerful way to protect copyright from DM-based imitation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.299072265625
        },
        {
            "paperId": "1d98fb80898ebca144081c5ddedfea15858b26dc",
            "corpusId": 271628269,
            "title": "32\u20101: Improving QD Backplane Defect Image Generation Using Automatic Masking in Diffusion Models",
            "venue": "SID Symposium Digest of Technical Papers",
            "year": 2024,
            "referenceCount": 5,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/sdtp.17544?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/sdtp.17544, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2314432014",
                    "name": "Zhihong Pan"
                },
                {
                    "authorId": "2314285712",
                    "name": "Rahul Shenoy"
                },
                {
                    "authorId": "2236826563",
                    "name": "Kaushik Balakrishnan"
                },
                {
                    "authorId": "2236764216",
                    "name": "Qisen Cheng"
                },
                {
                    "authorId": "40585524",
                    "name": "Janghwan Lee"
                },
                {
                    "authorId": "2314204625",
                    "name": "Yongmoon Jeon"
                },
                {
                    "authorId": "2236825653",
                    "name": "Deokyeong Jeong"
                },
                {
                    "authorId": "2314320044",
                    "name": "Jaewon Kim"
                }
            ],
            "abstract": "The modern QD backplane production line deploys deep learning\u2010based defect classifier, which uses backplane images, to the auto repair system. However, one key challenge in this application is the lack of defect images available for training when a new product is first introduced. To address this issue, generative models like GAN have been used to generate defect images for new products but the image quality and diversity are not optimal due to unstable training and mode collapse. In this paper, the newest diffusion model is applied for defect image generation. It is trained on both normal (defect\u2010free) and defect images from new Product A and existing Product B. This allows the model to learn defects from Product B and generate synthetic defect images of Product A from normal images. Using an innovative method to automatically apply masks in diffusion sampling, the generated synthetic images are demonstrated to achieve high image quality and good defect diversity while preserving image fidelity outside the defective region when compared to the input normal image. Experiments are also conducted to train classifiers with generated defect images from different sampling methods. The proposed automatic masking method outperforms other sampling methods in the experiments.",
            "corpus_id": "271628269",
            "text": "The modern QD backplane production line deploys deep learning\u2010based defect classifier, which uses backplane images, to the auto repair system. However, one key challenge in this application is the lack of defect images available for training when a new product is first introduced. To address this issue, generative models like GAN have been used to generate defect images for new products but the image quality and diversity are not optimal due to unstable training and mode collapse. In this paper, the newest diffusion model is applied for defect image generation. It is trained on both normal (defect\u2010free) and defect images from new Product A and existing Product B. This allows the model to learn defects from Product B and generate synthetic defect images of Product A from normal images. Using an innovative method to automatically apply masks in diffusion sampling, the generated synthetic images are demonstrated to achieve high image quality and good defect diversity while preserving image fidelity outside the defective region when compared to the input normal image. Experiments are also conducted to train classifiers with generated defect images from different sampling methods. The proposed automatic masking method outperforms other sampling methods in the experiments.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0022602081298828125
        }
    ],
    "quotes": {
        "cost": 0.11507400000000002,
        "quotes": [
            {
                "idx": 0,
                "key": "[257050406 | Vyas et al. | 2023 | Citations: 94]",
                "snippets": "Copyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Copyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[265352103 | Zhang et al. | 2023 | Citations: 10]",
                "snippets": "Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 158,
                        "end": 658,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[265551515 | Li et al. | 2023 | Citations: 5]",
                "snippets": "In recent years, the advancement of large generative models [17,(Sohl-Dickstein et al., 2015)(Song et al., 2020) has revolutionized high-quality image synthesis [34,(Rombach et al., 2021)(Saharia et al., 2022), paving the way for commercial applications that enable the public to effortlessly craft their own artworks and designs (Brooks et al., 2022)(Gal et al., 2022)(Kawar et al., 2022)(Lugmayr et al., 2022)(Ruiz et al., 2022)(Saharia et al., 2021). Nevertheless, these models exhibit notable memorization capabilities to produce generations highly similar to the training data (Carlini et al., 2023). This resemblance raises growing concerns about copyright infringement, especially when copyrighted data is used for training [12,16,49](Vyas et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[257050406 | Vyas et al. | 2023 | Citations: 94]": "There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.",
                    "[14888175 | Sohl-Dickstein et al. | 2015 | Citations: 7030]": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.",
                    "[227209335 | Song et al. | 2020 | Citations: 6585]": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",
                    "[243938678 | Saharia et al. | 2021 | Citations: 1647]": "This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io/ for an overview of the results and code.",
                    "[245335280 | Rombach et al. | 2021 | Citations: 15768]": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
                    "[246240274 | Lugmayr et al. | 2022 | Citations: 1424]": "Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image infor-mation. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks. Re-Paint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions. Github Repository: git.io/RePaint",
                    "[248986576 | Saharia et al. | 2022 | Citations: 6075]": "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.",
                    "[251253049 | Gal et al. | 2022 | Citations: 1897]": "Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new\"words\"in the embedding space of a frozen text-to-image model. These\"words\"can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https://textual-inversion.github.io",
                    "[251800180 | Ruiz et al. | 2022 | Citations: 2891]": "Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for \u201cpersonalization\u201d of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/",
                    "[252918469 | Kawar et al. | 2022 | Citations: 1104]": "Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently limited to one of the following: specific editing types (e.g., object overlay, style transfer), synthetically generated images, or requiring multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-based semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down, cause a bird to spread its wings, etc. \u2013 each within its single high-resolution user-provided natural image. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, called Imagic, leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of Imagic on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework. To better assess performance, we introduce TEdBench, a highly challenging image editing benchmark. We conduct a user study, whose findings show that human raters prefer Imagic to previous leading editing methods on TEdBench.",
                    "[253581213 | Brooks et al. | 2022 | Citations: 1833]": "We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.",
                    "[256389993 | Carlini et al. | 2023 | Citations: 617]": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 542,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 269
                            },
                            {
                                "start": 270,
                                "end": 402
                            },
                            {
                                "start": 403,
                                "end": 542
                            }
                        ],
                        "ref_mentions": [
                            "14888175",
                            "227209335",
                            "245335280",
                            "248986576",
                            "253581213",
                            "251253049",
                            "252918469",
                            "246240274",
                            "251800180",
                            "243938678",
                            "256389993",
                            "257050406"
                        ],
                        "quote": "In recent years, the advancement of large generative models [17,(Sohl-Dickstein et al., 2015)(Song et al., 2020) has revolutionized high-quality image synthesis [34,(Rombach et al., 2021)(Saharia et al., 2022), paving the way for commercial applications that enable the public to effortlessly craft their own artworks and designs (Brooks et al., 2022)(Gal et al., 2022)(Kawar et al., 2022)(Lugmayr et al., 2022)(Ruiz et al., 2022)(Saharia et al., 2021). Nevertheless, these models exhibit notable memorization capabilities to produce generations highly similar to the training data (Carlini et al., 2023). This resemblance raises growing concerns about copyright infringement, especially when copyrighted data is used for training [12,16,49](Vyas et al., 2023)."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[266900037 | Wang et al. | 2024 | Citations: 32]",
                "snippets": "In the context of copyright, a backdoor attack against diffusion models is a type of security attack designed to induce diffusion models to reproduce copyright-infringing outputs by manipulating the clean training dataset...In our approach, the trigger is a specific prompt, while the backdoors are poisoned image-caption pairs. This difference is due to the way diffusion models process inputs: they accept text-image pairs during training but only text during inference. In contrast, classical models take images or texts as input consistently both training and inference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Problem Formulation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 221,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 222
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the context of copyright, a backdoor attack against diffusion models is a type of security attack designed to induce diffusion models to reproduce copyright-infringing outputs by manipulating the clean training dataset"
                    },
                    {
                        "section_title": "Copyright Infringement Attack",
                        "pdf_hash": "",
                        "start": 2084,
                        "end": 2435,
                        "sentence_offsets": [
                            {
                                "start": 2084,
                                "end": 2188
                            },
                            {
                                "start": 2189,
                                "end": 2332
                            },
                            {
                                "start": 2333,
                                "end": 2434
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In our approach, the trigger is a specific prompt, while the backdoors are poisoned image-caption pairs. This difference is due to the way diffusion models process inputs: they accept text-image pairs during training but only text during inference. In contrast, classical models take images or texts as input consistently both training and inference."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[267412857 | Ren et al. | 2024 | Citations: 42]",
                "snippets": "The possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works. \n\nDifferent from directly employing DGMs to generate new images, DGMs are also usually trained or fine-tuned on some source images to effectively learn useful information from for future generations. Therefore, \"training-stage protection\" aims to add imperceptible noise to the copyrighted images, to break the training process of potential DGMs for data copyright protection.\n\nDeep Generative Model Watermarking is a common solution for model copyright protection. It involves incorporating distinct information, known as a watermark, into the models before their deployment. The embedded watermark can be retrieved from a potentially infringing model or its generated data to confirm any suspected copyright violations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works. \n\nDifferent from directly employing DGMs to generate new images, DGMs are also usually trained or fine-tuned on some source images to effectively learn useful information from for future generations. Therefore, \"training-stage protection\" aims to add imperceptible noise to the copyrighted images, to break the training process of potential DGMs for data copyright protection.\n\nDeep Generative Model Watermarking is a common solution for model copyright protection. It involves incorporating distinct information, known as a watermark, into the models before their deployment. The embedded watermark can be retrieved from a potentially infringing model or its generated data to confirm any suspected copyright violations.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[268201483 | Manduchi et al. | 2024 | Citations: 22]",
                "snippets": "Interestingly, recent works show that publicly available LLMs and large-scale text-to-image models can implicitly \"memorize\" training data, in the sense that points from the dataset can be (almost exactly) reconstructed, which potentially infringes on data privacy (Carlini et al., 2023)(Carlini et al., 2023)(Somepalli et al., 2022)[136]...In addition to privacy, recent advances in large-scale generative modeling can lead to unauthorized distribution or replication of training data resulting in copyright infringement liabilities...there are several outstanding technical challenges ranging from mitigating copyright infringements during dataset curation [24] to reliable detection of copyright violations in generated samples.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[254366634 | Somepalli et al. | 2022 | Citations: 329]": "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html",
                    "[256389993 | Carlini et al. | 2023 | Citations: 617]": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                },
                "metadata": [
                    {
                        "section_title": "Privacy and Copyright Infringement",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 280,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 281
                            }
                        ],
                        "ref_mentions": [
                            "256389993",
                            "256389993",
                            "254366634"
                        ],
                        "quote": "Interestingly, recent works show that publicly available LLMs and large-scale text-to-image models can implicitly \"memorize\" training data, in the sense that points from the dataset can be (almost exactly) reconstructed, which potentially infringes on data privacy (Carlini et al., 2023)(Carlini et al., 2023)(Somepalli et al., 2022)[136]"
                    },
                    {
                        "section_title": "Privacy and Copyright Infringement",
                        "pdf_hash": "",
                        "start": 917,
                        "end": 1110,
                        "sentence_offsets": [
                            {
                                "start": 917,
                                "end": 1113
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In addition to privacy, recent advances in large-scale generative modeling can lead to unauthorized distribution or replication of training data resulting in copyright infringement liabilities"
                    },
                    {
                        "section_title": "Privacy and Copyright Infringement",
                        "pdf_hash": "",
                        "start": 1178,
                        "end": 1374,
                        "sentence_offsets": [
                            {
                                "start": 1114,
                                "end": 1373
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "there are several outstanding technical challenges ranging from mitigating copyright infringements during dataset curation [24] to reliable detection of copyright violations in generated samples."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[268732888 | Taghanaki et al. | 2024 | Citations: 1]",
                "snippets": "The rapid adoption and deployment of these technologies have also raised significant ethical, legal, and technical challenges, particularly in the context of copyright infringement and data privacy (Franceschelli et al., 2021)12,(Vyas et al., 2023).At the heart of these concerns is the phenomenon known as \"generative parroting,\" where models produce outputs that are not sufficiently distinct from their training data (Carlini et al., 2023)(Somepalli et al., 2022), leading to the generation of content that closely mimics or even directly copies existing copyrighted materials.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[257050406 | Vyas et al. | 2023 | Citations: 94]": "There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.",
                    "[234777751 | Franceschelli et al. | 2021 | Citations: 64]": "Abstract Machine-generated artworks are now part of the contemporary art scene: they are attracting significant investments and they are presented in exhibitions together with those created by human artists. These artworks are mainly based on generative deep learning (GDL) techniques, which have seen a formidable development and remarkable refinement in the very recent years. Given the inherent characteristics of these techniques, a series of novel legal problems arise. In this article, we consider a set of key questions in the area of GDL for the arts, including the following: is it possible to use copyrighted works as training set for generative models? How do we legally store their copies in order to perform the training process? Who (if someone) will own the copyright on the generated data? We try to answer these questions considering the law in force in both the United States and the European Union, and potential future alternatives. We then extend our analysis to code generation, which is an emerging area of GDL. Finally, we also formulate a set of practical guidelines for artists and developers working on deep learning generated art, as well as some policy suggestions for policymakers.",
                    "[254366634 | Somepalli et al. | 2022 | Citations: 329]": "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html",
                    "[256389993 | Carlini et al. | 2023 | Citations: 617]": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 597,
                        "end": 1096,
                        "sentence_offsets": [
                            {
                                "start": 597,
                                "end": 805
                            },
                            {
                                "start": 805,
                                "end": 1096
                            }
                        ],
                        "ref_mentions": [
                            "234777751",
                            "257050406",
                            "256389993",
                            "254366634"
                        ],
                        "quote": "The rapid adoption and deployment of these technologies have also raised significant ethical, legal, and technical challenges, particularly in the context of copyright infringement and data privacy (Franceschelli et al., 2021)12,(Vyas et al., 2023).At the heart of these concerns is the phenomenon known as \"generative parroting,\" where models produce outputs that are not sufficiently distinct from their training data (Carlini et al., 2023)(Somepalli et al., 2022), leading to the generation of content that closely mimics or even directly copies existing copyrighted materials."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[269033217 | Lu et al. | 2024 | Citations: 8]",
                "snippets": "Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 741,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[270258236 | Chiba-Okabe et al. | 2024 | Citations: 1]",
                "snippets": "Generative models are trained on massive datasets that often contain copyrighted works and are capable of producing outputs closely resembling their training data, potentially resulting in violation of exclusive rights of copyright owners.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 426,
                        "end": 665,
                        "sentence_offsets": [
                            {
                                "start": 426,
                                "end": 667
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Generative models are trained on massive datasets that often contain copyrighted works and are capable of producing outputs closely resembling their training data, potentially resulting in violation of exclusive rights of copyright owners."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[273023255 | Chiba-Okabe | 2024 | Citations: 2]",
                "snippets": "In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content (Carlini et al., 2020)(Carlini et al., 2023)[39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[229156229 | Carlini et al. | 2020 | Citations: 1950]": "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
                    "[256389993 | Carlini et al. | 2023 | Citations: 617]": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                },
                "metadata": [
                    {
                        "section_title": "Risks of Copyright Infringement in Generative AI",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2369,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 303
                            },
                            {
                                "start": 304,
                                "end": 456
                            },
                            {
                                "start": 459,
                                "end": 604
                            },
                            {
                                "start": 605,
                                "end": 750
                            },
                            {
                                "start": 751,
                                "end": 969
                            },
                            {
                                "start": 970,
                                "end": 1200
                            },
                            {
                                "start": 1201,
                                "end": 1520
                            },
                            {
                                "start": 1521,
                                "end": 1689
                            },
                            {
                                "start": 1692,
                                "end": 1932
                            },
                            {
                                "start": 1933,
                                "end": 2237
                            },
                            {
                                "start": 2238,
                                "end": 2369
                            }
                        ],
                        "ref_mentions": [
                            "229156229",
                            "256389993"
                        ],
                        "quote": "In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content (Carlini et al., 2020)(Carlini et al., 2023)[39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[273654195 | Shi et al. | 2024 | Citations: 1]",
                "snippets": "The image generative models require large amount of training data, and it's impractical to only use noncopyrighted content to train the models. The model holder wants to leverage the high quality copyrighted data to learn the style and content and output high quality generated data. However, the generative models may generate some images that are very similar to the copyrighted training data, which may lead to copyright infringement [28].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 846,
                        "end": 1288,
                        "sentence_offsets": [
                            {
                                "start": 846,
                                "end": 989
                            },
                            {
                                "start": 990,
                                "end": 1129
                            },
                            {
                                "start": 1130,
                                "end": 1288
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The image generative models require large amount of training data, and it's impractical to only use noncopyrighted content to train the models. The model holder wants to leverage the high quality copyrighted data to learn the style and content and output high quality generated data. However, the generative models may generate some images that are very similar to the copyrighted training data, which may lead to copyright infringement [28]."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[276575866 | Xu et al. | 2025 | Citations: 1]",
                "snippets": "To mitigate IP infringement, two primary approaches have emerged: \n\n\u2022 Reducing Memorization During Training: Implementing differential privacy (Dwork et al., 2014) techniques during the training of generative models can help minimize the retention of specific data points, thereby reducing the risk of reproducing protected content (Abadi et al., 2016)(Chen et al., 2022)Dockhorn et al., 2022). \n\n\u2022 Prompt Engineering: Employing strategies such as negative prompts during the inference phase can exclude undesired concepts or elements from the generated output (Wang et al., 2024;He et al., 2024), or optimizing unsafe prompts (Chin et al., 2023;Rando et al., 2022), thereby avoiding the inclusion of IP-protected material.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207178262 | Dwork et al. | 2014 | Citations: 7735]": "The problem of privacy-preserving data analysis has a long history spanning multiple disciplines. As electronic data about individuals becomes increasingly detailed, and as technology enables ever more powerful collection and curation of these data, the need increases for a robust, meaningful, and mathematically rigorous definition of privacy, together with a computationally rich class of algorithms that satisfy this definition. Differential Privacy is such a definition.After motivating and discussing the meaning of differential privacy, the preponderance of this monograph is devoted to fundamental techniques for achieving differential privacy, and application of these techniques in creative combinations, using the query-release problem as an ongoing example. A key point is that, by rethinking the computational goal, one can often obtain far better results than would be achieved by methodically replacing each step of a non-private computation with a differentially private implementation. Despite some astonishingly powerful computational results, there are still fundamental limitations \u2014 not just on what can be achieved with differential privacy but on what can be achieved with any method that protects against a complete breakdown in privacy. Virtually all the algorithms discussed herein maintain differential privacy against adversaries of arbitrary computational power. Certain algorithms are computationally intensive, others are efficient. Computational complexity for the adversary and the algorithm are both discussed.We then turn from fundamentals to applications other than queryrelease, discussing differentially private methods for mechanism design and machine learning. The vast majority of the literature on differentially private algorithms considers a single, static, database that is subject to many analyses. Differential privacy in other models, including distributed databases and computations on data streams is discussed.Finally, we note that this work is meant as a thorough introduction to the problems and techniques of differential privacy, but is not intended to be an exhaustive survey \u2014 there is by now a vast amount of work in differential privacy, and we can cover only a small portion of it.",
                    "[207241585 | Abadi et al. | 2016 | Citations: 6165]": "Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",
                    "[250210875 | Chen et al. | 2022 | Citations: 28]": "Despite an increased demand for valuable data, the privacy concerns associated with sensitive datasets present a barrier to data sharing. One may use differentially private generative models to generate synthetic data. Unfortunately, generators are typically restricted to generating images of low-resolutions due to the limitation of noisy gradients. Here, we propose DPGEN, a network model designed to synthesize high-resolution natural images while satisfying differential privacy. In particular, we propose an energy-guided network trained on sanitized data to indicate the direction of the true data distribution via Langevin Markov chain Monte Carlo (MCMC) sampling method. In contrast to the state-of-the-art methods that can process only low-resolution images (e.g., MNIST and Fashion-MNIST), DPGEN can generate differentially private synthetic images with resolutions up to $128\\times 128$ with superior visual quality and data utility. Our code is available at https://github.com/chiamuyu/DPGEN"
                },
                "metadata": [
                    {
                        "section_title": "Vision Language Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 722,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 65
                            },
                            {
                                "start": 68,
                                "end": 393
                            },
                            {
                                "start": 396,
                                "end": 722
                            }
                        ],
                        "ref_mentions": [
                            "207178262",
                            "207241585",
                            "250210875"
                        ],
                        "quote": "To mitigate IP infringement, two primary approaches have emerged: \n\n\u2022 Reducing Memorization During Training: Implementing differential privacy (Dwork et al., 2014) techniques during the training of generative models can help minimize the retention of specific data points, thereby reducing the risk of reproducing protected content (Abadi et al., 2016)(Chen et al., 2022)Dockhorn et al., 2022). \n\n\u2022 Prompt Engineering: Employing strategies such as negative prompts during the inference phase can exclude undesired concepts or elements from the generated output (Wang et al., 2024;He et al., 2024), or optimizing unsafe prompts (Chin et al., 2023;Rando et al., 2022), thereby avoiding the inclusion of IP-protected material."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[278129333 | Chen et al. | 2025 | Citations: 1]",
                "snippets": "Recent research (Carlini et al., 2023)8,(Somepalli et al., 2022)[34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[254366634 | Somepalli et al. | 2022 | Citations: 329]": "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html",
                    "[256389993 | Carlini et al. | 2023 | Citations: 617]": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 357,
                        "end": 918,
                        "sentence_offsets": [
                            {
                                "start": 348,
                                "end": 689
                            },
                            {
                                "start": 690,
                                "end": 918
                            }
                        ],
                        "ref_mentions": [
                            "256389993",
                            "254366634"
                        ],
                        "quote": "Recent research (Carlini et al., 2023)8,(Somepalli et al., 2022)[34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[278338968 | Reissinger et al. | 2025 | Citations: 0]",
                "snippets": "Copyright issues in visual diffusion models arise when models replicate their training data within the image generation process. Therefore, to detect IP infringement, we compare the generated image with the images in the training data. Since training data is huge, a comparison to the entire dataset is prohibitively expensive. We therefore-similar to [25]-limit our comparison to a subset of the training data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Detecting IP Infringement",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 411,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 128
                            },
                            {
                                "start": 129,
                                "end": 235
                            },
                            {
                                "start": 236,
                                "end": 327
                            },
                            {
                                "start": 328,
                                "end": 410
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Copyright issues in visual diffusion models arise when models replicate their training data within the image generation process. Therefore, to detect IP infringement, we compare the generated image with the images in the training data. Since training data is huge, a comparison to the entire dataset is prohibitively expensive. We therefore-similar to [25]-limit our comparison to a subset of the training data."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.021705,
        "cot": "When analyzing the query about how copyright infringement occurs during training versus deployment phases of image generation models, I need to create a comprehensive structure that clearly distinguishes between these two phases and explores the specific mechanisms involved in each.\n\n1. \"Introduction to Copyright Infringement in Image Generation Models\" - This should be a synthesis section that provides background on the general problem of copyright infringement in generative AI models, explaining why this is a significant issue. This will help users understand the context before diving into specific phases.\n\n2. \"Copyright Infringement Mechanisms During Training Phase\" - This should be a synthesis section that explains how copyright issues arise during model training, including data scraping, memorization, and how copyrighted content becomes embedded in model weights.\n\n3. \"Copyright Infringement Mechanisms During Deployment Phase\" - This should be a synthesis section that explores how copyright infringement manifests when models are used to generate images, including reproduction of training data and the challenge of output provenance.\n\n4. \"Technical Differences Between Training and Deployment Infringement\" - This should be a synthesis section that directly compares the technical aspects of how infringement occurs in each phase, highlighting the key differences.\n\n5. \"Concealed Copyright Infringement Methods\" - This should be a synthesis section focusing on more subtle forms of copyright infringement that go beyond direct copying, including backdoor attacks and disguised training data.\n\n6. \"Mitigation Strategies for Copyright Infringement\" - This should be a synthesis section that covers approaches to address copyright concerns in both training and deployment phases.\n\n7. \"Legal Considerations and Liability\" - This should be a synthesis section exploring the legal framework around copyright infringement in AI models, including who bears responsibility during different phases.\n\nEach section will be a synthesis rather than a list because the topic requires complex explanation and analysis rather than enumeration of discrete items. The quotes provide detailed information that can be synthesized into cohesive explanations for each dimension.",
        "plan": {
            "Introduction to Copyright Infringement in Image Generation Models (synthesis)": [
                2,
                6,
                8
            ],
            "Copyright Infringement Mechanisms During Training Phase (synthesis)": [
                0,
                2,
                4,
                5,
                10,
                12
            ],
            "Copyright Infringement Mechanisms During Deployment Phase (synthesis)": [
                0,
                1,
                6,
                9,
                12
            ],
            "Technical Differences Between Training and Deployment Infringement (synthesis)": [
                0,
                3,
                7,
                9
            ],
            "Concealed Copyright Infringement Methods (synthesis)": [
                1,
                3,
                7
            ],
            "Mitigation Strategies for Copyright Infringement (synthesis)": [
                4,
                11,
                13
            ],
            "Legal Considerations and Liability (synthesis)": [
                0,
                9
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Copyright Infringement in Image Generation Models",
                "tldr": "Image generation models have raised significant copyright concerns due to their ability to closely replicate training data in their outputs. These models can memorize and reproduce copyrighted content during both training and deployment phases, creating legal and ethical challenges. (11 sources)",
                "text": "\nThe emergence of advanced generative models has revolutionized image synthesis, enabling users to easily create high-quality artworks and designs <Paper corpusId=\"265551515\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. These technologies, particularly diffusion models, have achieved unprecedented photorealism and creative capabilities <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper> <Paper corpusId=\"248986576\" paperTitle=\"(Saharia et al., 2022)\" isShortName></Paper>. The accessibility of such powerful tools has led to widespread commercial applications and creative possibilities, from text-to-image generation to sophisticated image editing <Paper corpusId=\"251800180\" paperTitle=\"(Ruiz et al., 2022)\" isShortName></Paper> <Paper corpusId=\"253581213\" paperTitle=\"(Brooks et al., 2022)\" isShortName></Paper>.\n\nHowever, these remarkable capabilities come with significant copyright concerns. Research has revealed that generative models can memorize and reproduce specific images from their training data <Paper corpusId=\"256389993\" paperTitle=\"(Carlini et al., 2023)\" isShortName></Paper>. This phenomenon, known as \"generative parroting,\" occurs when models produce outputs that are insufficiently distinct from their training data <Paper corpusId=\"268732888\" paperTitle=\"(Taghanaki et al., 2024)\" isShortName></Paper> <Paper corpusId=\"254366634\" paperTitle=\"(Somepalli et al., 2022)\" isShortName></Paper>. Studies have demonstrated that diffusion models, including popular systems like Stable Diffusion, can sometimes \"blatantly copy\" content from their training datasets <Paper corpusId=\"254366634\" paperTitle=\"(Somepalli et al., 2022)\" isShortName></Paper>.\n\nThe copyright infringement risk is particularly concerning because these generative models are typically trained on massive datasets containing copyrighted works <Paper corpusId=\"270258236\" paperTitle=\"(Chiba-Okabe et al., 2024)\" isShortName></Paper>. When the models reproduce content closely resembling protected works, they potentially violate the exclusive rights of copyright owners <Paper corpusId=\"270258236\" paperTitle=\"(Chiba-Okabe et al., 2024)\" isShortName></Paper>. These concerns extend beyond artistic content to include trademarked logos and even photographs of individual people <Paper corpusId=\"256389993\" paperTitle=\"(Carlini et al., 2023)\" isShortName></Paper>.\n\nThe legal implications of using copyrighted materials for training generative models remain complex and unresolved in many jurisdictions <Paper corpusId=\"234777751\" paperTitle=\"(Franceschelli et al., 2021)\" isShortName></Paper>. As these technologies continue to evolve and gain mainstream adoption, addressing the tension between technological innovation and copyright protection has become increasingly urgent <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>. Research is now focusing on developing approaches that can preserve generative capabilities while reducing the risk of copyright infringement through mechanisms that limit memorization and reproduction of protected content <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "In recent years, the advancement of large generative models [17,(Sohl-Dickstein et al., 2015)(Song et al., 2020) has revolutionized high-quality image synthesis [34,(Rombach et al., 2021)(Saharia et al., 2022), paving the way for commercial applications that enable the public to effortlessly craft their own artworks and designs (Brooks et al., 2022)(Gal et al., 2022)(Kawar et al., 2022)(Lugmayr et al., 2022)(Ruiz et al., 2022)(Saharia et al., 2021). Nevertheless, these models exhibit notable memorization capabilities to produce generations highly similar to the training data (Carlini et al., 2023). This resemblance raises growing concerns about copyright infringement, especially when copyrighted data is used for training [12,16,49](Vyas et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 265551515,
                            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
                            "authors": [
                                {
                                    "authorId": "2269413961",
                                    "name": "Xiang Li"
                                },
                                {
                                    "authorId": "2257038423",
                                    "name": "Qianli Shen"
                                },
                                {
                                    "authorId": "2244295298",
                                    "name": "Kenji Kawaguchi"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 5
                        },
                        "score": 0.5654296875
                    },
                    {
                        "id": "(Rombach et al., 2021)",
                        "snippets": [
                            "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
                        ],
                        "paper": {
                            "corpus_id": 245335280,
                            "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1660819540",
                                    "name": "Robin Rombach"
                                },
                                {
                                    "authorId": "119843260",
                                    "name": "A. Blattmann"
                                },
                                {
                                    "authorId": "2053482699",
                                    "name": "Dominik Lorenz"
                                },
                                {
                                    "authorId": "35175531",
                                    "name": "Patrick Esser"
                                },
                                {
                                    "authorId": "1796707",
                                    "name": "B. Ommer"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 15768
                        },
                        "score": 0
                    },
                    {
                        "id": "(Saharia et al., 2022)",
                        "snippets": [
                            "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results."
                        ],
                        "paper": {
                            "corpus_id": 248986576,
                            "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
                            "authors": [
                                {
                                    "authorId": "51497543",
                                    "name": "Chitwan Saharia"
                                },
                                {
                                    "authorId": "144333684",
                                    "name": "William Chan"
                                },
                                {
                                    "authorId": "2054003577",
                                    "name": "Saurabh Saxena"
                                },
                                {
                                    "authorId": "2111917831",
                                    "name": "Lala Li"
                                },
                                {
                                    "authorId": "21040156",
                                    "name": "Jay Whang"
                                },
                                {
                                    "authorId": "40081727",
                                    "name": "Emily L. Denton"
                                },
                                {
                                    "authorId": "81419386",
                                    "name": "Seyed Kamyar Seyed Ghasemipour"
                                },
                                {
                                    "authorId": "143990191",
                                    "name": "Burcu Karagol Ayan"
                                },
                                {
                                    "authorId": "1982213",
                                    "name": "S. S. Mahdavi"
                                },
                                {
                                    "authorId": "143826364",
                                    "name": "Raphael Gontijo Lopes"
                                },
                                {
                                    "authorId": "2887364",
                                    "name": "Tim Salimans"
                                },
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                },
                                {
                                    "authorId": "1793739",
                                    "name": "David J. Fleet"
                                },
                                {
                                    "authorId": "144739074",
                                    "name": "Mohammad Norouzi"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6075
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ruiz et al., 2022)",
                        "snippets": [
                            "Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for \"personalization\" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/"
                        ],
                        "paper": {
                            "corpus_id": 251800180,
                            "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
                            "authors": [
                                {
                                    "authorId": "31601235",
                                    "name": "Nataniel Ruiz"
                                },
                                {
                                    "authorId": "2167749913",
                                    "name": "Yuanzhen Li"
                                },
                                {
                                    "authorId": "2131639924",
                                    "name": "Varun Jampani"
                                },
                                {
                                    "authorId": "1782328",
                                    "name": "Y. Pritch"
                                },
                                {
                                    "authorId": "144544291",
                                    "name": "Michael Rubinstein"
                                },
                                {
                                    "authorId": "3451442",
                                    "name": "Kfir Aberman"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 2891
                        },
                        "score": 0
                    },
                    {
                        "id": "(Brooks et al., 2022)",
                        "snippets": [
                            "We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions."
                        ],
                        "paper": {
                            "corpus_id": 253581213,
                            "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
                            "authors": [
                                {
                                    "authorId": "2679394",
                                    "name": "Tim Brooks"
                                },
                                {
                                    "authorId": "2248172435",
                                    "name": "Aleksander Holynski"
                                },
                                {
                                    "authorId": "1763086",
                                    "name": "Alexei A. Efros"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 1833
                        },
                        "score": 0
                    },
                    {
                        "id": "(Carlini et al., 2023)",
                        "snippets": [
                            "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                        ],
                        "paper": {
                            "corpus_id": 256389993,
                            "title": "Extracting Training Data from Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "9200194",
                                    "name": "Jamie Hayes"
                                },
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "3482535",
                                    "name": "Vikash Sehwag"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "1718064",
                                    "name": "Borja Balle"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                }
                            ],
                            "year": 2023,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 617
                        },
                        "score": 0
                    },
                    {
                        "id": "(Taghanaki et al., 2024)",
                        "snippets": [
                            "The rapid adoption and deployment of these technologies have also raised significant ethical, legal, and technical challenges, particularly in the context of copyright infringement and data privacy (Franceschelli et al., 2021)12,(Vyas et al., 2023).At the heart of these concerns is the phenomenon known as \"generative parroting,\" where models produce outputs that are not sufficiently distinct from their training data (Carlini et al., 2023)(Somepalli et al., 2022), leading to the generation of content that closely mimics or even directly copies existing copyrighted materials."
                        ],
                        "paper": {
                            "corpus_id": 268732888,
                            "title": "Detecting Generative Parroting through Overfitting Masked Autoencoders",
                            "authors": [
                                {
                                    "authorId": "17803311",
                                    "name": "Saeid Asgari Taghanaki"
                                },
                                {
                                    "authorId": "2293723050",
                                    "name": "Joseph Lambourne"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.60107421875
                    },
                    {
                        "id": "(Somepalli et al., 2022)",
                        "snippets": [
                            "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html"
                        ],
                        "paper": {
                            "corpus_id": 254366634,
                            "title": "Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2003112028",
                                    "name": "Gowthami Somepalli"
                                },
                                {
                                    "authorId": "1824188732",
                                    "name": "Vasu Singla"
                                },
                                {
                                    "authorId": "121592562",
                                    "name": "Micah Goldblum"
                                },
                                {
                                    "authorId": "8284185",
                                    "name": "Jonas Geiping"
                                },
                                {
                                    "authorId": "1962083",
                                    "name": "T. Goldstein"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 329
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chiba-Okabe et al., 2024)",
                        "snippets": [
                            "Generative models are trained on massive datasets that often contain copyrighted works and are capable of producing outputs closely resembling their training data, potentially resulting in violation of exclusive rights of copyright owners."
                        ],
                        "paper": {
                            "corpus_id": 270258236,
                            "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
                            "authors": [
                                {
                                    "authorId": "2297800322",
                                    "name": "Hiroaki Chiba-Okabe"
                                },
                                {
                                    "authorId": "2278306561",
                                    "name": "Weijie J. Su"
                                }
                            ],
                            "year": 2024,
                            "venue": "Scientific Reports",
                            "n_citations": 1
                        },
                        "score": 0.51171875
                    },
                    {
                        "id": "(Franceschelli et al., 2021)",
                        "snippets": [
                            "Abstract Machine-generated artworks are now part of the contemporary art scene: they are attracting significant investments and they are presented in exhibitions together with those created by human artists. These artworks are mainly based on generative deep learning (GDL) techniques, which have seen a formidable development and remarkable refinement in the very recent years. Given the inherent characteristics of these techniques, a series of novel legal problems arise. In this article, we consider a set of key questions in the area of GDL for the arts, including the following: is it possible to use copyrighted works as training set for generative models? How do we legally store their copies in order to perform the training process? Who (if someone) will own the copyright on the generated data? We try to answer these questions considering the law in force in both the United States and the European Union, and potential future alternatives. We then extend our analysis to code generation, which is an emerging area of GDL. Finally, we also formulate a set of practical guidelines for artists and developers working on deep learning generated art, as well as some policy suggestions for policymakers."
                        ],
                        "paper": {
                            "corpus_id": 234777751,
                            "title": "Copyright in generative deep learning",
                            "authors": [
                                {
                                    "authorId": "2067291198",
                                    "name": "Giorgio Franceschelli"
                                },
                                {
                                    "authorId": "1806767",
                                    "name": "Mirco Musolesi"
                                }
                            ],
                            "year": 2021,
                            "venue": "Data & Policy",
                            "n_citations": 64
                        },
                        "score": 0
                    },
                    {
                        "id": "(Vyas et al., 2023)",
                        "snippets": [
                            "Copyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material."
                        ],
                        "paper": {
                            "corpus_id": 257050406,
                            "title": "On Provable Copyright Protection for Generative Models",
                            "authors": [
                                {
                                    "authorId": "145603901",
                                    "name": "Nikhil Vyas"
                                },
                                {
                                    "authorId": "144695232",
                                    "name": "S. Kakade"
                                },
                                {
                                    "authorId": "1697211",
                                    "name": "B. Barak"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 94
                        },
                        "score": 0.666015625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Copyright Infringement Mechanisms During Training Phase",
                "tldr": "During the training phase, copyright infringement occurs when models learn from and memorize copyrighted content without proper authorization. This form of infringement involves embedding copyrighted material within model weights and parameters, creating both privacy and intellectual property concerns. (10 sources)",
                "text": "\nThe training phase of image generation models presents distinct copyright challenges that begin with data acquisition. These models require massive datasets that often include copyrighted images scraped from various sources, making it \"impractical to only use noncopyrighted content\" <Paper corpusId=\"273654195\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. The fundamental infringement mechanism during training is the unauthorized incorporation of copyrighted materials into the model's weights and parameters.\n\nRecent research has revealed that diffusion models and other generative architectures can memorize training data to a concerning degree <Paper corpusId=\"256389993\" paperTitle=\"(Carlini et al., 2023)\" isShortName></Paper> <Paper corpusId=\"254366634\" paperTitle=\"(Somepalli et al., 2022)\" isShortName></Paper>. This memorization can manifest in two primary forms: \"local memorization,\" where models reproduce specific parts of images such as foregrounds or backgrounds, and \"global memorization,\" where entire copyrighted images are encoded within the model's parameters <Paper corpusId=\"278129333\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>. Memorization is particularly problematic because it occurs implicitly during the normal training process without special techniques designed to deliberately copy content.\n\nThe scale and diversity of training data exacerbate this issue. Advanced image models like Stable Diffusion are trained on billions of images, making it virtually impossible to verify copyright status for every training sample <Paper corpusId=\"265551515\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. The technological innovations that enable these models to produce high-quality, photorealistic images\u2014particularly diffusion models\u2014are the same capabilities that increase copyright risks <Paper corpusId=\"265551515\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"14888175\" paperTitle=\"(Sohl-Dickstein et al., 2015)\" isShortName></Paper> <Paper corpusId=\"227209335\" paperTitle=\"(Song et al., 2020)\" isShortName></Paper>.\n\nThe legal analysis of training-phase infringement often centers around fair use doctrine, which varies significantly between jurisdictions <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>. While some jurisdictions may permit limited use of copyrighted materials for model training under fair use provisions, this remains a complex and evolving legal question without clear resolution.\n\nFrom a technical perspective, training-phase protections are beginning to emerge. Some researchers propose \"training-stage protection\" mechanisms that add imperceptible noise to copyrighted images specifically to disrupt the training processes of unauthorized models <Paper corpusId=\"267412857\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper>. Similarly, watermarking techniques for deep generative models are being developed to establish clear ownership of model architecture and training data <Paper corpusId=\"267412857\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper>.\n\nThe problems of memorization during training extend beyond copyright concerns to include privacy issues, especially when training data contains personal information or identifiable images of individuals <Paper corpusId=\"268201483\" paperTitle=\"(Manduchi et al., 2024)\" isShortName></Paper>. These dual concerns\u2014privacy and copyright\u2014create significant technical and legal challenges for developers and deployers of generative AI systems.",
                "citations": [
                    {
                        "id": "(Shi et al., 2024)",
                        "snippets": [
                            "The image generative models require large amount of training data, and it's impractical to only use noncopyrighted content to train the models. The model holder wants to leverage the high quality copyrighted data to learn the style and content and output high quality generated data. However, the generative models may generate some images that are very similar to the copyrighted training data, which may lead to copyright infringement [28]."
                        ],
                        "paper": {
                            "corpus_id": 273654195,
                            "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
                            "authors": [
                                {
                                    "authorId": "2319419519",
                                    "name": "Zhuan Shi"
                                },
                                {
                                    "authorId": "2328664079",
                                    "name": "Yifei Song"
                                },
                                {
                                    "authorId": "2318236128",
                                    "name": "Xiaoli Tang"
                                },
                                {
                                    "authorId": "2287820224",
                                    "name": "Lingjuan Lyu"
                                },
                                {
                                    "authorId": "2054858128",
                                    "name": "Boi Faltings"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.54052734375
                    },
                    {
                        "id": "(Carlini et al., 2023)",
                        "snippets": [
                            "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                        ],
                        "paper": {
                            "corpus_id": 256389993,
                            "title": "Extracting Training Data from Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "9200194",
                                    "name": "Jamie Hayes"
                                },
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "3482535",
                                    "name": "Vikash Sehwag"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "1718064",
                                    "name": "Borja Balle"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                }
                            ],
                            "year": 2023,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 617
                        },
                        "score": 0
                    },
                    {
                        "id": "(Somepalli et al., 2022)",
                        "snippets": [
                            "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html"
                        ],
                        "paper": {
                            "corpus_id": 254366634,
                            "title": "Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2003112028",
                                    "name": "Gowthami Somepalli"
                                },
                                {
                                    "authorId": "1824188732",
                                    "name": "Vasu Singla"
                                },
                                {
                                    "authorId": "121592562",
                                    "name": "Micah Goldblum"
                                },
                                {
                                    "authorId": "8284185",
                                    "name": "Jonas Geiping"
                                },
                                {
                                    "authorId": "1962083",
                                    "name": "T. Goldstein"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 329
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2025)",
                        "snippets": [
                            "Recent research (Carlini et al., 2023)8,(Somepalli et al., 2022)[34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content."
                        ],
                        "paper": {
                            "corpus_id": 278129333,
                            "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1696291",
                                    "name": "Chen Chen"
                                },
                                {
                                    "authorId": "51023221",
                                    "name": "Daochang Liu"
                                },
                                {
                                    "authorId": "2302950741",
                                    "name": "Mubarak Shah"
                                },
                                {
                                    "authorId": "2288626806",
                                    "name": "Chang Xu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.6982421875
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "In recent years, the advancement of large generative models [17,(Sohl-Dickstein et al., 2015)(Song et al., 2020) has revolutionized high-quality image synthesis [34,(Rombach et al., 2021)(Saharia et al., 2022), paving the way for commercial applications that enable the public to effortlessly craft their own artworks and designs (Brooks et al., 2022)(Gal et al., 2022)(Kawar et al., 2022)(Lugmayr et al., 2022)(Ruiz et al., 2022)(Saharia et al., 2021). Nevertheless, these models exhibit notable memorization capabilities to produce generations highly similar to the training data (Carlini et al., 2023). This resemblance raises growing concerns about copyright infringement, especially when copyrighted data is used for training [12,16,49](Vyas et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 265551515,
                            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
                            "authors": [
                                {
                                    "authorId": "2269413961",
                                    "name": "Xiang Li"
                                },
                                {
                                    "authorId": "2257038423",
                                    "name": "Qianli Shen"
                                },
                                {
                                    "authorId": "2244295298",
                                    "name": "Kenji Kawaguchi"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 5
                        },
                        "score": 0.5654296875
                    },
                    {
                        "id": "(Sohl-Dickstein et al., 2015)",
                        "snippets": [
                            "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm."
                        ],
                        "paper": {
                            "corpus_id": 14888175,
                            "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
                            "authors": [
                                {
                                    "authorId": "1407546424",
                                    "name": "Jascha Narain Sohl-Dickstein"
                                },
                                {
                                    "authorId": "2144479710",
                                    "name": "Eric A. Weiss"
                                },
                                {
                                    "authorId": "2333223",
                                    "name": "Niru Maheswaranathan"
                                },
                                {
                                    "authorId": "25769960",
                                    "name": "S. Ganguli"
                                }
                            ],
                            "year": 2015,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 7030
                        },
                        "score": 0
                    },
                    {
                        "id": "(Song et al., 2020)",
                        "snippets": [
                            "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model."
                        ],
                        "paper": {
                            "corpus_id": 227209335,
                            "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
                            "authors": [
                                {
                                    "authorId": "115504645",
                                    "name": "Yang Song"
                                },
                                {
                                    "authorId": "1407546424",
                                    "name": "Jascha Narain Sohl-Dickstein"
                                },
                                {
                                    "authorId": "1726807",
                                    "name": "Diederik P. Kingma"
                                },
                                {
                                    "authorId": "2109224633",
                                    "name": "Abhishek Kumar"
                                },
                                {
                                    "authorId": "2490652",
                                    "name": "Stefano Ermon"
                                },
                                {
                                    "authorId": "16443937",
                                    "name": "Ben Poole"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 6585
                        },
                        "score": 0
                    },
                    {
                        "id": "(Vyas et al., 2023)",
                        "snippets": [
                            "Copyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material."
                        ],
                        "paper": {
                            "corpus_id": 257050406,
                            "title": "On Provable Copyright Protection for Generative Models",
                            "authors": [
                                {
                                    "authorId": "145603901",
                                    "name": "Nikhil Vyas"
                                },
                                {
                                    "authorId": "144695232",
                                    "name": "S. Kakade"
                                },
                                {
                                    "authorId": "1697211",
                                    "name": "B. Barak"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 94
                        },
                        "score": 0.666015625
                    },
                    {
                        "id": "(Ren et al., 2024)",
                        "snippets": [
                            "The possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works. \n\nDifferent from directly employing DGMs to generate new images, DGMs are also usually trained or fine-tuned on some source images to effectively learn useful information from for future generations. Therefore, \"training-stage protection\" aims to add imperceptible noise to the copyrighted images, to break the training process of potential DGMs for data copyright protection.\n\nDeep Generative Model Watermarking is a common solution for model copyright protection. It involves incorporating distinct information, known as a watermark, into the models before their deployment. The embedded watermark can be retrieved from a potentially infringing model or its generated data to confirm any suspected copyright violations."
                        ],
                        "paper": {
                            "corpus_id": 267412857,
                            "title": "Copyright Protection in Generative AI: A Technical Perspective",
                            "authors": [
                                {
                                    "authorId": "2256589810",
                                    "name": "Jie Ren"
                                },
                                {
                                    "authorId": "2253881697",
                                    "name": "Han Xu"
                                },
                                {
                                    "authorId": "2185740224",
                                    "name": "Pengfei He"
                                },
                                {
                                    "authorId": "2218740984",
                                    "name": "Yingqian Cui"
                                },
                                {
                                    "authorId": "2253682835",
                                    "name": "Shenglai Zeng"
                                },
                                {
                                    "authorId": "2282560420",
                                    "name": "Jiankun Zhang"
                                },
                                {
                                    "authorId": "2256788829",
                                    "name": "Hongzhi Wen"
                                },
                                {
                                    "authorId": "46496977",
                                    "name": "Jiayuan Ding"
                                },
                                {
                                    "authorId": "2253533415",
                                    "name": "Hui Liu"
                                },
                                {
                                    "authorId": "2267019992",
                                    "name": "Yi Chang"
                                },
                                {
                                    "authorId": "2115879611",
                                    "name": "Jiliang Tang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 42
                        },
                        "score": 0.5537109375
                    },
                    {
                        "id": "(Manduchi et al., 2024)",
                        "snippets": [
                            "Interestingly, recent works show that publicly available LLMs and large-scale text-to-image models can implicitly \"memorize\" training data, in the sense that points from the dataset can be (almost exactly) reconstructed, which potentially infringes on data privacy (Carlini et al., 2023)(Carlini et al., 2023)(Somepalli et al., 2022)[136]",
                            "In addition to privacy, recent advances in large-scale generative modeling can lead to unauthorized distribution or replication of training data resulting in copyright infringement liabilities",
                            "there are several outstanding technical challenges ranging from mitigating copyright infringements during dataset curation [24] to reliable detection of copyright violations in generated samples."
                        ],
                        "paper": {
                            "corpus_id": 268201483,
                            "title": "On the Challenges and Opportunities in Generative AI",
                            "authors": [
                                {
                                    "authorId": "11180167",
                                    "name": "Laura Manduchi"
                                },
                                {
                                    "authorId": "2054151596",
                                    "name": "Kushagra Pandey"
                                },
                                {
                                    "authorId": "36347405",
                                    "name": "Robert Bamler"
                                },
                                {
                                    "authorId": "2326993383",
                                    "name": "Ryan Cotterell"
                                },
                                {
                                    "authorId": "2289611494",
                                    "name": "Sina Daubener"
                                },
                                {
                                    "authorId": "2161485495",
                                    "name": "Sophie Fellenz"
                                },
                                {
                                    "authorId": "2280912684",
                                    "name": "Asja Fischer"
                                },
                                {
                                    "authorId": "2289611496",
                                    "name": "Thomas Gartner"
                                },
                                {
                                    "authorId": "7965286",
                                    "name": "Matthias Kirchler"
                                },
                                {
                                    "authorId": "2749512",
                                    "name": "M. Kloft"
                                },
                                {
                                    "authorId": "2258745503",
                                    "name": "Yingzhen Li"
                                },
                                {
                                    "authorId": "2261493650",
                                    "name": "Christoph Lippert"
                                },
                                {
                                    "authorId": "2289612000",
                                    "name": "Gerard de Melo"
                                },
                                {
                                    "authorId": "2268322296",
                                    "name": "Eric T. Nalisnick"
                                },
                                {
                                    "authorId": "2289612053",
                                    "name": "Bjorn Ommer"
                                },
                                {
                                    "authorId": "2289611796",
                                    "name": "Rajesh Ranganath"
                                },
                                {
                                    "authorId": "2258717579",
                                    "name": "Maja Rudolph"
                                },
                                {
                                    "authorId": "2260806834",
                                    "name": "Karen Ullrich"
                                },
                                {
                                    "authorId": "1749506",
                                    "name": "Guy Van den Broeck"
                                },
                                {
                                    "authorId": "2260809546",
                                    "name": "Julia E Vogt"
                                },
                                {
                                    "authorId": "2289783943",
                                    "name": "Yixin Wang"
                                },
                                {
                                    "authorId": "39798982",
                                    "name": "F. Wenzel"
                                },
                                {
                                    "authorId": "2289610270",
                                    "name": "Frank Wood"
                                },
                                {
                                    "authorId": "2258707737",
                                    "name": "Stephan Mandt"
                                },
                                {
                                    "authorId": "41031794",
                                    "name": "Vincent Fortuin"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 22
                        },
                        "score": 0.50341796875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Copyright Infringement Mechanisms During Deployment Phase",
                "tldr": "During deployment, copyright infringement occurs when image generation models produce outputs that replicate or closely mimic copyrighted works in response to user prompts. This phase creates unique legal challenges because the model doesn't track the provenance of its outputs, making it difficult for users to verify if generated content infringes on existing copyrights. (9 sources)",
                "text": "\nThe deployment phase presents distinct copyright infringement mechanisms that differ from those in the training phase. During deployment, infringement occurs when a user provides a prompt to the model and receives an output that either directly copies or is substantially similar to copyrighted material from the training data <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>. Unlike search engines that maintain provenance information, generative models don't track the sources of their outputs, leaving users without a straightforward way to verify if generated content infringes on existing copyrights <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>.\n\nResearch has revealed that users can trigger copyright infringement even without intentionally attempting to replicate protected works. Recent studies have expanded beyond examining direct copyright-related prompts to investigate \"subtler forms of infringement, where even indirect prompts can trigger copyright issues\" <Paper corpusId=\"265352103\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>. This unintentional infringement occurs because diffusion models can memorize and reproduce training data during inference, generating content that incorporates elements from copyrighted materials without explicit instructions to do so <Paper corpusId=\"278129333\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper> <Paper corpusId=\"254366634\" paperTitle=\"(Somepalli et al., 2022)\" isShortName></Paper> <Paper corpusId=\"256389993\" paperTitle=\"(Carlini et al., 2023)\" isShortName></Paper>.\n\nThe legal landscape around deployment-phase infringement introduces complex questions of liability. Both the user who prompts the model and the AI developer who provides the service could potentially be considered direct or indirect infringers, depending on the specific circumstances <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>. Importantly, copyright infringement may occur even when neither the user nor the developer intentionally aims to replicate copyrighted material <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>.\n\nEstablishing copyright infringement for AI-generated content faces evidentiary challenges similar to traditional copyright cases. Due to the \"uninterpretability of generative models,\" direct evidence of copying is difficult to obtain <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>. Instead, access and similarity typically serve as primary evidence of infringement <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>. The inclusion of copyrighted works in training data may constitute sufficient \"access\" from a legal perspective, particularly since studies have demonstrated that exposure to specific content during training correlates with a higher probability of producing similar outputs <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper> <Paper corpusId=\"229156229\" paperTitle=\"(Carlini et al., 2020)\" isShortName></Paper> <Paper corpusId=\"256389993\" paperTitle=\"(Carlini et al., 2023)\" isShortName></Paper>.\n\nA particularly concerning aspect of deployment-phase infringement is that latent diffusion models can produce images similar to copyrighted works even without direct inclusion of those works in the training data. These models can reproduce content from other images that retain similar latent information, further complicating the determination of infringement <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>. This capability, combined with the \"generative parroting\" phenomenon where models produce outputs insufficiently distinct from their training data, creates significant ethical, legal, and technical challenges <Paper corpusId=\"268732888\" paperTitle=\"(Taghanaki et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper> <Paper corpusId=\"234777751\" paperTitle=\"(Franceschelli et al., 2021)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Vyas et al., 2023)",
                        "snippets": [
                            "Copyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material."
                        ],
                        "paper": {
                            "corpus_id": 257050406,
                            "title": "On Provable Copyright Protection for Generative Models",
                            "authors": [
                                {
                                    "authorId": "145603901",
                                    "name": "Nikhil Vyas"
                                },
                                {
                                    "authorId": "144695232",
                                    "name": "S. Kakade"
                                },
                                {
                                    "authorId": "1697211",
                                    "name": "B. Barak"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 94
                        },
                        "score": 0.666015625
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues."
                        ],
                        "paper": {
                            "corpus_id": 265352103,
                            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2267877984",
                                    "name": "Yang Zhang"
                                },
                                {
                                    "authorId": "2267728071",
                                    "name": "Teoh Tze Tzun"
                                },
                                {
                                    "authorId": "2267727392",
                                    "name": "Lim Wei Hern"
                                },
                                {
                                    "authorId": "2267866973",
                                    "name": "Haonan Wang"
                                },
                                {
                                    "authorId": "2256995496",
                                    "name": "Kenji Kawaguchi"
                                }
                            ],
                            "year": 2023,
                            "venue": "",
                            "n_citations": 10
                        },
                        "score": 0.716796875
                    },
                    {
                        "id": "(Chen et al., 2025)",
                        "snippets": [
                            "Recent research (Carlini et al., 2023)8,(Somepalli et al., 2022)[34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content."
                        ],
                        "paper": {
                            "corpus_id": 278129333,
                            "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1696291",
                                    "name": "Chen Chen"
                                },
                                {
                                    "authorId": "51023221",
                                    "name": "Daochang Liu"
                                },
                                {
                                    "authorId": "2302950741",
                                    "name": "Mubarak Shah"
                                },
                                {
                                    "authorId": "2288626806",
                                    "name": "Chang Xu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.6982421875
                    },
                    {
                        "id": "(Somepalli et al., 2022)",
                        "snippets": [
                            "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html"
                        ],
                        "paper": {
                            "corpus_id": 254366634,
                            "title": "Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2003112028",
                                    "name": "Gowthami Somepalli"
                                },
                                {
                                    "authorId": "1824188732",
                                    "name": "Vasu Singla"
                                },
                                {
                                    "authorId": "121592562",
                                    "name": "Micah Goldblum"
                                },
                                {
                                    "authorId": "8284185",
                                    "name": "Jonas Geiping"
                                },
                                {
                                    "authorId": "1962083",
                                    "name": "T. Goldstein"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 329
                        },
                        "score": 0
                    },
                    {
                        "id": "(Carlini et al., 2023)",
                        "snippets": [
                            "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                        ],
                        "paper": {
                            "corpus_id": 256389993,
                            "title": "Extracting Training Data from Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "9200194",
                                    "name": "Jamie Hayes"
                                },
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "3482535",
                                    "name": "Vikash Sehwag"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "1718064",
                                    "name": "Borja Balle"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                }
                            ],
                            "year": 2023,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 617
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chiba-Okabe, 2024)",
                        "snippets": [
                            "In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content (Carlini et al., 2020)(Carlini et al., 2023)[39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works."
                        ],
                        "paper": {
                            "corpus_id": 273023255,
                            "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety",
                            "authors": [
                                {
                                    "authorId": "2297800322",
                                    "name": "Hiroaki Chiba-Okabe"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.56689453125
                    },
                    {
                        "id": "(Carlini et al., 2020)",
                        "snippets": [
                            "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models."
                        ],
                        "paper": {
                            "corpus_id": 229156229,
                            "title": "Extracting Training Data from Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "1404060687",
                                    "name": "Ariel Herbert-Voss"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "31035595",
                                    "name": "Tom B. Brown"
                                },
                                {
                                    "authorId": "143711382",
                                    "name": "D. Song"
                                },
                                {
                                    "authorId": "1758110",
                                    "name": "\u00da. Erlingsson"
                                },
                                {
                                    "authorId": "3046437",
                                    "name": "Alina Oprea"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2020,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 1950
                        },
                        "score": 0
                    },
                    {
                        "id": "(Taghanaki et al., 2024)",
                        "snippets": [
                            "The rapid adoption and deployment of these technologies have also raised significant ethical, legal, and technical challenges, particularly in the context of copyright infringement and data privacy (Franceschelli et al., 2021)12,(Vyas et al., 2023).At the heart of these concerns is the phenomenon known as \"generative parroting,\" where models produce outputs that are not sufficiently distinct from their training data (Carlini et al., 2023)(Somepalli et al., 2022), leading to the generation of content that closely mimics or even directly copies existing copyrighted materials."
                        ],
                        "paper": {
                            "corpus_id": 268732888,
                            "title": "Detecting Generative Parroting through Overfitting Masked Autoencoders",
                            "authors": [
                                {
                                    "authorId": "17803311",
                                    "name": "Saeid Asgari Taghanaki"
                                },
                                {
                                    "authorId": "2293723050",
                                    "name": "Joseph Lambourne"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.60107421875
                    },
                    {
                        "id": "(Franceschelli et al., 2021)",
                        "snippets": [
                            "Abstract Machine-generated artworks are now part of the contemporary art scene: they are attracting significant investments and they are presented in exhibitions together with those created by human artists. These artworks are mainly based on generative deep learning (GDL) techniques, which have seen a formidable development and remarkable refinement in the very recent years. Given the inherent characteristics of these techniques, a series of novel legal problems arise. In this article, we consider a set of key questions in the area of GDL for the arts, including the following: is it possible to use copyrighted works as training set for generative models? How do we legally store their copies in order to perform the training process? Who (if someone) will own the copyright on the generated data? We try to answer these questions considering the law in force in both the United States and the European Union, and potential future alternatives. We then extend our analysis to code generation, which is an emerging area of GDL. Finally, we also formulate a set of practical guidelines for artists and developers working on deep learning generated art, as well as some policy suggestions for policymakers."
                        ],
                        "paper": {
                            "corpus_id": 234777751,
                            "title": "Copyright in generative deep learning",
                            "authors": [
                                {
                                    "authorId": "2067291198",
                                    "name": "Giorgio Franceschelli"
                                },
                                {
                                    "authorId": "1806767",
                                    "name": "Mirco Musolesi"
                                }
                            ],
                            "year": 2021,
                            "venue": "Data & Policy",
                            "n_citations": 64
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Technical Differences Between Training and Deployment Infringement",
                "tldr": "Training-phase infringement involves encoding copyrighted content within model weights through data memorization, while deployment-phase infringement occurs when models generate outputs resembling protected works in response to user prompts. These phases differ in their technical mechanisms, liability considerations, and the methods used to detect or conceal potential copyright violations. (6 sources)",
                "text": "\nThe technical mechanisms of copyright infringement differ significantly between the training and deployment phases of image generation models. During the training phase, infringement occurs when algorithms directly access copyrighted material, with the learned model potentially storing \"verbatim copies of some of this material\" implicitly encoded in its weights <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>. This form of infringement primarily involves the model developers who collect and use the training data, raising questions about fair use that parallel those in other contexts like search engine indexing and book digitization <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>.\n\nIn contrast, deployment-phase infringement presents a fundamentally different technical challenge. Here, a user provides a prompt to the model and receives an output that may be \"either a verbatim copy or substantially similar to some copyrighted training data\" <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>. Unlike training-phase infringement, deployment involves an interaction between the user's prompt and the model's learned parameters, creating a more complex chain of causality for copyright violations.\n\nA key technical distinction between these phases lies in how diffusion models process inputs. During training, these models accept text-image pairs, whereas during inference (deployment), they typically only receive text prompts <Paper corpusId=\"266900037\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. This difference in input processing creates unique vulnerability patterns in each phase. For example, backdoor attacks\u2014which are designed to induce copyright-infringing outputs\u2014must account for this asymmetry, with triggers implemented as specific prompts during deployment while backdoors are poisoned image-caption pairs during training <Paper corpusId=\"266900037\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nThe detection of copyright infringement also differs technically between phases. Training-phase infringement might be identified through inspection of the training dataset, though this approach has significant limitations <Paper corpusId=\"269033217\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. Deployment-phase infringement is typically detected through output similarity to known copyrighted works, but establishing a causal link to training data remains challenging due to the \"uninterpretability of generative models\" <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>.\n\nRecent research has identified more sophisticated mechanisms for concealed copyright infringement that exploit the technical differences between phases. For instance, latent diffusion models can produce images similar to copyrighted works even without direct inclusion of those works in the training data by reproducing content from other images with similar latent information <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>. Additionally, it's possible to construct visual \"disguises\" that appear drastically different from copyrighted samples yet still induce the effect of training on those samples, effectively circumventing visual auditing tools <Paper corpusId=\"269033217\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>.\n\nFrom an evidentiary perspective, the technical distinctions between phases create different challenges for establishing infringement. While training-phase infringement might be demonstrated through direct analysis of the training process, deployment-phase infringement typically relies on \"access and similarity\" as primary evidence <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>. Studies have shown that exposure to specific content during training correlates with a higher probability of producing similar outputs <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper> <Paper corpusId=\"229156229\" paperTitle=\"(Carlini et al., 2020)\" isShortName></Paper> <Paper corpusId=\"256389993\" paperTitle=\"(Carlini et al., 2023)\" isShortName></Paper>, suggesting that inclusion of copyrighted works in training data may constitute sufficient \"access\" from a legal standpoint, even without explicit memorization.",
                "citations": [
                    {
                        "id": "(Vyas et al., 2023)",
                        "snippets": [
                            "Copyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material."
                        ],
                        "paper": {
                            "corpus_id": 257050406,
                            "title": "On Provable Copyright Protection for Generative Models",
                            "authors": [
                                {
                                    "authorId": "145603901",
                                    "name": "Nikhil Vyas"
                                },
                                {
                                    "authorId": "144695232",
                                    "name": "S. Kakade"
                                },
                                {
                                    "authorId": "1697211",
                                    "name": "B. Barak"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 94
                        },
                        "score": 0.666015625
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "In the context of copyright, a backdoor attack against diffusion models is a type of security attack designed to induce diffusion models to reproduce copyright-infringing outputs by manipulating the clean training dataset",
                            "In our approach, the trigger is a specific prompt, while the backdoors are poisoned image-caption pairs. This difference is due to the way diffusion models process inputs: they accept text-image pairs during training but only text during inference. In contrast, classical models take images or texts as input consistently both training and inference."
                        ],
                        "paper": {
                            "corpus_id": 266900037,
                            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
                            "authors": [
                                {
                                    "authorId": "2267866973",
                                    "name": "Haonan Wang"
                                },
                                {
                                    "authorId": "2257038423",
                                    "name": "Qianli Shen"
                                },
                                {
                                    "authorId": "2278794984",
                                    "name": "Yao Tong"
                                },
                                {
                                    "authorId": "2267877984",
                                    "name": "Yang Zhang"
                                },
                                {
                                    "authorId": "2256995496",
                                    "name": "Kenji Kawaguchi"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 32
                        },
                        "score": 0.6826171875
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools."
                        ],
                        "paper": {
                            "corpus_id": 269033217,
                            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2275053301",
                                    "name": "Yiwei Lu"
                                },
                                {
                                    "authorId": "2284800079",
                                    "name": "Matthew Y.R. Yang"
                                },
                                {
                                    "authorId": "2295948127",
                                    "name": "Zuoqiu Liu"
                                },
                                {
                                    "authorId": "2284763541",
                                    "name": "Gautam Kamath"
                                },
                                {
                                    "authorId": "2274963165",
                                    "name": "Yaoliang Yu"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 8
                        },
                        "score": 0.556640625
                    },
                    {
                        "id": "(Chiba-Okabe, 2024)",
                        "snippets": [
                            "In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content (Carlini et al., 2020)(Carlini et al., 2023)[39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works."
                        ],
                        "paper": {
                            "corpus_id": 273023255,
                            "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety",
                            "authors": [
                                {
                                    "authorId": "2297800322",
                                    "name": "Hiroaki Chiba-Okabe"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.56689453125
                    },
                    {
                        "id": "(Carlini et al., 2020)",
                        "snippets": [
                            "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models."
                        ],
                        "paper": {
                            "corpus_id": 229156229,
                            "title": "Extracting Training Data from Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "1404060687",
                                    "name": "Ariel Herbert-Voss"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "31035595",
                                    "name": "Tom B. Brown"
                                },
                                {
                                    "authorId": "143711382",
                                    "name": "D. Song"
                                },
                                {
                                    "authorId": "1758110",
                                    "name": "\u00da. Erlingsson"
                                },
                                {
                                    "authorId": "3046437",
                                    "name": "Alina Oprea"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2020,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 1950
                        },
                        "score": 0
                    },
                    {
                        "id": "(Carlini et al., 2023)",
                        "snippets": [
                            "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                        ],
                        "paper": {
                            "corpus_id": 256389993,
                            "title": "Extracting Training Data from Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "9200194",
                                    "name": "Jamie Hayes"
                                },
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "3482535",
                                    "name": "Vikash Sehwag"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "1718064",
                                    "name": "Borja Balle"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                }
                            ],
                            "year": 2023,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 617
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Concealed Copyright Infringement Methods",
                "tldr": "Beyond obvious copyright violations, sophisticated concealed infringement methods allow malicious actors to bypass traditional safeguards. These techniques include triggering copyright infringement through indirect prompts, implementing backdoor attacks, and creating visual disguises that induce the effect of training on copyrighted content while evading detection. (3 sources)",
                "text": "\nRecent research has uncovered increasingly sophisticated methods for concealing copyright infringement in image generation models. While early copyright studies focused primarily on direct, obvious violations, newer research examines \"subtler forms of infringement, where even indirect prompts can trigger copyright issues\" <Paper corpusId=\"265352103\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>. This means that even seemingly innocuous user prompts that don't explicitly reference copyrighted content can still produce outputs that infringe on protected works.\n\nA particularly concerning method involves backdoor attacks specifically designed to induce copyright violations. These attacks work by \"manipulating the clean training dataset\" to create hidden vulnerabilities that can later be exploited during deployment <Paper corpusId=\"266900037\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. The technical implementation of these attacks exploits the asymmetry between training and deployment phases: \"the trigger is a specific prompt, while the backdoors are poisoned image-caption pairs\" <Paper corpusId=\"266900037\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. This approach takes advantage of how diffusion models process different types of inputs during training versus inference, making the backdoors particularly difficult to detect.\n\nPerhaps most alarming are visual disguise techniques that can completely circumvent current copyright auditing mechanisms. Researchers have demonstrated that it's possible to construct \"a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it\" <Paper corpusId=\"269033217\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. These disguised training samples only require \"indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools\" <Paper corpusId=\"269033217\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This means that traditional approaches to detecting copyright infringement through visual inspection of training data are insufficient against sophisticated concealment methods.\n\nThe evolution of these concealed copyright infringement techniques poses significant challenges for developing effective safeguards. As image generation models become more sophisticated, the methods for concealing copyright violations are similarly advancing, creating a technical arms race between copyright protection mechanisms and increasingly subtle evasion techniques <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues."
                        ],
                        "paper": {
                            "corpus_id": 265352103,
                            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2267877984",
                                    "name": "Yang Zhang"
                                },
                                {
                                    "authorId": "2267728071",
                                    "name": "Teoh Tze Tzun"
                                },
                                {
                                    "authorId": "2267727392",
                                    "name": "Lim Wei Hern"
                                },
                                {
                                    "authorId": "2267866973",
                                    "name": "Haonan Wang"
                                },
                                {
                                    "authorId": "2256995496",
                                    "name": "Kenji Kawaguchi"
                                }
                            ],
                            "year": 2023,
                            "venue": "",
                            "n_citations": 10
                        },
                        "score": 0.716796875
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "In the context of copyright, a backdoor attack against diffusion models is a type of security attack designed to induce diffusion models to reproduce copyright-infringing outputs by manipulating the clean training dataset",
                            "In our approach, the trigger is a specific prompt, while the backdoors are poisoned image-caption pairs. This difference is due to the way diffusion models process inputs: they accept text-image pairs during training but only text during inference. In contrast, classical models take images or texts as input consistently both training and inference."
                        ],
                        "paper": {
                            "corpus_id": 266900037,
                            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
                            "authors": [
                                {
                                    "authorId": "2267866973",
                                    "name": "Haonan Wang"
                                },
                                {
                                    "authorId": "2257038423",
                                    "name": "Qianli Shen"
                                },
                                {
                                    "authorId": "2278794984",
                                    "name": "Yao Tong"
                                },
                                {
                                    "authorId": "2267877984",
                                    "name": "Yang Zhang"
                                },
                                {
                                    "authorId": "2256995496",
                                    "name": "Kenji Kawaguchi"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 32
                        },
                        "score": 0.6826171875
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools."
                        ],
                        "paper": {
                            "corpus_id": 269033217,
                            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2275053301",
                                    "name": "Yiwei Lu"
                                },
                                {
                                    "authorId": "2284800079",
                                    "name": "Matthew Y.R. Yang"
                                },
                                {
                                    "authorId": "2295948127",
                                    "name": "Zuoqiu Liu"
                                },
                                {
                                    "authorId": "2284763541",
                                    "name": "Gautam Kamath"
                                },
                                {
                                    "authorId": "2274963165",
                                    "name": "Yaoliang Yu"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 8
                        },
                        "score": 0.556640625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Mitigation Strategies for Copyright Infringement",
                "tldr": "Various approaches have been developed to mitigate copyright infringement in image generation models, focusing on both the training and deployment phases. These strategies include reducing memorization through differential privacy, implementing training-stage protection mechanisms, and developing model watermarking techniques to establish ownership. (6 sources)",
                "text": "\nTo address the copyright challenges posed by image generation models, researchers have developed several mitigation strategies targeting both training and deployment phases. These approaches aim to balance the technological benefits of generative models with the protection of intellectual property rights.\n\nOne primary approach involves reducing memorization during the training process. Differential privacy techniques can help minimize the retention of specific data points, thereby reducing the risk of reproducing protected content <Paper corpusId=\"276575866\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"207178262\" paperTitle=\"(Dwork et al., 2014)\" isShortName></Paper> <Paper corpusId=\"207241585\" paperTitle=\"(Abadi et al., 2016)\" isShortName></Paper> <Paper corpusId=\"250210875\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>. By adding carefully calibrated noise to the training process, these methods can limit the model's ability to memorize and subsequently reproduce copyrighted materials while still learning useful patterns from the data.\n\nFor content owners seeking to protect their works, \"training-stage protection\" mechanisms offer a proactive solution. These techniques involve adding imperceptible noise to copyrighted images specifically designed to disrupt the training processes of unauthorized models <Paper corpusId=\"267412857\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper>. This approach prevents unauthorized parties from effectively training or fine-tuning models on protected content, addressing concerns about data replication and unauthorized modifications through techniques like DreamBooth.\n\nDuring the deployment phase, prompt engineering strategies can help prevent copyright infringement. These include using negative prompts to exclude protected concepts or elements from generated outputs and optimizing potentially unsafe prompts to avoid the inclusion of IP-protected material <Paper corpusId=\"276575866\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>. Such techniques give users and system administrators more control over the content generation process.\n\nModel copyright protection is another important consideration. Deep Generative Model Watermarking has emerged as a common solution, incorporating distinct information into models before deployment. These embedded watermarks can later be retrieved from potentially infringing models or their generated data to confirm suspected copyright violations <Paper corpusId=\"267412857\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper>. This approach helps establish clear ownership of model architecture and generated content.\n\nDetection mechanisms also play a crucial role in mitigating copyright infringement. Researchers have developed methods to compare generated images with training data to identify potential violations. Given the prohibitive cost of comparing against entire massive datasets, these approaches typically limit comparisons to relevant subsets of training data <Paper corpusId=\"278338968\" paperTitle=\"(Reissinger et al., 2025)\" isShortName></Paper>. While not preventing infringement directly, such detection capabilities can support enforcement efforts and promote accountability.\n\nAs the field evolves, the development of more sophisticated mitigation strategies remains an active area of research. The challenge lies in balancing the innovative potential of generative AI with appropriate safeguards for intellectual property rights across both training and deployment phases.",
                "citations": [
                    {
                        "id": "(Xu et al., 2025)",
                        "snippets": [
                            "To mitigate IP infringement, two primary approaches have emerged: \n\n\u2022 Reducing Memorization During Training: Implementing differential privacy (Dwork et al., 2014) techniques during the training of generative models can help minimize the retention of specific data points, thereby reducing the risk of reproducing protected content (Abadi et al., 2016)(Chen et al., 2022)Dockhorn et al., 2022). \n\n\u2022 Prompt Engineering: Employing strategies such as negative prompts during the inference phase can exclude undesired concepts or elements from the generated output (Wang et al., 2024;He et al., 2024), or optimizing unsafe prompts (Chin et al., 2023;Rando et al., 2022), thereby avoiding the inclusion of IP-protected material."
                        ],
                        "paper": {
                            "corpus_id": 276575866,
                            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
                            "authors": [
                                {
                                    "authorId": "2346987106",
                                    "name": "Qipan Xu"
                                },
                                {
                                    "authorId": "2307564873",
                                    "name": "Zhenting Wang"
                                },
                                {
                                    "authorId": "2202044790",
                                    "name": "Xiaoxiao He"
                                },
                                {
                                    "authorId": "2302817621",
                                    "name": "Ligong Han"
                                },
                                {
                                    "authorId": "2346993695",
                                    "name": "Ruixiang Tang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.625
                    },
                    {
                        "id": "(Dwork et al., 2014)",
                        "snippets": [
                            "The problem of privacy-preserving data analysis has a long history spanning multiple disciplines. As electronic data about individuals becomes increasingly detailed, and as technology enables ever more powerful collection and curation of these data, the need increases for a robust, meaningful, and mathematically rigorous definition of privacy, together with a computationally rich class of algorithms that satisfy this definition. Differential Privacy is such a definition.After motivating and discussing the meaning of differential privacy, the preponderance of this monograph is devoted to fundamental techniques for achieving differential privacy, and application of these techniques in creative combinations, using the query-release problem as an ongoing example. A key point is that, by rethinking the computational goal, one can often obtain far better results than would be achieved by methodically replacing each step of a non-private computation with a differentially private implementation. Despite some astonishingly powerful computational results, there are still fundamental limitations \u2014 not just on what can be achieved with differential privacy but on what can be achieved with any method that protects against a complete breakdown in privacy. Virtually all the algorithms discussed herein maintain differential privacy against adversaries of arbitrary computational power. Certain algorithms are computationally intensive, others are efficient. Computational complexity for the adversary and the algorithm are both discussed.We then turn from fundamentals to applications other than queryrelease, discussing differentially private methods for mechanism design and machine learning. The vast majority of the literature on differentially private algorithms considers a single, static, database that is subject to many analyses. Differential privacy in other models, including distributed databases and computations on data streams is discussed.Finally, we note that this work is meant as a thorough introduction to the problems and techniques of differential privacy, but is not intended to be an exhaustive survey \u2014 there is by now a vast amount of work in differential privacy, and we can cover only a small portion of it."
                        ],
                        "paper": {
                            "corpus_id": 207178262,
                            "title": "The Algorithmic Foundations of Differential Privacy",
                            "authors": [
                                {
                                    "authorId": "1781565",
                                    "name": "C. Dwork"
                                },
                                {
                                    "authorId": "1682008",
                                    "name": "Aaron Roth"
                                }
                            ],
                            "year": 2014,
                            "venue": "Foundations and Trends\u00ae in Theoretical Computer Science",
                            "n_citations": 7735
                        },
                        "score": 0
                    },
                    {
                        "id": "(Abadi et al., 2016)",
                        "snippets": [
                            "Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality."
                        ],
                        "paper": {
                            "corpus_id": 207241585,
                            "title": "Deep Learning with Differential Privacy",
                            "authors": [
                                {
                                    "authorId": "2057642721",
                                    "name": "Mart\u00edn Abadi"
                                },
                                {
                                    "authorId": "1396184193",
                                    "name": "Andy Chu"
                                },
                                {
                                    "authorId": "153440022",
                                    "name": "I. Goodfellow"
                                },
                                {
                                    "authorId": "145057514",
                                    "name": "H. B. McMahan"
                                },
                                {
                                    "authorId": "145591745",
                                    "name": "Ilya Mironov"
                                },
                                {
                                    "authorId": "35210462",
                                    "name": "Kunal Talwar"
                                },
                                {
                                    "authorId": "2152832173",
                                    "name": "Li Zhang"
                                }
                            ],
                            "year": 2016,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 6165
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2022)",
                        "snippets": [
                            "Despite an increased demand for valuable data, the privacy concerns associated with sensitive datasets present a barrier to data sharing. One may use differentially private generative models to generate synthetic data. Unfortunately, generators are typically restricted to generating images of low-resolutions due to the limitation of noisy gradients. Here, we propose DPGEN, a network model designed to synthesize high-resolution natural images while satisfying differential privacy. In particular, we propose an energy-guided network trained on sanitized data to indicate the direction of the true data distribution via Langevin Markov chain Monte Carlo (MCMC) sampling method. In contrast to the state-of-the-art methods that can process only low-resolution images (e.g., MNIST and Fashion-MNIST), DPGEN can generate differentially private synthetic images with resolutions up to $128\\times 128$ with superior visual quality and data utility. Our code is available at https://github.com/chiamuyu/DPGEN"
                        ],
                        "paper": {
                            "corpus_id": 250210875,
                            "title": "DPGEN: Differentially Private Generative Energy-Guided Network for Natural Image Synthesis",
                            "authors": [
                                {
                                    "authorId": "2154737261",
                                    "name": "Jiawei Chen"
                                },
                                {
                                    "authorId": "2015326",
                                    "name": "Chia-Mu Yu"
                                },
                                {
                                    "authorId": "1599691800",
                                    "name": "Ching-Chia Kao"
                                },
                                {
                                    "authorId": "2174450003",
                                    "name": "Tzai-Wei Pang"
                                },
                                {
                                    "authorId": "144854327",
                                    "name": "Chun-Shien Lu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 28
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ren et al., 2024)",
                        "snippets": [
                            "The possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works. \n\nDifferent from directly employing DGMs to generate new images, DGMs are also usually trained or fine-tuned on some source images to effectively learn useful information from for future generations. Therefore, \"training-stage protection\" aims to add imperceptible noise to the copyrighted images, to break the training process of potential DGMs for data copyright protection.\n\nDeep Generative Model Watermarking is a common solution for model copyright protection. It involves incorporating distinct information, known as a watermark, into the models before their deployment. The embedded watermark can be retrieved from a potentially infringing model or its generated data to confirm any suspected copyright violations."
                        ],
                        "paper": {
                            "corpus_id": 267412857,
                            "title": "Copyright Protection in Generative AI: A Technical Perspective",
                            "authors": [
                                {
                                    "authorId": "2256589810",
                                    "name": "Jie Ren"
                                },
                                {
                                    "authorId": "2253881697",
                                    "name": "Han Xu"
                                },
                                {
                                    "authorId": "2185740224",
                                    "name": "Pengfei He"
                                },
                                {
                                    "authorId": "2218740984",
                                    "name": "Yingqian Cui"
                                },
                                {
                                    "authorId": "2253682835",
                                    "name": "Shenglai Zeng"
                                },
                                {
                                    "authorId": "2282560420",
                                    "name": "Jiankun Zhang"
                                },
                                {
                                    "authorId": "2256788829",
                                    "name": "Hongzhi Wen"
                                },
                                {
                                    "authorId": "46496977",
                                    "name": "Jiayuan Ding"
                                },
                                {
                                    "authorId": "2253533415",
                                    "name": "Hui Liu"
                                },
                                {
                                    "authorId": "2267019992",
                                    "name": "Yi Chang"
                                },
                                {
                                    "authorId": "2115879611",
                                    "name": "Jiliang Tang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 42
                        },
                        "score": 0.5537109375
                    },
                    {
                        "id": "(Reissinger et al., 2025)",
                        "snippets": [
                            "Copyright issues in visual diffusion models arise when models replicate their training data within the image generation process. Therefore, to detect IP infringement, we compare the generated image with the images in the training data. Since training data is huge, a comparison to the entire dataset is prohibitively expensive. We therefore-similar to [25]-limit our comparison to a subset of the training data."
                        ],
                        "paper": {
                            "corpus_id": 278338968,
                            "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
                            "authors": [
                                {
                                    "authorId": "2359254079",
                                    "name": "Lena Reissinger"
                                },
                                {
                                    "authorId": "2256011059",
                                    "name": "Yuanyuan Li"
                                },
                                {
                                    "authorId": "23107750",
                                    "name": "Anna Haensch"
                                },
                                {
                                    "authorId": "2254269177",
                                    "name": "Neeraj Sarna"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.63134765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Legal Considerations and Liability",
                "tldr": "The legal landscape for copyright infringement in image generation models is complex, with potential liability for both AI developers and users. The unique characteristics of these models, including their opacity and access to vast training datasets, create unprecedented challenges for applying traditional copyright principles. (4 sources)",
                "text": "\nThe question of liability for copyright infringement in image generation models involves multiple parties and complex legal considerations. Copyright infringement can occur in two distinct phases: training and deployment. During the training phase, the algorithm directly accesses copyrighted material, potentially encoding verbatim copies within its weights. These training-phase issues share similarities with other data-scraping contexts like search engine indexing and book digitization, where fair use considerations are paramount <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>.\n\nThe deployment phase introduces a different liability scenario, where a user prompts the model to generate content that may directly copy or substantially resemble copyrighted training data. Unlike search engines, generative models don't track the provenance of their outputs, making it difficult for users to verify potential infringement <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>. This creates a legal gray area where both the user and the AI developer may potentially be considered direct or indirect infringers, depending on specific circumstances <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>.\n\nA critical legal consideration is that copyright infringement may occur even without intentional replication of protected content. Neither the user nor the developer needs to deliberately aim to copy copyrighted material for infringement to take place <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>. This unintentional infringement creates significant challenges for liability determination and risk management.\n\nThe opacity of generative models further complicates legal proceedings. Due to the \"uninterpretability\" of these models, providing direct evidence of copying is extremely difficult. Instead, legal cases will likely rely on access and similarity as primary evidence of infringement, similar to traditional copyright disputes <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>. The inclusion of copyrighted works in training data may constitute sufficient \"access\" from a legal perspective, especially since studies have demonstrated that exposure to specific content during training correlates with a higher probability of producing similar outputs <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper> <Paper corpusId=\"229156229\" paperTitle=\"(Carlini et al., 2020)\" isShortName></Paper> <Paper corpusId=\"256389993\" paperTitle=\"(Carlini et al., 2023)\" isShortName></Paper>.\n\nA unique characteristic of generative models that distinguishes them from human creators is their ability to train on massive, diverse datasets often scraped from the internet. Furthermore, direct inclusion of copyrighted material in training data isn't even necessary for latent diffusion models to produce similar images, as they can reproduce content from other images with similar latent information <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>. This capability significantly increases the likelihood that these models have what could legally be construed as \"access\" to copyrighted works, further complicating liability determinations.\n\nAs the technology continues to evolve, the legal framework for addressing copyright infringement in generative AI remains in flux. The unprecedented capabilities of these models create new challenges for applying traditional copyright principles, suggesting that updated legal approaches may be necessary to balance innovation with intellectual property protection <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Vyas et al., 2023)",
                        "snippets": [
                            "Copyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material."
                        ],
                        "paper": {
                            "corpus_id": 257050406,
                            "title": "On Provable Copyright Protection for Generative Models",
                            "authors": [
                                {
                                    "authorId": "145603901",
                                    "name": "Nikhil Vyas"
                                },
                                {
                                    "authorId": "144695232",
                                    "name": "S. Kakade"
                                },
                                {
                                    "authorId": "1697211",
                                    "name": "B. Barak"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 94
                        },
                        "score": 0.666015625
                    },
                    {
                        "id": "(Chiba-Okabe, 2024)",
                        "snippets": [
                            "In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content (Carlini et al., 2020)(Carlini et al., 2023)[39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works."
                        ],
                        "paper": {
                            "corpus_id": 273023255,
                            "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety",
                            "authors": [
                                {
                                    "authorId": "2297800322",
                                    "name": "Hiroaki Chiba-Okabe"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.56689453125
                    },
                    {
                        "id": "(Carlini et al., 2020)",
                        "snippets": [
                            "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models."
                        ],
                        "paper": {
                            "corpus_id": 229156229,
                            "title": "Extracting Training Data from Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "1404060687",
                                    "name": "Ariel Herbert-Voss"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "31035595",
                                    "name": "Tom B. Brown"
                                },
                                {
                                    "authorId": "143711382",
                                    "name": "D. Song"
                                },
                                {
                                    "authorId": "1758110",
                                    "name": "\u00da. Erlingsson"
                                },
                                {
                                    "authorId": "3046437",
                                    "name": "Alina Oprea"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2020,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 1950
                        },
                        "score": 0
                    },
                    {
                        "id": "(Carlini et al., 2023)",
                        "snippets": [
                            "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                        ],
                        "paper": {
                            "corpus_id": 256389993,
                            "title": "Extracting Training Data from Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "9200194",
                                    "name": "Jamie Hayes"
                                },
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "3482535",
                                    "name": "Vikash Sehwag"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "1718064",
                                    "name": "Borja Balle"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                }
                            ],
                            "year": 2023,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 617
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.195519
    }
}