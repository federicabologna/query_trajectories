{
    "query": "How do distinct linguistic capabilities such as syntax, world knowledge, and reasoning emerge at different phases during language model training, and what are the characteristic phase transitions observed?",
    "user_id": "lib_user",
    "task_id": "78b4c0de-fdd4-4ae7-9f8e-cc0f69530457",
    "timestamp": "2025-06-23T23:51:11.570190",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 41,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.35506200000000004,
    "decomposed_query": {
        "rewritten_query": "How distinct linguistic capabilities such as syntax, world knowledge, and reasoning emerge at different phases during language model training, and what are the characteristic phase transitions observed.",
        "keyword_query": "linguistic capabilities syntax world knowledge reasoning emergence phases language model training characteristic phase transitions",
        "search_filters": {
            "fieldsOfStudy": "Computer Science,Linguistics"
        },
        "cost": 0.009957,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Triple Phase Transitions: Understanding the Learning Dynamics of Large Language Models from a Neuroscience Perspective",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 64,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.20779, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283312221",
                    "name": "Yuko Nakagi"
                },
                {
                    "authorId": "2348095768",
                    "name": "Keigo Tada"
                },
                {
                    "authorId": "2348095652",
                    "name": "Sota Yoshino"
                },
                {
                    "authorId": "2346109244",
                    "name": "Shinji Nishimoto"
                },
                {
                    "authorId": "2346108815",
                    "name": "Yu Takagi"
                }
            ],
            "abstract": "Large language models (LLMs) often exhibit abrupt emergent behavior, whereby new abilities arise at certain points during their training. This phenomenon, commonly referred to as a ''phase transition'', remains poorly understood. In this study, we conduct an integrative analysis of such phase transitions by examining three interconnected perspectives: the similarity between LLMs and the human brain, the internal states of LLMs, and downstream task performance. We propose a novel interpretation for the learning dynamics of LLMs that vary in both training data and architecture, revealing that three phase transitions commonly emerge across these models during training: (1) alignment with the entire brain surges as LLMs begin adhering to task instructions Brain Alignment and Instruction Following, (2) unexpectedly, LLMs diverge from the brain during a period in which downstream task accuracy temporarily stagnates Brain Detachment and Stagnation, and (3) alignment with the brain reoccurs as LLMs become capable of solving the downstream tasks Brain Realignment and Consolidation. These findings illuminate the underlying mechanisms of phase transitions in LLMs, while opening new avenues for interdisciplinary research bridging AI and neuroscience.",
            "corpus_id": 276724796,
            "sentences": [
                {
                    "corpus_id": "276724796",
                    "title": "Triple Phase Transitions: Understanding the Learning Dynamics of Large Language Models from a Neuroscience Perspective",
                    "text": "Large language models (LLMs) often exhibit abrupt emergent behavior, whereby new abilities arise at certain points during their training. This phenomenon, commonly referred to as a ''phase transition'', remains poorly understood. In this study, we conduct an integrative analysis of such phase transitions by examining three interconnected perspectives: the similarity between LLMs and the human brain, the internal states of LLMs, and downstream task performance. We propose a novel interpretation for the learning dynamics of LLMs that vary in both training data and architecture, revealing that three phase transitions commonly emerge across these models during training: (1) alignment with the entire brain surges as LLMs begin adhering to task instructions Brain Alignment and Instruction Following, (2) unexpectedly, LLMs diverge from the brain during a period in which downstream task accuracy temporarily stagnates Brain Detachment and Stagnation, and (3) alignment with the brain reoccurs as LLMs become capable of solving the downstream tasks Brain Realignment and Consolidation. These findings illuminate the underlying mechanisms of phase transitions in LLMs, while opening new avenues for interdisciplinary research bridging AI and neuroscience.",
                    "score": 0.38704866882156796,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97021484375
                },
                {
                    "corpus_id": "276724796",
                    "title": "Triple Phase Transitions: Understanding the Learning Dynamics of Large Language Models from a Neuroscience Perspective",
                    "text": "It is crucial to note that although these metrics confirm that fundamental transformations occur in the representation space, they do not fully explain the specific underlying mechanisms. By examining how these transformations relate to both the brain and downstream tasks, as in this study, a more comprehensive and nuanced understanding of the underlying processes can be achieved. \n\nIn summary, this study has elucidated the existence of three distinct phase transitions during LLM training, as evidenced by three perspectives: alignment with the human brain, shifts in internal representations pertinent to downstream tasks, and task performance. We interpreted these transitions as indispensable internal transformations that enable the model to acquire downstream task capabilities. This study is the first to show that changes in brain alignment, internal representation, and model performance advance in tandem by leveraging the human brain-the only known system (aside from LLMs) capable of processing complex language. These findings highlight the critical importance of examining multiple signals to gain a comprehensive understanding of LLM learning dynamics, including emergent phenomena. Furthermore, they suggest the potential to harness human brain activity in the pursuit of explainable and human-aligned language models. 2014. doi: 10.1073/pnas.1403112111. URL https://www.pnas.org/doi/abs/10.1073/pnas. 1403112111. \n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.",
                    "score": 0.37955109953353117,
                    "section_title": "Discussion and conclusions",
                    "char_start_offset": 29546,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 383
                        },
                        {
                            "start": 386,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 788
                        },
                        {
                            "start": 789,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1201
                        },
                        {
                            "start": 1202,
                            "end": 1338
                        },
                        {
                            "start": 1339,
                            "end": 1344
                        },
                        {
                            "start": 1345,
                            "end": 1374
                        },
                        {
                            "start": 1375,
                            "end": 1421
                        },
                        {
                            "start": 1422,
                            "end": 1433
                        },
                        {
                            "start": 1436,
                            "end": 1507
                        },
                        {
                            "start": 1508,
                            "end": 1561
                        },
                        {
                            "start": 1562,
                            "end": 1659
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90478515625
                },
                {
                    "corpus_id": "276724796",
                    "title": "Triple Phase Transitions: Understanding the Learning Dynamics of Large Language Models from a Neuroscience Perspective",
                    "text": "Previous research linking model representations to brain activity (Yamins et al., 2014;G \u00fcc \u00b8l \u00fc & van Gerven, 2015;Schrimpf et al., 2021;Caucheteux et al., 2023) has not fully addressed such learning dynamics. While Antonello et al. (2023) reported a rise in alignment with increasing model size, our findings depict a more intricate path: an initial surge in brain alignment, a decline, and a final resurgence. This suggests that LLMs may adopt distinct computational strategies at various stages, rather than gradually acquiring brain-like language representations. Moreover, alignment with semantic regions in the temporal cortex generally rises over the course of training, implying a dynamic reorganization in how models correspond to brain activity. By capturing shifts in LLM neuron activations related to downstream tasks, we elucidated how task-specific neurons form and how much specialization they share among tasks. These findings align with previous studies showing neurons that encode particular linguistic information (Tenney et al., 2019;Dai et al., 2022;Wang et al., 2022;Gurnee et al., 2023). We observed that neurons contributing substantially to certain tasks emerge abruptly in later layers as training progresses. Moreover, the variations in neuron sharing across tasks suggest that differences in activation patterns-reflecting task-specific characteristics (e.g., required capabilities, difficulty levels, and answer formats)-are indeed discernible at the neuronal level. Taken together, our results offer a more comprehensive view of LLM neurons during the learning process, building on past work that focused solely on trained models. Further in-depth analyses of these neurons would be a fascinating avenue for future research. \n\nFinally, we extended the findings of Cheng & Antonello (2024), who demonstrated a positive correlation between the dimensionality of LLM activations in trained models and the alignment of those LLMs with the brain. Our results confirm that this correlation remains robust during training, suggesting that the observed phase transitions are tied to changes in activation dimensionality. It is crucial to note that although these metrics confirm that fundamental transformations occur in the representation space, they do not fully explain the specific underlying mechanisms.",
                    "score": 0.3959881719302649,
                    "section_title": "Discussion and conclusions",
                    "char_start_offset": 27402,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 210
                        },
                        {
                            "start": 211,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 568
                        },
                        {
                            "start": 569,
                            "end": 756
                        },
                        {
                            "start": 757,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 1111
                        },
                        {
                            "start": 1112,
                            "end": 1236
                        },
                        {
                            "start": 1237,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1661
                        },
                        {
                            "start": 1662,
                            "end": 1755
                        },
                        {
                            "start": 1758,
                            "end": 1972
                        },
                        {
                            "start": 1973,
                            "end": 2143
                        },
                        {
                            "start": 2144,
                            "end": 2331
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 87,
                            "end": 116,
                            "matchedPaperCorpusId": "3515742"
                        },
                        {
                            "start": 116,
                            "end": 138,
                            "matchedPaperCorpusId": "222359195"
                        },
                        {
                            "start": 138,
                            "end": 162,
                            "matchedPaperCorpusId": "257309545"
                        },
                        {
                            "start": 1034,
                            "end": 1055,
                            "matchedPaperCorpusId": "108300988"
                        },
                        {
                            "start": 1055,
                            "end": 1072,
                            "matchedPaperCorpusId": "233296761"
                        },
                        {
                            "start": 1072,
                            "end": 1090,
                            "matchedPaperCorpusId": "253510792"
                        },
                        {
                            "start": 1090,
                            "end": 1110,
                            "matchedPaperCorpusId": "258437237"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8974609375
                }
            ],
            "relevance_judgement": 0.97021484375,
            "relevance_judgment_input_expanded": "# Title: Triple Phase Transitions: Understanding the Learning Dynamics of Large Language Models from a Neuroscience Perspective\n# Venue: arXiv.org\n# Authors: Yuko Nakagi, Keigo Tada, Sota Yoshino, Shinji Nishimoto, Yu Takagi\n## Abstract\nLarge language models (LLMs) often exhibit abrupt emergent behavior, whereby new abilities arise at certain points during their training. This phenomenon, commonly referred to as a ''phase transition'', remains poorly understood. In this study, we conduct an integrative analysis of such phase transitions by examining three interconnected perspectives: the similarity between LLMs and the human brain, the internal states of LLMs, and downstream task performance. We propose a novel interpretation for the learning dynamics of LLMs that vary in both training data and architecture, revealing that three phase transitions commonly emerge across these models during training: (1) alignment with the entire brain surges as LLMs begin adhering to task instructions Brain Alignment and Instruction Following, (2) unexpectedly, LLMs diverge from the brain during a period in which downstream task accuracy temporarily stagnates Brain Detachment and Stagnation, and (3) alignment with the brain reoccurs as LLMs become capable of solving the downstream tasks Brain Realignment and Consolidation. These findings illuminate the underlying mechanisms of phase transitions in LLMs, while opening new avenues for interdisciplinary research bridging AI and neuroscience.\n## Discussion and conclusions\nPrevious research linking model representations to brain activity (Yamins et al., 2014;G \u00fcc \u00b8l \u00fc & van Gerven, 2015;Schrimpf et al., 2021;Caucheteux et al., 2023) has not fully addressed such learning dynamics. While Antonello et al. (2023) reported a rise in alignment with increasing model size, our findings depict a more intricate path: an initial surge in brain alignment, a decline, and a final resurgence. This suggests that LLMs may adopt distinct computational strategies at various stages, rather than gradually acquiring brain-like language representations. Moreover, alignment with semantic regions in the temporal cortex generally rises over the course of training, implying a dynamic reorganization in how models correspond to brain activity. By capturing shifts in LLM neuron activations related to downstream tasks, we elucidated how task-specific neurons form and how much specialization they share among tasks. These findings align with previous studies showing neurons that encode particular linguistic information (Tenney et al., 2019;Dai et al., 2022;Wang et al., 2022;Gurnee et al., 2023). We observed that neurons contributing substantially to certain tasks emerge abruptly in later layers as training progresses. Moreover, the variations in neuron sharing across tasks suggest that differences in activation patterns-reflecting task-specific characteristics (e.g., required capabilities, difficulty levels, and answer formats)-are indeed discernible at the neuronal level. Taken together, our results offer a more comprehensive view of LLM neurons during the learning process, building on past work that focused solely on trained models. Further in-depth analyses of these neurons would be a fascinating avenue for future research. \n\nFinally, we extended the findings of Cheng & Antonello (2024), who demonstrated a positive correlation between the dimensionality of LLM activations in trained models and the alignment of those LLMs with the brain. Our results confirm that this correlation remains robust during training, suggesting that the observed phase transitions are tied to changes in activation dimensionality. It is crucial to note that although these metrics confirm that fundamental transformations occur in the representation space, they do not fully explain the specific underlying mechanisms.\n...\nIt is crucial to note that although these metrics confirm that fundamental transformations occur in the representation space, they do not fully explain the specific underlying mechanisms. By examining how these transformations relate to both the brain and downstream tasks, as in this study, a more comprehensive and nuanced understanding of the underlying processes can be achieved. \n\nIn summary, this study has elucidated the existence of three distinct phase transitions during LLM training, as evidenced by three perspectives: alignment with the human brain, shifts in internal representations pertinent to downstream tasks, and task performance. We interpreted these transitions as indispensable internal transformations that enable the model to acquire downstream task capabilities. This study is the first to show that changes in brain alignment, internal representation, and model performance advance in tandem by leveraging the human brain-the only known system (aside from LLMs) capable of processing complex language. These findings highlight the critical importance of examining multiple signals to gain a comprehensive understanding of LLM learning dynamics, including emergent phenomena. Furthermore, they suggest the potential to harness human brain activity in the pursuit of explainable and human-aligned language models. 2014. doi: 10.1073/pnas.1403112111. URL https://www.pnas.org/doi/abs/10.1073/pnas. 1403112111. \n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.",
            "reference_string": "[276724796 | Nakagi et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 60,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.16484, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1416353805",
                    "name": "Max M\u00fcller-Eberstein"
                },
                {
                    "authorId": "3449407",
                    "name": "Rob van der Goot"
                },
                {
                    "authorId": "2256991041",
                    "name": "Barbara Plank"
                },
                {
                    "authorId": "2261492323",
                    "name": "Ivan Titov"
                }
            ],
            "abstract": "Representational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic probing suite, which enables direct comparisons of not just task performance, but their representational subspaces, we analyze nine tasks covering syntax, semantics and reasoning, across 2M pre-training steps and five seeds. We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization. Measuring cross-task similarity further reveals that linguistically related tasks share information throughout training, and do so more during the critical phase of learning than before or after. Our findings have implications for model interpretability, multi-task learning, and learning from limited data.",
            "corpus_id": 264451507,
            "sentences": [
                {
                    "corpus_id": "264451507",
                    "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training",
                    "text": "Representational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic probing suite, which enables direct comparisons of not just task performance, but their representational subspaces, we analyze nine tasks covering syntax, semantics and reasoning, across 2M pre-training steps and five seeds. We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization. Measuring cross-task similarity further reveals that linguistically related tasks share information throughout training, and do so more during the critical phase of learning than before or after. Our findings have implications for model interpretability, multi-task learning, and learning from limited data.",
                    "score": 0.5488074885559356,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96728515625
                }
            ],
            "relevance_judgement": 0.96728515625,
            "relevance_judgment_input_expanded": "# Title: Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Max M\u00fcller-Eberstein, Rob van der Goot, Barbara Plank, Ivan Titov\n## Abstract\nRepresentational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic probing suite, which enables direct comparisons of not just task performance, but their representational subspaces, we analyze nine tasks covering syntax, semantics and reasoning, across 2M pre-training steps and five seeds. We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization. Measuring cross-task similarity further reveals that linguistically related tasks share information throughout training, and do so more during the critical phase of learning than before or after. Our findings have implications for model interpretability, multi-task learning, and learning from limited data.\n",
            "reference_string": "[264451507 | Muller-Eberstein et al. | 2023 | Citations: 10]"
        },
        {
            "title": "How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 23,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.06394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2342412237",
                    "name": "Tatsuro Inaba"
                },
                {
                    "authorId": "2261744024",
                    "name": "Kentaro Inui"
                },
                {
                    "authorId": "2302320083",
                    "name": "Yusuke Miyao"
                },
                {
                    "authorId": "50856622",
                    "name": "Yohei Oseki"
                },
                {
                    "authorId": "2266692",
                    "name": "Benjamin Heinzerling"
                },
                {
                    "authorId": "2310230516",
                    "name": "Yu Takagi"
                }
            ],
            "abstract": "Large Language Models (LLMs) demonstrate remarkable multilingual capabilities and broad knowledge. However, the internal mechanisms underlying the development of these capabilities remain poorly understood. To investigate this, we analyze how the information encoded in LLMs' internal representations evolves during the training process. Specifically, we train sparse autoencoders at multiple checkpoints of the model and systematically compare the interpretative results across these stages. Our findings suggest that LLMs initially acquire language-specific knowledge independently, followed by cross-linguistic correspondences. Moreover, we observe that after mastering token-level knowledge, the model transitions to learning higher-level, abstract concepts, indicating the development of more conceptual understanding.",
            "corpus_id": 276902722,
            "sentences": [
                {
                    "corpus_id": "276902722",
                    "title": "How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders",
                    "text": "Large Language Models (LLMs) demonstrate remarkable multilingual capabilities and broad knowledge. However, the internal mechanisms underlying the development of these capabilities remain poorly understood. To investigate this, we analyze how the information encoded in LLMs' internal representations evolves during the training process. Specifically, we train sparse autoencoders at multiple checkpoints of the model and systematically compare the interpretative results across these stages. Our findings suggest that LLMs initially acquire language-specific knowledge independently, followed by cross-linguistic correspondences. Moreover, we observe that after mastering token-level knowledge, the model transitions to learning higher-level, abstract concepts, indicating the development of more conceptual understanding.",
                    "score": 0.440952831375588,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95263671875
                }
            ],
            "relevance_judgement": 0.95263671875,
            "relevance_judgment_input_expanded": "# Title: How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders\n# Venue: arXiv.org\n# Authors: Tatsuro Inaba, Kentaro Inui, Yusuke Miyao, Yohei Oseki, Benjamin Heinzerling, Yu Takagi\n## Abstract\nLarge Language Models (LLMs) demonstrate remarkable multilingual capabilities and broad knowledge. However, the internal mechanisms underlying the development of these capabilities remain poorly understood. To investigate this, we analyze how the information encoded in LLMs' internal representations evolves during the training process. Specifically, we train sparse autoencoders at multiple checkpoints of the model and systematically compare the interpretative results across these stages. Our findings suggest that LLMs initially acquire language-specific knowledge independently, followed by cross-linguistic correspondences. Moreover, we observe that after mastering token-level knowledge, the model transitions to learning higher-level, abstract concepts, indicating the development of more conceptual understanding.\n",
            "reference_string": "[276902722 | Inaba et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 94,
            "citation_count": 74,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.07311",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.07311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "13336152",
                    "name": "Angelica Chen"
                },
                {
                    "authorId": "2240524527",
                    "name": "Ravid Schwartz-Ziv"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                },
                {
                    "authorId": "2240527814",
                    "name": "Matthew L. Leavitt"
                },
                {
                    "authorId": "2362960",
                    "name": "Naomi Saphra"
                }
            ],
            "abstract": "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.",
            "corpus_id": 261822542,
            "sentences": [
                {
                    "corpus_id": "261822542",
                    "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                    "text": "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens. \n\nTo better understand the role of interpretable artifacts in model development, we analyze and manipulate these artifacts during training. We focus on a case study of Syntactic Attention Structure (SAS), a model behavior thought to relate to grammatical structure. By measuring and controlling the emergence of SAS, we deepen our understanding of the relationship between the internal structural traits and extrinsic capabilities of masked language models (MLMs). \n\nSAS occurs when a model learns specialized attention heads that focus on a word's syntactic neighbors. This behavior emerges naturally during conventional MLM pre-training (Clark et al., 2019;Voita et al., 2019;Manning et al., 2020). We observe an abrupt spike in SAS at a consistent point in training, and explore its impact on MLM capabilities by manipulating SAS during training. Our observations paint a picture of how interpretability artifacts may represent simplicity biases that compete with other learning strategies during MLM training. In summary, our main contributions are: \n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes.",
                    "score": 0.6030716557195096,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 144
                        },
                        {
                            "start": 145,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 657
                        },
                        {
                            "start": 658,
                            "end": 807
                        },
                        {
                            "start": 808,
                            "end": 888
                        },
                        {
                            "start": 891,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1353
                        },
                        {
                            "start": 1356,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1589
                        },
                        {
                            "start": 1590,
                            "end": 1738
                        },
                        {
                            "start": 1739,
                            "end": 1902
                        },
                        {
                            "start": 1903,
                            "end": 1942
                        },
                        {
                            "start": 1945,
                            "end": 2186
                        },
                        {
                            "start": 2187,
                            "end": 2262
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 588,
                            "end": 612,
                            "matchedPaperCorpusId": "253117181"
                        },
                        {
                            "start": 1567,
                            "end": 1588,
                            "matchedPaperCorpusId": "219315567"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9501953125
                },
                {
                    "corpus_id": "261822542",
                    "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                    "text": "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.",
                    "score": 0.4321832711590916,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9326171875
                },
                {
                    "corpus_id": "261822542",
                    "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                    "text": "At the onset of this stage (which we call the structure onset), SAS spikes. After the spike, the model starts handling complex linguistic phenomena correctly, as signaled by a break in BLiMP score (which we call the capabilities onset). Although the functional complexity of the model declines for the rest of training, it increases between these breaks. \u2022 We introduce a regularizer to examine the causal role of SAS (defined in Section 2.2) and use it to show that SAS is necessary for handling complex linguistic phenomena (Section 4.2) and that SAS competes with an alternative strategy that exhibits its own break in the loss curve, which we call the alternative strategy onset. \u2022 Section 4.3 shows that briefly suppressing SAS improves model quality and accelerates convergence. \n\nSuppressing past the alternative strategy onset damages performance and blocks SAS long-term, suggesting this phase transition terminates a critical learning period. BERT first learns to focus on syntactic neighbors with specialized attention heads, and then exhibits grammatical capabilities in its MLM objective. The former (internal) and the latter (external) model behaviors both emerge abruptly, at moments we respectively call the structure onset (\u25b2) and capabilities onset ( ) (quantified as described in Section 2.3). We separately visualize three runs with different seeds, noting that these seeds differ in the stability of Unlabeled Attachment Score (UAS; see Section 2.1) after the structure onset, but uniformly show that SAS emerges almost entirely in a brief window of time. We show (a) MLM loss, with 95% confidence intervals across samples bynonparametric bootstrapping; (b) internal grammar structure, measured by UAS on the parse induced by the attention distributions; and (c) external grammar capabilities, measured by average BLiMP accuracy with 95% confidence intervals across tasks by nonparametric bootstrapping.",
                    "score": 0.39068232981712503,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 2202,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 75
                        },
                        {
                            "start": 76,
                            "end": 236
                        },
                        {
                            "start": 237,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 683
                        },
                        {
                            "start": 684,
                            "end": 784
                        },
                        {
                            "start": 787,
                            "end": 952
                        },
                        {
                            "start": 953,
                            "end": 1101
                        },
                        {
                            "start": 1102,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1576
                        },
                        {
                            "start": 1577,
                            "end": 1924
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8818359375
                },
                {
                    "corpus_id": "261822542",
                    "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                    "text": "Most work on scaling laws (Kaplan et al., 2020) presents test loss as a quantity that homogeneously responds to the scale of training, declining by a power law relative to the size of the corpus. In the MLM setting, we instead identify a precipitous drop in the loss curve of BERT Base (Fig. 1(a)), consistently spanning 20K-30K timesteps of training across various random seeds. We now show how this rapid learning stage can be interpreted as the composition of two distinct phase transitions. \n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset. We show similar trajectories on the MultiBERTs (Sellam et al., 2022) reproductions (Appendix F). \n\nBy observing these phase transitions, we can see that the internal representation of grammar, in the form of syntactic attention, precipitates the external observation of grammatical behavior, in the form of correct language modeling judgements on linguistically challenging examples. This is not only a single breakthrough during training, but a sequence of breakthroughs that appear to be dependent on each other. We might compare this to the \"checkmate in one\" BIG-Bench task, a known breakthrough behavior in autoregressive language models (Srivastava et al., 2022). Only at a large scale can models accurately identify checkmate moves, but further exploration revealed that the model was progressing in a linear fashion at offering consistently valid chess moves before that point.",
                    "score": 0.4517074706112117,
                    "section_title": "THE SYNTAX ACQUISITION PHASE",
                    "char_start_offset": 11881,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 494
                        },
                        {
                            "start": 497,
                            "end": 655
                        },
                        {
                            "start": 656,
                            "end": 766
                        },
                        {
                            "start": 767,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 1049
                        },
                        {
                            "start": 1050,
                            "end": 1211
                        },
                        {
                            "start": 1212,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1356
                        },
                        {
                            "start": 1359,
                            "end": 1643
                        },
                        {
                            "start": 1644,
                            "end": 1774
                        },
                        {
                            "start": 1775,
                            "end": 1929
                        },
                        {
                            "start": 1930,
                            "end": 2145
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1307,
                            "end": 1328,
                            "matchedPaperCorpusId": "235683534"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85205078125
                }
            ],
            "relevance_judgement": 0.9501953125,
            "relevance_judgment_input_expanded": "# Title: Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs\n# Venue: International Conference on Learning Representations\n# Authors: Angelica Chen, Ravid Schwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt, Naomi Saphra\n## Abstract\nMost interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.\n## INTRODUCTION\nWhile language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens. \n\nTo better understand the role of interpretable artifacts in model development, we analyze and manipulate these artifacts during training. We focus on a case study of Syntactic Attention Structure (SAS), a model behavior thought to relate to grammatical structure. By measuring and controlling the emergence of SAS, we deepen our understanding of the relationship between the internal structural traits and extrinsic capabilities of masked language models (MLMs). \n\nSAS occurs when a model learns specialized attention heads that focus on a word's syntactic neighbors. This behavior emerges naturally during conventional MLM pre-training (Clark et al., 2019;Voita et al., 2019;Manning et al., 2020). We observe an abrupt spike in SAS at a consistent point in training, and explore its impact on MLM capabilities by manipulating SAS during training. Our observations paint a picture of how interpretability artifacts may represent simplicity biases that compete with other learning strategies during MLM training. In summary, our main contributions are: \n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes.\n...\nAt the onset of this stage (which we call the structure onset), SAS spikes. After the spike, the model starts handling complex linguistic phenomena correctly, as signaled by a break in BLiMP score (which we call the capabilities onset). Although the functional complexity of the model declines for the rest of training, it increases between these breaks. \u2022 We introduce a regularizer to examine the causal role of SAS (defined in Section 2.2) and use it to show that SAS is necessary for handling complex linguistic phenomena (Section 4.2) and that SAS competes with an alternative strategy that exhibits its own break in the loss curve, which we call the alternative strategy onset. \u2022 Section 4.3 shows that briefly suppressing SAS improves model quality and accelerates convergence. \n\nSuppressing past the alternative strategy onset damages performance and blocks SAS long-term, suggesting this phase transition terminates a critical learning period. BERT first learns to focus on syntactic neighbors with specialized attention heads, and then exhibits grammatical capabilities in its MLM objective. The former (internal) and the latter (external) model behaviors both emerge abruptly, at moments we respectively call the structure onset (\u25b2) and capabilities onset ( ) (quantified as described in Section 2.3). We separately visualize three runs with different seeds, noting that these seeds differ in the stability of Unlabeled Attachment Score (UAS; see Section 2.1) after the structure onset, but uniformly show that SAS emerges almost entirely in a brief window of time. We show (a) MLM loss, with 95% confidence intervals across samples bynonparametric bootstrapping; (b) internal grammar structure, measured by UAS on the parse induced by the attention distributions; and (c) external grammar capabilities, measured by average BLiMP accuracy with 95% confidence intervals across tasks by nonparametric bootstrapping.\n\n## THE SYNTAX ACQUISITION PHASE\nMost work on scaling laws (Kaplan et al., 2020) presents test loss as a quantity that homogeneously responds to the scale of training, declining by a power law relative to the size of the corpus. In the MLM setting, we instead identify a precipitous drop in the loss curve of BERT Base (Fig. 1(a)), consistently spanning 20K-30K timesteps of training across various random seeds. We now show how this rapid learning stage can be interpreted as the composition of two distinct phase transitions. \n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset. We show similar trajectories on the MultiBERTs (Sellam et al., 2022) reproductions (Appendix F). \n\nBy observing these phase transitions, we can see that the internal representation of grammar, in the form of syntactic attention, precipitates the external observation of grammatical behavior, in the form of correct language modeling judgements on linguistically challenging examples. This is not only a single breakthrough during training, but a sequence of breakthroughs that appear to be dependent on each other. We might compare this to the \"checkmate in one\" BIG-Bench task, a known breakthrough behavior in autoregressive language models (Srivastava et al., 2022). Only at a large scale can models accurately identify checkmate moves, but further exploration revealed that the model was progressing in a linear fashion at offering consistently valid chess moves before that point.",
            "reference_string": "[261822542 | Chen et al. | 2023 | Citations: 74]"
        },
        {
            "title": "A non-ergodic framework for understanding emergent capabilities in Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.01638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2338713039",
                    "name": "Javier Marin"
                }
            ],
            "abstract": "Large language models have emergent capabilities that come unexpectedly at scale, but we need a theoretical framework to explain why and how they emerge. We prove that language models are actually non-ergodic systems while providing a mathematical framework based on Stuart Kauffman's theory of the adjacent possible (TAP) to explain capability emergence. Our resource-constrained TAP equation demonstrates how architectural, training, and contextual constraints interact to shape model capabilities through phase transitions in semantic space. We prove through experiments with three different language models that capacities emerge through discrete transitions guided by constraint interactions and path-dependent exploration. This framework provides a theoretical basis for understanding emergence in language models and guides the development of architectures that can guide capability emergence.",
            "corpus_id": 275324224,
            "sentences": [
                {
                    "corpus_id": "275324224",
                    "title": "A non-ergodic framework for understanding emergent capabilities in Large Language Models",
                    "text": "Large language models have emergent capabilities that come unexpectedly at scale, but we need a theoretical framework to explain why and how they emerge. We prove that language models are actually non-ergodic systems while providing a mathematical framework based on Stuart Kauffman's theory of the adjacent possible (TAP) to explain capability emergence. Our resource-constrained TAP equation demonstrates how architectural, training, and contextual constraints interact to shape model capabilities through phase transitions in semantic space. We prove through experiments with three different language models that capacities emerge through discrete transitions guided by constraint interactions and path-dependent exploration. This framework provides a theoretical basis for understanding emergence in language models and guides the development of architectures that can guide capability emergence.",
                    "score": 0.5219359393399943,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94921875
                }
            ],
            "relevance_judgement": 0.94921875,
            "relevance_judgment_input_expanded": "# Title: A non-ergodic framework for understanding emergent capabilities in Large Language Models\n# Venue: arXiv.org\n# Authors: Javier Marin\n## Abstract\nLarge language models have emergent capabilities that come unexpectedly at scale, but we need a theoretical framework to explain why and how they emerge. We prove that language models are actually non-ergodic systems while providing a mathematical framework based on Stuart Kauffman's theory of the adjacent possible (TAP) to explain capability emergence. Our resource-constrained TAP equation demonstrates how architectural, training, and contextual constraints interact to shape model capabilities through phase transitions in semantic space. We prove through experiments with three different language models that capacities emerge through discrete transitions guided by constraint interactions and path-dependent exploration. This framework provides a theoretical basis for understanding emergence in language models and guides the development of architectures that can guide capability emergence.\n",
            "reference_string": "[275324224 | Marin | 2025 | Citations: 0]"
        },
        {
            "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 77,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01830, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2006905770",
                    "name": "Badr AlKhamissi"
                },
                {
                    "authorId": "1780803442",
                    "name": "Greta Tuckute"
                },
                {
                    "authorId": "2329164095",
                    "name": "Yingtian Tang"
                },
                {
                    "authorId": "2092548838",
                    "name": "T. Binhuraib"
                },
                {
                    "authorId": "2691021",
                    "name": "Antoine Bosselut"
                },
                {
                    "authorId": "8551292",
                    "name": "Martin Schrimpf"
                }
            ],
            "abstract": "Large language models (LLMs) exhibit remarkable similarity to neural activity in the human language network. However, the key properties of language shaping brain-like representations, and their evolution during training as a function of different tasks remain unclear. We here benchmark 34 training checkpoints spanning 300B tokens across 8 different model sizes to analyze how brain alignment relates to linguistic competence. Specifically, we find that brain alignment tracks the development of formal linguistic competence -- i.e., knowledge of linguistic rules -- more closely than functional linguistic competence. While functional competence, which involves world knowledge and reasoning, continues to develop throughout training, its relationship with brain alignment is weaker, suggesting that the human language network primarily encodes formal linguistic structure rather than broader cognitive functions. We further show that model size is not a reliable predictor of brain alignment when controlling for feature size and find that the correlation between next-word prediction, behavioral alignment and brain alignment fades once models surpass human language proficiency. Finally, using the largest set of rigorous neural language benchmarks to date, we show that language brain alignment benchmarks remain unsaturated, highlighting opportunities for improving future models. Taken together, our findings suggest that the human language network is best modeled by formal, rather than functional, aspects of language.",
            "corpus_id": 276766647,
            "sentences": [
                {
                    "corpus_id": "276766647",
                    "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network",
                    "text": "In this work, we investigated how brain alignment in LLMs evolves throughout training, revealing different learning processes at play. We demonstrated that alignment with the human language network (LN) primarily correlates with formal linguistic competence Mahowald et al. (2024), peaking and saturating early in training. In contrast, functional linguistic competence, which involves world knowledge and reasoning, continues to grow beyond this stage. These findings suggest that the LN primarily encodes syntactic and compositional structure, in line with the literature of language neuroscience Fedorenko et al. (2024), while broader linguistic functions may rely on other cognitive systems beyond the LN. We also showed that model size is not a reliable predictor of brain alignment when controlling for feature size. Instead, alignment is determined by architectural inductive biases, token integration mechanisms, and training dynamics. Our standardized brain-scoring framework eliminates contextualization biases from previous work, ensuring more rigorous evaluations. Finally, we demonstrated that current brain alignment benchmarks are not saturated, indicating that LLMs can still be improved in modeling human language processing. Together, these findings challenge prior assumptions about how alignment emerges in LLMs and provide new insights into the relationship between artificial and biological language processing.",
                    "score": 0.4157778868427594,
                    "section_title": "Conclusion & Future Work",
                    "char_start_offset": 24602,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 323
                        },
                        {
                            "start": 324,
                            "end": 453
                        },
                        {
                            "start": 454,
                            "end": 709
                        },
                        {
                            "start": 710,
                            "end": 822
                        },
                        {
                            "start": 823,
                            "end": 943
                        },
                        {
                            "start": 944,
                            "end": 1076
                        },
                        {
                            "start": 1077,
                            "end": 1242
                        },
                        {
                            "start": 1243,
                            "end": 1433
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 258,
                            "end": 280,
                            "matchedPaperCorpusId": "268551442"
                        },
                        {
                            "start": 599,
                            "end": 622,
                            "matchedPaperCorpusId": "269112224"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94287109375
                },
                {
                    "corpus_id": "276766647",
                    "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network",
                    "text": "To assess formal linguistic competence, we use two benchmarks: BLIMP (Warstadt et al., 2019) and SYNTAXGYM (Gauthier et al., 2020). BLIMP evaluates key grammatical phenomena in English through 67 tasks, each containing 1,000 minimal pairs designed to test specific contrasts in syntax, morphology, and semantics. Complementing this, SYNTAXGYM consists of 31 tasks that systematically measure the syntactic knowledge of language models. Together, these benchmarks provide a robust framework for evaluating how well LLMs acquire and apply linguistic rules. \n\nFunctional Linguistic Competence Functional competence extends beyond linguistic rules, engaging a broader set of cognitive mechanisms. To assess this, we use six benchmarks covering world knowledge (ARC-EASY, ARC-CHALLENGE (Clark et al., 2018)), social reasoning (SOCIAL IQA (Sap et al., 2019)), physical reasoning (PIQA (Bisk et al., 2019)), and commonsense reasoning (WINOGRANDE (Sakaguchi et al., 2019), HELLASWAG (Zellers et al., 2019)). Together, these benchmarks provide a comprehensive evaluation of an LLM's ability to reason, infer implicit knowledge, and navigate real-world contexts.",
                    "score": 0.4814950333985718,
                    "section_title": "Formal Linguistic Competence",
                    "char_start_offset": 10180,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 435
                        },
                        {
                            "start": 436,
                            "end": 554
                        },
                        {
                            "start": 557,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 999
                        },
                        {
                            "start": 1000,
                            "end": 1152
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 107,
                            "end": 130,
                            "matchedPaperCorpusId": "220060899"
                        },
                        {
                            "start": 833,
                            "end": 851,
                            "matchedPaperCorpusId": "128296356"
                        },
                        {
                            "start": 879,
                            "end": 898,
                            "matchedPaperCorpusId": "208290939"
                        },
                        {
                            "start": 975,
                            "end": 997,
                            "matchedPaperCorpusId": "159041722"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83740234375
                }
            ],
            "relevance_judgement": 0.94287109375,
            "relevance_judgment_input_expanded": "# Title: From Language to Cognition: How LLMs Outgrow the Human Language Network\n# Venue: arXiv.org\n# Authors: Badr AlKhamissi, Greta Tuckute, Yingtian Tang, T. Binhuraib, Antoine Bosselut, Martin Schrimpf\n## Abstract\nLarge language models (LLMs) exhibit remarkable similarity to neural activity in the human language network. However, the key properties of language shaping brain-like representations, and their evolution during training as a function of different tasks remain unclear. We here benchmark 34 training checkpoints spanning 300B tokens across 8 different model sizes to analyze how brain alignment relates to linguistic competence. Specifically, we find that brain alignment tracks the development of formal linguistic competence -- i.e., knowledge of linguistic rules -- more closely than functional linguistic competence. While functional competence, which involves world knowledge and reasoning, continues to develop throughout training, its relationship with brain alignment is weaker, suggesting that the human language network primarily encodes formal linguistic structure rather than broader cognitive functions. We further show that model size is not a reliable predictor of brain alignment when controlling for feature size and find that the correlation between next-word prediction, behavioral alignment and brain alignment fades once models surpass human language proficiency. Finally, using the largest set of rigorous neural language benchmarks to date, we show that language brain alignment benchmarks remain unsaturated, highlighting opportunities for improving future models. Taken together, our findings suggest that the human language network is best modeled by formal, rather than functional, aspects of language.\n## Formal Linguistic Competence\nTo assess formal linguistic competence, we use two benchmarks: BLIMP (Warstadt et al., 2019) and SYNTAXGYM (Gauthier et al., 2020). BLIMP evaluates key grammatical phenomena in English through 67 tasks, each containing 1,000 minimal pairs designed to test specific contrasts in syntax, morphology, and semantics. Complementing this, SYNTAXGYM consists of 31 tasks that systematically measure the syntactic knowledge of language models. Together, these benchmarks provide a robust framework for evaluating how well LLMs acquire and apply linguistic rules. \n\nFunctional Linguistic Competence Functional competence extends beyond linguistic rules, engaging a broader set of cognitive mechanisms. To assess this, we use six benchmarks covering world knowledge (ARC-EASY, ARC-CHALLENGE (Clark et al., 2018)), social reasoning (SOCIAL IQA (Sap et al., 2019)), physical reasoning (PIQA (Bisk et al., 2019)), and commonsense reasoning (WINOGRANDE (Sakaguchi et al., 2019), HELLASWAG (Zellers et al., 2019)). Together, these benchmarks provide a comprehensive evaluation of an LLM's ability to reason, infer implicit knowledge, and navigate real-world contexts.\n\n## Conclusion & Future Work\nIn this work, we investigated how brain alignment in LLMs evolves throughout training, revealing different learning processes at play. We demonstrated that alignment with the human language network (LN) primarily correlates with formal linguistic competence Mahowald et al. (2024), peaking and saturating early in training. In contrast, functional linguistic competence, which involves world knowledge and reasoning, continues to grow beyond this stage. These findings suggest that the LN primarily encodes syntactic and compositional structure, in line with the literature of language neuroscience Fedorenko et al. (2024), while broader linguistic functions may rely on other cognitive systems beyond the LN. We also showed that model size is not a reliable predictor of brain alignment when controlling for feature size. Instead, alignment is determined by architectural inductive biases, token integration mechanisms, and training dynamics. Our standardized brain-scoring framework eliminates contextualization biases from previous work, ensuring more rigorous evaluations. Finally, we demonstrated that current brain alignment benchmarks are not saturated, indicating that LLMs can still be improved in modeling human language processing. Together, these findings challenge prior assumptions about how alignment emerges in LLMs and provide new insights into the relationship between artificial and biological language processing.",
            "reference_string": "[276766647 | AlKhamissi et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Probing Across Time: What Does RoBERTa Know and When?",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2021,
            "reference_count": 42,
            "citation_count": 87,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.findings-emnlp.71.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.07885, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Leo Z. Liu"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "11348687",
                    "name": "Jungo Kasai"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                },
                {
                    "authorId": "144365875",
                    "name": "Noah A. Smith"
                }
            ],
            "abstract": "Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.",
            "corpus_id": 233289478,
            "sentences": [
                {
                    "corpus_id": "233289478",
                    "title": "Probing Across Time: What Does RoBERTa Know and When?",
                    "text": "Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.",
                    "score": 0.48342877481182756,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92138671875
                }
            ],
            "relevance_judgement": 0.92138671875,
            "relevance_judgment_input_expanded": "# Title: Probing Across Time: What Does RoBERTa Know and When?\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Leo Z. Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, Noah A. Smith\n## Abstract\nModels of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.\n",
            "reference_string": "[233289478 | Liu et al. | 2021 | Citations: 87]"
        },
        {
            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 58,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.15419, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2087001989",
                    "name": "Tyler A. Chang"
                },
                {
                    "authorId": "144035504",
                    "name": "Z. Tu"
                },
                {
                    "authorId": "24316216",
                    "name": "B. Bergen"
                }
            ],
            "abstract": "Abstract How do language models learn to make predictions during pre-training? To study this, we extract learning curves from five autoregressive English language model pre-training runs, for 1M unseen tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We also find that individual tokens often exhibit sudden increases or decreases in loss that are surprisingly consistent across pre-training runs. To better understand these fluctuations, we quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be \u201cforgotten\u201d during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Based on our results, we argue for the existence of sequential learning dependencies between different model capabilities, and we characterize language model learning as early n-gram learning before gradual refinement of tail n-gram predictions.",
            "corpus_id": 261277016,
            "sentences": [
                {
                    "corpus_id": "261277016",
                    "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                    "text": "Our work provides evidence that there exist other capabilities or types of generalizations (e.g. non-syntactic abilities or even more fine-grained syntactic sub-abilities) that can only be learned after others, or at least only once the model reaches some particular state. Isolating these sequential dependencies is an exciting direction for future work. \n\nN-gram learning and refinement. As a further step towards understanding fine-grained learning patterns in language models, our work investigates whether simple statistical regularities can explain learning patterns such as the sudden loss changes discussed above. We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts ( \u00a76.2). High probability n-grams in particular are less likely to be \"forgotten\", suggesting that evolving model generalizations throughout pre-training have larger effects on low-probability n-grams. Combined with findings that language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; Chang and Bergen, 2022b), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; Liu et al., 2021). \n\nRobust model deployment. Our work also has implications for robust model deployment. High token frequencies and n-gram probabilities are by far the most influential predictors of early and stable learning in language models ( \u00a76.2, with marginal additional effects of context lengths and likelihoods). As language models are deployed in do-mains with highly-specific vocabulary terms (e.g. healthcare, law, and finance; Yang et al., 2024), the accurate prediction of infrequent domain-specific terms during text generation is likely to require extensive pre-training (late acquisition, likely mitigated by large pre-training datasets). Such domainspecific text generation is also likely to be unstable across models and pre-training steps (high variability, potentially more difficult to mitigate).",
                    "score": 0.4097270538154639,
                    "section_title": "Discussion",
                    "char_start_offset": 29451,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 96
                        },
                        {
                            "start": 97,
                            "end": 273
                        },
                        {
                            "start": 274,
                            "end": 355
                        },
                        {
                            "start": 358,
                            "end": 389
                        },
                        {
                            "start": 390,
                            "end": 621
                        },
                        {
                            "start": 622,
                            "end": 769
                        },
                        {
                            "start": 770,
                            "end": 962
                        },
                        {
                            "start": 963,
                            "end": 1338
                        },
                        {
                            "start": 1339,
                            "end": 1388
                        },
                        {
                            "start": 1391,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1475
                        },
                        {
                            "start": 1476,
                            "end": 1692
                        },
                        {
                            "start": 1693,
                            "end": 1780
                        },
                        {
                            "start": 1781,
                            "end": 2026
                        },
                        {
                            "start": 2027,
                            "end": 2189
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1112,
                            "end": 1136,
                            "matchedPaperCorpusId": "238408423"
                        },
                        {
                            "start": 1370,
                            "end": 1387,
                            "matchedPaperCorpusId": "233289478"
                        },
                        {
                            "start": 1811,
                            "end": 1829,
                            "matchedPaperCorpusId": "258331833"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91845703125
                },
                {
                    "corpus_id": "261277016",
                    "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                    "text": "Previous work has studied the pre-training dynamics of language models (Saphra and Lopez, 2019). Choshen et al. (2022) and Evanson et al. (2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020;Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context. \n\nIndeed, previous work has studied how individual tokens are learned during pre-training. \n\nFor example, word learning is highly dependent on word frequency (Chang and Bergen, 2022b). Larger models memorize more examples during pre-training without overfitting (Tirumala et al., 2022), but the time step that a model sees an example does not affect memorization (Biderman et al., 2023). Most similar to our work, Xia et al. (2023) collect learning curves for individual tokens in context, finding that some examples exhibit a \"doubledescent\" trend where they first increase then decrease in surprisal. All of the studies above collect language model learning curves during pre-training, either for individual examples or targeted benchmark performance. Here, we introduce metrics to characterize such curves, we identify general learning patterns, and we isolate text features that are predictive of learning speed and stability.",
                    "score": 0.4539565577215165,
                    "section_title": "Related Work",
                    "char_start_offset": 3961,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 96
                        },
                        {
                            "start": 97,
                            "end": 291
                        },
                        {
                            "start": 292,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 603
                        },
                        {
                            "start": 604,
                            "end": 682
                        },
                        {
                            "start": 683,
                            "end": 783
                        },
                        {
                            "start": 786,
                            "end": 874
                        },
                        {
                            "start": 877,
                            "end": 968
                        },
                        {
                            "start": 969,
                            "end": 1171
                        },
                        {
                            "start": 1172,
                            "end": 1386
                        },
                        {
                            "start": 1387,
                            "end": 1537
                        },
                        {
                            "start": 1538,
                            "end": 1714
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 71,
                            "end": 95,
                            "matchedPaperCorpusId": "267903883"
                        },
                        {
                            "start": 123,
                            "end": 144,
                            "matchedPaperCorpusId": "259089351"
                        },
                        {
                            "start": 422,
                            "end": 443,
                            "matchedPaperCorpusId": "222140842"
                        },
                        {
                            "start": 443,
                            "end": 460,
                            "matchedPaperCorpusId": "233289478"
                        },
                        {
                            "start": 942,
                            "end": 967,
                            "matchedPaperCorpusId": "238408423"
                        },
                        {
                            "start": 1046,
                            "end": 1069,
                            "matchedPaperCorpusId": "248986465"
                        },
                        {
                            "start": 1147,
                            "end": 1170,
                            "matchedPaperCorpusId": "257921893"
                        },
                        {
                            "start": 1198,
                            "end": 1215,
                            "matchedPaperCorpusId": "254877112"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90576171875
                },
                {
                    "corpus_id": "261277016",
                    "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                    "text": "Language models have received unprecedented attention in recent years due to impressive performance on natural language tasks (e.g. OpenAI, 2022;Google, 2023;Anthropic, 2023). However, these models are initialized as random word (token) generators, and it remains unclear how the models achieve complex linguistic abilities during pre-training. Previous work has investigated when syntactic, semantic, and reasoning abilities emerge (Liu et al., 2021;Evanson et al., 2023), quantified ages of acquisition for tokens averaged over contexts (Chang and Bergen, 2022b), and extracted learning curves for individual examples (Xia et al., 2023). However, features that influence individual learning curves have yet to be identified (e.g. n-gram probabilities and context lengths). Given any token in context, it is largely unknown when or how stably that token would be learned. \n\nFrom a scientific perspective, understanding when examples are learned by language models can provide insights into possible mechanisms for language acquisition. Regardless of their similarity to human language processing, language models are exemplars of how learning from language statistics alone (i.e. \"distributional\" learning) can lead to complex linguistic abilities (Chang and Bergen, 2022b;Warstadt and Bowman, 2023;Mahowald et al., 2023). Notably, despite smoothly decreasing corpus-level loss and independent and identically distributed (i.i.d.) data throughout pre-training, individual text examples exhibit learning curves with sudden decreases and increases in loss ( \u00a75 and Xia et al., 2023). This highlights the importance of examining individual example learning curves for pre-training dynamics research; aggregate curves often do not capture the fluctuations exhibited by individual examples. Our work seeks to characterize these fine-grained convergence patterns in terms of simpler distributional statistics. \n\nFrom a practical perspective, understanding language model learning curves can inform the pre-training and deployment of language models. Learning curve results might allow NLP practitioners to determine how much pre-training is necessary for different capabilities and what behaviors will remain stable after additional pre-training (e.g. \"continual learning\" on more recent data; Jin et al., 2022).",
                    "score": 0.44848114772615866,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 344
                        },
                        {
                            "start": 345,
                            "end": 639
                        },
                        {
                            "start": 640,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 872
                        },
                        {
                            "start": 875,
                            "end": 1036
                        },
                        {
                            "start": 1037,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1582
                        },
                        {
                            "start": 1583,
                            "end": 1786
                        },
                        {
                            "start": 1787,
                            "end": 1904
                        },
                        {
                            "start": 1907,
                            "end": 2044
                        },
                        {
                            "start": 2045,
                            "end": 2246
                        },
                        {
                            "start": 2247,
                            "end": 2307
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 433,
                            "end": 451,
                            "matchedPaperCorpusId": "233289478"
                        },
                        {
                            "start": 451,
                            "end": 472,
                            "matchedPaperCorpusId": "259089351"
                        },
                        {
                            "start": 539,
                            "end": 564,
                            "matchedPaperCorpusId": "238408423"
                        },
                        {
                            "start": 620,
                            "end": 638,
                            "matchedPaperCorpusId": "254877112"
                        },
                        {
                            "start": 1249,
                            "end": 1274,
                            "matchedPaperCorpusId": "238408423"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8828125
                }
            ],
            "relevance_judgement": 0.91845703125,
            "relevance_judgment_input_expanded": "# Title: Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Tyler A. Chang, Z. Tu, B. Bergen\n## Abstract\nAbstract How do language models learn to make predictions during pre-training? To study this, we extract learning curves from five autoregressive English language model pre-training runs, for 1M unseen tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We also find that individual tokens often exhibit sudden increases or decreases in loss that are surprisingly consistent across pre-training runs. To better understand these fluctuations, we quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be \u201cforgotten\u201d during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Based on our results, we argue for the existence of sequential learning dependencies between different model capabilities, and we characterize language model learning as early n-gram learning before gradual refinement of tail n-gram predictions.\n## Introduction\nLanguage models have received unprecedented attention in recent years due to impressive performance on natural language tasks (e.g. OpenAI, 2022;Google, 2023;Anthropic, 2023). However, these models are initialized as random word (token) generators, and it remains unclear how the models achieve complex linguistic abilities during pre-training. Previous work has investigated when syntactic, semantic, and reasoning abilities emerge (Liu et al., 2021;Evanson et al., 2023), quantified ages of acquisition for tokens averaged over contexts (Chang and Bergen, 2022b), and extracted learning curves for individual examples (Xia et al., 2023). However, features that influence individual learning curves have yet to be identified (e.g. n-gram probabilities and context lengths). Given any token in context, it is largely unknown when or how stably that token would be learned. \n\nFrom a scientific perspective, understanding when examples are learned by language models can provide insights into possible mechanisms for language acquisition. Regardless of their similarity to human language processing, language models are exemplars of how learning from language statistics alone (i.e. \"distributional\" learning) can lead to complex linguistic abilities (Chang and Bergen, 2022b;Warstadt and Bowman, 2023;Mahowald et al., 2023). Notably, despite smoothly decreasing corpus-level loss and independent and identically distributed (i.i.d.) data throughout pre-training, individual text examples exhibit learning curves with sudden decreases and increases in loss ( \u00a75 and Xia et al., 2023). This highlights the importance of examining individual example learning curves for pre-training dynamics research; aggregate curves often do not capture the fluctuations exhibited by individual examples. Our work seeks to characterize these fine-grained convergence patterns in terms of simpler distributional statistics. \n\nFrom a practical perspective, understanding language model learning curves can inform the pre-training and deployment of language models. Learning curve results might allow NLP practitioners to determine how much pre-training is necessary for different capabilities and what behaviors will remain stable after additional pre-training (e.g. \"continual learning\" on more recent data; Jin et al., 2022).\n\n## Related Work\nPrevious work has studied the pre-training dynamics of language models (Saphra and Lopez, 2019). Choshen et al. (2022) and Evanson et al. (2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020;Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context. \n\nIndeed, previous work has studied how individual tokens are learned during pre-training. \n\nFor example, word learning is highly dependent on word frequency (Chang and Bergen, 2022b). Larger models memorize more examples during pre-training without overfitting (Tirumala et al., 2022), but the time step that a model sees an example does not affect memorization (Biderman et al., 2023). Most similar to our work, Xia et al. (2023) collect learning curves for individual tokens in context, finding that some examples exhibit a \"doubledescent\" trend where they first increase then decrease in surprisal. All of the studies above collect language model learning curves during pre-training, either for individual examples or targeted benchmark performance. Here, we introduce metrics to characterize such curves, we identify general learning patterns, and we isolate text features that are predictive of learning speed and stability.\n\n## Discussion\nOur work provides evidence that there exist other capabilities or types of generalizations (e.g. non-syntactic abilities or even more fine-grained syntactic sub-abilities) that can only be learned after others, or at least only once the model reaches some particular state. Isolating these sequential dependencies is an exciting direction for future work. \n\nN-gram learning and refinement. As a further step towards understanding fine-grained learning patterns in language models, our work investigates whether simple statistical regularities can explain learning patterns such as the sudden loss changes discussed above. We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts ( \u00a76.2). High probability n-grams in particular are less likely to be \"forgotten\", suggesting that evolving model generalizations throughout pre-training have larger effects on low-probability n-grams. Combined with findings that language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; Chang and Bergen, 2022b), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; Liu et al., 2021). \n\nRobust model deployment. Our work also has implications for robust model deployment. High token frequencies and n-gram probabilities are by far the most influential predictors of early and stable learning in language models ( \u00a76.2, with marginal additional effects of context lengths and likelihoods). As language models are deployed in do-mains with highly-specific vocabulary terms (e.g. healthcare, law, and finance; Yang et al., 2024), the accurate prediction of infrequent domain-specific terms during text generation is likely to require extensive pre-training (late acquisition, likely mitigated by large pre-training datasets). Such domainspecific text generation is also likely to be unstable across models and pre-training steps (high variability, potentially more difficult to mitigate).",
            "reference_string": "[261277016 | Chang et al. | 2023 | Citations: 13]"
        },
        {
            "title": "Are formal and functional linguistic mechanisms dissociated in language models?",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 86,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.11302, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350350287",
                    "name": "Michael Hanna"
                },
                {
                    "authorId": "2346327043",
                    "name": "Yonatan Belinkov"
                },
                {
                    "authorId": "2346109852",
                    "name": "Sandro Pezzelle"
                }
            ],
            "abstract": "Although large language models (LLMs) are increasingly capable, these capabilities are unevenly distributed: they excel at formal linguistic tasks, such as producing fluent, grammatical text, but struggle more with functional linguistic tasks like reasoning and consistent fact retrieval. Inspired by neuroscience, recent work suggests that to succeed on both formal and functional linguistic tasks, LLMs should use different mechanisms for each; such localization could either be built-in or emerge spontaneously through training. In this paper, we ask: do current models, with fast-improving functional linguistic abilities, exhibit distinct localization of formal and functional linguistic mechanisms? We answer this by finding and comparing the\"circuits\", or minimal computational subgraphs, responsible for various formal and functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that while there is indeed little overlap between circuits for formal and functional tasks, there is also little overlap between formal linguistic tasks, as exists in the human brain. Thus, a single formal linguistic network, unified and distinct from functional task circuits, remains elusive. However, in terms of cross-task faithfulness - the ability of one circuit to solve another's task - we observe a separation between formal and functional mechanisms, suggesting that shared mechanisms between formal tasks may exist.",
            "corpus_id": 277043826,
            "sentences": [
                {
                    "corpus_id": "277043826",
                    "title": "Are formal and functional linguistic mechanisms dissociated in language models?",
                    "text": "Although large language models (LLMs) are increasingly capable, these capabilities are unevenly distributed: they excel at formal linguistic tasks, such as producing fluent, grammatical text, but struggle more with functional linguistic tasks like reasoning and consistent fact retrieval. Inspired by neuroscience, recent work suggests that to succeed on both formal and functional linguistic tasks, LLMs should use different mechanisms for each; such localization could either be built-in or emerge spontaneously through training. In this paper, we ask: do current models, with fast-improving functional linguistic abilities, exhibit distinct localization of formal and functional linguistic mechanisms? We answer this by finding and comparing the\"circuits\", or minimal computational subgraphs, responsible for various formal and functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that while there is indeed little overlap between circuits for formal and functional tasks, there is also little overlap between formal linguistic tasks, as exists in the human brain. Thus, a single formal linguistic network, unified and distinct from functional task circuits, remains elusive. However, in terms of cross-task faithfulness - the ability of one circuit to solve another's task - we observe a separation between formal and functional mechanisms, suggesting that shared mechanisms between formal tasks may exist.",
                    "score": 0.5196507304291635,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90869140625
                }
            ],
            "relevance_judgement": 0.90869140625,
            "relevance_judgment_input_expanded": "# Title: Are formal and functional linguistic mechanisms dissociated in language models?\n# Venue: arXiv.org\n# Authors: Michael Hanna, Yonatan Belinkov, Sandro Pezzelle\n## Abstract\nAlthough large language models (LLMs) are increasingly capable, these capabilities are unevenly distributed: they excel at formal linguistic tasks, such as producing fluent, grammatical text, but struggle more with functional linguistic tasks like reasoning and consistent fact retrieval. Inspired by neuroscience, recent work suggests that to succeed on both formal and functional linguistic tasks, LLMs should use different mechanisms for each; such localization could either be built-in or emerge spontaneously through training. In this paper, we ask: do current models, with fast-improving functional linguistic abilities, exhibit distinct localization of formal and functional linguistic mechanisms? We answer this by finding and comparing the\"circuits\", or minimal computational subgraphs, responsible for various formal and functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that while there is indeed little overlap between circuits for formal and functional tasks, there is also little overlap between formal linguistic tasks, as exists in the human brain. Thus, a single formal linguistic network, unified and distinct from functional task circuits, remains elusive. However, in terms of cross-task faithfulness - the ability of one circuit to solve another's task - we observe a separation between formal and functional mechanisms, suggesting that shared mechanisms between formal tasks may exist.\n",
            "reference_string": "[277043826 | Hanna et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Hallucination of Multimodal Large Language Models: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 192,
            "citation_count": 197,
            "influential_citation_count": 10,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.18930, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237427303",
                    "name": "Zechen Bai"
                },
                {
                    "authorId": "2299164401",
                    "name": "Pichao Wang"
                },
                {
                    "authorId": "39102205",
                    "name": "Tianjun Xiao"
                },
                {
                    "authorId": "2264160722",
                    "name": "Tong He"
                },
                {
                    "authorId": "2008848359",
                    "name": "Zongbo Han"
                },
                {
                    "authorId": "2298907819",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2269732179",
                    "name": "Mike Zheng Shou"
                }
            ],
            "abstract": "This survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated significant advancements and remarkable abilities in multimodal tasks. Despite these promising developments, MLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination, which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real-world applications. This problem has attracted increasing attention, prompting efforts to detect and mitigate such inaccuracies. We review recent advances in identifying, evaluating, and mitigating these hallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and strategies developed to address this issue. Additionally, we analyze the current challenges and limitations, formulating open questions that delineate potential pathways for future research. By drawing the granular classification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field. Through our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners alike. Resources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination.",
            "corpus_id": 269449935,
            "sentences": [
                {
                    "corpus_id": "269449935",
                    "title": "Hallucination of Multimodal Large Language Models: A Survey",
                    "text": "Before moving to multimodal large language models, it is essential to introduce the concept of large language models. Typically, LLMs encompass a range of transformer-based models that are extensively trained on vast textual datasets. Prominent examples include GPT-3 [14], PaLM [31], LLaMA [153], and . Through scaling both data volume and model capacity, LLMs demonstrate notable emergent capabilities, including In-Context Learning [14], Chain-of-Thought prompting [168] and instruction following [130], among others. \n\nThe characteristics and behaviors of LLMs are intricately linked to their training processes. LLMs typically undergo three primary training stages: pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). Below, we provide a concise overview of each stage to facilitate comprehension. \n\nPre-trianing. Pre-training serves as a fundamental phase in the learning process of LLMs [219]. During this stage, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence. By undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities. This pre-training process establishes a solid groundwork for the models to undertake subsequent fine-tuning tasks effectively. \n\nSupervised Fine-Tuning. Although pre-training equips LLMs with substantial knowledge and skills, it's important to acknowledge that its primary focus is on optimizing for completion. Consequently, pre-trained LLMs essentially function as completion machines, which may create a misalignment between the objective of predicting the next word within LLMs and the user's objective of obtaining desired responses. To address this disparity, the concept of Supervised Fine-Tuning (SFT) [204] has been introduced. SFT involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, thereby enhancing the capabilities and controllability of LLMs. \n\nReinforcement Learning from Human Feedback. Although SFT has made strides in enabling LLMs to adhere to user instructions, there remains a need for further alignment with human preferences.",
                    "score": 0.3957890807559977,
                    "section_title": "Large Language Models",
                    "char_start_offset": 5855,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 117
                        },
                        {
                            "start": 118,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 303
                        },
                        {
                            "start": 304,
                            "end": 520
                        },
                        {
                            "start": 523,
                            "end": 616
                        },
                        {
                            "start": 617,
                            "end": 769
                        },
                        {
                            "start": 770,
                            "end": 849
                        },
                        {
                            "start": 852,
                            "end": 865
                        },
                        {
                            "start": 866,
                            "end": 947
                        },
                        {
                            "start": 948,
                            "end": 1076
                        },
                        {
                            "start": 1077,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1400
                        },
                        {
                            "start": 1403,
                            "end": 1426
                        },
                        {
                            "start": 1427,
                            "end": 1585
                        },
                        {
                            "start": 1586,
                            "end": 1812
                        },
                        {
                            "start": 1813,
                            "end": 1910
                        },
                        {
                            "start": 1911,
                            "end": 2078
                        },
                        {
                            "start": 2081,
                            "end": 2124
                        },
                        {
                            "start": 2125,
                            "end": 2270
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 268,
                            "end": 272,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 279,
                            "end": 283,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 435,
                            "end": 439,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 468,
                            "end": 473,
                            "matchedPaperCorpusId": "246411621"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90673828125
                }
            ],
            "relevance_judgement": 0.90673828125,
            "relevance_judgment_input_expanded": "# Title: Hallucination of Multimodal Large Language Models: A Survey\n# Venue: arXiv.org\n# Authors: Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, Mike Zheng Shou\n## Abstract\nThis survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated significant advancements and remarkable abilities in multimodal tasks. Despite these promising developments, MLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination, which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real-world applications. This problem has attracted increasing attention, prompting efforts to detect and mitigate such inaccuracies. We review recent advances in identifying, evaluating, and mitigating these hallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and strategies developed to address this issue. Additionally, we analyze the current challenges and limitations, formulating open questions that delineate potential pathways for future research. By drawing the granular classification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field. Through our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners alike. Resources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination.\n## Large Language Models\nBefore moving to multimodal large language models, it is essential to introduce the concept of large language models. Typically, LLMs encompass a range of transformer-based models that are extensively trained on vast textual datasets. Prominent examples include GPT-3 [14], PaLM [31], LLaMA [153], and . Through scaling both data volume and model capacity, LLMs demonstrate notable emergent capabilities, including In-Context Learning [14], Chain-of-Thought prompting [168] and instruction following [130], among others. \n\nThe characteristics and behaviors of LLMs are intricately linked to their training processes. LLMs typically undergo three primary training stages: pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). Below, we provide a concise overview of each stage to facilitate comprehension. \n\nPre-trianing. Pre-training serves as a fundamental phase in the learning process of LLMs [219]. During this stage, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence. By undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities. This pre-training process establishes a solid groundwork for the models to undertake subsequent fine-tuning tasks effectively. \n\nSupervised Fine-Tuning. Although pre-training equips LLMs with substantial knowledge and skills, it's important to acknowledge that its primary focus is on optimizing for completion. Consequently, pre-trained LLMs essentially function as completion machines, which may create a misalignment between the objective of predicting the next word within LLMs and the user's objective of obtaining desired responses. To address this disparity, the concept of Supervised Fine-Tuning (SFT) [204] has been introduced. SFT involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, thereby enhancing the capabilities and controllability of LLMs. \n\nReinforcement Learning from Human Feedback. Although SFT has made strides in enabling LLMs to adhere to user instructions, there remains a need for further alignment with human preferences.",
            "reference_string": "[269449935 | Bai et al. | 2024 | Citations: 197]"
        },
        {
            "title": "Contextualizing Search Queries In-Context Learning for Conversational Rewriting with LLMs",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 24,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.15009, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347119144",
                    "name": "Raymond Wilson"
                },
                {
                    "authorId": "2346835859",
                    "name": "Chase Carter"
                },
                {
                    "authorId": "2346835565",
                    "name": "Cole Graham"
                }
            ],
            "abstract": "Conversational query rewriting is crucial for effective conversational search, yet traditional supervised methods require substantial labeled data, which is scarce in low-resource settings. This paper introduces Prompt-Guided In-Context Learning, a novel approach that leverages the in-context learning capabilities of Large Language Models (LLMs) for few-shot conversational query rewriting. Our method employs carefully designed prompts, incorporating task descriptions, input/output format specifications, and a small set of illustrative examples, to guide pre-trained LLMs to generate context-independent queries without explicit fine-tuning. Extensive experiments on benchmark datasets, TREC and Taskmaster-1, demonstrate that our approach significantly outperforms strong baselines, including supervised models and contrastive co-training methods, across various evaluation metrics such as BLEU, ROUGE-L, Success Rate, and MRR. Ablation studies confirm the importance of in-context examples, and human evaluations further validate the superior fluency, relevance, and context utilization of our generated rewrites. The results highlight the potential of prompt-guided in-context learning as an efficient and effective paradigm for low-resource conversational query rewriting, reducing the reliance on extensive labeled data and complex training procedures.",
            "corpus_id": 276557765,
            "sentences": [
                {
                    "corpus_id": "276557765",
                    "title": "Contextualizing Search Queries In-Context Learning for Conversational Rewriting with LLMs",
                    "text": "Large Language Models (LLMs) have emerged as transformative technologies in Natural Language Processing (NLP), demonstrating remarkable capabilities across a wide spectrum of tasks, including text generation, machine translation, and complex reasoning [14]. The generalization capabilities of LLMs, especially in multi-capability scenarios, are a subject of ongoing research [15]. These models, typically based on the Transformer architecture [16], are pre-trained on massive text corpora, enabling them to acquire extensive world knowledge and linguistic proficiency. The groundbreaking work on Transformer networks [16] laid the foundation for modern LLMs, introducing the attention mechanism as a core building block and demonstrating its effectiveness in capturing long-range dependencies in text. \n\nEarly LLMs, such as GPT [17], showcased the potential of generative pre-training for improving language understanding. Pre-trained models like EventBERT demonstrate the effectiveness of this approach for tasks like event correlation reasoning [11]. GPT and its successors, like GPT-3 [18], demonstrated impressive few-shot learning abilities, enabling them to perform novel tasks with only a few examples provided in the input prompt. Visual in-context learning has been shown to be effective for VLMs [3]. [18] highlighted the in-context learning paradigm, where LLMs adapt to new tasks without explicit gradient updates, simply by conditioning on input-output demonstrations. Scaling laws for neural language models have further revealed that model performance consistently improves with increasing model size, dataset size, and computational resources, suggesting a path towards even more powerful LLMs by scaling up training [19]. Research also explores training specialized VLMs, such as for medical applications, using techniques like abnormal-aware feedback [20]. Furthermore, understanding and improving long-context reasoning in VLMs is an important direction [21]. \n\nBeyond model architecture and scale, prompting strategies have become increasingly important for eliciting desired behaviors from LLMs. Chain-of-Thought (CoT) prompting, introduced by [22], is a notable example, demonstrating that providing step-by-step reasoning examples in prompts can unlock and enhance the reasoning capabilities of LLMs on complex tasks.",
                    "score": 0.38230348970801564,
                    "section_title": "B. Large Language Models",
                    "char_start_offset": 7589,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 380
                        },
                        {
                            "start": 381,
                            "end": 568
                        },
                        {
                            "start": 569,
                            "end": 801
                        },
                        {
                            "start": 804,
                            "end": 922
                        },
                        {
                            "start": 923,
                            "end": 1052
                        },
                        {
                            "start": 1053,
                            "end": 1238
                        },
                        {
                            "start": 1239,
                            "end": 1310
                        },
                        {
                            "start": 1311,
                            "end": 1481
                        },
                        {
                            "start": 1482,
                            "end": 1738
                        },
                        {
                            "start": 1739,
                            "end": 1874
                        },
                        {
                            "start": 1875,
                            "end": 1978
                        },
                        {
                            "start": 1981,
                            "end": 2116
                        },
                        {
                            "start": 2117,
                            "end": 2340
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 252,
                            "end": 256,
                            "matchedPaperCorpusId": "275405666"
                        },
                        {
                            "start": 443,
                            "end": 447,
                            "matchedPaperCorpusId": "243936778"
                        },
                        {
                            "start": 617,
                            "end": 621,
                            "matchedPaperCorpusId": "243936778"
                        },
                        {
                            "start": 1047,
                            "end": 1051,
                            "matchedPaperCorpusId": "238744494"
                        },
                        {
                            "start": 1088,
                            "end": 1092,
                            "matchedPaperCorpusId": "248987346"
                        },
                        {
                            "start": 1306,
                            "end": 1309,
                            "matchedPaperCorpusId": "267750174"
                        },
                        {
                            "start": 1311,
                            "end": 1315,
                            "matchedPaperCorpusId": "248987346"
                        },
                        {
                            "start": 1733,
                            "end": 1737,
                            "matchedPaperCorpusId": "210861095"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89990234375
                }
            ],
            "relevance_judgement": 0.89990234375,
            "relevance_judgment_input_expanded": "# Title: Contextualizing Search Queries In-Context Learning for Conversational Rewriting with LLMs\n# Venue: arXiv.org\n# Authors: Raymond Wilson, Chase Carter, Cole Graham\n## Abstract\nConversational query rewriting is crucial for effective conversational search, yet traditional supervised methods require substantial labeled data, which is scarce in low-resource settings. This paper introduces Prompt-Guided In-Context Learning, a novel approach that leverages the in-context learning capabilities of Large Language Models (LLMs) for few-shot conversational query rewriting. Our method employs carefully designed prompts, incorporating task descriptions, input/output format specifications, and a small set of illustrative examples, to guide pre-trained LLMs to generate context-independent queries without explicit fine-tuning. Extensive experiments on benchmark datasets, TREC and Taskmaster-1, demonstrate that our approach significantly outperforms strong baselines, including supervised models and contrastive co-training methods, across various evaluation metrics such as BLEU, ROUGE-L, Success Rate, and MRR. Ablation studies confirm the importance of in-context examples, and human evaluations further validate the superior fluency, relevance, and context utilization of our generated rewrites. The results highlight the potential of prompt-guided in-context learning as an efficient and effective paradigm for low-resource conversational query rewriting, reducing the reliance on extensive labeled data and complex training procedures.\n## B. Large Language Models\nLarge Language Models (LLMs) have emerged as transformative technologies in Natural Language Processing (NLP), demonstrating remarkable capabilities across a wide spectrum of tasks, including text generation, machine translation, and complex reasoning [14]. The generalization capabilities of LLMs, especially in multi-capability scenarios, are a subject of ongoing research [15]. These models, typically based on the Transformer architecture [16], are pre-trained on massive text corpora, enabling them to acquire extensive world knowledge and linguistic proficiency. The groundbreaking work on Transformer networks [16] laid the foundation for modern LLMs, introducing the attention mechanism as a core building block and demonstrating its effectiveness in capturing long-range dependencies in text. \n\nEarly LLMs, such as GPT [17], showcased the potential of generative pre-training for improving language understanding. Pre-trained models like EventBERT demonstrate the effectiveness of this approach for tasks like event correlation reasoning [11]. GPT and its successors, like GPT-3 [18], demonstrated impressive few-shot learning abilities, enabling them to perform novel tasks with only a few examples provided in the input prompt. Visual in-context learning has been shown to be effective for VLMs [3]. [18] highlighted the in-context learning paradigm, where LLMs adapt to new tasks without explicit gradient updates, simply by conditioning on input-output demonstrations. Scaling laws for neural language models have further revealed that model performance consistently improves with increasing model size, dataset size, and computational resources, suggesting a path towards even more powerful LLMs by scaling up training [19]. Research also explores training specialized VLMs, such as for medical applications, using techniques like abnormal-aware feedback [20]. Furthermore, understanding and improving long-context reasoning in VLMs is an important direction [21]. \n\nBeyond model architecture and scale, prompting strategies have become increasingly important for eliciting desired behaviors from LLMs. Chain-of-Thought (CoT) prompting, introduced by [22], is a notable example, demonstrating that providing step-by-step reasoning examples in prompts can unlock and enhance the reasoning capabilities of LLMs on complex tasks.",
            "reference_string": "[276557765 | Wilson et al. | 2025 | Citations: 1]"
        },
        {
            "title": "The Grammar-Learning Trajectories of Neural Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 67,
            "citation_count": 29,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.acl-long.568.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.06096, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "41019330",
                    "name": "Leshem Choshen"
                },
                {
                    "authorId": "94064232",
                    "name": "Guy Hacohen"
                },
                {
                    "authorId": "1789171",
                    "name": "D. Weinshall"
                },
                {
                    "authorId": "2769805",
                    "name": "Omri Abend"
                }
            ],
            "abstract": "The learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker. To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make. In this paper, we show that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance. These findings suggest that there is some mutual inductive bias that underlies these models\u2019 learning of linguistic phenomena. Taking inspiration from psycholinguistics, we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs.Leveraging these findings, we compare the relative performance on different phenomena at varying learning stages with simpler reference models. Results suggest that NLMs exhibit consistent \u201cdevelopmental\u201d stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them.",
            "corpus_id": 237491997,
            "sentences": [
                {
                    "corpus_id": "237491997",
                    "title": "The Grammar-Learning Trajectories of Neural Language Models",
                    "text": "We showed that NLMs learn English grammatical phenomena in a consistent order, and subsequently investigated the emerging trajectory. Our findings suggest that NLMs present consistent and informative trends. This finding suggests a path for studying NLMs' acquired behavior through their learning dynamics, as a useful complementary perspective to the study of final representations.\n\nFuture work will consider the impact of additional factors, architectures and learning phases that appear only later in training. We hope that this work will increase the affinity between the knowledge and methodologies employed in developmental studies, and those used for studying NLMs. Our goal is to obtain a better understanding of what makes linguistic generalization complex or simple to learn, for both humans and NLMs.",
                    "score": 0.398657787598625,
                    "section_title": "Summary and Conclusions",
                    "char_start_offset": 31707,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89453125
                }
            ],
            "relevance_judgement": 0.89453125,
            "relevance_judgment_input_expanded": "# Title: The Grammar-Learning Trajectories of Neural Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Leshem Choshen, Guy Hacohen, D. Weinshall, Omri Abend\n## Abstract\nThe learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker. To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make. In this paper, we show that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance. These findings suggest that there is some mutual inductive bias that underlies these models\u2019 learning of linguistic phenomena. Taking inspiration from psycholinguistics, we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs.Leveraging these findings, we compare the relative performance on different phenomena at varying learning stages with simpler reference models. Results suggest that NLMs exhibit consistent \u201cdevelopmental\u201d stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them.\n## Summary and Conclusions\nWe showed that NLMs learn English grammatical phenomena in a consistent order, and subsequently investigated the emerging trajectory. Our findings suggest that NLMs present consistent and informative trends. This finding suggests a path for studying NLMs' acquired behavior through their learning dynamics, as a useful complementary perspective to the study of final representations.\n\nFuture work will consider the impact of additional factors, architectures and learning phases that appear only later in training. We hope that this work will increase the affinity between the knowledge and methodologies employed in developmental studies, and those used for studying NLMs. Our goal is to obtain a better understanding of what makes linguistic generalization complex or simple to learn, for both humans and NLMs.",
            "reference_string": "[237491997 | Choshen et al. | 2021 | Citations: 29]"
        },
        {
            "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
            "venue": "IEEE Access",
            "year": 2025,
            "reference_count": 205,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2025.3554586",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3554586?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3554586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326763710",
                    "name": "Dingzong Zhang"
                },
                {
                    "authorId": "2345374431",
                    "name": "Devi Listiyani"
                },
                {
                    "authorId": "2302811064",
                    "name": "Priyanka Singh"
                },
                {
                    "authorId": "2309099465",
                    "name": "Manoranjan Mohanty"
                }
            ],
            "abstract": "In the era of Large Language Models (LLMs), Knowledge Distillation (KD) enables the transfer of capabilities from proprietary LLMs to open-source models. This survey provides a detailed discussion of the basic principles, algorithms, and implementation methods of knowledge distillation. It explores KD\u2019s impact on LLMs, emphasizing its utility in model compression, performance enhancement, and self-improvement. Through the analysis of practical examples such as DistilBERT, TinyBERT, and MobileBERT, the paper demonstrates how knowledge distillation can markedly enhance the efficiency and applicability of large language models in real-world scenarios. The discussion encompasses the varied applications of KD across multiple domains, including industrial systems, embedded systems, Natural Language Processing (NLP), multi-modal processing, and vertical domains, such as medicine, law, science, finance, and materials science. This survey outlines current KD methodologies and future research directions, highlighting its role in advancing AI technologies and fostering innovation across different sectors.",
            "corpus_id": 277398866,
            "sentences": [
                {
                    "corpus_id": "277398866",
                    "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
                    "text": "The core principles of LLMs are fundamentally rooted in the Transformer architecture [26]. This architecture has revolutionized the field of natural language processing by introducing several key features that enhance model performance and efficiency. LLMs leverage a two-phase training approach that includes pre-training on large-scale text data followed by fine-tuning on specific tasks. This strategy allows the models to learn rich linguistic knowledge and contextual information, making them highly versatile and powerful for a wide range of applications. \n\n\u2022 Self-Attention Mechanism: One of the most significant innovations in the Transformer architecture is the self-attention mechanism. This mechanism captures long-range dependencies and contextual information by computing the similarity between each element in the input sequence and all other elements. Unlike traditional sequential processing methods, which process tokens one at a time, the self-attention mechanism allows the model to focus on different parts of the input simultaneously. This parallel processing capability improves both training and inference efficiency, enabling the model to handle larger datasets and more complex tasks [26]. \n\nThe self-attention mechanism ensures that the model can dynamically weigh the importance of different words in a sequence based on their relevance to the current context, leading to more accurate and coherent text generation. \n\n\u2022 Simultaneous Processing: Another crucial advantage of the Transformer architecture is its ability to process all elements of the input sequence simultaneously. This is a departure from the sequential nature of earlier models like RNNs and LSTMs, which process tokens in a stepby-step manner. The simultaneous processing enabled by Transformers not only accelerates the training process but also allows the model to maintain a global view of the entire sequence, enhancing its ability to capture complex dependencies and interactions between words. \n\n\u2022 Pre-Training Phase: During the pre-training phase, the model is exposed to a vast corpus of text data, learning to predict the next word in a sentence. This unsupervised learning process enables the model to acquire a deep understanding of syntax, semantics, and world knowledge. By learning from diverse and extensive text data, the model builds a comprehensive language representation that can be applied to various downstream tasks [28]. The pre-training phase is crucial as it equips the model with a broad base of knowledge that can be fine-tuned for specific applications.",
                    "score": 0.38180665414069753,
                    "section_title": "4) CORE PRINCIPLES OF LLMs",
                    "char_start_offset": 16536,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 90
                        },
                        {
                            "start": 91,
                            "end": 251
                        },
                        {
                            "start": 252,
                            "end": 390
                        },
                        {
                            "start": 391,
                            "end": 561
                        },
                        {
                            "start": 564,
                            "end": 696
                        },
                        {
                            "start": 697,
                            "end": 866
                        },
                        {
                            "start": 867,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1214
                        },
                        {
                            "start": 1217,
                            "end": 1442
                        },
                        {
                            "start": 1445,
                            "end": 1606
                        },
                        {
                            "start": 1607,
                            "end": 1738
                        },
                        {
                            "start": 1739,
                            "end": 1994
                        },
                        {
                            "start": 1997,
                            "end": 2150
                        },
                        {
                            "start": 2151,
                            "end": 2278
                        },
                        {
                            "start": 2279,
                            "end": 2439
                        },
                        {
                            "start": 2440,
                            "end": 2577
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 2434,
                            "end": 2438,
                            "matchedPaperCorpusId": "160025533"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89404296875
                }
            ],
            "relevance_judgement": 0.89404296875,
            "relevance_judgment_input_expanded": "# Title: Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models\n# Venue: IEEE Access\n# Authors: Dingzong Zhang, Devi Listiyani, Priyanka Singh, Manoranjan Mohanty\n## Abstract\nIn the era of Large Language Models (LLMs), Knowledge Distillation (KD) enables the transfer of capabilities from proprietary LLMs to open-source models. This survey provides a detailed discussion of the basic principles, algorithms, and implementation methods of knowledge distillation. It explores KD\u2019s impact on LLMs, emphasizing its utility in model compression, performance enhancement, and self-improvement. Through the analysis of practical examples such as DistilBERT, TinyBERT, and MobileBERT, the paper demonstrates how knowledge distillation can markedly enhance the efficiency and applicability of large language models in real-world scenarios. The discussion encompasses the varied applications of KD across multiple domains, including industrial systems, embedded systems, Natural Language Processing (NLP), multi-modal processing, and vertical domains, such as medicine, law, science, finance, and materials science. This survey outlines current KD methodologies and future research directions, highlighting its role in advancing AI technologies and fostering innovation across different sectors.\n## 4) CORE PRINCIPLES OF LLMs\nThe core principles of LLMs are fundamentally rooted in the Transformer architecture [26]. This architecture has revolutionized the field of natural language processing by introducing several key features that enhance model performance and efficiency. LLMs leverage a two-phase training approach that includes pre-training on large-scale text data followed by fine-tuning on specific tasks. This strategy allows the models to learn rich linguistic knowledge and contextual information, making them highly versatile and powerful for a wide range of applications. \n\n\u2022 Self-Attention Mechanism: One of the most significant innovations in the Transformer architecture is the self-attention mechanism. This mechanism captures long-range dependencies and contextual information by computing the similarity between each element in the input sequence and all other elements. Unlike traditional sequential processing methods, which process tokens one at a time, the self-attention mechanism allows the model to focus on different parts of the input simultaneously. This parallel processing capability improves both training and inference efficiency, enabling the model to handle larger datasets and more complex tasks [26]. \n\nThe self-attention mechanism ensures that the model can dynamically weigh the importance of different words in a sequence based on their relevance to the current context, leading to more accurate and coherent text generation. \n\n\u2022 Simultaneous Processing: Another crucial advantage of the Transformer architecture is its ability to process all elements of the input sequence simultaneously. This is a departure from the sequential nature of earlier models like RNNs and LSTMs, which process tokens in a stepby-step manner. The simultaneous processing enabled by Transformers not only accelerates the training process but also allows the model to maintain a global view of the entire sequence, enhancing its ability to capture complex dependencies and interactions between words. \n\n\u2022 Pre-Training Phase: During the pre-training phase, the model is exposed to a vast corpus of text data, learning to predict the next word in a sentence. This unsupervised learning process enables the model to acquire a deep understanding of syntax, semantics, and world knowledge. By learning from diverse and extensive text data, the model builds a comprehensive language representation that can be applied to various downstream tasks [28]. The pre-training phase is crucial as it equips the model with a broad base of knowledge that can be fine-tuned for specific applications.",
            "reference_string": "[277398866 | Zhang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Are Emergent Abilities in Large Language Models just In-Context Learning?",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 82,
            "citation_count": 100,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.01809",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.01809, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237947104",
                    "name": "Sheng Lu"
                },
                {
                    "authorId": "2081604910",
                    "name": "Irina Bigoulaeva"
                },
                {
                    "authorId": "1490892639",
                    "name": "Rachneet Sachdeva"
                },
                {
                    "authorId": "3467205",
                    "name": "H. T. Madabushi"
                },
                {
                    "authorId": "1730400",
                    "name": "Iryna Gurevych"
                }
            ],
            "abstract": "Large language models, comprising billions of parameters and pre-trained on extensive web-scale corpora, have been claimed to acquire certain capabilities without having been specifically trained on them. These capabilities, referred to as\"emergent abilities,\"have been a driving force in discussions regarding the potentials and risks of language models. A key challenge in evaluating emergent abilities is that they are confounded by model competencies that arise through alternative prompting techniques, including in-context learning, which is the ability of models to complete a task based on a few examples. We present a novel theory that explains emergent abilities, taking into account their potential confounding factors, and rigorously substantiate this theory through over 1000 experiments. Our findings suggest that purported emergent abilities are not truly emergent, but result from a combination of in-context learning, model memory, and linguistic knowledge. Our work is a foundational step in explaining language model performance, providing a template for their efficient use and clarifying the paradox of their ability to excel in some instances while faltering in others. Thus, we demonstrate that their capabilities should not be overestimated.",
            "corpus_id": 261531236,
            "sentences": [
                {
                    "corpus_id": "261531236",
                    "title": "Are Emergent Abilities in Large Language Models just In-Context Learning?",
                    "text": "One of the most captivating aspects of pre-trained language models (PLMs) is their capacity to acquire a wide range of knowledge across different domains, while being trained primarily through masked language modelling, a task requiring models to predict masked tokens in their input (Tenney et al., 2019;Petroni et al., 2019). The diverse abilities of PLMs can be categorised into two broad types: formal linguistic abilities and functional linguistic abilities. Formal linguistic abilities refer to the understanding of language rules and patterns, which PLMs, for example, BERT (Devlin et al., 2019) are known to excel at (Tenney et al., 2019;Petroni et al., 2019). The latter category includes a range of abilities akin to human cognition that are necessary for real-world language use and comprehension, such as commonsense knowledge and social awareness. While PLMs excel at formal linguistic abilities, they have faced challenges in developing functional linguistic abilities (Mahowald et al., 2023). \n\nThe introduction of Large Language Models (LLMs), which are typically generative PLMs scaled up to billions of parameters and trained on vast, web-scale data corpora, is changing this landscape (Brown et al., 2020;Chowdhery et al., 2023;Touvron et al., 2023a,b). Recent works indicate that LLMs exhibit emergent abilities, as measured by their above random performance without explicit training on tasks, including those tasks that explicitly require some form of reasoning. An emergent ability was first defined as an ability to solve a task which is absent in smaller models, but present in LLMs. This definition, introduced approximately concurrently by two works (Wei et al., 2022b;Srivastava et al., 2023), is based on the more general definition of emergence in physics: \"Emergence is when quantitative changes in a system result in qualitative changes in behaviour\" (Anderson, 1972). Emergent abilities are implied due to LLMs' capacity to perform above the random baseline on the corresponding tasks without explicit training on those same tasks.",
                    "score": 0.5002929014404388,
                    "section_title": "Introduction, Motivation and Context",
                    "char_start_offset": 39,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 463
                        },
                        {
                            "start": 464,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 860
                        },
                        {
                            "start": 861,
                            "end": 1007
                        },
                        {
                            "start": 1010,
                            "end": 1272
                        },
                        {
                            "start": 1273,
                            "end": 1484
                        },
                        {
                            "start": 1485,
                            "end": 1608
                        },
                        {
                            "start": 1609,
                            "end": 1900
                        },
                        {
                            "start": 1901,
                            "end": 2064
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 284,
                            "end": 305,
                            "matchedPaperCorpusId": "108300988"
                        },
                        {
                            "start": 305,
                            "end": 326,
                            "matchedPaperCorpusId": "202539551"
                        },
                        {
                            "start": 581,
                            "end": 602,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 625,
                            "end": 646,
                            "matchedPaperCorpusId": "108300988"
                        },
                        {
                            "start": 646,
                            "end": 667,
                            "matchedPaperCorpusId": "202539551"
                        },
                        {
                            "start": 1224,
                            "end": 1247,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 1677,
                            "end": 1696,
                            "matchedPaperCorpusId": "249674500"
                        },
                        {
                            "start": 1696,
                            "end": 1720,
                            "matchedPaperCorpusId": "263625818"
                        },
                        {
                            "start": 1883,
                            "end": 1899,
                            "matchedPaperCorpusId": "2221052"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89306640625
                }
            ],
            "relevance_judgement": 0.89306640625,
            "relevance_judgment_input_expanded": "# Title: Are Emergent Abilities in Large Language Models just In-Context Learning?\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, H. T. Madabushi, Iryna Gurevych\n## Abstract\nLarge language models, comprising billions of parameters and pre-trained on extensive web-scale corpora, have been claimed to acquire certain capabilities without having been specifically trained on them. These capabilities, referred to as\"emergent abilities,\"have been a driving force in discussions regarding the potentials and risks of language models. A key challenge in evaluating emergent abilities is that they are confounded by model competencies that arise through alternative prompting techniques, including in-context learning, which is the ability of models to complete a task based on a few examples. We present a novel theory that explains emergent abilities, taking into account their potential confounding factors, and rigorously substantiate this theory through over 1000 experiments. Our findings suggest that purported emergent abilities are not truly emergent, but result from a combination of in-context learning, model memory, and linguistic knowledge. Our work is a foundational step in explaining language model performance, providing a template for their efficient use and clarifying the paradox of their ability to excel in some instances while faltering in others. Thus, we demonstrate that their capabilities should not be overestimated.\n## Introduction, Motivation and Context\nOne of the most captivating aspects of pre-trained language models (PLMs) is their capacity to acquire a wide range of knowledge across different domains, while being trained primarily through masked language modelling, a task requiring models to predict masked tokens in their input (Tenney et al., 2019;Petroni et al., 2019). The diverse abilities of PLMs can be categorised into two broad types: formal linguistic abilities and functional linguistic abilities. Formal linguistic abilities refer to the understanding of language rules and patterns, which PLMs, for example, BERT (Devlin et al., 2019) are known to excel at (Tenney et al., 2019;Petroni et al., 2019). The latter category includes a range of abilities akin to human cognition that are necessary for real-world language use and comprehension, such as commonsense knowledge and social awareness. While PLMs excel at formal linguistic abilities, they have faced challenges in developing functional linguistic abilities (Mahowald et al., 2023). \n\nThe introduction of Large Language Models (LLMs), which are typically generative PLMs scaled up to billions of parameters and trained on vast, web-scale data corpora, is changing this landscape (Brown et al., 2020;Chowdhery et al., 2023;Touvron et al., 2023a,b). Recent works indicate that LLMs exhibit emergent abilities, as measured by their above random performance without explicit training on tasks, including those tasks that explicitly require some form of reasoning. An emergent ability was first defined as an ability to solve a task which is absent in smaller models, but present in LLMs. This definition, introduced approximately concurrently by two works (Wei et al., 2022b;Srivastava et al., 2023), is based on the more general definition of emergence in physics: \"Emergence is when quantitative changes in a system result in qualitative changes in behaviour\" (Anderson, 1972). Emergent abilities are implied due to LLMs' capacity to perform above the random baseline on the corresponding tasks without explicit training on those same tasks.",
            "reference_string": "[261531236 | Lu et al. | 2023 | Citations: 100]"
        },
        {
            "title": "On the Unexpected Abilities of Large Language Models",
            "venue": "Adaptive Behavior",
            "year": 2023,
            "reference_count": 77,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.09720",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.09720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3015062",
                    "name": "S. Nolfi"
                }
            ],
            "abstract": "Large Language Models (LLMs) are capable of displaying a wide range of abilities that are not directly connected with the task for which they are trained: predicting the next words of human-written texts. In this article, I review recent research investigating the cognitive abilities developed by LLMs and their relation to human cognition. I discuss the nature of the indirect process that leads to the acquisition of these cognitive abilities, their relation to other indirect processes, and the implications for the acquisition of integrated abilities. Moreover, I propose the factors that enable the development of abilities that are related only very indirectly to the proximal objective of the training task. Finally, I discuss whether the full set of capabilities that LLMs could possibly develop is predictable.",
            "corpus_id": 261049443,
            "sentences": [
                {
                    "corpus_id": "261049443",
                    "title": "On the Unexpected Abilities of Large Language Models",
                    "text": "An important issue that needs to be investigated is how well the skills acquired by LLMs are predictable. This question has important implications for AI safety and alignment, since the impossibility of predicting the abilities that will be developed by larger models implies that these models could acquire undesired and dangerous capabilities, without warning (Schaeffer et al., 2023). \n\nThe performance of large language models scales as a power-law with model size, dataset size, and amount of computation used for training (Kaplan et al., 2020). This implies that the overall performance (prediction error) of these systems is predictable. In other words, it implies that the overall performance that can be obtained by increasing the size of the model and/or the training time can be extrapolated based on the performance displayed by models that are smaller or less trained. However, the specific abilities that will be developed by a model of a certain size are not necessarily predictable. \n\nThe interest in this topic was raised by the publication of an influential article by Wei et al. (2022) entitled \"Emergent abilities of large language models\". The authors observe that several abilities, such as in-context learning, instruction following, step-by-step reasoning, and arithmetic skills, are not present in smaller models. They appear in larger models only. Moreover, they show that the acquisition process of these abilities in sufficiently large models is characterized by a sharp transition between a phase in which the system does not show any progress in the ability and a subsequent phase in which the system progresses. According to the authors, these two observations imply that predicting the abilities that will be acquired by scaled-up models will be impossible. In other words, they claim that the abilities that will be developed by scaled-up models cannot be extrapolated from what we know about smaller models. Wei et al. (2022) later questioned the presence of sharp transitions during the training process and showed that the occurrence of sharp or more continuous transitions in the acquisition process depends on the metric used to evaluate the performance. The occurrence of sharp transitions might thus be induced by the utilization of specific metrics.",
                    "score": 0.4302732890678317,
                    "section_title": "Predictability and emergence",
                    "char_start_offset": 18193,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 105
                        },
                        {
                            "start": 106,
                            "end": 387
                        },
                        {
                            "start": 390,
                            "end": 550
                        },
                        {
                            "start": 551,
                            "end": 644
                        },
                        {
                            "start": 645,
                            "end": 881
                        },
                        {
                            "start": 882,
                            "end": 998
                        },
                        {
                            "start": 1001,
                            "end": 1160
                        },
                        {
                            "start": 1161,
                            "end": 1338
                        },
                        {
                            "start": 1339,
                            "end": 1373
                        },
                        {
                            "start": 1374,
                            "end": 1642
                        },
                        {
                            "start": 1643,
                            "end": 1789
                        },
                        {
                            "start": 1790,
                            "end": 1941
                        },
                        {
                            "start": 1942,
                            "end": 2192
                        },
                        {
                            "start": 2193,
                            "end": 2290
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1087,
                            "end": 1104,
                            "matchedPaperCorpusId": "249674500"
                        },
                        {
                            "start": 1942,
                            "end": 1959,
                            "matchedPaperCorpusId": "249674500"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88623046875
                }
            ],
            "relevance_judgement": 0.88623046875,
            "relevance_judgment_input_expanded": "# Title: On the Unexpected Abilities of Large Language Models\n# Venue: Adaptive Behavior\n# Authors: S. Nolfi\n## Abstract\nLarge Language Models (LLMs) are capable of displaying a wide range of abilities that are not directly connected with the task for which they are trained: predicting the next words of human-written texts. In this article, I review recent research investigating the cognitive abilities developed by LLMs and their relation to human cognition. I discuss the nature of the indirect process that leads to the acquisition of these cognitive abilities, their relation to other indirect processes, and the implications for the acquisition of integrated abilities. Moreover, I propose the factors that enable the development of abilities that are related only very indirectly to the proximal objective of the training task. Finally, I discuss whether the full set of capabilities that LLMs could possibly develop is predictable.\n## Predictability and emergence\nAn important issue that needs to be investigated is how well the skills acquired by LLMs are predictable. This question has important implications for AI safety and alignment, since the impossibility of predicting the abilities that will be developed by larger models implies that these models could acquire undesired and dangerous capabilities, without warning (Schaeffer et al., 2023). \n\nThe performance of large language models scales as a power-law with model size, dataset size, and amount of computation used for training (Kaplan et al., 2020). This implies that the overall performance (prediction error) of these systems is predictable. In other words, it implies that the overall performance that can be obtained by increasing the size of the model and/or the training time can be extrapolated based on the performance displayed by models that are smaller or less trained. However, the specific abilities that will be developed by a model of a certain size are not necessarily predictable. \n\nThe interest in this topic was raised by the publication of an influential article by Wei et al. (2022) entitled \"Emergent abilities of large language models\". The authors observe that several abilities, such as in-context learning, instruction following, step-by-step reasoning, and arithmetic skills, are not present in smaller models. They appear in larger models only. Moreover, they show that the acquisition process of these abilities in sufficiently large models is characterized by a sharp transition between a phase in which the system does not show any progress in the ability and a subsequent phase in which the system progresses. According to the authors, these two observations imply that predicting the abilities that will be acquired by scaled-up models will be impossible. In other words, they claim that the abilities that will be developed by scaled-up models cannot be extrapolated from what we know about smaller models. Wei et al. (2022) later questioned the presence of sharp transitions during the training process and showed that the occurrence of sharp or more continuous transitions in the acquisition process depends on the metric used to evaluate the performance. The occurrence of sharp transitions might thus be induced by the utilization of specific metrics.",
            "reference_string": "[261049443 | Nolfi | 2023 | Citations: 11]"
        },
        {
            "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 84,
            "citation_count": 138,
            "influential_citation_count": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.05492",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05492, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51490462",
                    "name": "Guanting Dong"
                },
                {
                    "authorId": "2114128654",
                    "name": "Hongyi Yuan"
                },
                {
                    "authorId": "2257001403",
                    "name": "Keming Lu"
                },
                {
                    "authorId": "2257039734",
                    "name": "Chengpeng Li"
                },
                {
                    "authorId": "2065790119",
                    "name": "Mingfeng Xue"
                },
                {
                    "authorId": "2248487202",
                    "name": "Dayiheng Liu"
                },
                {
                    "authorId": "2203795932",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "2112340945",
                    "name": "Zheng Yuan"
                },
                {
                    "authorId": "2257314035",
                    "name": "Chang Zhou"
                },
                {
                    "authorId": "2237981776",
                    "name": "Jingren Zhou"
                }
            ],
            "abstract": "Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specifically focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.",
            "corpus_id": 263830318,
            "sentences": [
                {
                    "corpus_id": "263830318",
                    "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition",
                    "text": "Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specifically focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.",
                    "score": 0.39662625871133195,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88525390625
                }
            ],
            "relevance_judgement": 0.88525390625,
            "relevance_judgment_input_expanded": "# Title: How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, Jingren Zhou\n## Abstract\nLarge language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specifically focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.\n",
            "reference_string": "[263830318 | Dong et al. | 2023 | Citations: 138]"
        },
        {
            "title": "Birth of a Transformer: A Memory Viewpoint",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 61,
            "citation_count": 96,
            "influential_citation_count": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.00802",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.00802, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2269602",
                    "name": "A. Bietti"
                },
                {
                    "authorId": "1387995815",
                    "name": "Vivien A. Cabannes"
                },
                {
                    "authorId": "3365029",
                    "name": "Diane Bouchacourt"
                },
                {
                    "authorId": "1681054",
                    "name": "H. J\u00e9gou"
                },
                {
                    "authorId": "52184096",
                    "name": "L. Bottou"
                }
            ],
            "abstract": "Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an\"induction head\"mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties.",
            "corpus_id": 258999187,
            "sentences": [
                {
                    "corpus_id": "258999187",
                    "title": "Birth of a Transformer: A Memory Viewpoint",
                    "text": "As large language models (LLMs) are growing in usage and deployment, it is increasingly important to open the black box and understand how they work. A better understanding can help with interpretability of how these models make decisions, and will be crucial to improve these models and mitigate their failure cases, such as hallucinations or reasoning errors. \n\nAn important ingredient in the success of recent LLMs is their ability to learn and reason from information present in their context [5]. These \"in-context\" learning capabilities are often attributed to the transformer architecture [46], in particular its self-attention blocks, which are able to carefully select parts of the input sequence in order to infer plausible next tokens. Additionally, predictions may require \"global\" knowledge, such as syntactic rules or general facts, which may not appear in the context and thus needs to be stored in the model. \n\nIn order to better understand how transformers develop these capabilities during training, we introduce a synthetic dataset that exhibits both aspects. It consists of sequences generated from a bigram language model, but where some of the bigrams are specific to each sequence. Then, the model needs to rely on in-context learning for good prediction on the sequence-specific bigrams, while the global bigrams can be guessed from global statistics conditioned on the current token. While one-layer transformers fail to reliably predict the in-context bigrams, we find that two-layer transformers succeed by developing an induction head mechanism [14,36], namely a \"circuit\" of two attention heads that allows the transformer to predict b from a context [\u2022 \u2022 \u2022 , a, b, \u2022 \u2022 \u2022 , a], and which appears to be ubiquitous in transformer language models [36,48]. \n\nIn order to obtain a fine-grained understanding of how this in-context mechanism emerges during training, we further simplify the two-layer architecture by freezing some of the layers at random initialization, including embeddings and value matrices. This focuses our study on attention and feed-forward mechanisms, while avoiding the difficulty of learning representations, which may require complex nonlinear dynamics [13,29,40].",
                    "score": 0.4058661151266508,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 361
                        },
                        {
                            "start": 364,
                            "end": 501
                        },
                        {
                            "start": 502,
                            "end": 746
                        },
                        {
                            "start": 747,
                            "end": 924
                        },
                        {
                            "start": 927,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1408
                        },
                        {
                            "start": 1409,
                            "end": 1781
                        },
                        {
                            "start": 1784,
                            "end": 2034
                        },
                        {
                            "start": 2035,
                            "end": 2215
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 497,
                            "end": 500,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 596,
                            "end": 600,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1777,
                            "end": 1780,
                            "matchedPaperCorpusId": "253244237"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8837890625
                }
            ],
            "relevance_judgement": 0.8837890625,
            "relevance_judgment_input_expanded": "# Title: Birth of a Transformer: A Memory Viewpoint\n# Venue: Neural Information Processing Systems\n# Authors: A. Bietti, Vivien A. Cabannes, Diane Bouchacourt, H. J\u00e9gou, L. Bottou\n## Abstract\nLarge language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an\"induction head\"mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties.\n## Introduction\nAs large language models (LLMs) are growing in usage and deployment, it is increasingly important to open the black box and understand how they work. A better understanding can help with interpretability of how these models make decisions, and will be crucial to improve these models and mitigate their failure cases, such as hallucinations or reasoning errors. \n\nAn important ingredient in the success of recent LLMs is their ability to learn and reason from information present in their context [5]. These \"in-context\" learning capabilities are often attributed to the transformer architecture [46], in particular its self-attention blocks, which are able to carefully select parts of the input sequence in order to infer plausible next tokens. Additionally, predictions may require \"global\" knowledge, such as syntactic rules or general facts, which may not appear in the context and thus needs to be stored in the model. \n\nIn order to better understand how transformers develop these capabilities during training, we introduce a synthetic dataset that exhibits both aspects. It consists of sequences generated from a bigram language model, but where some of the bigrams are specific to each sequence. Then, the model needs to rely on in-context learning for good prediction on the sequence-specific bigrams, while the global bigrams can be guessed from global statistics conditioned on the current token. While one-layer transformers fail to reliably predict the in-context bigrams, we find that two-layer transformers succeed by developing an induction head mechanism [14,36], namely a \"circuit\" of two attention heads that allows the transformer to predict b from a context [\u2022 \u2022 \u2022 , a, b, \u2022 \u2022 \u2022 , a], and which appears to be ubiquitous in transformer language models [36,48]. \n\nIn order to obtain a fine-grained understanding of how this in-context mechanism emerges during training, we further simplify the two-layer architecture by freezing some of the layers at random initialization, including embeddings and value matrices. This focuses our study on attention and feed-forward mechanisms, while avoiding the difficulty of learning representations, which may require complex nonlinear dynamics [13,29,40].",
            "reference_string": "[258999187 | Bietti et al. | 2023 | Citations: 96]"
        },
        {
            "title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13259, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326447675",
                    "name": "Qiyuan Yang"
                },
                {
                    "authorId": "2294722814",
                    "name": "Pengda Wang"
                },
                {
                    "authorId": "2326296255",
                    "name": "Luke D. Plonsky"
                },
                {
                    "authorId": "2250843586",
                    "name": "Frederick L. Oswald"
                },
                {
                    "authorId": "7315244",
                    "name": "Hanjie Chen"
                }
            ],
            "abstract": "We examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning. Using this framework, we evaluate the generative capacities of LMs using methods from linguistic research. Results indicate that although recent LMs outperform earlier models in overall performance, their developmental trajectory does not strictly follow the path of human language acquisition. Notably, in generation tasks, LMs are more similar to human performance in areas where information is easier to extract from the corpus, such as average word length, clauses, and auxiliary verbs. Newer LMs did not exhibit significant progress in terms of specific dimensions, such as clauses and auxiliary verbs, where the variation across corpora is relatively limited. Register theory offers a plausible explanation for these observations, suggesting that the linguistic features of the training data have a substantial impact on the models' abilities.",
            "corpus_id": 273403825,
            "sentences": [
                {
                    "corpus_id": "273403825",
                    "title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition",
                    "text": "As one of the unique abilities of humans, the acquisition of language has long been a key area of research in psycholinguistics. During the process of language acquisition, humans go through multiple stages, from imitation and rule learning to complex contextual understanding (Goldberg, 2005). These stages bear some resemblance to the way current LMs are trained. For instance, LMs learn the statistical patterns and grammatical rules of language through training on large-scale data, similar to how infants develop language abilities by receiving a vast amount of input through listening and speaking. If we design theory-driven tests based on the human language acquisition process to evaluate the capabilities of LMs, it could help us better understand the nature, potential, and limitations of LMs in their development. \n\nOur work draws on classical theories of human language development to assess LMs in terms of a three-stage human language development framework (Chomsky, 2014;Loban, 1976;Pinker, 2003), as shown in Figure 1. The first stage involves developing basic language understanding, similar to early language acquisition in infants. At this stage, we evaluate the model's ability to recognize vocabulary, grasp syntax, and perform simple reasoning. In the second stage, the focus shifts to mastering complex grammar and semantics, where the model demonstrates a deeper understanding of language rules and logical relationships between sentences. The third stage assesses advanced language abilities, evaluating the model's capacity for complex reasoning and logical analysis. \n\nWe further investigate another theory: register theory in linguistics, which posits that different language use scenarios influence the form and structure of language (Halliday, 1977;Matthiessen, 1993). This theory offers insights into the extent to which models' abilities depend on the linguistic features encountered in specific situations, referred to as registers. In LMs, the training corpus will reflect some registers but not others, which can raise general questions or concerns about the generaliz-ability and biases contained in any given corpus. \n\nWe evaluated 15 LMs from 2019 to 2024, excluding instruction fine-tuned or chat versions, with varying parameter sizes (see \u00a74.1).",
                    "score": 0.47977210142702814,
                    "section_title": "Introduction",
                    "char_start_offset": 2100,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 294
                        },
                        {
                            "start": 295,
                            "end": 365
                        },
                        {
                            "start": 366,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 825
                        },
                        {
                            "start": 828,
                            "end": 1035
                        },
                        {
                            "start": 1036,
                            "end": 1151
                        },
                        {
                            "start": 1152,
                            "end": 1267
                        },
                        {
                            "start": 1268,
                            "end": 1464
                        },
                        {
                            "start": 1465,
                            "end": 1594
                        },
                        {
                            "start": 1597,
                            "end": 1799
                        },
                        {
                            "start": 1800,
                            "end": 1966
                        },
                        {
                            "start": 1967,
                            "end": 2154
                        },
                        {
                            "start": 2157,
                            "end": 2287
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88330078125
                }
            ],
            "relevance_judgement": 0.88330078125,
            "relevance_judgment_input_expanded": "# Title: From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition\n# Venue: arXiv.org\n# Authors: Qiyuan Yang, Pengda Wang, Luke D. Plonsky, Frederick L. Oswald, Hanjie Chen\n## Abstract\nWe examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning. Using this framework, we evaluate the generative capacities of LMs using methods from linguistic research. Results indicate that although recent LMs outperform earlier models in overall performance, their developmental trajectory does not strictly follow the path of human language acquisition. Notably, in generation tasks, LMs are more similar to human performance in areas where information is easier to extract from the corpus, such as average word length, clauses, and auxiliary verbs. Newer LMs did not exhibit significant progress in terms of specific dimensions, such as clauses and auxiliary verbs, where the variation across corpora is relatively limited. Register theory offers a plausible explanation for these observations, suggesting that the linguistic features of the training data have a substantial impact on the models' abilities.\n## Introduction\nAs one of the unique abilities of humans, the acquisition of language has long been a key area of research in psycholinguistics. During the process of language acquisition, humans go through multiple stages, from imitation and rule learning to complex contextual understanding (Goldberg, 2005). These stages bear some resemblance to the way current LMs are trained. For instance, LMs learn the statistical patterns and grammatical rules of language through training on large-scale data, similar to how infants develop language abilities by receiving a vast amount of input through listening and speaking. If we design theory-driven tests based on the human language acquisition process to evaluate the capabilities of LMs, it could help us better understand the nature, potential, and limitations of LMs in their development. \n\nOur work draws on classical theories of human language development to assess LMs in terms of a three-stage human language development framework (Chomsky, 2014;Loban, 1976;Pinker, 2003), as shown in Figure 1. The first stage involves developing basic language understanding, similar to early language acquisition in infants. At this stage, we evaluate the model's ability to recognize vocabulary, grasp syntax, and perform simple reasoning. In the second stage, the focus shifts to mastering complex grammar and semantics, where the model demonstrates a deeper understanding of language rules and logical relationships between sentences. The third stage assesses advanced language abilities, evaluating the model's capacity for complex reasoning and logical analysis. \n\nWe further investigate another theory: register theory in linguistics, which posits that different language use scenarios influence the form and structure of language (Halliday, 1977;Matthiessen, 1993). This theory offers insights into the extent to which models' abilities depend on the linguistic features encountered in specific situations, referred to as registers. In LMs, the training corpus will reflect some registers but not others, which can raise general questions or concerns about the generaliz-ability and biases contained in any given corpus. \n\nWe evaluated 15 LMs from 2019 to 2024, excluding instruction fine-tuned or chat versions, with varying parameter sizes (see \u00a74.1).",
            "reference_string": "[273403825 | Yang et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Probing LLMs for Joint Encoding of Linguistic Categories",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.18696, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2168285763",
                    "name": "Giulio Starace"
                },
                {
                    "authorId": "2262444926",
                    "name": "Konstantinos Papakostas"
                },
                {
                    "authorId": "2067163164",
                    "name": "Rochelle Choenni"
                },
                {
                    "authorId": "31713235",
                    "name": "Apostolos Panagiotopoulos"
                },
                {
                    "authorId": "2168109752",
                    "name": "Matteo Rosati"
                },
                {
                    "authorId": "2262444923",
                    "name": "Alina Leidinger"
                },
                {
                    "authorId": "2262445370",
                    "name": "Ekaterina Shutova"
                }
            ],
            "abstract": "Large Language Models (LLMs) exhibit impressive performance on a range of NLP tasks, due to the general-purpose linguistic knowledge acquired during pretraining. Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing. Yet, little is known about how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations. In this paper, we propose a framework for testing the joint encoding of linguistic categories in LLMs. Focusing on syntax, we find evidence of joint encoding both at the same (related part-of-speech (POS) classes) and different (POS classes and related syntactic dependency relations) levels of linguistic hierarchy. Our cross-lingual experiments show that the same patterns hold across languages in multilingual LLMs.",
            "corpus_id": 264590507,
            "sentences": [
                {
                    "corpus_id": "264590507",
                    "title": "Probing LLMs for Joint Encoding of Linguistic Categories",
                    "text": "Large Language Models (LLMs) exhibit impressive performance on a range of NLP tasks, due to the general-purpose linguistic knowledge acquired during pretraining. Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing. Yet, little is known about how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations. In this paper, we propose a framework for testing the joint encoding of linguistic categories in LLMs. Focusing on syntax, we find evidence of joint encoding both at the same (related part-of-speech (POS) classes) and different (POS classes and related syntactic dependency relations) levels of linguistic hierarchy. Our cross-lingual experiments show that the same patterns hold across languages in multilingual LLMs.",
                    "score": 0.388804166726843,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8818359375
                }
            ],
            "relevance_judgement": 0.8818359375,
            "relevance_judgment_input_expanded": "# Title: Probing LLMs for Joint Encoding of Linguistic Categories\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Giulio Starace, Konstantinos Papakostas, Rochelle Choenni, Apostolos Panagiotopoulos, Matteo Rosati, Alina Leidinger, Ekaterina Shutova\n## Abstract\nLarge Language Models (LLMs) exhibit impressive performance on a range of NLP tasks, due to the general-purpose linguistic knowledge acquired during pretraining. Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing. Yet, little is known about how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations. In this paper, we propose a framework for testing the joint encoding of linguistic categories in LLMs. Focusing on syntax, we find evidence of joint encoding both at the same (related part-of-speech (POS) classes) and different (POS classes and related syntactic dependency relations) levels of linguistic hierarchy. Our cross-lingual experiments show that the same patterns hold across languages in multilingual LLMs.\n",
            "reference_string": "[264590507 | Starace et al. | 2023 | Citations: 7]"
        },
        {
            "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 104,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.01444, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323997802",
                    "name": "Jin Hwa Lee"
                },
                {
                    "authorId": "2187058673",
                    "name": "Thomas Jiralerspong"
                },
                {
                    "authorId": "2324060912",
                    "name": "Lei Yu"
                },
                {
                    "authorId": "1865800402",
                    "name": "Y. Bengio"
                },
                {
                    "authorId": "2323783941",
                    "name": "Emily Cheng"
                }
            ],
            "abstract": "By virtue of linguistic compositionality, few syntactic rules and a finite lexicon can generate an unbounded number of sentences. That is, language, though seemingly high-dimensional, can be explained using relatively few degrees of freedom. An open question is whether contemporary language models (LMs) reflect the intrinsic simplicity of language that is enabled by compositionality. We take a geometric view of this problem by relating the degree of compositionality in a dataset to the intrinsic dimension (ID) of its representations under an LM, a measure of feature complexity. We find not only that the degree of dataset compositionality is reflected in representations' ID, but that the relationship between compositionality and geometric complexity arises due to learned linguistic features over training. Finally, our analyses reveal a striking contrast between nonlinear and linear dimensionality, showing they respectively encode semantic and superficial aspects of linguistic composition.",
            "corpus_id": 273025704,
            "sentences": [
                {
                    "corpus_id": "273025704",
                    "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime",
                    "text": "We just saw how dimensionality scales with size, and now we investigate its change over time. We find that feature complexity is highly related to the LM's linguistic capabilities, assessed using the eval-harness benchmark performance, over training. Figure 3 shows the evolution of I d on the k = 1 dataset (top), where each curve is one layer, with the evolution of LM performance on the benchmark tasks (bottom), where each curve plots performance on an individual task. We observe in Figure 3 that, for all models, the evolution of representational dimensionality closely tracks a sudden transition in LM task performance. In Figure 3 (2024), who observed in BERT models a similar two-part I d transition on the training corpus; they showed that the two extrema corresponding to the dip and uptick in I d temporally coincided with the onset of higher-order linguistic capabilities. Together, results show that representational complexity can signify whether and when LMs learn linguistic structure. Crucially, we show that the phase transition exists for inputs beyond in-distribution data, which was the subject of (Chen et al., 2024), and, furthermore, beyond grammatical data (Figure G.4) as a more general property of LM processing.",
                    "score": 0.4225733805389244,
                    "section_title": "EVOLUTION OF REPRESENTATIONAL GEOMETRY TRACKS EMERGENT LINGUISTIC ABILITIES OVER TRAINING",
                    "char_start_offset": 19171,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 94,
                            "end": 250
                        },
                        {
                            "start": 251,
                            "end": 473
                        },
                        {
                            "start": 474,
                            "end": 626
                        },
                        {
                            "start": 627,
                            "end": 885
                        },
                        {
                            "start": 886,
                            "end": 1002
                        },
                        {
                            "start": 1003,
                            "end": 1240
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1120,
                            "end": 1139,
                            "matchedPaperCorpusId": "261822542"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88037109375
                },
                {
                    "corpus_id": "273025704",
                    "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime",
                    "text": "Most research on LMs focuses on the final configuration of the model at the end of pre-training. Yet, recent work shows that learning dynamics can elucidate the behavior and computational mechanisms of LMs (Chen et al., 2024;Singh et al., 2024;Tigges et al., 2024). It has been found that, over training, LMs' weight matrices become higher-rank (Abbe et al., 2023), their representations higher dimensional (Cheng et al., 2024), and their gradients increasingly diffuse (Weber et al., 2024). Over finetuning, representational dimensionality has been found to change in concert with geometric properties like cluster reorganization (Doimo et al., 2024). \n\nPhase transitions during LM training have been found for some, but not all, aspects of language learning. Negative evidence includes that LM circuits involved in linguistic subtasks are stable (Tigges et al., 2024) and gradually reinforced (Weber et al., 2024) over training. Positive evidence for learning phase transitions includes that the ID of BERT's final [CLS] representation tracks sudden syntax acquisition and drops in training loss (Chen et al., 2024), with similar observations on Transformers trained on formal languages (Lubana et al., 2024). Our work supplements these results by investigating how the interaction between compositional understanding of language and geometric complexity of its representation arises over training.",
                    "score": 0.4910683718909633,
                    "section_title": "Language model training dynamics",
                    "char_start_offset": 5069,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 96
                        },
                        {
                            "start": 97,
                            "end": 265
                        },
                        {
                            "start": 266,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 652
                        },
                        {
                            "start": 655,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 930
                        },
                        {
                            "start": 931,
                            "end": 1211
                        },
                        {
                            "start": 1212,
                            "end": 1400
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 206,
                            "end": 225,
                            "matchedPaperCorpusId": "261822542"
                        },
                        {
                            "start": 225,
                            "end": 244,
                            "matchedPaperCorpusId": "265157721"
                        },
                        {
                            "start": 345,
                            "end": 364,
                            "matchedPaperCorpusId": "259138551"
                        },
                        {
                            "start": 470,
                            "end": 490,
                            "matchedPaperCorpusId": "270370801"
                        },
                        {
                            "start": 895,
                            "end": 915,
                            "matchedPaperCorpusId": "270370801"
                        },
                        {
                            "start": 1098,
                            "end": 1117,
                            "matchedPaperCorpusId": "261822542"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8544921875
                }
            ],
            "relevance_judgement": 0.88037109375,
            "relevance_judgment_input_expanded": "# Title: Geometric Signatures of Compositionality Across a Language Model's Lifetime\n# Venue: arXiv.org\n# Authors: Jin Hwa Lee, Thomas Jiralerspong, Lei Yu, Y. Bengio, Emily Cheng\n## Abstract\nBy virtue of linguistic compositionality, few syntactic rules and a finite lexicon can generate an unbounded number of sentences. That is, language, though seemingly high-dimensional, can be explained using relatively few degrees of freedom. An open question is whether contemporary language models (LMs) reflect the intrinsic simplicity of language that is enabled by compositionality. We take a geometric view of this problem by relating the degree of compositionality in a dataset to the intrinsic dimension (ID) of its representations under an LM, a measure of feature complexity. We find not only that the degree of dataset compositionality is reflected in representations' ID, but that the relationship between compositionality and geometric complexity arises due to learned linguistic features over training. Finally, our analyses reveal a striking contrast between nonlinear and linear dimensionality, showing they respectively encode semantic and superficial aspects of linguistic composition.\n## Language model training dynamics\nMost research on LMs focuses on the final configuration of the model at the end of pre-training. Yet, recent work shows that learning dynamics can elucidate the behavior and computational mechanisms of LMs (Chen et al., 2024;Singh et al., 2024;Tigges et al., 2024). It has been found that, over training, LMs' weight matrices become higher-rank (Abbe et al., 2023), their representations higher dimensional (Cheng et al., 2024), and their gradients increasingly diffuse (Weber et al., 2024). Over finetuning, representational dimensionality has been found to change in concert with geometric properties like cluster reorganization (Doimo et al., 2024). \n\nPhase transitions during LM training have been found for some, but not all, aspects of language learning. Negative evidence includes that LM circuits involved in linguistic subtasks are stable (Tigges et al., 2024) and gradually reinforced (Weber et al., 2024) over training. Positive evidence for learning phase transitions includes that the ID of BERT's final [CLS] representation tracks sudden syntax acquisition and drops in training loss (Chen et al., 2024), with similar observations on Transformers trained on formal languages (Lubana et al., 2024). Our work supplements these results by investigating how the interaction between compositional understanding of language and geometric complexity of its representation arises over training.\n\n## EVOLUTION OF REPRESENTATIONAL GEOMETRY TRACKS EMERGENT LINGUISTIC ABILITIES OVER TRAINING\nWe just saw how dimensionality scales with size, and now we investigate its change over time. We find that feature complexity is highly related to the LM's linguistic capabilities, assessed using the eval-harness benchmark performance, over training. Figure 3 shows the evolution of I d on the k = 1 dataset (top), where each curve is one layer, with the evolution of LM performance on the benchmark tasks (bottom), where each curve plots performance on an individual task. We observe in Figure 3 that, for all models, the evolution of representational dimensionality closely tracks a sudden transition in LM task performance. In Figure 3 (2024), who observed in BERT models a similar two-part I d transition on the training corpus; they showed that the two extrema corresponding to the dip and uptick in I d temporally coincided with the onset of higher-order linguistic capabilities. Together, results show that representational complexity can signify whether and when LMs learn linguistic structure. Crucially, we show that the phase transition exists for inputs beyond in-distribution data, which was the subject of (Chen et al., 2024), and, furthermore, beyond grammatical data (Figure G.4) as a more general property of LM processing.",
            "reference_string": "[273025704 | Lee et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Modelling Language Acquisition through Syntactico-Semantic Pattern Finding",
            "venue": "Findings",
            "year": 2023,
            "reference_count": 48,
            "citation_count": 15,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.findings-eacl.99.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.findings-eacl.99, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2187454476",
                    "name": "Jonas Doumen"
                },
                {
                    "authorId": "2972424",
                    "name": "Katrien Beuls"
                },
                {
                    "authorId": "150266649",
                    "name": "Paul Van Eecke"
                }
            ],
            "abstract": "Usage-based theories of language acquisition have extensively documented the processes by which children acquire language through communicative interaction. Notably, Tomasello (2003) distinguishes two main cognitive capacities that underlie human language acquisition: intention reading and pattern finding. Intention reading is the process by which children try to continuously reconstruct the intended meaning of their interlocutors. Pattern finding refers to the process that allows them to distil linguistic schemata from multiple communicative interactions. Even though the fields of cognitive science and psycholinguistics have studied these processes in depth, no faithful computational operationalisations of these mechanisms through which children learn language exist to date. The research on which we report in this paper aims to fill part of this void by introducing a computational operationalisation of syntactico-semantic pattern finding. Concretely, we present a methodology for learning grammars based on similarities and differences in the form and meaning of linguistic observations alone. Our methodology is able to learn compositional lexical and item-based constructions of variable extent and degree of abstraction, along with a network of emergent syntactic categories. We evaluate our methodology on the CLEVR benchmark dataset and show that the methodology allows for fast, incremental and effective learning. The constructions and categorial network that result from the learning process are fully transparent and bidirectional, facilitating both language comprehension and production. Theoretically, our model provides computational evidence for the learnability of usage-based constructionist theories of language acquisition. Practically, the techniques that we present facilitate the learning of computationally tractable, usage-based construction grammars, which are applicable for natural language understanding and production tasks.",
            "corpus_id": 258378235,
            "sentences": [
                {
                    "corpus_id": "258378235",
                    "title": "Modelling Language Acquisition through Syntactico-Semantic Pattern Finding",
                    "text": "The scientific contribution of the methodology and experiments presented in this paper is twofold. On the one hand, they provide computational evidence for the cognitive plausibility of constructivist theories of language acquisition. These theories, as most prominently put forward by Tomasello (2003), attribute the ability of children to acquire language to two main cognitive capacities: intention reading and pattern finding. Intention reading deals with reconstructing the intended meaning of observed utterances, while pattern finding implements generalisation processes that distil these reconstructed utterance-meaning pairs into abstract schemata embodying the linguistic knowledge of a language user. These schemata can then be used to fulfil the communicative function of language through the comprehension and production of natural language expressions. The methodology introduced in this paper presents a mechanistic model of the pattern finding capacity. Based on utterances paired with a representation of their meaning, the learning algorithm gradually builds up an inventory of concrete to abstract form-meaning mappings, called constructions, along with a network of emergent grammatical categories that captures how the constructions of the grammar can combine to collaboratively comprehend and produce utterances. The experiments show that a small number of general learning operators, which become active if an utterance cannot be successfully processed by the grammar learnt so far, effectively leads to learning dynamics that are similar to those described in the psycholinguistic literature (Pine and Lieven, 1997;Tomasello, 2003;Ambridge and Lieven, 2015). In the first phase of the learning process, the learner acquires holistic mappings between utterances and their meaning representation. Soon after that, holophrase constructions are generalised to item-based constructions that integrate a variable slot. At the same time, this generalisation process leads to the emergence of slot-filling constructions, here called lexical constructions. Along with the item-based and lexical constructions, a network of grammatical categories emerges, capturing the distribution of construction slots and their observed fillers. In a third phase, more abstract itembased constructions emerge, with an increasingly large number of variable slots. In the final phase of the learning process, most constructions have already been acquired and most remaining impasses can be solved by adding new links to the categorial network. The learning dynamics are influenced by the degree of entrenchment of constructions.",
                    "score": 0.3770818612161002,
                    "section_title": "Discussion and Conclusion",
                    "char_start_offset": 26459,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 98
                        },
                        {
                            "start": 99,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 430
                        },
                        {
                            "start": 431,
                            "end": 711
                        },
                        {
                            "start": 712,
                            "end": 866
                        },
                        {
                            "start": 867,
                            "end": 969
                        },
                        {
                            "start": 970,
                            "end": 1334
                        },
                        {
                            "start": 1335,
                            "end": 1682
                        },
                        {
                            "start": 1683,
                            "end": 1818
                        },
                        {
                            "start": 1819,
                            "end": 1936
                        },
                        {
                            "start": 1937,
                            "end": 2071
                        },
                        {
                            "start": 2072,
                            "end": 2246
                        },
                        {
                            "start": 2247,
                            "end": 2363
                        },
                        {
                            "start": 2364,
                            "end": 2542
                        },
                        {
                            "start": 2543,
                            "end": 2627
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1616,
                            "end": 1639,
                            "matchedPaperCorpusId": "145808085"
                        },
                        {
                            "start": 1655,
                            "end": 1681,
                            "matchedPaperCorpusId": "60893172"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87255859375
                }
            ],
            "relevance_judgement": 0.87255859375,
            "relevance_judgment_input_expanded": "# Title: Modelling Language Acquisition through Syntactico-Semantic Pattern Finding\n# Venue: Findings\n# Authors: Jonas Doumen, Katrien Beuls, Paul Van Eecke\n## Abstract\nUsage-based theories of language acquisition have extensively documented the processes by which children acquire language through communicative interaction. Notably, Tomasello (2003) distinguishes two main cognitive capacities that underlie human language acquisition: intention reading and pattern finding. Intention reading is the process by which children try to continuously reconstruct the intended meaning of their interlocutors. Pattern finding refers to the process that allows them to distil linguistic schemata from multiple communicative interactions. Even though the fields of cognitive science and psycholinguistics have studied these processes in depth, no faithful computational operationalisations of these mechanisms through which children learn language exist to date. The research on which we report in this paper aims to fill part of this void by introducing a computational operationalisation of syntactico-semantic pattern finding. Concretely, we present a methodology for learning grammars based on similarities and differences in the form and meaning of linguistic observations alone. Our methodology is able to learn compositional lexical and item-based constructions of variable extent and degree of abstraction, along with a network of emergent syntactic categories. We evaluate our methodology on the CLEVR benchmark dataset and show that the methodology allows for fast, incremental and effective learning. The constructions and categorial network that result from the learning process are fully transparent and bidirectional, facilitating both language comprehension and production. Theoretically, our model provides computational evidence for the learnability of usage-based constructionist theories of language acquisition. Practically, the techniques that we present facilitate the learning of computationally tractable, usage-based construction grammars, which are applicable for natural language understanding and production tasks.\n## Discussion and Conclusion\nThe scientific contribution of the methodology and experiments presented in this paper is twofold. On the one hand, they provide computational evidence for the cognitive plausibility of constructivist theories of language acquisition. These theories, as most prominently put forward by Tomasello (2003), attribute the ability of children to acquire language to two main cognitive capacities: intention reading and pattern finding. Intention reading deals with reconstructing the intended meaning of observed utterances, while pattern finding implements generalisation processes that distil these reconstructed utterance-meaning pairs into abstract schemata embodying the linguistic knowledge of a language user. These schemata can then be used to fulfil the communicative function of language through the comprehension and production of natural language expressions. The methodology introduced in this paper presents a mechanistic model of the pattern finding capacity. Based on utterances paired with a representation of their meaning, the learning algorithm gradually builds up an inventory of concrete to abstract form-meaning mappings, called constructions, along with a network of emergent grammatical categories that captures how the constructions of the grammar can combine to collaboratively comprehend and produce utterances. The experiments show that a small number of general learning operators, which become active if an utterance cannot be successfully processed by the grammar learnt so far, effectively leads to learning dynamics that are similar to those described in the psycholinguistic literature (Pine and Lieven, 1997;Tomasello, 2003;Ambridge and Lieven, 2015). In the first phase of the learning process, the learner acquires holistic mappings between utterances and their meaning representation. Soon after that, holophrase constructions are generalised to item-based constructions that integrate a variable slot. At the same time, this generalisation process leads to the emergence of slot-filling constructions, here called lexical constructions. Along with the item-based and lexical constructions, a network of grammatical categories emerges, capturing the distribution of construction slots and their observed fillers. In a third phase, more abstract itembased constructions emerge, with an increasingly large number of variable slots. In the final phase of the learning process, most constructions have already been acquired and most remaining impasses can be solved by adding new links to the categorial network. The learning dynamics are influenced by the degree of entrenchment of constructions.",
            "reference_string": "[258378235 | Doumen et al. | 2023 | Citations: 15]"
        },
        {
            "title": "Emergent Abilities in Large Language Models: A Survey",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 97,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.05788, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2229196235",
                    "name": "Leonardo Berti"
                },
                {
                    "authorId": "2325903265",
                    "name": "Flavio Giorgi"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                }
            ],
            "abstract": "Large Language Models (LLMs) are leading a new technological revolution as one of the most promising research streams toward artificial general intelligence. The scaling of these models, accomplished by increasing the number of parameters and the magnitude of the training datasets, has been linked to various so-called emergent abilities that were previously unobserved. These emergent abilities, ranging from advanced reasoning and in-context learning to coding and problem-solving, have sparked an intense scientific debate: Are they truly emergent, or do they simply depend on external factors, such as training dynamics, the type of problems, or the chosen metric? What underlying mechanism causes them? Despite their transformative potential, emergent abilities remain poorly understood, leading to misconceptions about their definition, nature, predictability, and implications. In this work, we shed light on emergent abilities by conducting a comprehensive review of the phenomenon, addressing both its scientific underpinnings and real-world consequences. We first critically analyze existing definitions, exposing inconsistencies in conceptualizing emergent abilities. We then explore the conditions under which these abilities appear, evaluating the role of scaling laws, task complexity, pre-training loss, quantization, and prompting strategies. Our review extends beyond traditional LLMs and includes Large Reasoning Models (LRMs), which leverage reinforcement learning and inference-time search to amplify reasoning and self-reflection. However, emergence is not inherently positive. As AI systems gain autonomous reasoning capabilities, they also develop harmful behaviors, including deception, manipulation, and reward hacking. We highlight growing concerns about safety and governance, emphasizing the need for better evaluation frameworks and regulatory oversight.",
            "corpus_id": 276903421,
            "sentences": [
                {
                    "corpus_id": "276903421",
                    "title": "Emergent Abilities in Large Language Models: A Survey",
                    "text": "The term emergent in the context of LLMs is often used to describe capabilities that arise implicitly as models learn language patterns and structures through next-token prediction. These abilities are assessed through few-shot or zero-shot prompting, where models generalize to new tasks without undergoing explicit fine-tuning. This process, known as incontext learning (ICL), allows LLMs to infer new patterns and concepts solely from contextual information provided in the prompt. \n\nUnlike Wei et al.'s [87] definition of emergent abilities, which emphasizes sudden performance jumps with increased model scale, in-context learning does not necessarily require abrupt improvements. Instead, it refers to the gradual development of capabilities that enable LLMs to perform tasks for which they have not been explicitly trained. The research in this area primarily seeks to understand why LLMs generalize to new tasks without fine-tuning, what aspects of the training process contribute to this phenomenon, and how prompt design can be optimized to maximize ICL efficiency. \n\nTo explain in-context learning, various theories have emerged, ranging from statistical and structural perspectives to cognitive and algorithmic analogies. Some researchers attribute ICL to properties of input data distribution and label space structure [13,57], while others suggest that exposure to a diverse range of tasks during multitask-prompted learning facilitates generalization [69]. Another line of work explores how pretraining term frequencies influence a model's ability to recall and recombine information [66]. Some studies frame ICL as a form of Bayesian inference over the latent space of language [94], while others view it as a compositional recombination of linguistic structures [32]. Mechanistically, ICL has been linked to neural architectures, such as induction heads [24,59] and functional modules that emerge naturally during training [7,43,81,84]. Arora and Goyal [2] provide a theoretical framework in which LLMs develop skills through a bipartite \"skill graph\" that links training data to fundamental reasoning abilities, demonstrating how compositional generalization emerges as models scale. \n\nBeyond theoretical explanations, studies have examined the role of training data and model architecture in shaping ICL.",
                    "score": 0.4437833841268616,
                    "section_title": "IV. EMERGENT ABILITIES AS IN-CONTEXT LEARNING",
                    "char_start_offset": 47006,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 182,
                            "end": 329
                        },
                        {
                            "start": 330,
                            "end": 484
                        },
                        {
                            "start": 487,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 830
                        },
                        {
                            "start": 831,
                            "end": 1075
                        },
                        {
                            "start": 1078,
                            "end": 1233
                        },
                        {
                            "start": 1234,
                            "end": 1471
                        },
                        {
                            "start": 1472,
                            "end": 1604
                        },
                        {
                            "start": 1605,
                            "end": 1784
                        },
                        {
                            "start": 1785,
                            "end": 1953
                        },
                        {
                            "start": 1954,
                            "end": 2201
                        },
                        {
                            "start": 2204,
                            "end": 2323
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1332,
                            "end": 1336,
                            "matchedPaperCorpusId": "248665718"
                        },
                        {
                            "start": 1940,
                            "end": 1943,
                            "matchedPaperCorpusId": "258999187"
                        },
                        {
                            "start": 1943,
                            "end": 1946,
                            "matchedPaperCorpusId": "246823378"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87060546875
                }
            ],
            "relevance_judgement": 0.87060546875,
            "relevance_judgment_input_expanded": "# Title: Emergent Abilities in Large Language Models: A Survey\n# Venue: arXiv.org\n# Authors: Leonardo Berti, Flavio Giorgi, Gjergji Kasneci\n## Abstract\nLarge Language Models (LLMs) are leading a new technological revolution as one of the most promising research streams toward artificial general intelligence. The scaling of these models, accomplished by increasing the number of parameters and the magnitude of the training datasets, has been linked to various so-called emergent abilities that were previously unobserved. These emergent abilities, ranging from advanced reasoning and in-context learning to coding and problem-solving, have sparked an intense scientific debate: Are they truly emergent, or do they simply depend on external factors, such as training dynamics, the type of problems, or the chosen metric? What underlying mechanism causes them? Despite their transformative potential, emergent abilities remain poorly understood, leading to misconceptions about their definition, nature, predictability, and implications. In this work, we shed light on emergent abilities by conducting a comprehensive review of the phenomenon, addressing both its scientific underpinnings and real-world consequences. We first critically analyze existing definitions, exposing inconsistencies in conceptualizing emergent abilities. We then explore the conditions under which these abilities appear, evaluating the role of scaling laws, task complexity, pre-training loss, quantization, and prompting strategies. Our review extends beyond traditional LLMs and includes Large Reasoning Models (LRMs), which leverage reinforcement learning and inference-time search to amplify reasoning and self-reflection. However, emergence is not inherently positive. As AI systems gain autonomous reasoning capabilities, they also develop harmful behaviors, including deception, manipulation, and reward hacking. We highlight growing concerns about safety and governance, emphasizing the need for better evaluation frameworks and regulatory oversight.\n## IV. EMERGENT ABILITIES AS IN-CONTEXT LEARNING\nThe term emergent in the context of LLMs is often used to describe capabilities that arise implicitly as models learn language patterns and structures through next-token prediction. These abilities are assessed through few-shot or zero-shot prompting, where models generalize to new tasks without undergoing explicit fine-tuning. This process, known as incontext learning (ICL), allows LLMs to infer new patterns and concepts solely from contextual information provided in the prompt. \n\nUnlike Wei et al.'s [87] definition of emergent abilities, which emphasizes sudden performance jumps with increased model scale, in-context learning does not necessarily require abrupt improvements. Instead, it refers to the gradual development of capabilities that enable LLMs to perform tasks for which they have not been explicitly trained. The research in this area primarily seeks to understand why LLMs generalize to new tasks without fine-tuning, what aspects of the training process contribute to this phenomenon, and how prompt design can be optimized to maximize ICL efficiency. \n\nTo explain in-context learning, various theories have emerged, ranging from statistical and structural perspectives to cognitive and algorithmic analogies. Some researchers attribute ICL to properties of input data distribution and label space structure [13,57], while others suggest that exposure to a diverse range of tasks during multitask-prompted learning facilitates generalization [69]. Another line of work explores how pretraining term frequencies influence a model's ability to recall and recombine information [66]. Some studies frame ICL as a form of Bayesian inference over the latent space of language [94], while others view it as a compositional recombination of linguistic structures [32]. Mechanistically, ICL has been linked to neural architectures, such as induction heads [24,59] and functional modules that emerge naturally during training [7,43,81,84]. Arora and Goyal [2] provide a theoretical framework in which LLMs develop skills through a bipartite \"skill graph\" that links training data to fundamental reasoning abilities, demonstrating how compositional generalization emerges as models scale. \n\nBeyond theoretical explanations, studies have examined the role of training data and model architecture in shaping ICL.",
            "reference_string": "[276903421 | Berti et al. | 2025 | Citations: 6]"
        },
        {
            "title": "Understanding Knowledge Hijack Mechanism in In-context Learning through Associative Memory",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 62,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.11459, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2263247139",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2303397982",
                    "name": "Issei Sato"
                }
            ],
            "abstract": "In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks without fine-tuning by leveraging contextual information provided within a prompt. However, ICL relies not only on contextual clues but also on the global knowledge acquired during pretraining for the next token prediction. Analyzing this process has been challenging due to the complex computational circuitry of LLMs. This paper investigates the balance between in-context information and pretrained bigram knowledge in token prediction, focusing on the induction head mechanism, a key component in ICL. Leveraging the fact that a two-layer transformer can implement the induction head mechanism with associative memories, we theoretically analyze the logits when a two-layer transformer is given prompts generated by a bigram model. In the experiments, we design specific prompts to evaluate whether the outputs of a two-layer transformer align with the theoretical results.",
            "corpus_id": 274776135,
            "sentences": [
                {
                    "corpus_id": "274776135",
                    "title": "Understanding Knowledge Hijack Mechanism in In-context Learning through Associative Memory",
                    "text": "In recent years, transformer-based models, such as BERT [13] and GPT [42,43,5], have achieved remarkable success in natural language processing. Especially, in-context learning (ICL) [14] has emerged as a groundbreaking capability within large language models (LLMs), enabling them to adapt to new tasks without traditional fine-tuning. Instead, these models leverage patterns from a prompt or input sequence, effectively learning \"in context\" by interpreting examples or instructions provided in real-time [33,56]. The capability for ICL [5] in language models develops gradually during a \"phase change\" observed in the early stages of training. At the same time, models acquire the ability to complete complex patterns through induction heads, suggesting that induction heads are a key mechanistic basis of ICL [40]. \n\nIn the context of ICL, Jiang et al. [25] discovered a phenomenon called context hijacking. This phenomenon occurs when altering the context disrupts fact recall, causing the model to generate incorrect outputs influenced by the in-context knowledge provided in the prompt. Similarly, ICL can exhibit oversight of in-context knowledge provided in the prompt, where the model relies solely on the knowledge acquired during pretraining (global knowledge) while ignoring the in-context knowledge. These types of knowledge hijacking are thought to arise from either discarding part of the prompt's information or overly trusting the information in the prompt. Conversely, if the model can (i) comprehensively use the information in the prompt and (ii) balance in-context knowledge and global knowledge, hijacking can be prevented. In relation to the induction head, we demonstrate that employing relative positional encoding (RPE) allows the transformer to comprehensively use the information in the prompt. Furthermore, we show that even a three-layer transformer without positional encoding can achieve comprehensive use of information. Understanding how to avoid knowledge hijacking is essential for the safe and reliable use of ICL capabilities in LLMs and is a matter of significant societal importance.",
                    "score": 0.39794612230242793,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 144
                        },
                        {
                            "start": 145,
                            "end": 336
                        },
                        {
                            "start": 337,
                            "end": 515
                        },
                        {
                            "start": 516,
                            "end": 646
                        },
                        {
                            "start": 647,
                            "end": 818
                        },
                        {
                            "start": 821,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 1093
                        },
                        {
                            "start": 1094,
                            "end": 1313
                        },
                        {
                            "start": 1314,
                            "end": 1475
                        },
                        {
                            "start": 1476,
                            "end": 1646
                        },
                        {
                            "start": 1647,
                            "end": 1823
                        },
                        {
                            "start": 1824,
                            "end": 1954
                        },
                        {
                            "start": 1955,
                            "end": 2124
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 73,
                            "end": 76,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 183,
                            "end": 187,
                            "matchedPaperCorpusId": "13490401"
                        },
                        {
                            "start": 507,
                            "end": 511,
                            "matchedPaperCorpusId": "231632658"
                        },
                        {
                            "start": 857,
                            "end": 861,
                            "matchedPaperCorpusId": "270737871"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.869140625
                }
            ],
            "relevance_judgement": 0.869140625,
            "relevance_judgment_input_expanded": "# Title: Understanding Knowledge Hijack Mechanism in In-context Learning through Associative Memory\n# Venue: arXiv.org\n# Authors: Shuo Wang, Issei Sato\n## Abstract\nIn-context learning (ICL) enables large language models (LLMs) to adapt to new tasks without fine-tuning by leveraging contextual information provided within a prompt. However, ICL relies not only on contextual clues but also on the global knowledge acquired during pretraining for the next token prediction. Analyzing this process has been challenging due to the complex computational circuitry of LLMs. This paper investigates the balance between in-context information and pretrained bigram knowledge in token prediction, focusing on the induction head mechanism, a key component in ICL. Leveraging the fact that a two-layer transformer can implement the induction head mechanism with associative memories, we theoretically analyze the logits when a two-layer transformer is given prompts generated by a bigram model. In the experiments, we design specific prompts to evaluate whether the outputs of a two-layer transformer align with the theoretical results.\n## Introduction\nIn recent years, transformer-based models, such as BERT [13] and GPT [42,43,5], have achieved remarkable success in natural language processing. Especially, in-context learning (ICL) [14] has emerged as a groundbreaking capability within large language models (LLMs), enabling them to adapt to new tasks without traditional fine-tuning. Instead, these models leverage patterns from a prompt or input sequence, effectively learning \"in context\" by interpreting examples or instructions provided in real-time [33,56]. The capability for ICL [5] in language models develops gradually during a \"phase change\" observed in the early stages of training. At the same time, models acquire the ability to complete complex patterns through induction heads, suggesting that induction heads are a key mechanistic basis of ICL [40]. \n\nIn the context of ICL, Jiang et al. [25] discovered a phenomenon called context hijacking. This phenomenon occurs when altering the context disrupts fact recall, causing the model to generate incorrect outputs influenced by the in-context knowledge provided in the prompt. Similarly, ICL can exhibit oversight of in-context knowledge provided in the prompt, where the model relies solely on the knowledge acquired during pretraining (global knowledge) while ignoring the in-context knowledge. These types of knowledge hijacking are thought to arise from either discarding part of the prompt's information or overly trusting the information in the prompt. Conversely, if the model can (i) comprehensively use the information in the prompt and (ii) balance in-context knowledge and global knowledge, hijacking can be prevented. In relation to the induction head, we demonstrate that employing relative positional encoding (RPE) allows the transformer to comprehensively use the information in the prompt. Furthermore, we show that even a three-layer transformer without positional encoding can achieve comprehensive use of information. Understanding how to avoid knowledge hijacking is essential for the safe and reliable use of ICL capabilities in LLMs and is a matter of significant societal importance.",
            "reference_string": "[274776135 | Wang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 14,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.12580, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2065612731",
                    "name": "Laura Ruis"
                },
                {
                    "authorId": "2331416367",
                    "name": "Maximilian Mozes"
                },
                {
                    "authorId": "2028949",
                    "name": "Juhan Bae"
                },
                {
                    "authorId": "1419486332",
                    "name": "S. Kamalakara"
                },
                {
                    "authorId": "2303258186",
                    "name": "Dwarak Talupuru"
                },
                {
                    "authorId": "153563548",
                    "name": "Acyr F. Locatelli"
                },
                {
                    "authorId": "2066422293",
                    "name": "Robert Kirk"
                },
                {
                    "authorId": "1389854357",
                    "name": "Tim Rocktaschel"
                },
                {
                    "authorId": "1864353",
                    "name": "Edward Grefenstette"
                },
                {
                    "authorId": "2300370708",
                    "name": "Max Bartolo"
                }
            ],
            "abstract": "The capabilities and limitations of Large Language Models have been sketched out in great detail in recent years, providing an intriguing yet conflicting picture. On the one hand, LLMs demonstrate a general ability to solve problems. On the other hand, they show surprising reasoning gaps when compared to humans, casting doubt on the robustness of their generalisation strategies. The sheer volume of data used in the design of LLMs has precluded us from applying the method traditionally used to measure generalisation: train-test set separation. To overcome this, we study what kind of generalisation strategies LLMs employ when performing reasoning tasks by investigating the pretraining data they rely on. For two models of different sizes (7B and 35B) and 2.5B of their pretraining tokens, we identify what documents influence the model outputs for three simple mathematical reasoning tasks and contrast this to the data that are influential for answering factual questions. We find that, while the models rely on mostly distinct sets of data for each factual question, a document often has a similar influence across different reasoning questions within the same task, indicating the presence of procedural knowledge. We further find that the answers to factual questions often show up in the most influential data. However, for reasoning questions the answers usually do not show up as highly influential, nor do the answers to the intermediate reasoning steps. When we characterise the top ranked documents for the reasoning questions qualitatively, we confirm that the influential documents often contain procedural knowledge, like demonstrating how to obtain a solution using formulae or code. Our findings indicate that the approach to reasoning the models use is unlike retrieval, and more like a generalisable strategy that synthesises procedural knowledge from documents doing a similar form of reasoning.",
            "corpus_id": 274141509,
            "sentences": [
                {
                    "corpus_id": "274141509",
                    "title": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models",
                    "text": "The capabilities and limitations of Large Language Models have been sketched out in great detail in recent years, providing an intriguing yet conflicting picture. On the one hand, LLMs demonstrate a general ability to solve problems. On the other hand, they show surprising reasoning gaps when compared to humans, casting doubt on the robustness of their generalisation strategies. The sheer volume of data used in the design of LLMs has precluded us from applying the method traditionally used to measure generalisation: train-test set separation. To overcome this, we study what kind of generalisation strategies LLMs employ when performing reasoning tasks by investigating the pretraining data they rely on. For two models of different sizes (7B and 35B) and 2.5B of their pretraining tokens, we identify what documents influence the model outputs for three simple mathematical reasoning tasks and contrast this to the data that are influential for answering factual questions. We find that, while the models rely on mostly distinct sets of data for each factual question, a document often has a similar influence across different reasoning questions within the same task, indicating the presence of procedural knowledge. We further find that the answers to factual questions often show up in the most influential data. However, for reasoning questions the answers usually do not show up as highly influential, nor do the answers to the intermediate reasoning steps. When we characterise the top ranked documents for the reasoning questions qualitatively, we confirm that the influential documents often contain procedural knowledge, like demonstrating how to obtain a solution using formulae or code. Our findings indicate that the approach to reasoning the models use is unlike retrieval, and more like a generalisable strategy that synthesises procedural knowledge from documents doing a similar form of reasoning.",
                    "score": 0.38089619036960126,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8662109375
                }
            ],
            "relevance_judgement": 0.8662109375,
            "relevance_judgment_input_expanded": "# Title: Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models\n# Venue: arXiv.org\n# Authors: Laura Ruis, Maximilian Mozes, Juhan Bae, S. Kamalakara, Dwarak Talupuru, Acyr F. Locatelli, Robert Kirk, Tim Rocktaschel, Edward Grefenstette, Max Bartolo\n## Abstract\nThe capabilities and limitations of Large Language Models have been sketched out in great detail in recent years, providing an intriguing yet conflicting picture. On the one hand, LLMs demonstrate a general ability to solve problems. On the other hand, they show surprising reasoning gaps when compared to humans, casting doubt on the robustness of their generalisation strategies. The sheer volume of data used in the design of LLMs has precluded us from applying the method traditionally used to measure generalisation: train-test set separation. To overcome this, we study what kind of generalisation strategies LLMs employ when performing reasoning tasks by investigating the pretraining data they rely on. For two models of different sizes (7B and 35B) and 2.5B of their pretraining tokens, we identify what documents influence the model outputs for three simple mathematical reasoning tasks and contrast this to the data that are influential for answering factual questions. We find that, while the models rely on mostly distinct sets of data for each factual question, a document often has a similar influence across different reasoning questions within the same task, indicating the presence of procedural knowledge. We further find that the answers to factual questions often show up in the most influential data. However, for reasoning questions the answers usually do not show up as highly influential, nor do the answers to the intermediate reasoning steps. When we characterise the top ranked documents for the reasoning questions qualitatively, we confirm that the influential documents often contain procedural knowledge, like demonstrating how to obtain a solution using formulae or code. Our findings indicate that the approach to reasoning the models use is unlike retrieval, and more like a generalisable strategy that synthesises procedural knowledge from documents doing a similar form of reasoning.\n",
            "reference_string": "[274141509 | Ruis et al. | 2024 | Citations: 14]"
        },
        {
            "title": "Revisiting the Superficial Alignment Hypothesis",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 35,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03717, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345822162",
                    "name": "Mohit Raghavendra"
                },
                {
                    "authorId": "2151210591",
                    "name": "Vaskar Nath"
                },
                {
                    "authorId": "2265402399",
                    "name": "Sean M. Hendryx"
                }
            ],
            "abstract": "The Superficial Alignment Hypothesis posits that almost all of a language model's abilities and knowledge are learned during pre-training, while post-training is about giving a model the right style and format. We re-examine these claims by empirically studying the scaling behavior of post-training with increasing finetuning examples and evaluating them using objective task-specific standardized benchmarks. Through experiments with the Llama-3, Mistral, and Llama-2 model families of multiple sizes, we observe that, similar to the pre-training scaling laws, post-training task performance scales as a power law against the number of finetuning examples. This power law relationship holds across a broad array of capabilities, including mathematical reasoning, coding, instruction following, and multihop-reasoning. In addition, for tasks like math and multihop reasoning, we observe that a handful of examples merely align the model stylistically but do not saturate performance on the benchmarks. Model performance is instead correlated with its reasoning ability and it improves significantly with more examples, illustrating the need for holistic evaluation programs leveraging objective benchmarks in addition to measurement of alignment to human preferences. We also observe that language models are not necessarily limited to using knowledge learned during pre-training. With appropriate post-training, a model's ability to integrate new knowledge greatly improves on downstream tasks like multihop question-answering. Taken together, these results shed new light on the Superficial Alignment Hypothesis, suggesting that it is, at best, an over-simplification.",
            "corpus_id": 273186633,
            "sentences": [
                {
                    "corpus_id": "273186633",
                    "title": "Revisiting the Superficial Alignment Hypothesis",
                    "text": "The Superficial Alignment Hypothesis posits that almost all of a language model's abilities and knowledge are learned during pre-training, while post-training is about giving a model the right style and format. We re-examine these claims by empirically studying the scaling behavior of post-training with increasing finetuning examples and evaluating them using objective task-specific standardized benchmarks. Through experiments with the Llama-3, Mistral, and Llama-2 model families of multiple sizes, we observe that, similar to the pre-training scaling laws, post-training task performance scales as a power law against the number of finetuning examples. This power law relationship holds across a broad array of capabilities, including mathematical reasoning, coding, instruction following, and multihop-reasoning. In addition, for tasks like math and multihop reasoning, we observe that a handful of examples merely align the model stylistically but do not saturate performance on the benchmarks. Model performance is instead correlated with its reasoning ability and it improves significantly with more examples, illustrating the need for holistic evaluation programs leveraging objective benchmarks in addition to measurement of alignment to human preferences. We also observe that language models are not necessarily limited to using knowledge learned during pre-training. With appropriate post-training, a model's ability to integrate new knowledge greatly improves on downstream tasks like multihop question-answering. Taken together, these results shed new light on the Superficial Alignment Hypothesis, suggesting that it is, at best, an over-simplification.",
                    "score": 0.3822884136400235,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86328125
                }
            ],
            "relevance_judgement": 0.86328125,
            "relevance_judgment_input_expanded": "# Title: Revisiting the Superficial Alignment Hypothesis\n# Venue: arXiv.org\n# Authors: Mohit Raghavendra, Vaskar Nath, Sean M. Hendryx\n## Abstract\nThe Superficial Alignment Hypothesis posits that almost all of a language model's abilities and knowledge are learned during pre-training, while post-training is about giving a model the right style and format. We re-examine these claims by empirically studying the scaling behavior of post-training with increasing finetuning examples and evaluating them using objective task-specific standardized benchmarks. Through experiments with the Llama-3, Mistral, and Llama-2 model families of multiple sizes, we observe that, similar to the pre-training scaling laws, post-training task performance scales as a power law against the number of finetuning examples. This power law relationship holds across a broad array of capabilities, including mathematical reasoning, coding, instruction following, and multihop-reasoning. In addition, for tasks like math and multihop reasoning, we observe that a handful of examples merely align the model stylistically but do not saturate performance on the benchmarks. Model performance is instead correlated with its reasoning ability and it improves significantly with more examples, illustrating the need for holistic evaluation programs leveraging objective benchmarks in addition to measurement of alignment to human preferences. We also observe that language models are not necessarily limited to using knowledge learned during pre-training. With appropriate post-training, a model's ability to integrate new knowledge greatly improves on downstream tasks like multihop question-answering. Taken together, these results shed new light on the Superficial Alignment Hypothesis, suggesting that it is, at best, an over-simplification.\n",
            "reference_string": "[273186633 | Raghavendra et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Unveiling A Core Linguistic Region in Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 23,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14928, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257315251",
                    "name": "Jun Zhao"
                },
                {
                    "authorId": "2262654002",
                    "name": "Zhihao Zhang"
                },
                {
                    "authorId": "2262551692",
                    "name": "Yide Ma"
                },
                {
                    "authorId": "2257376355",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2067331064",
                    "name": "Tao Gui"
                },
                {
                    "authorId": "2262686179",
                    "name": "Luhui Gao"
                },
                {
                    "authorId": "2257129989",
                    "name": "Xuanjing Huang"
                }
            ],
            "abstract": "Brain localization, which describes the association between specific regions of the brain and their corresponding functions, is widely accepted in the field of cognitive science as an objective fact. Today's large language models (LLMs) possess human-level linguistic competence and can execute complex tasks requiring abstract knowledge and reasoning. To deeply understand the inherent mechanisms of intelligence emergence in LLMs, this paper conducts an analogical research using brain localization as a prototype. We have discovered a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. This core region exhibits significant dimension dependency, and perturbations to even a single parameter on specific dimensions can lead to a loss of linguistic competence. Furthermore, we observe that an improvement in linguistic competence does not necessarily accompany an elevation in the model's knowledge level, which might imply the existence of regions of domain knowledge that are dissociated from the linguistic region. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence. In the future, we will continue to investigate knowledge regions within LLMs and the interactions between them.",
            "corpus_id": 264591406,
            "sentences": [
                {
                    "corpus_id": "264591406",
                    "title": "Unveiling A Core Linguistic Region in Large Language Models",
                    "text": "Inspired by cognitive science research, this paper investigates whether specific functional regions exist within LLMs. We identify a core region specifically responsible for language processing within LLMs. This region occupies only about 1% of the model's parameters but plays a crucial role in maintaining the overall linguistic competence of the model. Invalid changes in the parameters of this region can severely impair the model's linguistic competence. We also observe a pronounced dimension dependence in the core region of linguistic competence. Surprisingly, in a large model like LLaMA-13B, which boasts 13 billion parameters, altering just one parameter could potentially inflict significant damage to its linguistic competence. This study further elucidates the relationship between linguistic competence and knowledge in large language models. We find that an improvement in linguistic competence does not necessarily imply an enhancement in knowledge level. This suggests the presence of a knowledge storage region in LLMs that operates independently of language processing. In summary, the findings of this paper shed new light on how the capabilities and knowledge are structured in large language models and help explain why the pre-training and fine-tuning processes of these large models differ significantly from their smaller predecessors.",
                    "score": 0.4938029974423907,
                    "section_title": "CONCLUSIONS",
                    "char_start_offset": 25015,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 119,
                            "end": 206
                        },
                        {
                            "start": 207,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 554
                        },
                        {
                            "start": 555,
                            "end": 740
                        },
                        {
                            "start": 741,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 972
                        },
                        {
                            "start": 973,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1361
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85986328125
                }
            ],
            "relevance_judgement": 0.85986328125,
            "relevance_judgment_input_expanded": "# Title: Unveiling A Core Linguistic Region in Large Language Models\n# Venue: arXiv.org\n# Authors: Jun Zhao, Zhihao Zhang, Yide Ma, Qi Zhang, Tao Gui, Luhui Gao, Xuanjing Huang\n## Abstract\nBrain localization, which describes the association between specific regions of the brain and their corresponding functions, is widely accepted in the field of cognitive science as an objective fact. Today's large language models (LLMs) possess human-level linguistic competence and can execute complex tasks requiring abstract knowledge and reasoning. To deeply understand the inherent mechanisms of intelligence emergence in LLMs, this paper conducts an analogical research using brain localization as a prototype. We have discovered a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. This core region exhibits significant dimension dependency, and perturbations to even a single parameter on specific dimensions can lead to a loss of linguistic competence. Furthermore, we observe that an improvement in linguistic competence does not necessarily accompany an elevation in the model's knowledge level, which might imply the existence of regions of domain knowledge that are dissociated from the linguistic region. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence. In the future, we will continue to investigate knowledge regions within LLMs and the interactions between them.\n## CONCLUSIONS\nInspired by cognitive science research, this paper investigates whether specific functional regions exist within LLMs. We identify a core region specifically responsible for language processing within LLMs. This region occupies only about 1% of the model's parameters but plays a crucial role in maintaining the overall linguistic competence of the model. Invalid changes in the parameters of this region can severely impair the model's linguistic competence. We also observe a pronounced dimension dependence in the core region of linguistic competence. Surprisingly, in a large model like LLaMA-13B, which boasts 13 billion parameters, altering just one parameter could potentially inflict significant damage to its linguistic competence. This study further elucidates the relationship between linguistic competence and knowledge in large language models. We find that an improvement in linguistic competence does not necessarily imply an enhancement in knowledge level. This suggests the presence of a knowledge storage region in LLMs that operates independently of language processing. In summary, the findings of this paper shed new light on how the capabilities and knowledge are structured in large language models and help explain why the pre-training and fine-tuning processes of these large models differ significantly from their smaller predecessors.",
            "reference_string": "[264591406 | Zhao et al. | 2023 | Citations: 6]"
        },
        {
            "title": "Abrupt Learning in Transformers: A Case Study on Matrix Completion",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.22244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2124445996",
                    "name": "Pulkit Gopalani"
                },
                {
                    "authorId": "35573359",
                    "name": "Ekdeep Singh Lubana"
                },
                {
                    "authorId": "2293775823",
                    "name": "Wei Hu"
                }
            ],
            "abstract": "Recent analysis on the training dynamics of Transformers has unveiled an interesting characteristic: the training loss plateaus for a significant number of training steps, and then suddenly (and sharply) drops to near--optimal values. To understand this phenomenon in depth, we formulate the low-rank matrix completion problem as a masked language modeling (MLM) task, and show that it is possible to train a BERT model to solve this task to low error. Furthermore, the loss curve shows a plateau early in training followed by a sudden drop to near-optimal values, despite no changes in the training procedure or hyper-parameters. To gain interpretability insights into this sudden drop, we examine the model's predictions, attention heads, and hidden states before and after this transition. Concretely, we observe that (a) the model transitions from simply copying the masked input to accurately predicting the masked entries; (b) the attention heads transition to interpretable patterns relevant to the task; and (c) the embeddings and hidden states encode information relevant to the problem. We also analyze the training dynamics of individual model components to understand the sudden drop in loss.",
            "corpus_id": 273661991,
            "sentences": [
                {
                    "corpus_id": "273661991",
                    "title": "Abrupt Learning in Transformers: A Case Study on Matrix Completion",
                    "text": "Large Language Models (LLMs) have revolutionized the field of natural language processing (NLP). However, there are still gaps in our understanding of these models, leading to challenges in controlling their behavior. As a pertinent example, the training of these models appears to demonstrate sudden improvements in metrics correlated with various capabilities [8], prompting questions about whether learning of a given capability can be predicted by tracking predefined progress measures and why such sudden changes occur. If undesirable capabilities can suddenly 'emerge' (despite any explicit supervision for them) [16], such sudden changes can be a challenge for AI regulation [21]. \n\nTo better understand such sudden changes during model training, this work investigates training BERT [12] on the classical mathematical task of low-rank matrix completion (LRMC) [6]. Making an analogy with masked language modeling (MLM), where sudden learning of syntactical structures was recently demonstrated [8], we argue matrix completion captures the core aspect of this learning problem (Fig. 1): given some relevant context (observed tokens), fill the missing elements (masked tokens). Specifically, we assume access to a matrix with some fraction of its entries missing, and would like to complete the missing entries of this matrix assuming the ground truth matrix is low-rank. We find that despite being a simplified abstraction of MLM, this setting already demonstrates a sharp decrease in loss as the model undergoes training (Fig. 1 (B)), preceded by a loss plateau for a significant number of training steps (akin to Chen et al. [8]). The simplicity of our setting further affords us interpretability, as we find that the point of sudden drop coincides with a precise change in how the model solves the task-we call this change an algorithmic transition. Specifically, we show that the pre-transition model simply copies the input (predicting 0 at masked positions), while the post-transition model accurately predicts missing values at masked positions. To perform the latter, distinctive changes occur in the model's attention heads during the period of sudden drop, wherein the model learns to identify relevant positional information to combine various elements in the input matrix and compute missing entries for matrix completion.",
                    "score": 0.3856871831954551,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 96
                        },
                        {
                            "start": 97,
                            "end": 217
                        },
                        {
                            "start": 218,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 687
                        },
                        {
                            "start": 690,
                            "end": 872
                        },
                        {
                            "start": 873,
                            "end": 1183
                        },
                        {
                            "start": 1184,
                            "end": 1377
                        },
                        {
                            "start": 1378,
                            "end": 1639
                        },
                        {
                            "start": 1640,
                            "end": 1859
                        },
                        {
                            "start": 1860,
                            "end": 2059
                        },
                        {
                            "start": 2060,
                            "end": 2341
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 362,
                            "end": 365,
                            "matchedPaperCorpusId": "261822542"
                        },
                        {
                            "start": 619,
                            "end": 623,
                            "matchedPaperCorpusId": "246867298"
                        },
                        {
                            "start": 791,
                            "end": 795,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 868,
                            "end": 871,
                            "matchedPaperCorpusId": "8061516"
                        },
                        {
                            "start": 1002,
                            "end": 1005,
                            "matchedPaperCorpusId": "261822542"
                        },
                        {
                            "start": 1634,
                            "end": 1637,
                            "matchedPaperCorpusId": "261822542"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85986328125
                }
            ],
            "relevance_judgement": 0.85986328125,
            "relevance_judgment_input_expanded": "# Title: Abrupt Learning in Transformers: A Case Study on Matrix Completion\n# Venue: Neural Information Processing Systems\n# Authors: Pulkit Gopalani, Ekdeep Singh Lubana, Wei Hu\n## Abstract\nRecent analysis on the training dynamics of Transformers has unveiled an interesting characteristic: the training loss plateaus for a significant number of training steps, and then suddenly (and sharply) drops to near--optimal values. To understand this phenomenon in depth, we formulate the low-rank matrix completion problem as a masked language modeling (MLM) task, and show that it is possible to train a BERT model to solve this task to low error. Furthermore, the loss curve shows a plateau early in training followed by a sudden drop to near-optimal values, despite no changes in the training procedure or hyper-parameters. To gain interpretability insights into this sudden drop, we examine the model's predictions, attention heads, and hidden states before and after this transition. Concretely, we observe that (a) the model transitions from simply copying the masked input to accurately predicting the masked entries; (b) the attention heads transition to interpretable patterns relevant to the task; and (c) the embeddings and hidden states encode information relevant to the problem. We also analyze the training dynamics of individual model components to understand the sudden drop in loss.\n## Introduction\nLarge Language Models (LLMs) have revolutionized the field of natural language processing (NLP). However, there are still gaps in our understanding of these models, leading to challenges in controlling their behavior. As a pertinent example, the training of these models appears to demonstrate sudden improvements in metrics correlated with various capabilities [8], prompting questions about whether learning of a given capability can be predicted by tracking predefined progress measures and why such sudden changes occur. If undesirable capabilities can suddenly 'emerge' (despite any explicit supervision for them) [16], such sudden changes can be a challenge for AI regulation [21]. \n\nTo better understand such sudden changes during model training, this work investigates training BERT [12] on the classical mathematical task of low-rank matrix completion (LRMC) [6]. Making an analogy with masked language modeling (MLM), where sudden learning of syntactical structures was recently demonstrated [8], we argue matrix completion captures the core aspect of this learning problem (Fig. 1): given some relevant context (observed tokens), fill the missing elements (masked tokens). Specifically, we assume access to a matrix with some fraction of its entries missing, and would like to complete the missing entries of this matrix assuming the ground truth matrix is low-rank. We find that despite being a simplified abstraction of MLM, this setting already demonstrates a sharp decrease in loss as the model undergoes training (Fig. 1 (B)), preceded by a loss plateau for a significant number of training steps (akin to Chen et al. [8]). The simplicity of our setting further affords us interpretability, as we find that the point of sudden drop coincides with a precise change in how the model solves the task-we call this change an algorithmic transition. Specifically, we show that the pre-transition model simply copies the input (predicting 0 at masked positions), while the post-transition model accurately predicts missing values at masked positions. To perform the latter, distinctive changes occur in the model's attention heads during the period of sudden drop, wherein the model learns to identify relevant positional information to combine various elements in the input matrix and compute missing entries for matrix completion.",
            "reference_string": "[273661991 | Gopalani et al. | 2024 | Citations: 4]"
        },
        {
            "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 36,
            "citation_count": 267,
            "influential_citation_count": 37,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.07759, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315830",
                    "name": "Ronen Eldan"
                },
                {
                    "authorId": "152244300",
                    "name": "Yuan-Fang Li"
                }
            ],
            "abstract": "Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.",
            "corpus_id": 258686446,
            "sentences": [
                {
                    "corpus_id": "258686446",
                    "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
                    "text": "Natural language is rich and diverse. It is not only a system of rules and symbols, but also a way of conveying and interpreting meaning [32]. To understand and produce language, one needs not only to master the technical rules of grammar and knowledge of vocabulary, but also to have sufficient factual information and to be able to reason logically and contextually. Therefore, autoregressive language models, which are able to generate coherent English text, must have acquired some degree of these capabilities as well. For example, consider the following incomplete sentence: Jack was hungry, so he went looking for \u27e8 \u27e9 To complete this sentence in a sensible way, the language model needs to know that hunger is a state that motivates people to seek food, and that food is a category of things that can satisfy hunger. It also needs to choose a word that fits the syntactic and semantic constraints of the sentence (such as \"a snack\"), and that is plausible given the situation and the background knowledge. \n\nAn example that illustrates the need for reasoning is: \n\nLily wanted to get either a cat or a dog. Her mother didn't let her get a dog so instead she \u27e8 \u27e9 \n\nTo complete this sentence, the language model needs to invoke reasoning: it needs to apply the principle of disjunction elimination: if Lily wants either a cat or a dog, and she cannot get a dog, then she must choose a cat. It also needs to choose a words that expresses Lily's intention or action that is coherent with the tone and style of the text. Language models have been shown to exhibit a range of emergent abilities, such as summarization, arithmetic, translation, and commonsense reasoning, as they are scaled up in size and trained on diverse and large corpora [24,4,5,21]. These abilities suggest that language models are not only learning the surface patterns of language, but also acquiring some degree of semantic and logical understanding of the world and the text. However, it is not clear at what scale these abilities emerge, and how they depend on the model architecture and the data distribution. \n\nPerhaps the most fundamental ability for a language model is to produce coherent and fluent English text, which, as we discussed above, requires not only grammatical and lexical knowledge, but also factual information and contextual reasoning.",
                    "score": 0.40815515993594265,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 37
                        },
                        {
                            "start": 38,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 368
                        },
                        {
                            "start": 369,
                            "end": 523
                        },
                        {
                            "start": 524,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 1013
                        },
                        {
                            "start": 1016,
                            "end": 1070
                        },
                        {
                            "start": 1073,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1169
                        },
                        {
                            "start": 1172,
                            "end": 1395
                        },
                        {
                            "start": 1396,
                            "end": 1523
                        },
                        {
                            "start": 1524,
                            "end": 1756
                        },
                        {
                            "start": 1757,
                            "end": 1953
                        },
                        {
                            "start": 1954,
                            "end": 2089
                        },
                        {
                            "start": 2092,
                            "end": 2335
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 137,
                            "end": 141,
                            "matchedPaperCorpusId": "56798209"
                        },
                        {
                            "start": 1744,
                            "end": 1748,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 1748,
                            "end": 1750,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85888671875
                }
            ],
            "relevance_judgement": 0.85888671875,
            "relevance_judgment_input_expanded": "# Title: TinyStories: How Small Can Language Models Be and Still Speak Coherent English?\n# Venue: arXiv.org\n# Authors: Ronen Eldan, Yuan-Fang Li\n## Abstract\nLanguage models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.\n## Introduction\nNatural language is rich and diverse. It is not only a system of rules and symbols, but also a way of conveying and interpreting meaning [32]. To understand and produce language, one needs not only to master the technical rules of grammar and knowledge of vocabulary, but also to have sufficient factual information and to be able to reason logically and contextually. Therefore, autoregressive language models, which are able to generate coherent English text, must have acquired some degree of these capabilities as well. For example, consider the following incomplete sentence: Jack was hungry, so he went looking for \u27e8 \u27e9 To complete this sentence in a sensible way, the language model needs to know that hunger is a state that motivates people to seek food, and that food is a category of things that can satisfy hunger. It also needs to choose a word that fits the syntactic and semantic constraints of the sentence (such as \"a snack\"), and that is plausible given the situation and the background knowledge. \n\nAn example that illustrates the need for reasoning is: \n\nLily wanted to get either a cat or a dog. Her mother didn't let her get a dog so instead she \u27e8 \u27e9 \n\nTo complete this sentence, the language model needs to invoke reasoning: it needs to apply the principle of disjunction elimination: if Lily wants either a cat or a dog, and she cannot get a dog, then she must choose a cat. It also needs to choose a words that expresses Lily's intention or action that is coherent with the tone and style of the text. Language models have been shown to exhibit a range of emergent abilities, such as summarization, arithmetic, translation, and commonsense reasoning, as they are scaled up in size and trained on diverse and large corpora [24,4,5,21]. These abilities suggest that language models are not only learning the surface patterns of language, but also acquiring some degree of semantic and logical understanding of the world and the text. However, it is not clear at what scale these abilities emerge, and how they depend on the model architecture and the data distribution. \n\nPerhaps the most fundamental ability for a language model is to produce coherent and fluent English text, which, as we discussed above, requires not only grammatical and lexical knowledge, but also factual information and contextual reasoning.",
            "reference_string": "[258686446 | Eldan et al. | 2023 | Citations: 267]"
        },
        {
            "title": "What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2019,
            "reference_count": 41,
            "citation_count": 607,
            "influential_citation_count": 36,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00298/1923116/tacl_a_00298.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.13528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "37907837",
                    "name": "Allyson Ettinger"
                }
            ],
            "abstract": "Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation.",
            "corpus_id": 199001173,
            "sentences": [
                {
                    "corpus_id": "199001173",
                    "title": "What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models",
                    "text": "Pre-training of NLP models with a language modeling objective has recently gained popularity as a precursor to task-specific fine-tuning. Pretrained models like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018a) have advanced the state of the art in a wide variety of tasks, suggesting that these models acquire valuable, generalizable linguistic competence during the pretraining process. However, though we have established the benefits of language model pre-training, we have yet to understand what exactly about language these models learn during that process. \n\nThis paper aims to improve our understanding of what language models (LMs) know about language, by introducing a set of diagnostics targeting a range of linguistic capacities, drawn from human psycholinguistic experiments. Because of their origin in psycholinguistics, these diagnostics have two distinct advantages: they are carefully controlled to ask targeted questions about linguistic capabilities, and they are designed to ask these questions by examining word predictions in context, which allows us to study LMs without any need for task-specific fine-tuning. \n\nBeyond these advantages, our diagnostics distinguish themselves from existing tests for LMs in two primary ways. First, these tests have been chosen specifically for their capacity to reveal insensitivities in predictive models, as evidenced by patterns that they elicit in human brain responses. Second, each of these tests targets a set of linguistic capacities that extend beyond the primarily syntactic focus seen in existing LM diagnosticswe have tests targeting commonsense/pragmatic inference, semantic roles and event knowledge, category membership, and negation. Each of our diagnostics is set up to support tests of both word prediction accuracy and sensitivity to distinctions between good and bad context completions. Although we focus on the BERT model here as an illustrative case study, these diagnostics are applicable for testing of any language model. This paper makes two main contributions. First, we introduce a new set of targeted diagnostics for assessing linguistic capacities in language models. 1 Second, we apply these tests to shed light on strengths and weaknesses of the popular BERT model.",
                    "score": 0.39450226305380726,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 397
                        },
                        {
                            "start": 398,
                            "end": 572
                        },
                        {
                            "start": 575,
                            "end": 797
                        },
                        {
                            "start": 798,
                            "end": 1142
                        },
                        {
                            "start": 1145,
                            "end": 1257
                        },
                        {
                            "start": 1258,
                            "end": 1441
                        },
                        {
                            "start": 1442,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1874
                        },
                        {
                            "start": 1875,
                            "end": 2014
                        },
                        {
                            "start": 2015,
                            "end": 2055
                        },
                        {
                            "start": 2056,
                            "end": 2265
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 166,
                            "end": 187,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 197,
                            "end": 219,
                            "matchedPaperCorpusId": "3626819"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85791015625
                }
            ],
            "relevance_judgement": 0.85791015625,
            "relevance_judgment_input_expanded": "# Title: What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Allyson Ettinger\n## Abstract\nPre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation.\n## Introduction\nPre-training of NLP models with a language modeling objective has recently gained popularity as a precursor to task-specific fine-tuning. Pretrained models like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018a) have advanced the state of the art in a wide variety of tasks, suggesting that these models acquire valuable, generalizable linguistic competence during the pretraining process. However, though we have established the benefits of language model pre-training, we have yet to understand what exactly about language these models learn during that process. \n\nThis paper aims to improve our understanding of what language models (LMs) know about language, by introducing a set of diagnostics targeting a range of linguistic capacities, drawn from human psycholinguistic experiments. Because of their origin in psycholinguistics, these diagnostics have two distinct advantages: they are carefully controlled to ask targeted questions about linguistic capabilities, and they are designed to ask these questions by examining word predictions in context, which allows us to study LMs without any need for task-specific fine-tuning. \n\nBeyond these advantages, our diagnostics distinguish themselves from existing tests for LMs in two primary ways. First, these tests have been chosen specifically for their capacity to reveal insensitivities in predictive models, as evidenced by patterns that they elicit in human brain responses. Second, each of these tests targets a set of linguistic capacities that extend beyond the primarily syntactic focus seen in existing LM diagnosticswe have tests targeting commonsense/pragmatic inference, semantic roles and event knowledge, category membership, and negation. Each of our diagnostics is set up to support tests of both word prediction accuracy and sensitivity to distinctions between good and bad context completions. Although we focus on the BERT model here as an illustrative case study, these diagnostics are applicable for testing of any language model. This paper makes two main contributions. First, we introduce a new set of targeted diagnostics for assessing linguistic capacities in language models. 1 Second, we apply these tests to shed light on strengths and weaknesses of the popular BERT model.",
            "reference_string": "[199001173 | Ettinger | 2019 | Citations: 607]"
        },
        {
            "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
            "venue": "ACM Trans. Inf. Syst.",
            "year": 2023,
            "reference_count": 287,
            "citation_count": 903,
            "influential_citation_count": 30,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3703155",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.05232, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2265930173",
                    "name": "Lei Huang"
                },
                {
                    "authorId": "2248673673",
                    "name": "Weijiang Yu"
                },
                {
                    "authorId": "2265878959",
                    "name": "Weitao Ma"
                },
                {
                    "authorId": "2208739098",
                    "name": "Weihong Zhong"
                },
                {
                    "authorId": "51056532",
                    "name": "Zhangyin Feng"
                },
                {
                    "authorId": "2256768984",
                    "name": "Haotian Wang"
                },
                {
                    "authorId": "1500384901",
                    "name": "Qianglong Chen"
                },
                {
                    "authorId": "2247980601",
                    "name": "Weihua Peng"
                },
                {
                    "authorId": "2674998",
                    "name": "Xiaocheng Feng"
                },
                {
                    "authorId": "2257004102",
                    "name": "Bing Qin"
                },
                {
                    "authorId": "2238862997",
                    "name": "Ting Liu"
                }
            ],
            "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.",
            "corpus_id": 265067168,
            "sentences": [
                {
                    "corpus_id": "265067168",
                    "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                    "text": "The attributes and behaviors of LLMs are deeply intertwined with their training processes. LLMs undergo three primary training stages: pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). Analyzing these stages provides insight into hallucination origins in LLMs, as each stage equips the model with specific capabilities. Pre-training. Pre-training is generally considered a crucial stage for LLM to acquire knowledge and skills (Zhou et al., 2023a). Language models, during pre-training, aim to predict the next token in a sequence autoregressively. Through selfsupervised training on extensive textual corpora, the model acquires knowledge of language syntax, world knowledge, and reasoning abilities, providing a robust foundation for subsequent fine-tuning tasks. Besides, recent research (Sutskever, 2023;Del\u00e9tang et al., 2023) suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence of language models lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world. \n\nSupervised Fine-Tuning. While LLMs acquire substantial knowledge and capabilities during the pre-training stage, it's crucial to recognize that pretraining primarily optimizes for completion. Consequently, pre-trained LLMs fundamentally served as completion machines, which can lead to a misalignment between the next-word prediction objective of LLMs and the user's objective of obtaining desired responses. To bridge this gap, SFT (Zhang et al., 2023d) has been introduced, which involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, resulting in enhanced capabilities and improved controllability of LLMs. Furthermore, recent studies (Chung et al., 2022;Iyer et al., 2022) have confirmed the effectiveness of supervised fine-tuning to achieve exceptional performance on unseen tasks, showcasing their remarkable generalization abilities. \n\nReinforcement Learning from Human Feedback. While the SFT process successfully enables LLMs to follow user instructions, there is still room for them to better align with human preferences.",
                    "score": 0.42970717763971567,
                    "section_title": "Training Stages of Large Language Models",
                    "char_start_offset": 7928,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 90
                        },
                        {
                            "start": 91,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 368
                        },
                        {
                            "start": 369,
                            "end": 382
                        },
                        {
                            "start": 383,
                            "end": 497
                        },
                        {
                            "start": 498,
                            "end": 597
                        },
                        {
                            "start": 598,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 980
                        },
                        {
                            "start": 981,
                            "end": 1079
                        },
                        {
                            "start": 1080,
                            "end": 1193
                        },
                        {
                            "start": 1196,
                            "end": 1219
                        },
                        {
                            "start": 1220,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1604
                        },
                        {
                            "start": 1605,
                            "end": 1850
                        },
                        {
                            "start": 1851,
                            "end": 2082
                        },
                        {
                            "start": 2085,
                            "end": 2128
                        },
                        {
                            "start": 2129,
                            "end": 2274
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1899,
                            "end": 1917,
                            "matchedPaperCorpusId": "255096269"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.857421875
                }
            ],
            "relevance_judgement": 0.857421875,
            "relevance_judgment_input_expanded": "# Title: A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions\n# Venue: ACM Trans. Inf. Syst.\n# Authors: Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu\n## Abstract\nThe emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.\n## Training Stages of Large Language Models\nThe attributes and behaviors of LLMs are deeply intertwined with their training processes. LLMs undergo three primary training stages: pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). Analyzing these stages provides insight into hallucination origins in LLMs, as each stage equips the model with specific capabilities. Pre-training. Pre-training is generally considered a crucial stage for LLM to acquire knowledge and skills (Zhou et al., 2023a). Language models, during pre-training, aim to predict the next token in a sequence autoregressively. Through selfsupervised training on extensive textual corpora, the model acquires knowledge of language syntax, world knowledge, and reasoning abilities, providing a robust foundation for subsequent fine-tuning tasks. Besides, recent research (Sutskever, 2023;Del\u00e9tang et al., 2023) suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence of language models lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world. \n\nSupervised Fine-Tuning. While LLMs acquire substantial knowledge and capabilities during the pre-training stage, it's crucial to recognize that pretraining primarily optimizes for completion. Consequently, pre-trained LLMs fundamentally served as completion machines, which can lead to a misalignment between the next-word prediction objective of LLMs and the user's objective of obtaining desired responses. To bridge this gap, SFT (Zhang et al., 2023d) has been introduced, which involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, resulting in enhanced capabilities and improved controllability of LLMs. Furthermore, recent studies (Chung et al., 2022;Iyer et al., 2022) have confirmed the effectiveness of supervised fine-tuning to achieve exceptional performance on unseen tasks, showcasing their remarkable generalization abilities. \n\nReinforcement Learning from Human Feedback. While the SFT process successfully enables LLMs to follow user instructions, there is still room for them to better align with human preferences.",
            "reference_string": "[265067168 | Huang et al. | 2023 | Citations: 903]"
        },
        {
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 421,
            "citation_count": 215,
            "influential_citation_count": 12,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.06627, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2412497",
                    "name": "Kyle Mahowald"
                },
                {
                    "authorId": "2017088782",
                    "name": "Anna A. Ivanova"
                },
                {
                    "authorId": "38412172",
                    "name": "I. Blank"
                },
                {
                    "authorId": "1931482",
                    "name": "N. Kanwisher"
                },
                {
                    "authorId": "1763295",
                    "name": "J. Tenenbaum"
                },
                {
                    "authorId": "144733430",
                    "name": "Evelina Fedorenko"
                }
            ],
            "abstract": "Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become --\"thinking machines\", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.",
            "corpus_id": 255941592,
            "sentences": [
                {
                    "corpus_id": "255941592",
                    "title": "Dissociating language and thought in large language models: a cognitive perspective",
                    "text": "Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become --\"thinking machines\", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.",
                    "score": 0.5537037883310325,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.853515625
                }
            ],
            "relevance_judgement": 0.853515625,
            "relevance_judgment_input_expanded": "# Title: Dissociating language and thought in large language models: a cognitive perspective\n# Venue: arXiv.org\n# Authors: Kyle Mahowald, Anna A. Ivanova, I. Blank, N. Kanwisher, J. Tenenbaum, Evelina Fedorenko\n## Abstract\nToday's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become --\"thinking machines\", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.\n",
            "reference_string": "[255941592 | Mahowald et al. | 2023 | Citations: 215]"
        },
        {
            "title": "The LLM Language Network: A Neuroscientific Approach for Identifying Causally Task-Relevant Units",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 63,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.02280, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2006905770",
                    "name": "Badr AlKhamissi"
                },
                {
                    "authorId": "1780803442",
                    "name": "Greta Tuckute"
                },
                {
                    "authorId": "2691021",
                    "name": "Antoine Bosselut"
                },
                {
                    "authorId": "8551292",
                    "name": "Martin Schrimpf"
                }
            ],
            "abstract": "Large language models (LLMs) exhibit remarkable capabilities on not just language tasks, but also various tasks that are not linguistic in nature, such as logical reasoning and social inference. In the human brain, neuroscience has identified a core language system that selectively and causally supports language processing. We here ask whether similar specialization for language emerges in LLMs. We identify language-selective units within 18 popular LLMs, using the same localization approach that is used in neuroscience. We then establish the causal role of these units by demonstrating that ablating LLM language-selective units -- but not random units -- leads to drastic deficits in language tasks. Correspondingly, language-selective LLM units are more aligned to brain recordings from the human language system than random units. Finally, we investigate whether our localization method extends to other cognitive domains: while we find specialized networks in some LLMs for reasoning and social capabilities, there are substantial differences among models. These findings provide functional and causal evidence for specialization in large language models, and highlight parallels with the functional organization in the brain.",
            "corpus_id": 273821157,
            "sentences": [
                {
                    "corpus_id": "273821157",
                    "title": "The LLM Language Network: A Neuroscientific Approach for Identifying Causally Task-Relevant Units",
                    "text": "Large language models (LLMs) exhibit remarkable capabilities on not just language tasks, but also various tasks that are not linguistic in nature, such as logical reasoning and social inference. In the human brain, neuroscience has identified a core language system that selectively and causally supports language processing. We here ask whether similar specialization for language emerges in LLMs. We identify language-selective units within 18 popular LLMs, using the same localization approach that is used in neuroscience. We then establish the causal role of these units by demonstrating that ablating LLM language-selective units -- but not random units -- leads to drastic deficits in language tasks. Correspondingly, language-selective LLM units are more aligned to brain recordings from the human language system than random units. Finally, we investigate whether our localization method extends to other cognitive domains: while we find specialized networks in some LLMs for reasoning and social capabilities, there are substantial differences among models. These findings provide functional and causal evidence for specialization in large language models, and highlight parallels with the functional organization in the brain.",
                    "score": 0.3869161053408935,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.853515625
                }
            ],
            "relevance_judgement": 0.853515625,
            "relevance_judgment_input_expanded": "# Title: The LLM Language Network: A Neuroscientific Approach for Identifying Causally Task-Relevant Units\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Badr AlKhamissi, Greta Tuckute, Antoine Bosselut, Martin Schrimpf\n## Abstract\nLarge language models (LLMs) exhibit remarkable capabilities on not just language tasks, but also various tasks that are not linguistic in nature, such as logical reasoning and social inference. In the human brain, neuroscience has identified a core language system that selectively and causally supports language processing. We here ask whether similar specialization for language emerges in LLMs. We identify language-selective units within 18 popular LLMs, using the same localization approach that is used in neuroscience. We then establish the causal role of these units by demonstrating that ablating LLM language-selective units -- but not random units -- leads to drastic deficits in language tasks. Correspondingly, language-selective LLM units are more aligned to brain recordings from the human language system than random units. Finally, we investigate whether our localization method extends to other cognitive domains: while we find specialized networks in some LLMs for reasoning and social capabilities, there are substantial differences among models. These findings provide functional and causal evidence for specialization in large language models, and highlight parallels with the functional organization in the brain.\n",
            "reference_string": "[273821157 | AlKhamissi et al. | 2024 | Citations: 12]"
        },
        {
            "title": "Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 15,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.03268, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115553132",
                    "name": "Xinyi Wang"
                },
                {
                    "authorId": "2039956094",
                    "name": "Alfonso Amayuelas"
                },
                {
                    "authorId": "2119058805",
                    "name": "Kexun Zhang"
                },
                {
                    "authorId": "2256983134",
                    "name": "Liangming Pan"
                },
                {
                    "authorId": "2109664620",
                    "name": "Wenhu Chen"
                },
                {
                    "authorId": "2257130314",
                    "name": "W. Wang"
                }
            ],
            "abstract": "Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and chain-of-thought (CoT) reasoning. More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and CoT datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step reasoning performance. code: https://github.com/WANGXinyiLinda/LM_random_walk",
            "corpus_id": 267412517,
            "sentences": [
                {
                    "corpus_id": "267412517",
                    "title": "Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation",
                    "text": "Recently, pre-trained large language models (LLMs) (Touvron et al., 2023a;b;Brown et al., 2020) have demonstrated remarkable capabilities in performing intricate reasoning tasks (Kojima et al., 2022). These tasks include problemsolving with world knowledge (Hendrycks et al., 2020;Suzgun et al., 2022), logical reasoning (Pan et al., 2023), and solving mathematical problems (Cobbe et al., 2021; Figure 1. We hypothesize that the pre-training corpus can be viewed as generated from random walks on a reasoning graph over world knowledge/concepts. With each node si representing concepts, pj can be viewed as arguments that connect them. Then we hypothesize that a language model (LM) training on such a corpus can be viewed as reasoning by a weighted aggregation of random walk paths that connect the entities in interest. PLM denote the LM distribution while PD denotes the random walk probability from the pre-training corpus. w 1 i denotes the weight assigned to the first random walk path by the LM for argument pi, and w 2 i denotes the weight assigned to the second random walk path. Hendrycks et al., 2021). These models are typically not explicitly fine-tuned to solve these tasks. Recent research (Jain et al., 2023) also suggests that the supervised fine-tuning process following pre-training only learns a wrapper on top of the already existing model capabilities, instead of learning new ones. It is intriguing to understand how next-token prediction pre-training contributes to the emergence of such reasoning capability. A better understanding of this matter can also inspire new pre-training/fine-tuning techniques to improve these important abilities of LLMs. \n\nIt is well-known that LLMs acquire emergent abilities through extensive pre-training (Wei et al., 2022a). In this paper, we focus on elucidating the emergence of reasoning ability -the capacity to draw novel conclusions from existing knowledge, which has been less studied. Many recent works also attempt to understand this phenomenon.",
                    "score": 0.3980955608233405,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 200
                        },
                        {
                            "start": 201,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 546
                        },
                        {
                            "start": 547,
                            "end": 636
                        },
                        {
                            "start": 637,
                            "end": 822
                        },
                        {
                            "start": 823,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1189
                        },
                        {
                            "start": 1190,
                            "end": 1405
                        },
                        {
                            "start": 1406,
                            "end": 1534
                        },
                        {
                            "start": 1535,
                            "end": 1675
                        },
                        {
                            "start": 1678,
                            "end": 1783
                        },
                        {
                            "start": 1784,
                            "end": 1951
                        },
                        {
                            "start": 1952,
                            "end": 2013
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 76,
                            "end": 95,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 257,
                            "end": 281,
                            "matchedPaperCorpusId": "221516475"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.849609375
                }
            ],
            "relevance_judgement": 0.849609375,
            "relevance_judgment_input_expanded": "# Title: Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation\n# Venue: International Conference on Machine Learning\n# Authors: Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan, Wenhu Chen, W. Wang\n## Abstract\nPre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and chain-of-thought (CoT) reasoning. More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and CoT datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step reasoning performance. code: https://github.com/WANGXinyiLinda/LM_random_walk\n## Introduction\nRecently, pre-trained large language models (LLMs) (Touvron et al., 2023a;b;Brown et al., 2020) have demonstrated remarkable capabilities in performing intricate reasoning tasks (Kojima et al., 2022). These tasks include problemsolving with world knowledge (Hendrycks et al., 2020;Suzgun et al., 2022), logical reasoning (Pan et al., 2023), and solving mathematical problems (Cobbe et al., 2021; Figure 1. We hypothesize that the pre-training corpus can be viewed as generated from random walks on a reasoning graph over world knowledge/concepts. With each node si representing concepts, pj can be viewed as arguments that connect them. Then we hypothesize that a language model (LM) training on such a corpus can be viewed as reasoning by a weighted aggregation of random walk paths that connect the entities in interest. PLM denote the LM distribution while PD denotes the random walk probability from the pre-training corpus. w 1 i denotes the weight assigned to the first random walk path by the LM for argument pi, and w 2 i denotes the weight assigned to the second random walk path. Hendrycks et al., 2021). These models are typically not explicitly fine-tuned to solve these tasks. Recent research (Jain et al., 2023) also suggests that the supervised fine-tuning process following pre-training only learns a wrapper on top of the already existing model capabilities, instead of learning new ones. It is intriguing to understand how next-token prediction pre-training contributes to the emergence of such reasoning capability. A better understanding of this matter can also inspire new pre-training/fine-tuning techniques to improve these important abilities of LLMs. \n\nIt is well-known that LLMs acquire emergent abilities through extensive pre-training (Wei et al., 2022a). In this paper, we focus on elucidating the emergence of reasoning ability -the capacity to draw novel conclusions from existing knowledge, which has been less studied. Many recent works also attempt to understand this phenomenon.",
            "reference_string": "[267412517 | Wang et al. | 2024 | Citations: 15]"
        },
        {
            "title": "Integrating Personality into Digital Humans: A Review of LLM-Driven Approaches for Virtual Reality",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 99,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.16457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313632583",
                    "name": "I. A. Brito"
                },
                {
                    "authorId": "2351602847",
                    "name": "J. S. Dollis"
                },
                {
                    "authorId": "2355407348",
                    "name": "Fernanda Bufon F\u00e4rber"
                },
                {
                    "authorId": "2351603831",
                    "name": "Pedro Schindler Freire Brasil Ribeiro"
                },
                {
                    "authorId": "2347519877",
                    "name": "Rafael Teixeira Sousa"
                },
                {
                    "authorId": "2349060811",
                    "name": "Arlindo Rodrigues Galv\u00e3o Filho"
                }
            ],
            "abstract": "The integration of large language models (LLMs) into virtual reality (VR) environments has opened new pathways for creating more immersive and interactive digital humans. By leveraging the generative capabilities of LLMs alongside multimodal outputs such as facial expressions and gestures, virtual agents can simulate human-like personalities and emotions, fostering richer and more engaging user experiences. This paper provides a comprehensive review of methods for enabling digital humans to adopt nuanced personality traits, exploring approaches such as zero-shot, few-shot, and fine-tuning. Additionally, it highlights the challenges of integrating LLM-driven personality traits into VR, including computational demands, latency issues, and the lack of standardized evaluation frameworks for multimodal interactions. By addressing these gaps, this work lays a foundation for advancing applications in education, therapy, and gaming, while fostering interdisciplinary collaboration to redefine human-computer interaction in VR.",
            "corpus_id": 277244499,
            "sentences": [
                {
                    "corpus_id": "277244499",
                    "title": "Integrating Personality into Digital Humans: A Review of LLM-Driven Approaches for Virtual Reality",
                    "text": "The field of natural language processing (NLP) has undergone rapid transformations since the introduction of the Transformer architecture [81]. This breakthrough has fundamentally reshaped NLP, leading to the proliferation of numerous language models, particularly decoder-based architectures, which have revolutionized text generation and paved the way for large language models (LLMs). These advanced neural networks redefine the boundaries of machine understanding and language generation, leveraging extensive datasets comprising billions or even trillions of words from diverse domains to maximize linguistic coverage and generalization [4]. \n\nThe unprecedented scale of LLMs, characterized by vast numbers of parameters, enables them to capture intricate linguistic patterns, including semantic relationships, syntactic structures, and long-range dependencies [77,4]. This capacity allows them to perform complex language-related tasks, such as advanced reasoning and contextual understanding. In particular, LLMs demonstrate emergent abilities, performing tasks that are not explicitly programmed but naturally arise from their scale and the complexity of their training process [85,37]. \n\nThe development of large language models generally involves two primary stages: pre-training and fine-tuning. The pre-training phase involves exposing the model to extensive volumes of unlabeled general knowledge text data through a language modeling task, most commonly framed as the next token prediction in a sequence [4]. Then, the resulting model passes through the finetuning phase, which refines the model's capabilities to perform a more specific set of tasks. This process leverages the pre-trained weights, which encode general knowledge about language, morphology, and syntax, and adjusts them using labeled datasets in a supervised learning framework [16]. This step, commonly referred to as Supervised Fine-Tuning (SFT), aligns the model's performance with the requirements of particular applications, significantly enhancing its ability to tackle domain-specific tasks [52]. \n\nWhile fine-tuning significantly enhances a model's domainspecific performance, it often requires access to a large amount of labeled data, which presents a substantial barrier, especially when dealing with specialized domains.",
                    "score": 0.3794178375541567,
                    "section_title": "LARGE LANGUAGE MODELS (LLMS)",
                    "char_start_offset": 8638,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 646
                        },
                        {
                            "start": 649,
                            "end": 873
                        },
                        {
                            "start": 874,
                            "end": 999
                        },
                        {
                            "start": 1000,
                            "end": 1194
                        },
                        {
                            "start": 1197,
                            "end": 1306
                        },
                        {
                            "start": 1307,
                            "end": 1522
                        },
                        {
                            "start": 1523,
                            "end": 1665
                        },
                        {
                            "start": 1666,
                            "end": 1865
                        },
                        {
                            "start": 1866,
                            "end": 2085
                        },
                        {
                            "start": 2088,
                            "end": 2314
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 138,
                            "end": 142,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 642,
                            "end": 645,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 870,
                            "end": 872,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1186,
                            "end": 1190,
                            "matchedPaperCorpusId": "249674500"
                        },
                        {
                            "start": 1518,
                            "end": 1521,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1860,
                            "end": 1864,
                            "matchedPaperCorpusId": "263830318"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84716796875
                }
            ],
            "relevance_judgement": 0.84716796875,
            "relevance_judgment_input_expanded": "# Title: Integrating Personality into Digital Humans: A Review of LLM-Driven Approaches for Virtual Reality\n# Venue: arXiv.org\n# Authors: I. A. Brito, J. S. Dollis, Fernanda Bufon F\u00e4rber, Pedro Schindler Freire Brasil Ribeiro, Rafael Teixeira Sousa, Arlindo Rodrigues Galv\u00e3o Filho\n## Abstract\nThe integration of large language models (LLMs) into virtual reality (VR) environments has opened new pathways for creating more immersive and interactive digital humans. By leveraging the generative capabilities of LLMs alongside multimodal outputs such as facial expressions and gestures, virtual agents can simulate human-like personalities and emotions, fostering richer and more engaging user experiences. This paper provides a comprehensive review of methods for enabling digital humans to adopt nuanced personality traits, exploring approaches such as zero-shot, few-shot, and fine-tuning. Additionally, it highlights the challenges of integrating LLM-driven personality traits into VR, including computational demands, latency issues, and the lack of standardized evaluation frameworks for multimodal interactions. By addressing these gaps, this work lays a foundation for advancing applications in education, therapy, and gaming, while fostering interdisciplinary collaboration to redefine human-computer interaction in VR.\n## LARGE LANGUAGE MODELS (LLMS)\nThe field of natural language processing (NLP) has undergone rapid transformations since the introduction of the Transformer architecture [81]. This breakthrough has fundamentally reshaped NLP, leading to the proliferation of numerous language models, particularly decoder-based architectures, which have revolutionized text generation and paved the way for large language models (LLMs). These advanced neural networks redefine the boundaries of machine understanding and language generation, leveraging extensive datasets comprising billions or even trillions of words from diverse domains to maximize linguistic coverage and generalization [4]. \n\nThe unprecedented scale of LLMs, characterized by vast numbers of parameters, enables them to capture intricate linguistic patterns, including semantic relationships, syntactic structures, and long-range dependencies [77,4]. This capacity allows them to perform complex language-related tasks, such as advanced reasoning and contextual understanding. In particular, LLMs demonstrate emergent abilities, performing tasks that are not explicitly programmed but naturally arise from their scale and the complexity of their training process [85,37]. \n\nThe development of large language models generally involves two primary stages: pre-training and fine-tuning. The pre-training phase involves exposing the model to extensive volumes of unlabeled general knowledge text data through a language modeling task, most commonly framed as the next token prediction in a sequence [4]. Then, the resulting model passes through the finetuning phase, which refines the model's capabilities to perform a more specific set of tasks. This process leverages the pre-trained weights, which encode general knowledge about language, morphology, and syntax, and adjusts them using labeled datasets in a supervised learning framework [16]. This step, commonly referred to as Supervised Fine-Tuning (SFT), aligns the model's performance with the requirements of particular applications, significantly enhancing its ability to tackle domain-specific tasks [52]. \n\nWhile fine-tuning significantly enhances a model's domainspecific performance, it often requires access to a large amount of labeled data, which presents a substantial barrier, especially when dealing with specialized domains.",
            "reference_string": "[277244499 | Brito et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Thought-Like-Pro: Enhancing Reasoning of Large Language Models through Self-Driven Prolog-based Chain-of-Though",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 66,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.14562, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2310866202",
                    "name": "Xiaoyu Tan"
                },
                {
                    "authorId": "2268085905",
                    "name": "Yongxin Deng"
                },
                {
                    "authorId": "1500386397",
                    "name": "Xihe Qiu"
                },
                {
                    "authorId": "2248606044",
                    "name": "Weidi Xu"
                },
                {
                    "authorId": "2268107128",
                    "name": "Chao Qu"
                },
                {
                    "authorId": "2266389707",
                    "name": "Wei Chu"
                },
                {
                    "authorId": "2266466742",
                    "name": "Yinghui Xu"
                },
                {
                    "authorId": "2192603365",
                    "name": "Yuan Qi"
                }
            ],
            "abstract": "Large language models (LLMs) have shown exceptional performance as general-purpose assistants, excelling across a variety of reasoning tasks. This achievement represents a significant step toward achieving artificial general intelligence (AGI). Despite these advancements, the effectiveness of LLMs often hinges on the specific prompting strategies employed, and there remains a lack of a robust framework to facilitate learning and generalization across diverse reasoning tasks. To address these challenges, we introduce a novel learning framework, THOUGHT-LIKE-PRO In this framework, we utilize imitation learning to imitate the Chain-of-Thought (CoT) process which is verified and translated from reasoning trajectories generated by a symbolic Prolog logic engine. This framework proceeds in a self-driven manner, that enables LLMs to formulate rules and statements from given instructions and leverage the symbolic Prolog engine to derive results. Subsequently, LLMs convert Prolog-derived successive reasoning trajectories into natural language CoT for imitation learning. Our empirical findings indicate that our proposed approach substantially enhances the reasoning abilities of LLMs and demonstrates robust generalization across out-of-distribution reasoning tasks.",
            "corpus_id": 271329091,
            "sentences": [
                {
                    "corpus_id": "271329091",
                    "title": "Thought-Like-Pro: Enhancing Reasoning of Large Language Models through Self-Driven Prolog-based Chain-of-Though",
                    "text": "Recent advancements in large language models (LLMs) have positioned them as versatile tools for a variety of tasks, achieved through a structured training process that includes pre-training on large text corpora, supervised fine-tuning, and reinforcement learning from human feedback (RLHF) (Casper et al., 2023). These phases equip LLMs with extensive knowledge, the ability to generate context-specific responses, and alignment with human values. One of the key factors contributing to the perception of LLMs as a precursor to artificial general intelligence (AGI) is their scalable and explainable reasoning capabilities (Zhao et al., 2023), which have demonstrated potential to solve multiple types of real-world complex logic reasoning problems, including natural language logical reasoning and arithmetic reasoning. \n\nGeneral logic reasoning tasks continue to present substantial challenges for LLMs. These challenges primarily emerge during the LLMs' reasoning processes (Ribeiro et al., 2023;Zhou et al., 2024). At the semantic level, the generation of accurate multihop inferential pathways and the assurance of validity at each reasoning step remain significant obstacles (Creswell et al., 2023;Kazemi et al., 2023). LLMs function in a self-regressive manner at the token level, processing information sequentially from left to right, one token at a time. This method can amplify errors: an inaccurate token prediction may negatively impact all subsequent tokens, leading to escalating inaccuracies and false reasoning outcomes. Interestingly, some studies (Ye and Durrett, 2022;Saparov and He, 2023;Ribeiro et al., 2023) have observed instances where LLMs deliver correct answers despite erroneous internal reasoning processes. This phenomenon highlights a misalignment between the reasoning processes and outcomes of LLMs, suggesting a weak causal relationship and underdeveloped implicit reasoning connections within the LLMs. \n\nTo address the challenges associated with generating reasoning trajectories in LLMs, several strategies have been developed to enhance cognitive alignment and logical coherence.",
                    "score": 0.39273711660877003,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 821
                        },
                        {
                            "start": 824,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 1019
                        },
                        {
                            "start": 1020,
                            "end": 1226
                        },
                        {
                            "start": 1227,
                            "end": 1365
                        },
                        {
                            "start": 1366,
                            "end": 1538
                        },
                        {
                            "start": 1539,
                            "end": 1738
                        },
                        {
                            "start": 1739,
                            "end": 1939
                        },
                        {
                            "start": 1942,
                            "end": 2119
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1182,
                            "end": 1205,
                            "matchedPaperCorpusId": "248887351"
                        },
                        {
                            "start": 1567,
                            "end": 1589,
                            "matchedPaperCorpusId": "252873674"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84521484375
                }
            ],
            "relevance_judgement": 0.84521484375,
            "relevance_judgment_input_expanded": "# Title: Thought-Like-Pro: Enhancing Reasoning of Large Language Models through Self-Driven Prolog-based Chain-of-Though\n# Venue: arXiv.org\n# Authors: Xiaoyu Tan, Yongxin Deng, Xihe Qiu, Weidi Xu, Chao Qu, Wei Chu, Yinghui Xu, Yuan Qi\n## Abstract\nLarge language models (LLMs) have shown exceptional performance as general-purpose assistants, excelling across a variety of reasoning tasks. This achievement represents a significant step toward achieving artificial general intelligence (AGI). Despite these advancements, the effectiveness of LLMs often hinges on the specific prompting strategies employed, and there remains a lack of a robust framework to facilitate learning and generalization across diverse reasoning tasks. To address these challenges, we introduce a novel learning framework, THOUGHT-LIKE-PRO In this framework, we utilize imitation learning to imitate the Chain-of-Thought (CoT) process which is verified and translated from reasoning trajectories generated by a symbolic Prolog logic engine. This framework proceeds in a self-driven manner, that enables LLMs to formulate rules and statements from given instructions and leverage the symbolic Prolog engine to derive results. Subsequently, LLMs convert Prolog-derived successive reasoning trajectories into natural language CoT for imitation learning. Our empirical findings indicate that our proposed approach substantially enhances the reasoning abilities of LLMs and demonstrates robust generalization across out-of-distribution reasoning tasks.\n## Introduction\nRecent advancements in large language models (LLMs) have positioned them as versatile tools for a variety of tasks, achieved through a structured training process that includes pre-training on large text corpora, supervised fine-tuning, and reinforcement learning from human feedback (RLHF) (Casper et al., 2023). These phases equip LLMs with extensive knowledge, the ability to generate context-specific responses, and alignment with human values. One of the key factors contributing to the perception of LLMs as a precursor to artificial general intelligence (AGI) is their scalable and explainable reasoning capabilities (Zhao et al., 2023), which have demonstrated potential to solve multiple types of real-world complex logic reasoning problems, including natural language logical reasoning and arithmetic reasoning. \n\nGeneral logic reasoning tasks continue to present substantial challenges for LLMs. These challenges primarily emerge during the LLMs' reasoning processes (Ribeiro et al., 2023;Zhou et al., 2024). At the semantic level, the generation of accurate multihop inferential pathways and the assurance of validity at each reasoning step remain significant obstacles (Creswell et al., 2023;Kazemi et al., 2023). LLMs function in a self-regressive manner at the token level, processing information sequentially from left to right, one token at a time. This method can amplify errors: an inaccurate token prediction may negatively impact all subsequent tokens, leading to escalating inaccuracies and false reasoning outcomes. Interestingly, some studies (Ye and Durrett, 2022;Saparov and He, 2023;Ribeiro et al., 2023) have observed instances where LLMs deliver correct answers despite erroneous internal reasoning processes. This phenomenon highlights a misalignment between the reasoning processes and outcomes of LLMs, suggesting a weak causal relationship and underdeveloped implicit reasoning connections within the LLMs. \n\nTo address the challenges associated with generating reasoning trajectories in LLMs, several strategies have been developed to enhance cognitive alignment and logical coherence.",
            "reference_string": "[271329091 | Tan et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models Using Minimal Pairs",
            "venue": "International Conference on Language Resources and Evaluation",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.17299, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257338995",
                    "name": "Linyang He"
                },
                {
                    "authorId": "2257019825",
                    "name": "Peili Chen"
                },
                {
                    "authorId": "2197254657",
                    "name": "Ercong Nie"
                },
                {
                    "authorId": "2257259408",
                    "name": "Yuanning Li"
                },
                {
                    "authorId": "2293392156",
                    "name": "Jonathan Brennan"
                }
            ],
            "abstract": "Inspired by cognitive neuroscience studies, we introduce a novel \u201cdecoding probing\u201d method that uses minimal pairs benchmark (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer. By treating the language model as the brain and its representations as \u201cneural activations\u201d, we decode grammaticality labels of minimal pairs from the intermediate layers\u2019 representations. This approach reveals: 1) Self-supervised language models capture abstract linguistic structures in intermediate layers that GloVe and RNN language models cannot learn. 2) Information about syntactic grammaticality is robustly captured through the first third layers of GPT-2 and also distributed in later layers. As sentence complexity increases, more layers are required for learning grammatical capabilities. 3) Morphological and semantics/syntax interface-related features are harder to capture than syntax. 4) For Transformer-based models, both embeddings and attentions capture grammatical features but show distinct patterns. Different attention heads exhibit similar tendencies toward various linguistic phenomena, but with varied contributions.",
            "corpus_id": 268691480,
            "sentences": [
                {
                    "corpus_id": "268691480",
                    "title": "Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models Using Minimal Pairs",
                    "text": "Inspired by cognitive neuroscience studies, we introduce a novel \u201cdecoding probing\u201d method that uses minimal pairs benchmark (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer. By treating the language model as the brain and its representations as \u201cneural activations\u201d, we decode grammaticality labels of minimal pairs from the intermediate layers\u2019 representations. This approach reveals: 1) Self-supervised language models capture abstract linguistic structures in intermediate layers that GloVe and RNN language models cannot learn. 2) Information about syntactic grammaticality is robustly captured through the first third layers of GPT-2 and also distributed in later layers. As sentence complexity increases, more layers are required for learning grammatical capabilities. 3) Morphological and semantics/syntax interface-related features are harder to capture than syntax. 4) For Transformer-based models, both embeddings and attentions capture grammatical features but show distinct patterns. Different attention heads exhibit similar tendencies toward various linguistic phenomena, but with varied contributions.",
                    "score": 0.4194654548337735,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84423828125
                }
            ],
            "relevance_judgement": 0.84423828125,
            "relevance_judgment_input_expanded": "# Title: Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models Using Minimal Pairs\n# Venue: International Conference on Language Resources and Evaluation\n# Authors: Linyang He, Peili Chen, Ercong Nie, Yuanning Li, Jonathan Brennan\n## Abstract\nInspired by cognitive neuroscience studies, we introduce a novel \u201cdecoding probing\u201d method that uses minimal pairs benchmark (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer. By treating the language model as the brain and its representations as \u201cneural activations\u201d, we decode grammaticality labels of minimal pairs from the intermediate layers\u2019 representations. This approach reveals: 1) Self-supervised language models capture abstract linguistic structures in intermediate layers that GloVe and RNN language models cannot learn. 2) Information about syntactic grammaticality is robustly captured through the first third layers of GPT-2 and also distributed in later layers. As sentence complexity increases, more layers are required for learning grammatical capabilities. 3) Morphological and semantics/syntax interface-related features are harder to capture than syntax. 4) For Transformer-based models, both embeddings and attentions capture grammatical features but show distinct patterns. Different attention heads exhibit similar tendencies toward various linguistic phenomena, but with varied contributions.\n",
            "reference_string": "[268691480 | He et al. | 2024 | Citations: 6]"
        },
        {
            "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14972, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2281641042",
                    "name": "Florin Cuconasu"
                },
                {
                    "authorId": "120709579",
                    "name": "Giovanni Trappolini"
                },
                {
                    "authorId": "2281641625",
                    "name": "Nicola Tonellotto"
                },
                {
                    "authorId": "2260653973",
                    "name": "Fabrizio Silvestri"
                }
            ],
            "abstract": "Retrieval Augmented Generation (RAG) represents a significant advancement in artificial intelligence combining a retrieval phase with a generative phase, with the latter typically being powered by large language models (LLMs). The current common practices in RAG involve using\"instructed\"LLMs, which are fine-tuned with supervised training to enhance their ability to follow instructions and are aligned with human preferences using state-of-the-art techniques. Contrary to popular belief, our study demonstrates that base models outperform their instructed counterparts in RAG tasks by 20% on average under our experimental settings. This finding challenges the prevailing assumptions about the superiority of instructed LLMs in RAG applications. Further investigations reveal a more nuanced situation, questioning fundamental aspects of RAG and suggesting the need for broader discussions on the topic; or, as Fromm would have it,\"Seldom is a glance at the statistics enough to understand the meaning of the figures\".",
            "corpus_id": 270688478,
            "sentences": [
                {
                    "corpus_id": "270688478",
                    "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
                    "text": "Here, we illustrate the training processes of LLMs, which consist of at least three main steps: pretraining, which is done on the task of next token prediction, instruction fine-tuning, and preference alignment.\n\nPre-training.Pre-training for large language models (Radford et al., 2018) involves an extensive unsupervised learning phase, where the model is exposed to a large corpus of text data to learn the underlying statistical properties of natural language.This process employs the next token prediction task, where the model is conditioned on a sequence of tokens w 1:i\u22121 and trained to predict the subsequent token w i , i.e., p(y) = n i p \u03b8 (w i |w 1:i\u22121 ).By iteratively processing vast and diverse textual datasets, the model learns linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies.This process delivers what is commonly called a \"base\" model.This first model is usually improved with further specialized training, as we will see now.\n\nInstruction Fine-Tuning.Instruction fine-tuning is the part of the training process of large language models aimed at enhancing their ability to follow specific directives and perform specialized tasks (Taori et al., 2023).This phase involves further training of the base model on curated datasets that include explicit instructions paired with corresponding responses.The objective is to teach the model how to interpret and execute various types of commands or queries effectively.During instruction fine-tuning, the model is exposed to a wide range of examples that illustrate how to respond to different types of prompts, from answering questions to generating summaries and more complex task-oriented interactions.The primary purpose of instruction fine-tuning is to improve the model's usability and performance in real-world applications.By aligning the model's behavior with the specific needs and expectations of users, instruction finetuning ensures that the model can handle diverse and nuanced tasks more effectively, enhancing its overall functionality and practical utility.This pro-cess might involve using an instruction template, where specific patterns and structures for providing instructions and expected responses are standardized to facilitate consistent and effective learning.It is worth noticing that this is still achieved with traditional supervised learning, unlike the next phase we are considering.",
                    "score": 0.3936219084400523,
                    "section_title": "LLM Training",
                    "char_start_offset": 6387,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 213,
                            "end": 226
                        },
                        {
                            "start": 226,
                            "end": 464
                        },
                        {
                            "start": 464,
                            "end": 667
                        },
                        {
                            "start": 667,
                            "end": 850
                        },
                        {
                            "start": 850,
                            "end": 911
                        },
                        {
                            "start": 911,
                            "end": 1002
                        },
                        {
                            "start": 1004,
                            "end": 1028
                        },
                        {
                            "start": 1028,
                            "end": 1227
                        },
                        {
                            "start": 1227,
                            "end": 1373
                        },
                        {
                            "start": 1373,
                            "end": 1487
                        },
                        {
                            "start": 1487,
                            "end": 1723
                        },
                        {
                            "start": 1723,
                            "end": 1849
                        },
                        {
                            "start": 1849,
                            "end": 2092
                        },
                        {
                            "start": 2092,
                            "end": 2305
                        },
                        {
                            "start": 2305,
                            "end": 2433
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8427734375
                }
            ],
            "relevance_judgement": 0.8427734375,
            "relevance_judgment_input_expanded": "# Title: A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems\n# Venue: arXiv.org\n# Authors: Florin Cuconasu, Giovanni Trappolini, Nicola Tonellotto, Fabrizio Silvestri\n## Abstract\nRetrieval Augmented Generation (RAG) represents a significant advancement in artificial intelligence combining a retrieval phase with a generative phase, with the latter typically being powered by large language models (LLMs). The current common practices in RAG involve using\"instructed\"LLMs, which are fine-tuned with supervised training to enhance their ability to follow instructions and are aligned with human preferences using state-of-the-art techniques. Contrary to popular belief, our study demonstrates that base models outperform their instructed counterparts in RAG tasks by 20% on average under our experimental settings. This finding challenges the prevailing assumptions about the superiority of instructed LLMs in RAG applications. Further investigations reveal a more nuanced situation, questioning fundamental aspects of RAG and suggesting the need for broader discussions on the topic; or, as Fromm would have it,\"Seldom is a glance at the statistics enough to understand the meaning of the figures\".\n## LLM Training\nHere, we illustrate the training processes of LLMs, which consist of at least three main steps: pretraining, which is done on the task of next token prediction, instruction fine-tuning, and preference alignment.\n\nPre-training.Pre-training for large language models (Radford et al., 2018) involves an extensive unsupervised learning phase, where the model is exposed to a large corpus of text data to learn the underlying statistical properties of natural language.This process employs the next token prediction task, where the model is conditioned on a sequence of tokens w 1:i\u22121 and trained to predict the subsequent token w i , i.e., p(y) = n i p \u03b8 (w i |w 1:i\u22121 ).By iteratively processing vast and diverse textual datasets, the model learns linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies.This process delivers what is commonly called a \"base\" model.This first model is usually improved with further specialized training, as we will see now.\n\nInstruction Fine-Tuning.Instruction fine-tuning is the part of the training process of large language models aimed at enhancing their ability to follow specific directives and perform specialized tasks (Taori et al., 2023).This phase involves further training of the base model on curated datasets that include explicit instructions paired with corresponding responses.The objective is to teach the model how to interpret and execute various types of commands or queries effectively.During instruction fine-tuning, the model is exposed to a wide range of examples that illustrate how to respond to different types of prompts, from answering questions to generating summaries and more complex task-oriented interactions.The primary purpose of instruction fine-tuning is to improve the model's usability and performance in real-world applications.By aligning the model's behavior with the specific needs and expectations of users, instruction finetuning ensures that the model can handle diverse and nuanced tasks more effectively, enhancing its overall functionality and practical utility.This pro-cess might involve using an instruction template, where specific patterns and structures for providing instructions and expected responses are standardized to facilitate consistent and effective learning.It is worth noticing that this is still achieved with traditional supervised learning, unlike the next phase we are considering.",
            "reference_string": "[270688478 | Cuconasu et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Training Dynamics of Contextual N-Grams in Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 35,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.00863, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2243240285",
                    "name": "Lucia Quirke"
                },
                {
                    "authorId": "2047549746",
                    "name": "Lovis Heindrich"
                },
                {
                    "authorId": "2056771333",
                    "name": "Wes Gurnee"
                },
                {
                    "authorId": "2051128902",
                    "name": "Neel Nanda"
                }
            ],
            "abstract": "Prior work has shown the existence of contextual neurons in language models, including a neuron that activates on German text. We show that this neuron exists within a broader contextual n-gram circuit: we find late layer neurons which recognize and continue n-grams common in German text, but which only activate if the German neuron is active. We investigate the formation of this circuit throughout training and find that it is an example of what we call a second-order circuit. In particular, both the constituent n-gram circuits and the German detection circuit which culminates in the German neuron form with independent functions early in training - the German detection circuit partially through modeling German unigram statistics, and the n-grams by boosting appropriate completions. Only after both circuits have already formed do they fit together into a second-order circuit. Contrary to the hypotheses presented in prior work, we find that the contextual n-gram circuit forms gradually rather than in a sudden phase transition. We further present a range of anomalous observations such as a simultaneous phase transition in many tasks coinciding with the learning rate warm-up, and evidence that many context neurons form simultaneously early in training but are later unlearned.",
            "corpus_id": 264935245,
            "sentences": [
                {
                    "corpus_id": "264935245",
                    "title": "Training Dynamics of Contextual N-Grams in Language Models",
                    "text": "Training dynamics Several works (Olsson et al., 2022;Nanda et al., 2023) discovered circuits in language models that seem causally relevant to emergent capabilities and can be ablated to reduce the capabilities. Michaud et al. (2023) postulated that many capabilities in language models emerge rapidly in phase transitions and proposed a model in which any capabilities which appear to instead form gradually can be decomposed into discrete, additive capabilities which emerge in independent phase transitions. Finally, several works (Varma et al., 2023;Nanda et al., 2023) observed the phenomenon of grokking, where a model that has already achieved near-perfect performance on the training data transitions to a mechanism that generalizes. \n\nWhile only a minority of circuits-style analysis has focused on model development, a wealth of results in the field of developmental interpretability have characterized the training dynamics of language models using other units of analysis. Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020;Wei et al., 2022). Raghu et al. (2017) proposed singular vector canonical correlation analysis, a tool which allows different model layers and networks to be compared, and showed that early network layers converge earlier in training than late layers. Saphra and Lopez (2020b,a) found that LSTM models learn hierarchical sequential features in natural language bottom up -shorter sequences are learned first and form the basis for the representation of longer sequences that contain the shorter ones. Finally, Saxe et al. (2019) found that the features are learned in an order determined by the magnitude of the corresponding singular value in the model. Dodge et al. (2020) found that differences in random parameter initializations and training data order contribute comparably to differences between learned representations of fine-tuning runs of the same model on identical data, and McCoy et al. (2020) found that identically trained BERT transformer models are consistent with this finding, varying widely in generalization performance.",
                    "score": 0.47622107315322565,
                    "section_title": "Related Work",
                    "char_start_offset": 17886,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 510
                        },
                        {
                            "start": 511,
                            "end": 741
                        },
                        {
                            "start": 744,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1255
                        },
                        {
                            "start": 1256,
                            "end": 1488
                        },
                        {
                            "start": 1489,
                            "end": 1737
                        },
                        {
                            "start": 1738,
                            "end": 1891
                        },
                        {
                            "start": 1892,
                            "end": 2279
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1216,
                            "end": 1237,
                            "matchedPaperCorpusId": "222140842"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83984375
                }
            ],
            "relevance_judgement": 0.83984375,
            "relevance_judgment_input_expanded": "# Title: Training Dynamics of Contextual N-Grams in Language Models\n# Venue: arXiv.org\n# Authors: Lucia Quirke, Lovis Heindrich, Wes Gurnee, Neel Nanda\n## Abstract\nPrior work has shown the existence of contextual neurons in language models, including a neuron that activates on German text. We show that this neuron exists within a broader contextual n-gram circuit: we find late layer neurons which recognize and continue n-grams common in German text, but which only activate if the German neuron is active. We investigate the formation of this circuit throughout training and find that it is an example of what we call a second-order circuit. In particular, both the constituent n-gram circuits and the German detection circuit which culminates in the German neuron form with independent functions early in training - the German detection circuit partially through modeling German unigram statistics, and the n-grams by boosting appropriate completions. Only after both circuits have already formed do they fit together into a second-order circuit. Contrary to the hypotheses presented in prior work, we find that the contextual n-gram circuit forms gradually rather than in a sudden phase transition. We further present a range of anomalous observations such as a simultaneous phase transition in many tasks coinciding with the learning rate warm-up, and evidence that many context neurons form simultaneously early in training but are later unlearned.\n## Related Work\nTraining dynamics Several works (Olsson et al., 2022;Nanda et al., 2023) discovered circuits in language models that seem causally relevant to emergent capabilities and can be ablated to reduce the capabilities. Michaud et al. (2023) postulated that many capabilities in language models emerge rapidly in phase transitions and proposed a model in which any capabilities which appear to instead form gradually can be decomposed into discrete, additive capabilities which emerge in independent phase transitions. Finally, several works (Varma et al., 2023;Nanda et al., 2023) observed the phenomenon of grokking, where a model that has already achieved near-perfect performance on the training data transitions to a mechanism that generalizes. \n\nWhile only a minority of circuits-style analysis has focused on model development, a wealth of results in the field of developmental interpretability have characterized the training dynamics of language models using other units of analysis. Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020;Wei et al., 2022). Raghu et al. (2017) proposed singular vector canonical correlation analysis, a tool which allows different model layers and networks to be compared, and showed that early network layers converge earlier in training than late layers. Saphra and Lopez (2020b,a) found that LSTM models learn hierarchical sequential features in natural language bottom up -shorter sequences are learned first and form the basis for the representation of longer sequences that contain the shorter ones. Finally, Saxe et al. (2019) found that the features are learned in an order determined by the magnitude of the corresponding singular value in the model. Dodge et al. (2020) found that differences in random parameter initializations and training data order contribute comparably to differences between learned representations of fine-tuning runs of the same model on identical data, and McCoy et al. (2020) found that identically trained BERT transformer models are consistent with this finding, varying widely in generalization performance.",
            "reference_string": "[264935245 | Quirke et al. | 2023 | Citations: 5]"
        },
        {
            "title": "SAIE Framework: Support Alone Isn't Enough - Advancing LLM Training with Adversarial Remarks",
            "venue": "European Conference on Artificial Intelligence",
            "year": 2023,
            "reference_count": 31,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.08107, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2150167420",
                    "name": "Mengsay Loem"
                },
                {
                    "authorId": "143655216",
                    "name": "Masahiro Kaneko"
                },
                {
                    "authorId": "1764004",
                    "name": "Naoaki Okazaki"
                }
            ],
            "abstract": "Large Language Models (LLMs) can justify or critique their predictions through discussions with other models or humans, thereby enriching their intrinsic understanding of instances. While proactive discussions in the inference phase have been shown to boost performance, such interactions have not been extensively explored during the training phase. We hypothesize that incorporating interactive discussions into the training process can enhance the models' understanding and improve their reasoning and verbal expression abilities during inference. This work introduces the SAIE framework, which facilitates supportive and adversarial discussions between learner and partner models. The learner model receives responses from the partner, and its parameters are then updated based on this discussion. This dynamic adjustment process continues throughout the training phase, responding to the evolving outputs of the learner model. Our empirical evaluation across various tasks, including math problems, commonsense reasoning, and multi-domain knowledge, demonstrates that models fine-tuned with the SAIE framework outperform those trained with conventional fine-tuning approaches. Furthermore, our method enhances the models' reasoning capabilities, improving both individual and multi-agent inference performance.",
            "corpus_id": 265157830,
            "sentences": [
                {
                    "corpus_id": "265157830",
                    "title": "SAIE Framework: Support Alone Isn't Enough - Advancing LLM Training with Adversarial Remarks",
                    "text": "Large Language Models (LLMs) can justify or critique their predictions through discussions with other models or humans, thereby enriching their intrinsic understanding of instances. While proactive discussions in the inference phase have been shown to boost performance, such interactions have not been extensively explored during the training phase. We hypothesize that incorporating interactive discussions into the training process can enhance the models' understanding and improve their reasoning and verbal expression abilities during inference. This work introduces the SAIE framework, which facilitates supportive and adversarial discussions between learner and partner models. The learner model receives responses from the partner, and its parameters are then updated based on this discussion. This dynamic adjustment process continues throughout the training phase, responding to the evolving outputs of the learner model. Our empirical evaluation across various tasks, including math problems, commonsense reasoning, and multi-domain knowledge, demonstrates that models fine-tuned with the SAIE framework outperform those trained with conventional fine-tuning approaches. Furthermore, our method enhances the models' reasoning capabilities, improving both individual and multi-agent inference performance.",
                    "score": 0.401472929743626,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8359375
                }
            ],
            "relevance_judgement": 0.8359375,
            "relevance_judgment_input_expanded": "# Title: SAIE Framework: Support Alone Isn't Enough - Advancing LLM Training with Adversarial Remarks\n# Venue: European Conference on Artificial Intelligence\n# Authors: Mengsay Loem, Masahiro Kaneko, Naoaki Okazaki\n## Abstract\nLarge Language Models (LLMs) can justify or critique their predictions through discussions with other models or humans, thereby enriching their intrinsic understanding of instances. While proactive discussions in the inference phase have been shown to boost performance, such interactions have not been extensively explored during the training phase. We hypothesize that incorporating interactive discussions into the training process can enhance the models' understanding and improve their reasoning and verbal expression abilities during inference. This work introduces the SAIE framework, which facilitates supportive and adversarial discussions between learner and partner models. The learner model receives responses from the partner, and its parameters are then updated based on this discussion. This dynamic adjustment process continues throughout the training phase, responding to the evolving outputs of the learner model. Our empirical evaluation across various tasks, including math problems, commonsense reasoning, and multi-domain knowledge, demonstrates that models fine-tuned with the SAIE framework outperform those trained with conventional fine-tuning approaches. Furthermore, our method enhances the models' reasoning capabilities, improving both individual and multi-agent inference performance.\n",
            "reference_string": "[265157830 | Loem et al. | 2023 | Citations: 5]"
        },
        {
            "title": "Benchmark Data Contamination of Large Language Models: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 164,
            "citation_count": 56,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153079650",
                    "name": "Cheng Xu"
                },
                {
                    "authorId": "2304954016",
                    "name": "Shuhao Guan"
                },
                {
                    "authorId": "2304952028",
                    "name": "Derek Greene"
                },
                {
                    "authorId": "2266906186",
                    "name": "Mohand-Tahar Kechadi"
                }
            ],
            "abstract": "The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing. However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC). This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process. This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks. The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications.",
            "corpus_id": 270285708,
            "sentences": [
                {
                    "corpus_id": "270285708",
                    "title": "Benchmark Data Contamination of Large Language Models: A Survey",
                    "text": "Emergent abilities manifest primarily in three ways: in-context learning [33], instruction following [127], and step-by-step reasoning [113].In-context learning abilities were first observed in GPT-3 [15] The instruction following ability enables the model to comprehend and execute tasks based on directives provided directly within the input prompt.For example, when provided with a prompt that includes specific instructions, such as \"summarize the following text\" or \"translate the following sentences into French, \" the model can understand and act upon these instructions, leveraging its pre-existing knowledge base and generalization capabilities [153].\n\nStep-by-step reasoning refers to the ability of models to break down complex problems or queries into smaller steps, processing each one sequentially to arrive at a final answer.This is often accomplished through chain-of-thought (CoT) prompting strategy [155], in this approach, the model sequentially processes each step, building upon prior steps to construct a comprehensive answer, and research has shown that chain-of-thought prompts can bring performance gains [155].\n\nThe abilities mentioned above enable LLMs to exhibit strong performance but also face several issues.A frequently-reported problem is the generation of so-called hallucinations [8], where LLMs generate text that superficially appears to be correct but is actually inaccurate.This problem is difficult to resolve completely, although it can be mitigated through alignment tuning strategies [109].While LLMs have learned general language patterns, they underperform in specialized domains, such as medicine or engineering.This may be related to catastrophic forgetting [76] or a scarcity of relevant training data.Furthermore, enabling LLMs to quickly learn the latest knowledge by updating weights remains an unresolved challenge [163].",
                    "score": 0.3791682091067085,
                    "section_title": "Large Language Models",
                    "char_start_offset": 9047,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 141,
                            "end": 351
                        },
                        {
                            "start": 351,
                            "end": 660
                        },
                        {
                            "start": 662,
                            "end": 840
                        },
                        {
                            "start": 840,
                            "end": 1136
                        },
                        {
                            "start": 1138,
                            "end": 1239
                        },
                        {
                            "start": 1239,
                            "end": 1413
                        },
                        {
                            "start": 1413,
                            "end": 1533
                        },
                        {
                            "start": 1533,
                            "end": 1658
                        },
                        {
                            "start": 1658,
                            "end": 1750
                        },
                        {
                            "start": 1750,
                            "end": 1873
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 101,
                            "end": 106,
                            "matchedPaperCorpusId": "239009562"
                        },
                        {
                            "start": 135,
                            "end": 140,
                            "matchedPaperCorpusId": "258048648"
                        },
                        {
                            "start": 200,
                            "end": 204,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 654,
                            "end": 659,
                            "matchedPaperCorpusId": "237416585"
                        },
                        {
                            "start": 917,
                            "end": 922,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 1130,
                            "end": 1135,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 1315,
                            "end": 1318,
                            "matchedPaperCorpusId": "256662612"
                        },
                        {
                            "start": 1527,
                            "end": 1532,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 1705,
                            "end": 1709,
                            "matchedPaperCorpusId": "22910766"
                        },
                        {
                            "start": 1867,
                            "end": 1872,
                            "matchedPaperCorpusId": "258833129"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83544921875
                }
            ],
            "relevance_judgement": 0.83544921875,
            "relevance_judgment_input_expanded": "# Title: Benchmark Data Contamination of Large Language Models: A Survey\n# Venue: arXiv.org\n# Authors: Cheng Xu, Shuhao Guan, Derek Greene, Mohand-Tahar Kechadi\n## Abstract\nThe rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing. However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC). This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process. This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks. The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications.\n## Large Language Models\nEmergent abilities manifest primarily in three ways: in-context learning [33], instruction following [127], and step-by-step reasoning [113].In-context learning abilities were first observed in GPT-3 [15] The instruction following ability enables the model to comprehend and execute tasks based on directives provided directly within the input prompt.For example, when provided with a prompt that includes specific instructions, such as \"summarize the following text\" or \"translate the following sentences into French, \" the model can understand and act upon these instructions, leveraging its pre-existing knowledge base and generalization capabilities [153].\n\nStep-by-step reasoning refers to the ability of models to break down complex problems or queries into smaller steps, processing each one sequentially to arrive at a final answer.This is often accomplished through chain-of-thought (CoT) prompting strategy [155], in this approach, the model sequentially processes each step, building upon prior steps to construct a comprehensive answer, and research has shown that chain-of-thought prompts can bring performance gains [155].\n\nThe abilities mentioned above enable LLMs to exhibit strong performance but also face several issues.A frequently-reported problem is the generation of so-called hallucinations [8], where LLMs generate text that superficially appears to be correct but is actually inaccurate.This problem is difficult to resolve completely, although it can be mitigated through alignment tuning strategies [109].While LLMs have learned general language patterns, they underperform in specialized domains, such as medicine or engineering.This may be related to catastrophic forgetting [76] or a scarcity of relevant training data.Furthermore, enabling LLMs to quickly learn the latest knowledge by updating weights remains an unresolved challenge [163].",
            "reference_string": "[270285708 | Xu et al. | 2024 | Citations: 56]"
        },
        {
            "title": "A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 64,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.08590, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316365571",
                    "name": "Geonhee Kim"
                },
                {
                    "authorId": "34102057",
                    "name": "Marco Valentino"
                },
                {
                    "authorId": "2242981659",
                    "name": "Andr\u00e9 Freitas"
                }
            ],
            "abstract": "Recent studies on logical reasoning in Language Models (LMs) have sparked a debate on whether they can learn systematic reasoning principles during pre-training or merely exploit superficial patterns in the training data. This paper presents a mechanistic interpretation of syllogistic reasoning in LMs to advance the understanding of internal dynamics. Specifically, we present a methodology for circuit discovery aimed at interpreting content-independent reasoning mechanisms. Through two distinct intervention methods, we uncover a sufficient and necessary circuit involving middle-term suppression that elucidates how LMs transfer information to derive valid conclusions from premises. Furthermore, we investigate how belief biases manifest in syllogistic reasoning, finding evidence of partial contamination from additional attention heads responsible for encoding commonsense and contextualized knowledge. Finally, we explore the generalization of the discovered mechanisms across various syllogistic schemes, model sizes and architectures, finding that the identified circuit is sufficient and necessary for the schemes on which the models achieve high downstream accuracy (>60%), and that the activation patterns apply to models of different families. Overall, our findings suggest that LMs indeed learn transferable content-independent reasoning mechanisms, but that, at the same time, such mechanisms do not involve generalizable and abstract logical primitives, being susceptible to contamination by the same world knowledge acquired during pre-training.",
            "corpus_id": 271892176,
            "sentences": [
                {
                    "corpus_id": "271892176",
                    "title": "A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive Language Models",
                    "text": "Recent studies on logical reasoning in Language Models (LMs) have sparked a debate on whether they can learn systematic reasoning principles during pre-training or merely exploit superficial patterns in the training data. This paper presents a mechanistic interpretation of syllogistic reasoning in LMs to advance the understanding of internal dynamics. Specifically, we present a methodology for circuit discovery aimed at interpreting content-independent reasoning mechanisms. Through two distinct intervention methods, we uncover a sufficient and necessary circuit involving middle-term suppression that elucidates how LMs transfer information to derive valid conclusions from premises. Furthermore, we investigate how belief biases manifest in syllogistic reasoning, finding evidence of partial contamination from additional attention heads responsible for encoding commonsense and contextualized knowledge. Finally, we explore the generalization of the discovered mechanisms across various syllogistic schemes, model sizes and architectures, finding that the identified circuit is sufficient and necessary for the schemes on which the models achieve high downstream accuracy (>60%), and that the activation patterns apply to models of different families. Overall, our findings suggest that LMs indeed learn transferable content-independent reasoning mechanisms, but that, at the same time, such mechanisms do not involve generalizable and abstract logical primitives, being susceptible to contamination by the same world knowledge acquired during pre-training.",
                    "score": 0.43779406003318855,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.830078125
                }
            ],
            "relevance_judgement": 0.830078125,
            "relevance_judgment_input_expanded": "# Title: A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive Language Models\n# Venue: arXiv.org\n# Authors: Geonhee Kim, Marco Valentino, Andr\u00e9 Freitas\n## Abstract\nRecent studies on logical reasoning in Language Models (LMs) have sparked a debate on whether they can learn systematic reasoning principles during pre-training or merely exploit superficial patterns in the training data. This paper presents a mechanistic interpretation of syllogistic reasoning in LMs to advance the understanding of internal dynamics. Specifically, we present a methodology for circuit discovery aimed at interpreting content-independent reasoning mechanisms. Through two distinct intervention methods, we uncover a sufficient and necessary circuit involving middle-term suppression that elucidates how LMs transfer information to derive valid conclusions from premises. Furthermore, we investigate how belief biases manifest in syllogistic reasoning, finding evidence of partial contamination from additional attention heads responsible for encoding commonsense and contextualized knowledge. Finally, we explore the generalization of the discovered mechanisms across various syllogistic schemes, model sizes and architectures, finding that the identified circuit is sufficient and necessary for the schemes on which the models achieve high downstream accuracy (>60%), and that the activation patterns apply to models of different families. Overall, our findings suggest that LMs indeed learn transferable content-independent reasoning mechanisms, but that, at the same time, such mechanisms do not involve generalizable and abstract logical primitives, being susceptible to contamination by the same world knowledge acquired during pre-training.\n",
            "reference_string": "[271892176 | Kim et al. | 2024 | Citations: 11]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "261822542",
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "text": "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens. \n\nTo better understand the role of interpretable artifacts in model development, we analyze and manipulate these artifacts during training. We focus on a case study of Syntactic Attention Structure (SAS), a model behavior thought to relate to grammatical structure. By measuring and controlling the emergence of SAS, we deepen our understanding of the relationship between the internal structural traits and extrinsic capabilities of masked language models (MLMs). \n\nSAS occurs when a model learns specialized attention heads that focus on a word's syntactic neighbors. This behavior emerges naturally during conventional MLM pre-training (Clark et al., 2019;Voita et al., 2019;Manning et al., 2020). We observe an abrupt spike in SAS at a consistent point in training, and explore its impact on MLM capabilities by manipulating SAS during training. Our observations paint a picture of how interpretability artifacts may represent simplicity biases that compete with other learning strategies during MLM training. In summary, our main contributions are: \n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes.",
            "score": 0.6030716557195096,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 1942
                },
                {
                    "start": 1945,
                    "end": 2186
                },
                {
                    "start": 2187,
                    "end": 2262
                }
            ],
            "ref_mentions": [
                {
                    "start": 588,
                    "end": 612,
                    "matchedPaperCorpusId": "253117181"
                },
                {
                    "start": 1567,
                    "end": 1588,
                    "matchedPaperCorpusId": "219315567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9501953125
        },
        {
            "corpus_id": "255941592",
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "text": "Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become --\"thinking machines\", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.",
            "score": 0.5537037883310325,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.853515625
        },
        {
            "corpus_id": "264451507",
            "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training",
            "text": "Representational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic probing suite, which enables direct comparisons of not just task performance, but their representational subspaces, we analyze nine tasks covering syntax, semantics and reasoning, across 2M pre-training steps and five seeds. We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization. Measuring cross-task similarity further reveals that linguistically related tasks share information throughout training, and do so more during the critical phase of learning than before or after. Our findings have implications for model interpretability, multi-task learning, and learning from limited data.",
            "score": 0.5488074885559356,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96728515625
        },
        {
            "corpus_id": "277043826",
            "title": "Are formal and functional linguistic mechanisms dissociated in language models?",
            "text": "Mahowald et al. furthermore claim that this distinction between formal and functional linguistic competence is reflected in the performance of today's large language models (LLMs). In particular, LLMs have strong formal linguistic competence as evidenced by their strong performance on syntax benchmarks and their general ability to output fluent and natural text (Hu et al. 2020;Gauthier et al. 2020;Warstadt et al. 2020). However, their functional linguistic competence is markedly worse: LLMs frequently output false reasoning, hallucinate untrue facts, and fail at complex social reasoning (Dziri et al. 2023;Xu, Jain, and Kankanhalli 2024;Strachan et al. 2024). \n\nMuch work has attempted to ameliorate these problems via retrieval-augmented generation or the use of chain-of-thought reasoning (Gao et al. 2024;Wei et al. 2022). However, Mahowald et al. offer another solution: perhaps LLMs would have stronger functional linguistic abilities if formal and functional linguistic abilities were as distinct in LLMs as they are in the human brain. Such a dissociation in LLMs could come about in two ways: either it could be explicitly built into them, or it could arise naturally via training, or the model's inductive biases. Today's LLMs have no explicit formalfunctional modularity, but whether any has arisen despite that fact is an open question. \n\nIn this paper, we seek to answer that question: to what extent are formal and functional linguistic competence dissociated in the internals of today's LLMs? If the two are not dissociated, new architectures or training procedures that bias models towards a formal-functional dissociation may be necessary to achieve this. If the two are already dissociated (despite the fact that LLMs struggle more with functional linguistic competence), this might indicate that dissociation does not suffice to improve LLMs' abilities.",
            "score": 0.53930153111339,
            "section_title": "Introduction",
            "char_start_offset": 2051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 666
                },
                {
                    "start": 669,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 380,
                    "matchedPaperCorpusId": "218571018"
                },
                {
                    "start": 380,
                    "end": 401,
                    "matchedPaperCorpusId": "220060899"
                },
                {
                    "start": 594,
                    "end": 613,
                    "matchedPaperCorpusId": "258967391"
                },
                {
                    "start": 644,
                    "end": 665,
                    "matchedPaperCorpusId": "269928651"
                },
                {
                    "start": 815,
                    "end": 830,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "275324224",
            "title": "A non-ergodic framework for understanding emergent capabilities in Large Language Models",
            "text": "Large language models have emergent capabilities that come unexpectedly at scale, but we need a theoretical framework to explain why and how they emerge. We prove that language models are actually non-ergodic systems while providing a mathematical framework based on Stuart Kauffman's theory of the adjacent possible (TAP) to explain capability emergence. Our resource-constrained TAP equation demonstrates how architectural, training, and contextual constraints interact to shape model capabilities through phase transitions in semantic space. We prove through experiments with three different language models that capacities emerge through discrete transitions guided by constraint interactions and path-dependent exploration. This framework provides a theoretical basis for understanding emergence in language models and guides the development of architectures that can guide capability emergence.",
            "score": 0.5219359393399943,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94921875
        },
        {
            "corpus_id": "277043826",
            "title": "Are formal and functional linguistic mechanisms dissociated in language models?",
            "text": "Although large language models (LLMs) are increasingly capable, these capabilities are unevenly distributed: they excel at formal linguistic tasks, such as producing fluent, grammatical text, but struggle more with functional linguistic tasks like reasoning and consistent fact retrieval. Inspired by neuroscience, recent work suggests that to succeed on both formal and functional linguistic tasks, LLMs should use different mechanisms for each; such localization could either be built-in or emerge spontaneously through training. In this paper, we ask: do current models, with fast-improving functional linguistic abilities, exhibit distinct localization of formal and functional linguistic mechanisms? We answer this by finding and comparing the\"circuits\", or minimal computational subgraphs, responsible for various formal and functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that while there is indeed little overlap between circuits for formal and functional tasks, there is also little overlap between formal linguistic tasks, as exists in the human brain. Thus, a single formal linguistic network, unified and distinct from functional task circuits, remains elusive. However, in terms of cross-task faithfulness - the ability of one circuit to solve another's task - we observe a separation between formal and functional mechanisms, suggesting that shared mechanisms between formal tasks may exist.",
            "score": 0.5196507304291635,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "261822542",
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "text": "While often, the performance of language models scales predictably (Kaplan et al., 2020;Srivastava et al., 2022), some tasks instead show breakthrough behavior where a single point along a scaling curve (i.e. compute/number of tokens seen/number of parameters versus test loss) shows a sudden improvement in performance (Srivastava et al., 2022;Wei et al., 2022;Caballero et al., 2023). One computational structure, the induction head, emerges in autoregressive language models at a discrete phase change (Olsson et al., 2022) and is associated with handling longer context sizes and in-context learning. In machine translation, Dankers et al. (2022) find a learning progression in which a Transformer first overgeneralizes the literal interpretations of idioms and then memorizes idiomatic behavior. When the training set makes grammatical rules ambiguous, Murty et al. (2023) show that language modelling eventually leads to phase transitions towards the hierarchical version of a rule over an alternative based on linear sequential order. Outside of NLP, phase changes are observed in the acquisition of concepts in strategy games (Lovering et al., 2022;McGrath et al., 2022) and arithmetic (Liu et al., 2022;Nanda et al., 2023). Our work also identifies a specific phase in MLM training, the SAS phase, and analyzes its role in performance and generalization behavior. \n\nWe also observe an alignment between phase changes in representational complexity and in generalization performance (Section 4.1), which parallels the observation of Finally, although we find the same phase transition occurs across multiple training runs, other work indicates that generalization capabilities are sensitive to random seed (Sellam et al., 2022;Juneja et al., 2023;Jordan, 2023). Furthermore, phase transitions may be primarily an artifact of poor hyperparameter settings (Liu et al., 2023), which lead to unstable optimization (Hu et al., 2023). Therefore, it is possible that these abrupt breakthroughs would vanish under the correct architecture and optimizer settings.",
            "score": 0.5146278098650336,
            "section_title": "B SYNTACTIC REGULARIZER",
            "char_start_offset": 33116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2062
                }
            ],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 385,
                    "matchedPaperCorpusId": "253117181"
                },
                {
                    "start": 629,
                    "end": 650,
                    "matchedPaperCorpusId": "236987160"
                },
                {
                    "start": 1134,
                    "end": 1157,
                    "matchedPaperCorpusId": "253534142"
                },
                {
                    "start": 1157,
                    "end": 1178,
                    "matchedPaperCorpusId": "244270420"
                },
                {
                    "start": 1194,
                    "end": 1212,
                    "matchedPaperCorpusId": "248965387"
                },
                {
                    "start": 1714,
                    "end": 1735,
                    "matchedPaperCorpusId": "235683534"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "268201735",
            "title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
            "text": "Human language processing, long studied in cognitive science and neuroscience, robustly attributes language and cognition to different brain areas, namely \"language network\" and \"multi-demand network\" (Duncan, 2010;Scott et al., 2017). The former is sensitive to linguistic regularities and formal operations, with damage leading to linguistic deficits, while the latter responds actively to various cognitively demanding processes, such as reasoning and memory. Similarly, prior analyses have identified core linguistic regions (Zhang et al., 2024b) and language-independent knowledge neurons (Chen et al., 2023) in LLMs, represented by different parameter sets and subnetworks, each contributing distinctly to language and reasoning tasks. \n\nMotivated by this separated relationship, we define the LLMs' capabilities as a 4-dimensional schema as shown in Table 1. Compared to the fine-grained definitions of high-level knowledge (Hendrycks et al., 2020;Yu et al., 2023) and reasoning skills (Ye et al., 2024) in related works, this formulation aims to dissociate languagerelated and cognition-related capabilities to define a broader range of both high-level and more fundamental low-level functionality. Additionally, it seeks to minimize the coupling between these capabilities to facilitate nuanced analysis (Sec. 3.1) and targeted improvement of the model (Sec. 3.2). LINGUISTIC KNOWLEDGE. To effectively generate language, LLMs first understand text at a basic linguistic level, including both grammar and semantics. Grammaticality encompasses the rules that govern language structure, spanning from the sounds (phonological) and words (lexical) to the arrangement of words in phrases and sentences. To capture this grammatical structure as comprehensively as possible, especially given the challenges conventional models face in benchmarks like BLiMP (Warstadt et al., 2020), we focus on four key skills: agreement (anaphora and subjectverb relationships), licensing (negative polarity items and reflexive pronouns), managing longdistance dependencies (filler-gap constructions and cleft sentences), and navigating garden-path sentences, which contain temporary ambiguities that must be resolved for correct understanding.",
            "score": 0.5122753783658031,
            "section_title": "Formulation of LLMs' Capabilities",
            "char_start_offset": 5295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 2230
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 215,
                    "matchedPaperCorpusId": "2480005"
                },
                {
                    "start": 215,
                    "end": 234,
                    "matchedPaperCorpusId": "3762948"
                },
                {
                    "start": 931,
                    "end": 955,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 955,
                    "end": 971,
                    "matchedPaperCorpusId": "256662612"
                },
                {
                    "start": 993,
                    "end": 1010,
                    "matchedPaperCorpusId": "259991144"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72119140625
        },
        {
            "corpus_id": "277043826",
            "title": "Are formal and functional linguistic mechanisms dissociated in language models?",
            "text": "A wide body of research has argued that language and thought are dissociated in the human brain (Fedorenko, Ivanova, and Regev 2024). That is, such research argues that the regions of the brain that respond differentially to well-formed linguistic input are distinct from those that respond to other structured inputs, such as mathematics, music, and code (Amalric and Dehaene 2019;Chen et al. 2023;Ivanova et al. 2020). They are also distinct from regions that respond to language-adjacent capabilities such as theory of mind and reasoning (Shain et al. 2022;Monti, Parsons, and Osherson 2009). \n\nThese regions of the brain, termed the language network, are thus selective for language (perhaps narrowly defined), and language alone. \n\nIn recent work, Mahowald et al. (2024) argue that these two types of stimuli, to which the language network does and does not respond, correspond to two distinct types of linguistic competence: formal and functional. Formal linguistic competence is necessary for \"getting the form of language right\". It involves the correct structuring of language at the sub-word, lexical, and sentence level; in other words, phonology, morphology, syntax, and lexical semantics are all domains of formal linguistic competence. Past neuroscientific work has shown all of these to be supported by the language network (Regev et al. 2024;Shain et al. 2024;Hu et al. 2022). \n\nIn contrast, functional linguistic competence involves abilities that allow speakers to achieve goals in the world, but may involve non-linguistic cognition. For example, speakers may engage in formal or pragmatic reasoning, or employ world knowledge in conversation, without such abilities being intrinsic to language. Similarly, speakers may use situation modeling skills to make sense of a narrative, or engage in theory of mind reasoning to understand their interlocutors' point of view, although language use need not entail the use of these abilities. Moreover, exercising these abilities does not engage the brain's language network. Mahowald et al. furthermore claim that this distinction between formal and functional linguistic competence is reflected in the performance of today's large language models (LLMs).",
            "score": 0.5104377542760604,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 132,
                    "matchedPaperCorpusId": "269112224"
                },
                {
                    "start": 356,
                    "end": 382,
                    "matchedPaperCorpusId": "58647234"
                },
                {
                    "start": 382,
                    "end": 399,
                    "matchedPaperCorpusId": "255523571"
                },
                {
                    "start": 541,
                    "end": 560,
                    "matchedPaperCorpusId": "250927208"
                },
                {
                    "start": 560,
                    "end": 594,
                    "matchedPaperCorpusId": "5624174"
                },
                {
                    "start": 753,
                    "end": 775,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 1339,
                    "end": 1358,
                    "matchedPaperCorpusId": "268511853"
                },
                {
                    "start": 1358,
                    "end": 1376,
                    "matchedPaperCorpusId": "269474005"
                },
                {
                    "start": 1376,
                    "end": 1391,
                    "matchedPaperCorpusId": "251323162"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5302734375
        },
        {
            "corpus_id": "261531236",
            "title": "Are Emergent Abilities in Large Language Models just In-Context Learning?",
            "text": "One of the most captivating aspects of pre-trained language models (PLMs) is their capacity to acquire a wide range of knowledge across different domains, while being trained primarily through masked language modelling, a task requiring models to predict masked tokens in their input (Tenney et al., 2019;Petroni et al., 2019). The diverse abilities of PLMs can be categorised into two broad types: formal linguistic abilities and functional linguistic abilities. Formal linguistic abilities refer to the understanding of language rules and patterns, which PLMs, for example, BERT (Devlin et al., 2019) are known to excel at (Tenney et al., 2019;Petroni et al., 2019). The latter category includes a range of abilities akin to human cognition that are necessary for real-world language use and comprehension, such as commonsense knowledge and social awareness. While PLMs excel at formal linguistic abilities, they have faced challenges in developing functional linguistic abilities (Mahowald et al., 2023). \n\nThe introduction of Large Language Models (LLMs), which are typically generative PLMs scaled up to billions of parameters and trained on vast, web-scale data corpora, is changing this landscape (Brown et al., 2020;Chowdhery et al., 2023;Touvron et al., 2023a,b). Recent works indicate that LLMs exhibit emergent abilities, as measured by their above random performance without explicit training on tasks, including those tasks that explicitly require some form of reasoning. An emergent ability was first defined as an ability to solve a task which is absent in smaller models, but present in LLMs. This definition, introduced approximately concurrently by two works (Wei et al., 2022b;Srivastava et al., 2023), is based on the more general definition of emergence in physics: \"Emergence is when quantitative changes in a system result in qualitative changes in behaviour\" (Anderson, 1972). Emergent abilities are implied due to LLMs' capacity to perform above the random baseline on the corresponding tasks without explicit training on those same tasks.",
            "score": 0.5002929014404388,
            "section_title": "Introduction, Motivation and Context",
            "char_start_offset": 39,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 305,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 305,
                    "end": 326,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 581,
                    "end": 602,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 625,
                    "end": 646,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 646,
                    "end": 667,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1224,
                    "end": 1247,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1677,
                    "end": 1696,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1696,
                    "end": 1720,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 1883,
                    "end": 1899,
                    "matchedPaperCorpusId": "2221052"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "278368201",
            "title": "Quiet Feature Learning in Algorithmic Tasks",
            "text": "Understanding how and when large language models acquire new capabilities has become an important question in the study of deep learning. While language models have demonstrated remarkable performance across a broad range of tasks, the precise mechanisms driving their improvements remain unknown. Recent discussions of \"emergent abilities\"where larger-scale models outperform baselines abruptly, even though smaller-scale counterparts exhibit little improvement -have led to debate over whether such phenomena are genuine or artifacts of measurement (Wei et al., 2022;Ganguli et al., 2022;Schaeffer et al., 2023). \n\nQuestions about emergent abilities are closely tied to the observation of scaling laws in model training (Kaplan et al., 2020;Ruan et al., 2024;Henighan et al., 2020;Dubey et al., 2024;OpenAI, 2023). These scaling laws typically show a smooth, power-law relationship between compute and model performance. However, most empirical demonstrations of these laws derive from heterogeneous data and tasks, leaving open the possibility that \"averaging out\" many distinct learning behaviors masks more abrupt transitions that occur for individual skills or subtasks. \n\nWe focus on ten foundational algorithmic problems spanning various input types. These algorithmic tasks have precisely defined solutions, making it straightforward to identify clear success criteria, isolate the specific features or subtasks the model must learn, and ensure that improvements cannot be attributed to memorization or partial heuristics. These tasks allow us to investigate fine-grained learning phenomena which might otherwise be obscured by heterogeneous data. \n\nOur key findings include: \n\n1. Phase transitions in scaling laws: We observe two distinct phases in scaling laws across tasks and task lengths. In the \"slow phase,\" loss improves minimally or remains flat. Then, loss drops rapidly (\"fast phase\"). We refer to the change between these two phases as phase transition. We observe this phase transition when we vary model size & training dataset size, fix model size & vary training dataset size and within individual training runs. \n\n2. Quiet features precede phase transitions: Models learn meaningful internal representations during the slow phase, but these features do not yet yield noticeable gains in the output loss (we call these \"quiet features\").",
            "score": 0.5002412781626308,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 614
                },
                {
                    "start": 617,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1176
                },
                {
                    "start": 1179,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1656
                },
                {
                    "start": 1659,
                    "end": 1684
                },
                {
                    "start": 1687,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2137
                },
                {
                    "start": 2140,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 551,
                    "end": 569,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 569,
                    "end": 590,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 590,
                    "end": 613,
                    "matchedPaperCorpusId": "258418299"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.798828125
        },
        {
            "corpus_id": "259202577",
            "title": "Eight challenges in developing theory of intelligence",
            "text": "The impressive problem-solving capabilities of Chat-GPT where GPT is a shorthand of generative pretrained transformer are leading the fourth industrial revolution. The Chat-GPT is based on large language models (LLMs) [98], which represents linguistic information as vectors in high dimensional state space, trained with a large text corpus in an autoregressive way (in analogy to the hypothesis that the brain is a prediction machine [99]), resulting in a complex statistical model of how the tokens in the training data correlate [100]. The computational model thus shows strong formal linguistic competence [101]. The LLM is also a few-shot or even zero-shot learner [102,103], i.e., the language model can perform a wide range of computationally challenging tasks with prompting alone (e..g, chain-of-thought prompting [104]). Remarkably, the LLMs display a qualitative leap in capability as the model complexity and sample complexity are both scaled up [105], akin to phase transitions in thermodynamic systems. \n\nIn contrast to the formal linguistic competence, the functional linguistic competence is argued to be weak [101]. This raises a fundamental question what the nature of intelligence is, or whether a single next-token context conditional prediction is a standard model of artificial general intelligence [106][107][108]. Human's reasoning capabilities in real-world problems rely on non-linguistic information as well, e.g., it is unpredictable when a creative idea for a scientist would come to a challenging problem at hand, which relies on reasoning about the implications along a sequence of thought. In a biological implementation, the language modules are separated from the other modules involving high-level cognition [101]. \n\nThe LLM explains hierarchical correlations in word pieces in the training corpora, rather than hidden casual dependencies. In other words, the neural network has not constructed a mental model of the world, which requires heterogeneous modular networks, thereby unlike humans. Therefore, the LLM does not know what it generate (as a generative model). Even if some key patterns of statistical regularities are absent in the training data, the model can generate perfect texts in terms of syntax.",
            "score": 0.49699387174465515,
            "section_title": "VIII. CHALLENGE VII-LARGE LANGUAGE MODELS",
            "char_start_offset": 19421,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 435,
                    "end": 439,
                    "matchedPaperCorpusId": "320913"
                },
                {
                    "start": 532,
                    "end": 537,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 610,
                    "end": 615,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 670,
                    "end": 675,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1126,
                    "end": 1131,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 1321,
                    "end": 1326,
                    "matchedPaperCorpusId": "251196636"
                },
                {
                    "start": 1326,
                    "end": 1331,
                    "matchedPaperCorpusId": "260496023"
                },
                {
                    "start": 1331,
                    "end": 1336,
                    "matchedPaperCorpusId": "195667729"
                },
                {
                    "start": 1743,
                    "end": 1748,
                    "matchedPaperCorpusId": "268551442"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.533203125
        },
        {
            "corpus_id": "264591406",
            "title": "Unveiling A Core Linguistic Region in Large Language Models",
            "text": "Inspired by cognitive science research, this paper investigates whether specific functional regions exist within LLMs. We identify a core region specifically responsible for language processing within LLMs. This region occupies only about 1% of the model's parameters but plays a crucial role in maintaining the overall linguistic competence of the model. Invalid changes in the parameters of this region can severely impair the model's linguistic competence. We also observe a pronounced dimension dependence in the core region of linguistic competence. Surprisingly, in a large model like LLaMA-13B, which boasts 13 billion parameters, altering just one parameter could potentially inflict significant damage to its linguistic competence. This study further elucidates the relationship between linguistic competence and knowledge in large language models. We find that an improvement in linguistic competence does not necessarily imply an enhancement in knowledge level. This suggests the presence of a knowledge storage region in LLMs that operates independently of language processing. In summary, the findings of this paper shed new light on how the capabilities and knowledge are structured in large language models and help explain why the pre-training and fine-tuning processes of these large models differ significantly from their smaller predecessors.",
            "score": 0.4938029974423907,
            "section_title": "CONCLUSIONS",
            "char_start_offset": 25015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1361
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85986328125
        },
        {
            "corpus_id": "273025704",
            "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime",
            "text": "Most research on LMs focuses on the final configuration of the model at the end of pre-training. Yet, recent work shows that learning dynamics can elucidate the behavior and computational mechanisms of LMs (Chen et al., 2024;Singh et al., 2024;Tigges et al., 2024). It has been found that, over training, LMs' weight matrices become higher-rank (Abbe et al., 2023), their representations higher dimensional (Cheng et al., 2024), and their gradients increasingly diffuse (Weber et al., 2024). Over finetuning, representational dimensionality has been found to change in concert with geometric properties like cluster reorganization (Doimo et al., 2024). \n\nPhase transitions during LM training have been found for some, but not all, aspects of language learning. Negative evidence includes that LM circuits involved in linguistic subtasks are stable (Tigges et al., 2024) and gradually reinforced (Weber et al., 2024) over training. Positive evidence for learning phase transitions includes that the ID of BERT's final [CLS] representation tracks sudden syntax acquisition and drops in training loss (Chen et al., 2024), with similar observations on Transformers trained on formal languages (Lubana et al., 2024). Our work supplements these results by investigating how the interaction between compositional understanding of language and geometric complexity of its representation arises over training.",
            "score": 0.4910683718909633,
            "section_title": "Language model training dynamics",
            "char_start_offset": 5069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1400
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 225,
                    "matchedPaperCorpusId": "261822542"
                },
                {
                    "start": 225,
                    "end": 244,
                    "matchedPaperCorpusId": "265157721"
                },
                {
                    "start": 345,
                    "end": 364,
                    "matchedPaperCorpusId": "259138551"
                },
                {
                    "start": 470,
                    "end": 490,
                    "matchedPaperCorpusId": "270370801"
                },
                {
                    "start": 895,
                    "end": 915,
                    "matchedPaperCorpusId": "270370801"
                },
                {
                    "start": 1098,
                    "end": 1117,
                    "matchedPaperCorpusId": "261822542"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8544921875
        },
        {
            "corpus_id": "277043826",
            "title": "Are formal and functional linguistic mechanisms dissociated in language models?",
            "text": "Building on the literature regarding the language network in the brain, Mahowald et al. (2024) propose a related distinction between formal and functional linguistic competence. They use formal linguistic competence to refer to linguistic abilities necessary to get the form of language right; formal linguistic competence refers to language abilities that we use to achieve goals or otherwise function with language. Notably, while formal linguistic competence cleanly maps onto the language network, functional linguistic competence includes only some non-language-network abilities; others, like music, are excluded. \n\nMahowald et al. divide formal and functional linguistic competence into subdomains (Table 1). Formal language competence includes subdomains like phonology, morphology, syntax, and lexical semantics. For Mahowald et al., phonology corresponds to the rules governing valid wordforms (i.e., phonotactics), while morphology involves the correct ordering of morphemes. Syntax involves not only correct word order, but also higher-level abilities like agreement (e.g. between subjects and verbs). \n\nLexical semantics entails using words correctly according to their part of speech, lexical category, or meaning. Mahowald et al. distinguish this category from semantics more broadly: general conceptual knowledge belongs to functional language competence. \n\nIn contrast, functional language competence consists of formal reasoning, world knowledge, situation modeling, and social reasoning. Formal reasoning includes math and logical abilities, while world knowledge includes facts and commonsense knowledge. Situation modeling entails the ability to track the state of a discourse, and the structure of narratives. Finally, social reasoning covers pragmatics and theory of mind. \n\nMahowald et al. note that LLMs have strong formal linguistic competence, succeeding on tests of syntactic ability and lexical semantics (Chang and Bergen 2024). It is more challenging to measure English LLMs' abilities in phonology, given that LLMs seldom produce novel phonemes, and morphology, given the relative simplicity of English morphology, but LLMs do seem to generate valid novel morphemes (McCoy et al. 2023). In contrast, models often struggle with functional tasks (Dziri et al. 2023;Strachan et al. 2024), though recent LLMs have markedly improved reasoning abilities due to intensive post-training.",
            "score": 0.48346939119903665,
            "section_title": "Formal and Functional Linguistic Competence in Language Models",
            "char_start_offset": 13017,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1113
                },
                {
                    "start": 1116,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1795
                },
                {
                    "start": 1798,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2218
                },
                {
                    "start": 2219,
                    "end": 2411
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 94,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 1934,
                    "end": 1957,
                    "matchedPaperCorpusId": "257636789"
                },
                {
                    "start": 2198,
                    "end": 2217,
                    "matchedPaperCorpusId": "244345615"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6962890625
        },
        {
            "corpus_id": "255941592",
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "text": "For our purposes here, we will use the term LLMs to refer primarily to \"pure\" language models (such as the original GPT-3) that are trained to predict held-out language tokens conditional on the immediate linguistic context, from large corpora of naturally observed language use. \n\nIn the rest of the paper, we formulate an account of what we should and should not expect from a model of language and evaluate contemporary LLMs within this framework. In Section 2, we elaborate on the constructs of formal and functional linguistic competence and motivate this distinction based on the evidence from human cognitive science and neuroscience. In Section 3, we discuss the successes of LLMs in achieving formal linguistic competence, showing that models trained on word filling-in/prediction tasks capture numerous complex linguistic phenomena. Then, in Section 4, we consider several domains required for functional linguistic competence-formal reasoning, world knowledge, situation modeling, and social-cognitive abilities-on which today's LLMs fail, or at least perform much worse than humans. In Section 5, we discuss the implications of our framework for building and evaluating future models of language and Artificial General Intelligence (AGI). Finally, in Section 6, we summarize our key conclusions. \n\n2 Formal vs. functional linguistic competence 2.1 What does linguistic competence entail?",
            "score": 0.48343209496622463,
            "section_title": "Introduction",
            "char_start_offset": 8287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 282,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1307
                },
                {
                    "start": 1310,
                    "end": 1399
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "233289478",
            "title": "Probing Across Time: What Does RoBERTa Know and When?",
            "text": "Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.",
            "score": 0.48342877481182756,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "275788769",
            "title": "Implicit Causality-biases in humans and LLMs as a tool for benchmarking LLM discourse capabilities",
            "text": "Despite recent successes of large language models (henceforth, LLMs), matching or exceeding human performance in wide array of tasks, it is still not clear to what extent they can be considered as models of human language (Milli\u00e8re, 2024) or human cognition (Mahowald et al., 2024). Ever since the emergence of large foundation language models, there has been a great interest in gaining a deeper understanding of their general linguistic and cognitive capabilities, ranging from studies into syntactic or semantic processing in LLMs (Gulordava et al., 2018;Blevins et al., 2018;Goldberg, 2019;Arefyev et al., 2020) to analyses of their arithmetic or logical reasoning abilities (Huang and Chang, 2022;Webb et al., 2023;Wu et al., 2023;Yuan et al., 2023). Since LLMs are trained on massive amounts of training data and learn huge amounts of parameters in their internal layers, it is often difficult to establish whether an LLM has \"really\" generalized a certain rule or piece of knowledge, or whether it memorized superficial token-level patterns (Weissweiler et al., 2023). This challenge exists in particular in domains where linguistic knowledge is closely intertwined with nonlinguistic aspects of cognition (Mahowald et al., 2024) such as, e.g., pragmatics and commonsense reasoning (Chang and Bergen, 2024). \n\nTo assess LLMs linguistic capabilities research has drawn from the methods of psychology, applying established experimental paradigms to language models. For example, Ettinger (2020) created a diagnostic suite for language models inspired by psycholinguistic tests, ranging over different linguistic disciplines. A study by Hawkins et al. (2020) compared human and model acceptability judgements for double object sentences, capturing the differences in their biases. Outside the realm of psycholinguistics, Binz and Schulz (2023) used vignette studies from cognitive psychology to gauge the cognitive abilities of GPT-3 (Brown et al., 2020).",
            "score": 0.48275277303730135,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1959
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 281,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 594,
                    "end": 615,
                    "matchedPaperCorpusId": "227231059"
                },
                {
                    "start": 702,
                    "end": 720,
                    "matchedPaperCorpusId": "254854575"
                },
                {
                    "start": 1213,
                    "end": 1236,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 1289,
                    "end": 1313,
                    "matchedPaperCorpusId": "257636789"
                },
                {
                    "start": 1484,
                    "end": 1499,
                    "matchedPaperCorpusId": "199001173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56884765625
        },
        {
            "corpus_id": "270440920",
            "title": "A Survey on Compositional Learning of AI Models: Theoretical and Experimetnal Practices",
            "text": "In addition to inconclusive theoretical studies on transformer limitations, there are controversial results on large language models. The most noteworthy study is on the emerging abilities and capabilities claimed to be unique in the large models. The emerging abilities relate to the generalization to new and complex tasks in LLMs. This kind of ability is also a feature of models' compositional learning ability, allowing them to perform in novel compositional situations (Yu et al., 2024). Multiple works have shown the existence of emergent abilities of LLMs (Wei et al., 2022). The recent work of Arora & Goyal (2023) provides a mathematical framework for identifying complex skills in language models. They use the LLM Scaling Rule to argue that emergent skills are the results of reducing excessive loss. This excessive loss enables the model to learn how to utilize and combine skills from downstream tasks during training. Their claims are based on the assumption that language inherently contains a random mix of complex skills. Although several experiments reveal these emerging capabilities, at least two papers disclaim their existence. The first group provides a theoretical proof based on a mathematical framework. They illustrate that the emerging ability appears due to the selected evaluation metrics that are nonlinear and discontinuous (Schaeffer et al., 2023). They show as an artifact of the evaluation metrics, even simple models such as CNNs can show emerging abilities. Therefore, they conclude that emerging abilities may not be a fundamental property of the large models. Moreover, Lu et al. (2024) provides an extensive empirical study with 1000 experiments on 22 tasks with different LLMs. However, given the inconsistency in some results and the unpredictability of emerging abilities, they do not find any strong evidence of how they emerge. They associate the performance with in-context learning techniques, memorization, and data contamination. However, a recent study presents a positive theoretical analysis of reasoning capabilities by investigating the chain of thought (CoT) (Wei et al., 2024) and draws a different conclusion. They argue that the log-precision transformer can perform fundamental operations such as multiplication and a look-up table. Consequently, it can solve linear equations and other reasoning problems if it stores all the input information.",
            "score": 0.4826846583710336,
            "section_title": "Large Language Models",
            "char_start_offset": 46514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2167
                },
                {
                    "start": 2168,
                    "end": 2292
                },
                {
                    "start": 2293,
                    "end": 2405
                }
            ],
            "ref_mentions": [
                {
                    "start": 475,
                    "end": 492,
                    "matchedPaperCorpusId": "264490642"
                },
                {
                    "start": 564,
                    "end": 582,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1357,
                    "end": 1381,
                    "matchedPaperCorpusId": "258418299"
                },
                {
                    "start": 1610,
                    "end": 1626,
                    "matchedPaperCorpusId": "261531236"
                },
                {
                    "start": 2115,
                    "end": 2133,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6298828125
        },
        {
            "corpus_id": "276766647",
            "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network",
            "text": "To assess formal linguistic competence, we use two benchmarks: BLIMP (Warstadt et al., 2019) and SYNTAXGYM (Gauthier et al., 2020). BLIMP evaluates key grammatical phenomena in English through 67 tasks, each containing 1,000 minimal pairs designed to test specific contrasts in syntax, morphology, and semantics. Complementing this, SYNTAXGYM consists of 31 tasks that systematically measure the syntactic knowledge of language models. Together, these benchmarks provide a robust framework for evaluating how well LLMs acquire and apply linguistic rules. \n\nFunctional Linguistic Competence Functional competence extends beyond linguistic rules, engaging a broader set of cognitive mechanisms. To assess this, we use six benchmarks covering world knowledge (ARC-EASY, ARC-CHALLENGE (Clark et al., 2018)), social reasoning (SOCIAL IQA (Sap et al., 2019)), physical reasoning (PIQA (Bisk et al., 2019)), and commonsense reasoning (WINOGRANDE (Sakaguchi et al., 2019), HELLASWAG (Zellers et al., 2019)). Together, these benchmarks provide a comprehensive evaluation of an LLM's ability to reason, infer implicit knowledge, and navigate real-world contexts.",
            "score": 0.4814950333985718,
            "section_title": "Formal Linguistic Competence",
            "char_start_offset": 10180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1152
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 130,
                    "matchedPaperCorpusId": "220060899"
                },
                {
                    "start": 833,
                    "end": 851,
                    "matchedPaperCorpusId": "128296356"
                },
                {
                    "start": 879,
                    "end": 898,
                    "matchedPaperCorpusId": "208290939"
                },
                {
                    "start": 975,
                    "end": 997,
                    "matchedPaperCorpusId": "159041722"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "273403825",
            "title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition",
            "text": "As one of the unique abilities of humans, the acquisition of language has long been a key area of research in psycholinguistics. During the process of language acquisition, humans go through multiple stages, from imitation and rule learning to complex contextual understanding (Goldberg, 2005). These stages bear some resemblance to the way current LMs are trained. For instance, LMs learn the statistical patterns and grammatical rules of language through training on large-scale data, similar to how infants develop language abilities by receiving a vast amount of input through listening and speaking. If we design theory-driven tests based on the human language acquisition process to evaluate the capabilities of LMs, it could help us better understand the nature, potential, and limitations of LMs in their development. \n\nOur work draws on classical theories of human language development to assess LMs in terms of a three-stage human language development framework (Chomsky, 2014;Loban, 1976;Pinker, 2003), as shown in Figure 1. The first stage involves developing basic language understanding, similar to early language acquisition in infants. At this stage, we evaluate the model's ability to recognize vocabulary, grasp syntax, and perform simple reasoning. In the second stage, the focus shifts to mastering complex grammar and semantics, where the model demonstrates a deeper understanding of language rules and logical relationships between sentences. The third stage assesses advanced language abilities, evaluating the model's capacity for complex reasoning and logical analysis. \n\nWe further investigate another theory: register theory in linguistics, which posits that different language use scenarios influence the form and structure of language (Halliday, 1977;Matthiessen, 1993). This theory offers insights into the extent to which models' abilities depend on the linguistic features encountered in specific situations, referred to as registers. In LMs, the training corpus will reflect some registers but not others, which can raise general questions or concerns about the generaliz-ability and biases contained in any given corpus. \n\nWe evaluated 15 LMs from 2019 to 2024, excluding instruction fine-tuned or chat versions, with varying parameter sizes (see \u00a74.1).",
            "score": 0.47977210142702814,
            "section_title": "Introduction",
            "char_start_offset": 2100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2154
                },
                {
                    "start": 2157,
                    "end": 2287
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88330078125
        },
        {
            "corpus_id": "255941592",
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "text": "Why should we care whether LLMs can think? One reason, of course, is a wealth of claims that contemporary LLMs are precursors to artificial general intelligence [AGI; e.g., Romero, 2022, Yan, 2021]. However, there is another reason, which is of substantial relevance to both AI researchers and language scientists: real-life language use is impossible without non-linguistic cognitive skills. Understanding a sentence, reasoning about its implications, and deciding what to say-these skills all rely on cognitive capacities that go way beyond lexical semantics or syntax. \n\nWe focus on four key capacities that are not language-specific but are nevertheless crucial for language use in real-life settings: i) formal reasoning-a host of abilities including logical reasoning, mathematical reasoning, relational reasoning, computational thinking, and novel problem solving; ii) world knowledge-knowledge of objects and their properties, actions, events, social agents, facts, and ideas; iii) situation modeling-the dynamic tracking of protagonists, locations, and events as a narrative/conversation unfolds over time; and iv) social reasoning-understanding the social context of linguistic exchanges, including what knowledge is shared, or in 'common ground', what the mental states of conversation participants are, and pragmatic reasoning ability. A simple conversation typically requires the use of all four of these capacities, yet none of them are specific to language use. Below, we provide evidence that these skills rely on non-language-specific processing mechanisms in humans and highlight LLMs' failures as relevant to each domain.",
            "score": 0.47820688033289677,
            "section_title": "Limitations of LLMs as real-life language users",
            "char_start_offset": 55051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1640
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 197,
                    "matchedPaperCorpusId": "263864874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5126953125
        },
        {
            "corpus_id": "264935245",
            "title": "Training Dynamics of Contextual N-Grams in Language Models",
            "text": "Training dynamics Several works (Olsson et al., 2022;Nanda et al., 2023) discovered circuits in language models that seem causally relevant to emergent capabilities and can be ablated to reduce the capabilities. Michaud et al. (2023) postulated that many capabilities in language models emerge rapidly in phase transitions and proposed a model in which any capabilities which appear to instead form gradually can be decomposed into discrete, additive capabilities which emerge in independent phase transitions. Finally, several works (Varma et al., 2023;Nanda et al., 2023) observed the phenomenon of grokking, where a model that has already achieved near-perfect performance on the training data transitions to a mechanism that generalizes. \n\nWhile only a minority of circuits-style analysis has focused on model development, a wealth of results in the field of developmental interpretability have characterized the training dynamics of language models using other units of analysis. Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020;Wei et al., 2022). Raghu et al. (2017) proposed singular vector canonical correlation analysis, a tool which allows different model layers and networks to be compared, and showed that early network layers converge earlier in training than late layers. Saphra and Lopez (2020b,a) found that LSTM models learn hierarchical sequential features in natural language bottom up -shorter sequences are learned first and form the basis for the representation of longer sequences that contain the shorter ones. Finally, Saxe et al. (2019) found that the features are learned in an order determined by the magnitude of the corresponding singular value in the model. Dodge et al. (2020) found that differences in random parameter initializations and training data order contribute comparably to differences between learned representations of fine-tuning runs of the same model on identical data, and McCoy et al. (2020) found that identically trained BERT transformer models are consistent with this finding, varying widely in generalization performance.",
            "score": 0.47622107315322565,
            "section_title": "Related Work",
            "char_start_offset": 17886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2279
                }
            ],
            "ref_mentions": [
                {
                    "start": 1216,
                    "end": 1237,
                    "matchedPaperCorpusId": "222140842"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83984375
        },
        {
            "corpus_id": "248157591",
            "title": "Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding",
            "text": "2) an evaluation procedure for probing and evaluating how well a language model captures reasoning skills for distinct types of linguistic phenomena. Targeted linguistic phenomena in CUR-RICULUM range from fundamental properties like named entity and coreference to complex ones like commonsense and deductive reasoning. With the CURRICULUM benchmark, we aim to investigate the following research questions:\n\n\u2022 Q1: Do language models trained on benchmark datasets have the ability to reason over a wide range of linguistic phenomena? \u2022 Q2: Are linguistic phenomena missing from the training data recoverable through inoculation? \u2022 Q3: Do language models learn a general reasoning skill of a phenomenon through inoculation?\n\nTo address the above questions, we empirically analyze NLI models trained on popular benchmark datasets through a zero-shot diagnostic test, inoculation by fine-tuning, hypothesis-only tests, and cross-distribution generalization tests. In addition, we closely study the low-data generalization performance of models sequentially trained on selected linguistic phenomena datasets.\n\nFor Q1, we observe that models trained on benchmark datasets, including adversarial data, do not have the reasoning ability for a large set of linguistic phenomena. Our results show that training on more datasets can help the model learn more types of reasoning but does not help the model acquire complex reasoning skills. Our benchmark exposes multiple knowledge gaps in large NLI models regarding diverse linguistic phenomena. For Q2, our analysis provides empirical evidence that either exploits the lack of recoverable linguistic phenomena in benchmark datasets or exposes models' inability to learn certain linguistic phenomena. We also show that, on some phenomena, models may rely heavily on superficial cues or artifacts existing in the hypothesis to reach high accuracy. For Q3, Our experiments show that a model's learning performance may not align with its generalization ability. Models fail to generalize across different difficulty distributions on many phenomena, sug-gesting the lack of a general reasoning skill. Models can generalize across distributions only on a limited number of phenomena.\n\nOverall, our proposed benchmark systematically maps out a wide range of specific linguistic skills required for language understanding and inference. We envision linguistic-phenomena-based evaluation to be an integral component of general linguistic intelligence. We hope CURRICULUM can serve as a useful evaluation tool that can",
            "score": 0.4688889589950752,
            "section_title": "Introduction",
            "char_start_offset": 3516,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73486328125
        },
        {
            "corpus_id": "255941592",
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "text": "In this paper, we have advanced the thesis that functional competence and formal linguistic competence are distinct capabilities, recruiting different machinery in the human brain. More broadly, most biological intelligent systems-including both human and non-human minds-are highly modular [e.g., Carruthers, 2002, 2005, Cosmides and Tooby, 1994, Fedorenko et al., 2011, Kanwisher et al., 1997, Meunier et al., 2010]. What can this modularity tell us about how to build better, more human-like models? \n\nWe argue that future language models can master both formal and functional linguistic competence by establishing a division of labor between the core language system and components for other cognitive processes, such as formal logic and social reasoning. We see at least two ways to implement this division of labor: explicitly building modularity into the architecture of the system (we call this Architectural Modularity) or naturally inducing modularity through the training process, both through the training data and the objective function (we call this Emergent Modularity). \n\nArchitectural Modularity has a long history; it involves stitching together separate components, perhaps with quite specialized architectures [e.g., Bottou andGallinari, 1990, Ronco andGawthrop, 1997]. More recent examples include a transformer language model paired with a separate memory module [e.g., Borgeaud et al., 2022, d'Autume et al., 2019, Liu et al., 2022a] or a model for visual question answering, which includes a language module, a vision module, and a reasoning module [e.g., Yi et al., 2018, Mao et al., 2019, Andreas et al., 2016, Hudson and Manning, 2019, Johnson et al., 2017]. Such modular models are capable of achieving high task performance, are more efficient (i.e., can be trained on smaller datasets and have lower memory demands), and show high generalizability (i.e., perform well on datasets with previously unseen properties). The modules of such models can be trained separately or together, similarly to how humans can flexibly combine different cognitive skills when learning to perform novel complex tasks.",
            "score": 0.4686501773789318,
            "section_title": "Modularity",
            "char_start_offset": 76646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2129
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 314,
                    "matchedPaperCorpusId": "7737830"
                },
                {
                    "start": 346,
                    "end": 370,
                    "matchedPaperCorpusId": "15092714"
                },
                {
                    "start": 370,
                    "end": 394,
                    "matchedPaperCorpusId": "86163"
                },
                {
                    "start": 1237,
                    "end": 1247,
                    "matchedPaperCorpusId": "628165"
                },
                {
                    "start": 1273,
                    "end": 1288,
                    "matchedPaperCorpusId": "16388614"
                },
                {
                    "start": 1436,
                    "end": 1456,
                    "matchedPaperCorpusId": "246240585"
                },
                {
                    "start": 1580,
                    "end": 1595,
                    "matchedPaperCorpusId": "52919654"
                },
                {
                    "start": 1635,
                    "end": 1661,
                    "matchedPaperCorpusId": "195847902"
                },
                {
                    "start": 1661,
                    "end": 1684,
                    "matchedPaperCorpusId": "31319559"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72119140625
        },
        {
            "corpus_id": "276617466",
            "title": "Language Models Grow Less Humanlike beyond Phase Transition",
            "text": "Figure 1: Phase transition (x-axis) and PPP peaks (yaxis) closely coincide with each other across the corpora studied in this work, except for MECO. Numbers in the parentheses are batch sizes; for Pythia models, regardless of the model size, the batch size is 2,048. Orange dashed lines, blue solid lines, and gray dashed lines represent the phase transition hypothesis, 2 billion tokens hypothesis, and the observed trendline drawn from the data, respectively. We find that the experimental results closely align with our phase transition hypothesis, meaning that the PPP peaks correspond to LM phase transition, beyond which PPP starts degrading. *p < .05; **p < .01. \n\nOne outstanding puzzle in these data is the observation that transformer-based LMs become maximally aligned with human sentence processing relatively early in pretraining, after which their fit to human data either plateaus or decreases. We will refer to the maxima of psychological fit during training as model's tipping point. Such tipping points present a conundrum: Why does the fit to human data decrease, even as the model's language modeling loss goes down? And what is the tipping point's underlying cause? \n\nPrevious work has observed that tipping points arXiv:2502.18802v1 [cs.CL] 26 Feb 2025 \n\nin models tend to occur after around 2 billion words of pretraining, and has suggested that this number-2 billion-is operative (Oh and Schuler, 2023a). We refer to this as the 2 Billion Hypothesis. In contrast, we suspect that the tipping point at 2 billion tokens is coincidental rather than causal. Instead, we hypothesize that the reversal of PPP is related to more general pretraining dynamics, specifically the presence of phase transitions, or periods in pretraining when new model capabilities emerge rapidly. While phase transition can be a general term used to describe any rapid change in model behaviors, we focus on what we term specialized heads phrase transitions, characterized by the emergence of specialized attention patterns relatively early in pretraining (Olsson et al., 2022;Chen et al., 2024a).",
            "score": 0.4659299543450347,
            "section_title": "Peak",
            "char_start_offset": 941,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 1404,
                    "end": 1427,
                    "matchedPaperCorpusId": "258298148"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73681640625
        },
        {
            "corpus_id": "244259822",
            "title": "The Emergence of a Complex Language Skill: Evidence from the Self-organization of Interpreting Competence in Interpreting Students",
            "text": "These studies, most of which were targeted at L2 writing (e.g., Larsen-Freeman, 2006;Verspoor, Schmid & Xu, 2012), but also a few at L2 reading (Wang, 2011), L2 listening (Dong, 2016) and L2 speaking (e.g., Larsen-Freeman, 2006;Yu & Lowie, 2019), have probed into individual variation, phase transition and non-linearity manifested during the developmental process of different language skills. By analyzing the linguistic outputs (e.g., sentence length and grammatical features in speaking and writing) or processing strategies (e.g., summarization and note-taking in reading and listening) at a number of time points during L2 learning, these studies converged on the findings that language skill development is a dynamic, non-linear process with both progress and attrition, characterized by self-adaptation and self-restructuring, and sensitive to learners' initial state. \n\nDespite the revealing findings yielded in the literature, one critical issue remains relatively unclear, i.e., how relevant cognitive abilities INTERACT in the complex dynamic systems of language learners, giving rise to the emergence of language skills. Language performance, according to the DST, depends on all the linguistic resources that learners could effectively assemble under their cognitive constraints, thus language competence should be viewed as a complex system consisting of not only its linguistic components (i.e., sub-systems like phonemes, morphemes, lexicon, syntax, etc.) but also relevant cognitive resources (e.g., memory capacity) that support (or constrain) language performance. Previous studies adopting the DST perspective have explored the evolvement of sub-systems, as well as their interactions, at the linguistic level, but little research has probed into the change of inter-componential relationship at the cognitive level. The present research intends to contribute to the issue of language skills development from this perspective.",
            "score": 0.45708736766171776,
            "section_title": "Introduction",
            "char_start_offset": 3367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1947
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 85,
                    "matchedPaperCorpusId": "143873677"
                },
                {
                    "start": 85,
                    "end": 113,
                    "matchedPaperCorpusId": "62398984"
                },
                {
                    "start": 171,
                    "end": 183,
                    "matchedPaperCorpusId": "64160625"
                },
                {
                    "start": 207,
                    "end": 228,
                    "matchedPaperCorpusId": "143873677"
                },
                {
                    "start": 228,
                    "end": 245,
                    "matchedPaperCorpusId": "199837408"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60791015625
        },
        {
            "corpus_id": "272831746",
            "title": "Unveiling Language Competence Neurons: A Psycholinguistic Approach to Model Interpretability",
            "text": "Psycholinguistic research has long been concerned with uncovering the cognitive processes that underpin language use. Theories in this field suggest that linguistic competence involves knowledge of language structure (syntax), meaning (semantics), and sound (phonology) that humans utilize to comprehend and generate language. Numerous experimental paradigms have been developed to study how the human brain processes these different aspects of language. For example, research on sound symbolism (K\u00f6hler, 1967;Cassidy et al., 1999) has demonstrated that certain sounds are perceived as inherently associated with particular shapes or concepts, while studies on implicit causality (Garvey and Caramazza, 1974) have revealed how language users process and assign causal relations during sentence comprehension. These psycholinguistic tasks offer a systematic and theory-driven way to investigate language competence, extending beyond superficial linguistic performance. Moreover, psycholinguistic findings provide a rich foundation for evaluating LLMs: if a model demonstrates the ability to replicate human performance in such tasks, it suggests that the model may be capturing deeper cognitive aspects of language. \n\nRecent studies have started applying psycholinguistic tasks to evaluate LLMs. For instance, Warstadt et al. (2019) used grammatical acceptability judgments to assess syntactic competence in LLMs, while Ettinger (2020) and Futrell (2019) have tested models like BERT on psycholinguistic tasks, including syntactic ambiguity resolution and structural priming, highlighting both the strengths and limitations of LLMs in mimicking human language processing. Michaelov and Bergen (2023) examined how LLMs internalize syntactic structures through structural priming tasks, while Huang et al. (2024) assessed their ability to resolve syntactic ambiguity. Additionally, Qiu et al. (2023) investigated LLMs' capacity for pragmatic reasoning. Cai et al. (2024) performed a systematic evaluation of human-like language use in models such as ChatGPT and Vicuna, showing that LLMs closely mirror human language patterns in many respects. \n\nOur study extends this line of work by using psycholinguistic tasks to examine whether specific neurons in GPT-2-XL encode representations that mirror human language competence, providing a new level of granularity in model interpretability.",
            "score": 0.45668597387358134,
            "section_title": "Psycholinguistics and Neural Representations",
            "char_start_offset": 4960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2141
                },
                {
                    "start": 2144,
                    "end": 2385
                }
            ],
            "ref_mentions": [
                {
                    "start": 510,
                    "end": 531,
                    "matchedPaperCorpusId": "144937396"
                },
                {
                    "start": 680,
                    "end": 708,
                    "matchedPaperCorpusId": "231993965"
                },
                {
                    "start": 1419,
                    "end": 1434,
                    "matchedPaperCorpusId": "199001173"
                },
                {
                    "start": 1790,
                    "end": 1809,
                    "matchedPaperCorpusId": "268195064"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6962890625
        },
        {
            "corpus_id": "255941592",
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "text": "Real-life language use requires integrating language into a broader cognitive framework. In this section, we have shown that many capacities required for language comprehension and production are, in fact, not specific to language and are supported by distinct brain circuits. In line with this distinction, models that master many syntactic and distributional properties of human language still cannot use language in human-like ways. In particular, they struggle when engaging in formal reasoning, fail to acquire comprehensive and consistent world knowledge, cannot track objects, relations and events in long inputs, and are unable to generate utterances intentionally or infer communicative intent from linguistic input. In other words, their functional language competence remains in its infancy. This is not to say that LLMs can only ever master formal linguistic competence. Some non-linguistic abilities that contemporary LLMs succeed at include various forms of general pattern completion (\"a, ab, abc, ?\"), style transfer, and long-and short-term memory. Nevertheless, their failure to master the four functional competence domains described in this section is quite notable: in line with evidence from cognitive neuroscience, LLMs' behavior highlights the difference between being good at language and being good at thought. The stark dissociation between formal and functional language competence in both humans and contemporary LLMs raises a question: is it reasonable to model these diverse capabilities using a single system and a single objective function? We turn to this question next.",
            "score": 0.4544195446551145,
            "section_title": "Interim conclusions",
            "char_start_offset": 74607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1604
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59375
        },
        {
            "corpus_id": "261277016",
            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
            "text": "Previous work has studied the pre-training dynamics of language models (Saphra and Lopez, 2019). Choshen et al. (2022) and Evanson et al. (2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020;Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context. \n\nIndeed, previous work has studied how individual tokens are learned during pre-training. \n\nFor example, word learning is highly dependent on word frequency (Chang and Bergen, 2022b). Larger models memorize more examples during pre-training without overfitting (Tirumala et al., 2022), but the time step that a model sees an example does not affect memorization (Biderman et al., 2023). Most similar to our work, Xia et al. (2023) collect learning curves for individual tokens in context, finding that some examples exhibit a \"doubledescent\" trend where they first increase then decrease in surprisal. All of the studies above collect language model learning curves during pre-training, either for individual examples or targeted benchmark performance. Here, we introduce metrics to characterize such curves, we identify general learning patterns, and we isolate text features that are predictive of learning speed and stability.",
            "score": 0.4539565577215165,
            "section_title": "Related Work",
            "char_start_offset": 3961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1714
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 95,
                    "matchedPaperCorpusId": "267903883"
                },
                {
                    "start": 123,
                    "end": 144,
                    "matchedPaperCorpusId": "259089351"
                },
                {
                    "start": 422,
                    "end": 443,
                    "matchedPaperCorpusId": "222140842"
                },
                {
                    "start": 443,
                    "end": 460,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 942,
                    "end": 967,
                    "matchedPaperCorpusId": "238408423"
                },
                {
                    "start": 1046,
                    "end": 1069,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 1147,
                    "end": 1170,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 1198,
                    "end": 1215,
                    "matchedPaperCorpusId": "254877112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90576171875
        },
        {
            "corpus_id": "261822542",
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "text": "Most work on scaling laws (Kaplan et al., 2020) presents test loss as a quantity that homogeneously responds to the scale of training, declining by a power law relative to the size of the corpus. In the MLM setting, we instead identify a precipitous drop in the loss curve of BERT Base (Fig. 1(a)), consistently spanning 20K-30K timesteps of training across various random seeds. We now show how this rapid learning stage can be interpreted as the composition of two distinct phase transitions. \n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset. We show similar trajectories on the MultiBERTs (Sellam et al., 2022) reproductions (Appendix F). \n\nBy observing these phase transitions, we can see that the internal representation of grammar, in the form of syntactic attention, precipitates the external observation of grammatical behavior, in the form of correct language modeling judgements on linguistically challenging examples. This is not only a single breakthrough during training, but a sequence of breakthroughs that appear to be dependent on each other. We might compare this to the \"checkmate in one\" BIG-Bench task, a known breakthrough behavior in autoregressive language models (Srivastava et al., 2022). Only at a large scale can models accurately identify checkmate moves, but further exploration revealed that the model was progressing in a linear fashion at offering consistently valid chess moves before that point.",
            "score": 0.4517074706112117,
            "section_title": "THE SYNTAX ACQUISITION PHASE",
            "char_start_offset": 11881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1356
                },
                {
                    "start": 1359,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2145
                }
            ],
            "ref_mentions": [
                {
                    "start": 1307,
                    "end": 1328,
                    "matchedPaperCorpusId": "235683534"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85205078125
        },
        {
            "corpus_id": "271161651",
            "title": "Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?",
            "text": "It is widely recognized that the capabilities of language models are predicated upon two distinct phases: pre-training and fine-tuning.On one hand, the method of pretraining is acknowledged to endow models with an understanding of context.Since it necessitates that language models predict masked words by comprehending the context (or the surrounding words), to elucidate, when describing a dog, one needs to employ descriptions such as a tailed mammal, mankind's best friend, etc., leveraging context to grasp the meaning of dog.On the other hand, the fine-tuning process equips language models with a degree of obedience to instructions and the ability for sustained dialogue, with its efficacy contingent upon the volume, variety of the final dataset, and the process of the model fine-tuning.\n\nContemporary large language models rely on the aforementioned pre-training and fine-tuning learning processes.Consequently, multiple-choice questions serve as one of the effective methods to evaluate the level of reasoning ability of the language model in a specific domain.This implies that the language model needs to provide answers according to the domain knowledge learned during the pre-training phase and the comprehensive abilities acquired during the fine-tuning phase, as required by the question.\n\nThe content we aim to evaluate primarily unfolds from two aspects:\n\n1.The themes encompassed by the multiple-choice questions represent the knowledge being assessed, aiming to evaluate the understanding of the model in terms of the concepts and semantics demonstrated within the sentences.2. This question format supports both the different mechanisms expression and the examination of the language model's logical reasoning abilities, assessing whether it can answer based on the fundamental principles of gold nanoparticle synthesis.\n\nIn order to obtain the binary accuracy of the model, for each multiple-choice questions, one point will be given to the model if it selected the gold answer, otherwise zero.This process will also be repeated with different temperature settings, i.e., from 0.1 to 0.9 with 5 steps.Finally, the average score of each model will be ranked.\n\nAs illustrated in Figure 3a, all models significantly surpass the random guessing baseline of 25% with a remarkable margin, consistent with their capabilities in general tasks.",
            "score": 0.4515202199135936,
            "section_title": "Results of Accuracy with Temperature",
            "char_start_offset": 15427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 531
                },
                {
                    "start": 531,
                    "end": 797
                },
                {
                    "start": 799,
                    "end": 909
                },
                {
                    "start": 909,
                    "end": 1073
                },
                {
                    "start": 1073,
                    "end": 1306
                },
                {
                    "start": 1308,
                    "end": 1374
                },
                {
                    "start": 1376,
                    "end": 1378
                },
                {
                    "start": 1378,
                    "end": 1597
                },
                {
                    "start": 1597,
                    "end": 1843
                },
                {
                    "start": 1845,
                    "end": 2018
                },
                {
                    "start": 2018,
                    "end": 2125
                },
                {
                    "start": 2125,
                    "end": 2181
                },
                {
                    "start": 2183,
                    "end": 2359
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59228515625
        },
        {
            "corpus_id": "270620163",
            "title": "Probing the Emergence of Cross-lingual Alignment during LLM Training",
            "text": "Concurrently, there is a rising interest in understanding the training dynamics of LLMs.Works that mainly examine monolingual English models report a steady trend in the acquisition of linguistic knowledge.Both Xia et al. (2023) and Choshen et al. (2022) argue that language acquisition undergoes the same order of phase transitions consistently across model scales, training objectives and random seeds.Chen et al. (2024) find that the emergence of syntactic structure in the attention scores of Transformer-based LMs is essential for grammar acquisition in LMs, but does not account for semantic knowledge acquisition.For multilingual training, Choenni et al. (2023) examine how data size and language variance affect the performance during fine-tuning.The experiments presented by Blevins et al. (2022) are the most reminiscent of      our work.They focus on the inconsistency between the emergence of in-language and cross-language abilities for encoder LMs, whereas we study the dynamics of neuron overlaps and the corresponding impact on downstream performance in autoregressive LMs.",
            "score": 0.45133094572016996,
            "section_title": "Knowledge Acquisition during Pre-training.",
            "char_start_offset": 21398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 88,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 620
                },
                {
                    "start": 620,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 848
                },
                {
                    "start": 848,
                    "end": 1089
                }
            ],
            "ref_mentions": [
                {
                    "start": 211,
                    "end": 228,
                    "matchedPaperCorpusId": "254877112"
                },
                {
                    "start": 233,
                    "end": 254,
                    "matchedPaperCorpusId": "237491997"
                },
                {
                    "start": 404,
                    "end": 422,
                    "matchedPaperCorpusId": "261822542"
                },
                {
                    "start": 647,
                    "end": 668,
                    "matchedPaperCorpusId": "258832559"
                },
                {
                    "start": 784,
                    "end": 805,
                    "matchedPaperCorpusId": "249017999"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69287109375
        },
        {
            "corpus_id": "277467536",
            "title": "Implicit In-Context Learning: Evidence from Artificial Language Experiments",
            "text": "Humans can acquire complex patterns implicitly through brief exposure, known as implicit learning, as evidenced by decades of artificial grammar and language-learning experiments in cognitive science (e.g. Reber, 1967;Saffran et al., 1996;G\u00f3mez & Gerken, 2000). Recent advancements in large language models (LLMs) have demonstrated similar remarkable linguistic capabilities, achieving human-like performance on diverse language tasks through in-context learning (ICL) -the ability to rapidly generalize from limited examples without explicit training updates (Brown et al., 2020;Webb et al., 2023). However, despite the striking parallels in outcomes, the cognitive and computational mechanisms underlying such rapid, example-based generalization in LLMs remain largely unknown. \n\nExisting research on in-context learning in LLMs is fragmented and primarily applicationoriented, often relying on heuristic methods such as prompt engineering or intuition-driven experimentation (e.g. Mavromatis et al., 2023;Ye et al., 2022). These approaches provide practical insights but fall short of elucidating underlying cognitive or computational processes. Concurrently, another line of research attempts to link transformer-based models to cognitive models, leveraging training or fine-tuning paradigms to explore representational similarities and cognitive plausibility (Goldstein et al., 2022;Binz & Schulz, 2023). However, these studies predominantly investigate learning mechanisms at the training or fine-tuning stages. Crucially, the process by which pre-trained models rapidly acquire structured linguistic knowledge during inference -when presented with limited examplesremains notably understudied. \n\nGiven this knowledge gap, the present study adopts an explicitly exploratory approach, systematically investigating whether LLMs exhibit implicit learning capacities analogous to human learners at the inferencing level. Our approach is firmly grounded in well-established cognitive science paradigm. Specifically, we adapt three classical artificial language learning experiments that have provided robust insights into human implicit learning across different linguistic domains: morphology (Schuler et al., 2016;Schuler, 2017), morphosyntax (Valian & Coulson, 1988), and finite-state syntax (Alamia et al., 2020).",
            "score": 0.4494701797368739,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 218,
                    "matchedPaperCorpusId": "145282756"
                },
                {
                    "start": 218,
                    "end": 239,
                    "matchedPaperCorpusId": "13321604"
                },
                {
                    "start": 239,
                    "end": 260,
                    "matchedPaperCorpusId": "15933380"
                },
                {
                    "start": 560,
                    "end": 580,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 580,
                    "end": 598,
                    "matchedPaperCorpusId": "254854575"
                },
                {
                    "start": 984,
                    "end": 1008,
                    "matchedPaperCorpusId": "264829036"
                },
                {
                    "start": 1364,
                    "end": 1388,
                    "matchedPaperCorpusId": "247316687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63623046875
        },
        {
            "corpus_id": "267523425",
            "title": "Six Fallacies in Substituting Large Language Models for Human Participants",
            "text": "The combination of pretraining (optimization for next-token prediction), post-training (finetuning), and prompt and inference design (e.g., reasoning instruction and inference time) enables LLMs to perform tasks once thought to exclusively require human intelligence (Box 1). This process equips LLMs with intricate linguistic knowledge, including syntactic rules and semantic relations-referred to as formal linguistic competence 16 . It allows LLMs to transcend mere memorization, enabling them to tackle complex, context-dependent tasks in language processing and generation 16,22 . It also empowers LLMs to infer underlying task structures and generate contextually appropriate responses-a form of instrumental knowledge that enhances their ability to solve nontrivial tasks in diverse contexts 23 . \n\nYet, at their core, LLMs have no minds 24 but are autoregressive statistical models that manipulate language-a task that is fundamentally different from that of humans, thus producing a kind of singular, ungrounded intelligence that fundamentally differs from  25 . \n\nIn contrast, tokens in LLMs lack real-world referents, meaning, and experience (cf. symbols in symbolic systems) 26 . LLMs manipulate tokens based solely on statistical patterns learned during training, devoid of real-world grounding, much like how Church encoding in lambda calculus represents data and operators purely through abstract functions defined by their input-output relationships-without any inherent meaning 27 . This lack of grounding detaches the models from the very physical and social realities they simulate, hindering them from obtaining functional linguistic competence-the use of language to achieve goals in the world 16 -and from acquiring the worldly knowledge necessary to approximate world models-representations of the real world that are structure-preserving and behaviorally efficacious 23,28 , such as cognitive maps, body schemas, or spatial schemas 29 . \n\nThus, while LLMs manifest proficient language use that contrasts with simply searching prerecorded strings of text (a lookup table of all possible conversations, as in Block's hypothetical Blockhead machine), they are not true language users: they do not possess intrinsic meaning, communicative intentions, or other internal states essential to human language users 30 .",
            "score": 0.4494026716159999,
            "section_title": "Token prediction as human intelligence fallacy",
            "char_start_offset": 8498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 1071
                },
                {
                    "start": 1074,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1960
                },
                {
                    "start": 1963,
                    "end": 2334
                }
            ],
            "ref_mentions": [
                {
                    "start": 431,
                    "end": 433,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 578,
                    "end": 581,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 799,
                    "end": 801,
                    "matchedPaperCorpusId": "263780995"
                },
                {
                    "start": 845,
                    "end": 847,
                    "matchedPaperCorpusId": "55303721"
                },
                {
                    "start": 1067,
                    "end": 1069,
                    "matchedPaperCorpusId": "268608487"
                },
                {
                    "start": 1495,
                    "end": 1497,
                    "matchedPaperCorpusId": "14181275"
                },
                {
                    "start": 1715,
                    "end": 1717,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 1891,
                    "end": 1894,
                    "matchedPaperCorpusId": "263780995"
                },
                {
                    "start": 1894,
                    "end": 1896,
                    "matchedPaperCorpusId": "254854575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7041015625
        },
        {
            "corpus_id": "267782481",
            "title": "Unveiling Linguistic Regions in Large Language Models",
            "text": "By conducting further pretraining across six languages and evaluating models' parameter importance (Section 2.2), we discover a region in LLMs corresponding to the core linguistic competence, which accounts for approximately 1% of the model's total parameters. As shown at the top of Figure 1, removing this region (setting parameters to zero) consistently leads to a significant decline in performance across 30 test languages (Section 3.2). \n\nFurthermore, by visualizing the core linguistic region (Figure 2), we observe that the linguistic core region of LLMs exhibits significant dimensional dependence. In certain dimensions, only perturbing a single parameter could lead to the model losing its linguistic competence (Section 3.3). Additionally, ablation study in 3.3 shows that beyond outlier dimensions, other non-outlier dimensions in this region are also critical. \n\nQ2: Beyond the core linguistic region within LLMs , do distinct monolingual regions exist that specifically influence individual languages? While LLMs possess strong multilingual capabilities, we discover that each individual language (or language with similar compositional elements or grammatical structures) encompasses independent regions within the LLMs. As shown in the middle of Figure 1, the analysis of the Russian sentences identifies a particular linguistic region that likewise exerts influence both on the Russian and Ukrainian language, both of which belong to the Slavic group (Section 3.4). \n\nQ3: If and how core linguistic regions affect further pre-training, how to utilize it to optimize further pre-training? After pre-training, core linguistic parameter regions of the LLMs are established for multilingual alignment. Notable shifts in these regions potentially lead to a decline in model lingual capabilities. Our findings reveal that freezing this core region can mitigate the issue of catastrophic forgetting (McCloskey and Cohen, 1989;Kemker et al., 2018), a common phenomenon observed during further pre-training of LLMs. As shown at the bottom of Figure 1, we investigate the impact of selectively freezing 5% key parameters of all parameters during further pre-training, compared to the full-scale fine-tuning technique. Findings indicate that this method facilitates comparable learning of the target language while concurrently decelerating the rate of language attrition for previously learned languages (Section 3.5).",
            "score": 0.4493137734100676,
            "section_title": "Core Linguistic Regions Distinct Monolingual Regions",
            "char_start_offset": 2493,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1483
                },
                {
                    "start": 1486,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2225
                },
                {
                    "start": 2226,
                    "end": 2426
                }
            ],
            "ref_mentions": [
                {
                    "start": 1937,
                    "end": 1957,
                    "matchedPaperCorpusId": "22910766"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6396484375
        },
        {
            "corpus_id": "261277016",
            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
            "text": "Language models have received unprecedented attention in recent years due to impressive performance on natural language tasks (e.g. OpenAI, 2022;Google, 2023;Anthropic, 2023). However, these models are initialized as random word (token) generators, and it remains unclear how the models achieve complex linguistic abilities during pre-training. Previous work has investigated when syntactic, semantic, and reasoning abilities emerge (Liu et al., 2021;Evanson et al., 2023), quantified ages of acquisition for tokens averaged over contexts (Chang and Bergen, 2022b), and extracted learning curves for individual examples (Xia et al., 2023). However, features that influence individual learning curves have yet to be identified (e.g. n-gram probabilities and context lengths). Given any token in context, it is largely unknown when or how stably that token would be learned. \n\nFrom a scientific perspective, understanding when examples are learned by language models can provide insights into possible mechanisms for language acquisition. Regardless of their similarity to human language processing, language models are exemplars of how learning from language statistics alone (i.e. \"distributional\" learning) can lead to complex linguistic abilities (Chang and Bergen, 2022b;Warstadt and Bowman, 2023;Mahowald et al., 2023). Notably, despite smoothly decreasing corpus-level loss and independent and identically distributed (i.i.d.) data throughout pre-training, individual text examples exhibit learning curves with sudden decreases and increases in loss ( \u00a75 and Xia et al., 2023). This highlights the importance of examining individual example learning curves for pre-training dynamics research; aggregate curves often do not capture the fluctuations exhibited by individual examples. Our work seeks to characterize these fine-grained convergence patterns in terms of simpler distributional statistics. \n\nFrom a practical perspective, understanding language model learning curves can inform the pre-training and deployment of language models. Learning curve results might allow NLP practitioners to determine how much pre-training is necessary for different capabilities and what behaviors will remain stable after additional pre-training (e.g. \"continual learning\" on more recent data; Jin et al., 2022).",
            "score": 0.44848114772615866,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1904
                },
                {
                    "start": 1907,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2246
                },
                {
                    "start": 2247,
                    "end": 2307
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 451,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 451,
                    "end": 472,
                    "matchedPaperCorpusId": "259089351"
                },
                {
                    "start": 539,
                    "end": 564,
                    "matchedPaperCorpusId": "238408423"
                },
                {
                    "start": 620,
                    "end": 638,
                    "matchedPaperCorpusId": "254877112"
                },
                {
                    "start": 1249,
                    "end": 1274,
                    "matchedPaperCorpusId": "238408423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "259202736",
            "title": "Revealing the structure of language model capabilities",
            "text": "These benchmarks provide a standardized way of testing LLMs across a variety of tasks, many of which were designed to test specific cognitive abilities. With data from this new population of LLMs across a range of these benchmark tasks, it is now becoming feasible to use an individual differences approach to investigate the structure of language model capabilities. \n\nWhat hypotheses, though, might we make about the structure of language model capabilities? It seems likely that the capabilities of LLMs are multifaceted. After all, we can find this multifaceted pattern across a number of different species, including humans, pigeons, and chimpanzees [10,29]. This idea also fits with the existing literature on LLMs-for example, there is evidence that new abilities can suddenly \"emerge\" at particular scales [34,30], which is consistent with the idea that certain abilities can be dissociated from one another. What is more uncertain is how these abilities are structured. For example, can the ability of language models to comprehend language be largely explained by a single broad ability, as is the case in humans [29]? Or are there multiple distinct abilities involved? Similarly, a range of studies have investigated the ability of LLMs to perform various kinds of reasoning, including quantitative reasoning, deductive reasoning, and commonsense reasoning [26,6,18]. But it remains unclear whether these different kinds of tasks rely on a set of distinct abilities or rather on a single, underlying reasoning ability. \n\nOnce we have identified the broad abilities of LLMs, we can begin to examine how model properties affect these different abilities. For example, a great deal of work has already been done to examine how the scale of a model affects its performance. In general, there is evidence increasing model size tends to improve performance, a pattern often referred to as \"scaling laws\" [30,15]. This pattern mirrors findings from cognitive science showing relationships between brain size and intelligence [13,20]. But at the level of specific tasks, the findings from the LLM literature are more complex and difficult to interpret. For example, data from BIG-bench suggests that model scale is closely related to performance on some tasks, but bears no relationship to performance on other tasks [30]. What are we to make of this pattern?",
            "score": 0.4483869426615086,
            "section_title": "Introduction",
            "char_start_offset": 2160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 367
                },
                {
                    "start": 370,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1529
                },
                {
                    "start": 1532,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2325
                },
                {
                    "start": 2326,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 659,
                    "end": 662,
                    "matchedPaperCorpusId": "148467552"
                },
                {
                    "start": 1123,
                    "end": 1127,
                    "matchedPaperCorpusId": "148467552"
                },
                {
                    "start": 1374,
                    "end": 1377,
                    "matchedPaperCorpusId": "250144408"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "210898819",
            "title": "Criticality in Pareto Optimal Grammars?",
            "text": "What are relevant levels of description when investigating human language? How are these levels connected to each other? Does one description yield smoothly into the next one such that different models lie naturally along a hierarchy containing each other? Or, instead, are there sharp transitions between one description and the next, such that to gain a little bit accuracy it is necessary to change our framework radically? Do different levels describe the same linguistic aspects with increasing (or decreasing) accuracy? Historically, answers to these questions were guided by intuition and resulted in subfields of study, from phonetics to syntax and semantics. Need for research at each level is acknowledged, but seldom are these different aspects brought together (with notable exceptions). Here, we propose a methodology to inspect empirical corpora systematically, and to extract from them, blindly, relevant phenomenological scales and interactions between them. Our methodology is rigorously grounded in information theory, multi-objective optimization, and statistical physics. Salient levels of linguistic description are readily interpretable in terms of energies, entropies, phase transitions, or criticality. Our results suggest a critical point in the description of human language, indicating that several complementary models are simultaneously necessary (and unavoidable) to describe it.",
            "score": 0.4478492255984838,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36767578125
        },
        {
            "corpus_id": "268201735",
            "title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
            "text": "The difference in problem-solving performance is significantly greater than the difference in  potentially showing that either pre-training or finetuning of LLMs can encode sufficient knowledge into the model, but the final task performance does not just depend on the amount or quality of knowledge. \n\nNotably, the crystallized performance (s1) of LLaMA 3-Instruct and LLaMA 3.1-Instruct has significantly improved compared to LLaMA 2-Chat. For example, in social modeling, LLaMA 3-Instruct scored 76.99, and LLaMA 3.1-Instruct scored 77.78, compared to LLaMA 2-Chat's 71.06. This suggests that these models better encode knowledge relevant to higher-level cognitive tasks, likely due to training and fine-tuning LLaMA 3 on higher-quality and larger-scale data compared to the LLaMA 2 model. Linguistic capabilities show a relatively weak correlation with cognitive capabilities. Figure 2 presents the correlation results between different capabilities. Both the language-related and cognitionrelated capabilities exhibit stronger (Pearson's r > 0.7 (Krippendorff, 2004)) intra-dimension correlation (e.g. world modeling vs. social modeling) when compared to inter-dimension correlation (e.g. world modeling vs. linguistic knowledge). This indicates that excellence in language processing does not necessarily equate to a similar level of cognitive capability. These results can also be observed from direct prompt evaluation, which assesses problem-solving performance (s 3 ) without prompting intermediate reasoning steps, further demonstrating the rationality of dissociating language and cognition. A possible reason behind it could be that dedicated structures of the model or subsets of parameters are highly correlated with language (Zhang et al., 2024b;Tang et al., 2024), whereas others serve as cognition (Chen et al., 2023), and they are optimized at different training stages and function as different mechanisms during inference, which has been verified by recent studies on knowledge locating and editing of LLMs Dai et al. (2022); Meng et al. (2022); Zhang et al. (2024a). The crystallized step impacts problem-solving more than the fluid step.",
            "score": 0.44658966452397525,
            "section_title": "Main results",
            "char_start_offset": 16984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 1051,
                    "end": 1070,
                    "matchedPaperCorpusId": "15407535"
                },
                {
                    "start": 2028,
                    "end": 2045,
                    "matchedPaperCorpusId": "233296761"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60888671875
        },
        {
            "corpus_id": "264451956",
            "title": "BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?",
            "text": "Implementing RLHF has shown promising results in aligning foundation models with human preferences. By using RLHF, models can undergo subtle yet significant improvements, such as refining tone (Liu, 2023), reducing biases and toxic elements (Bai et al., 2022), and enabling domain-specific content generation (Bang et al., 2023). The primary goal of this research is to explore whether the small pretrained model, with its limited data size, can also benefit from RLHF, thus potentially improving its overall performance. \n\nThe performance of small language models (SLMs) trained on large datasets has been observed to be poor, generating incoherent and repetitive text. Training large language models on limited data can lead to overfitting, making smaller models a potential solution to prevent overfitting (Warstadt et al., 2020c). Inspired by how humans acquire language and the BabyLM shared task, we explore downsizing the language used in models to observe the effects of pretraining. The main questions are whether small language models can generate coherent English text and if this ability is limited to larger, more complex models. It is also questioned whether the limited capacity of small models to memorize linguistic features-such as syntax, semantics, morphology, and phonology-leads to less creative outputs compared to larger models. For example, linguistic features are crucial for understanding and generating text, with a broader grasp potentially enabling more creative language use. Larger models, with their increased capacity, might capture a wider range of these features, possibly leading to more creative and nuanced language outputs. Conversely, small models might only learn basic or frequent linguistic patterns, potentially limiting their creative language generation capabilities. Previous research indicates that models can learn linguistic features with limited pretraining data but need more data to prioritize linguistic generalizations over superficial ones (Warstadt et al., 2020c). Some models fail to effectively use the linguistic features they learn during fine-tuning for natural language understanding tasks. The study aims to investigate whether GPT-2 models of varying sizes can acquire specific language patterns when finetuned with reinforcement learning and human feedback, aiming to enhance the model's storytelling abilities.",
            "score": 0.44651350686036206,
            "section_title": "Introduction",
            "char_start_offset": 2337,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2154
                },
                {
                    "start": 2155,
                    "end": 2378
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.501953125
        },
        {
            "corpus_id": "270285954",
            "title": "What Makes Language Models Good-enough?",
            "text": "Does a shallow architecture lead to good-enough language processing?Our first hypothesis is that a good-enough model requires a shallow architecture because it does not have to make a detailed syntactic representation of linguistic input.We confirm this hypothesis by finding that the shallow models exhibit a human-like good-enough performance in a way similar to their deeper version.Specifically, increasing the number of layers from 8 to 12 does not improve the models' human-like performance considerably.\n\nOur results shed light on whether language models can learn the dissociation between formal linguistic competence-knowledge of grammar-and functional linguistic competence-the ability to use language in real-world situations (Mahowald et al., 2023).The advent of seemingly well-behaved neural language models leads to an intensive investigation of their formal linguistic competence (Marvin and Linzen, 2018;Futrell et al., 2019;Warstadt et al., 2019bWarstadt et al., ,a, 2020)).Mahowald et al. (2023) conclude that language models show promising results in learning abstract linguistic rules and patterns, but it remains to be seen whether they can learn functional linguistic competence.Our results point to the possibility that deep as well as shallow models can learn human-like language processing that is good-enough for simple language use.Do fewer heads lead to good-enough language processing?Our second hypothesis is that the aimed model requires fewer attention heads because it does not need a strong working memory system.We find that among our model set, the model with the largest number of heads (H = 12) shows decreasing accuracy as the memory load increases, in a similar way as humans.This result does not confirm our hypothesis.\n\nWe can explain this result if the parallelism between the self-attention mechanism in Transformers and the human working memory system is specific to the retrieval phase.The human working memory system involves three phases, encoding, storage, and retrieval, at a coarse level of granularity (Baddeley, 1986(Baddeley, , 2000)).The models as well as humans achieve accuracy at the ceiling on items with correct entailment/yes labels, suggesting that they can store words that appear in contexts/premises.Thus, the observed accuracy pattern might not have to do with the storage phase of the working memory.",
            "score": 0.44559510514721995,
            "section_title": "Discussion",
            "char_start_offset": 27876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 68,
                    "end": 238
                },
                {
                    "start": 238,
                    "end": 386
                },
                {
                    "start": 386,
                    "end": 510
                },
                {
                    "start": 512,
                    "end": 761
                },
                {
                    "start": 761,
                    "end": 991
                },
                {
                    "start": 991,
                    "end": 1201
                },
                {
                    "start": 1201,
                    "end": 1359
                },
                {
                    "start": 1359,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1547
                },
                {
                    "start": 1547,
                    "end": 1716
                },
                {
                    "start": 1716,
                    "end": 1760
                },
                {
                    "start": 1762,
                    "end": 1932
                },
                {
                    "start": 1932,
                    "end": 2089
                },
                {
                    "start": 2089,
                    "end": 2265
                },
                {
                    "start": 2265,
                    "end": 2367
                }
            ],
            "ref_mentions": [
                {
                    "start": 895,
                    "end": 920,
                    "matchedPaperCorpusId": "52113185"
                },
                {
                    "start": 920,
                    "end": 941,
                    "matchedPaperCorpusId": "72940921"
                },
                {
                    "start": 941,
                    "end": 963,
                    "matchedPaperCorpusId": "44072099"
                },
                {
                    "start": 2069,
                    "end": 2088,
                    "matchedPaperCorpusId": "14333234"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38671875
        },
        {
            "corpus_id": "276903421",
            "title": "Emergent Abilities in Large Language Models: A Survey",
            "text": "The term emergent in the context of LLMs is often used to describe capabilities that arise implicitly as models learn language patterns and structures through next-token prediction. These abilities are assessed through few-shot or zero-shot prompting, where models generalize to new tasks without undergoing explicit fine-tuning. This process, known as incontext learning (ICL), allows LLMs to infer new patterns and concepts solely from contextual information provided in the prompt. \n\nUnlike Wei et al.'s [87] definition of emergent abilities, which emphasizes sudden performance jumps with increased model scale, in-context learning does not necessarily require abrupt improvements. Instead, it refers to the gradual development of capabilities that enable LLMs to perform tasks for which they have not been explicitly trained. The research in this area primarily seeks to understand why LLMs generalize to new tasks without fine-tuning, what aspects of the training process contribute to this phenomenon, and how prompt design can be optimized to maximize ICL efficiency. \n\nTo explain in-context learning, various theories have emerged, ranging from statistical and structural perspectives to cognitive and algorithmic analogies. Some researchers attribute ICL to properties of input data distribution and label space structure [13,57], while others suggest that exposure to a diverse range of tasks during multitask-prompted learning facilitates generalization [69]. Another line of work explores how pretraining term frequencies influence a model's ability to recall and recombine information [66]. Some studies frame ICL as a form of Bayesian inference over the latent space of language [94], while others view it as a compositional recombination of linguistic structures [32]. Mechanistically, ICL has been linked to neural architectures, such as induction heads [24,59] and functional modules that emerge naturally during training [7,43,81,84]. Arora and Goyal [2] provide a theoretical framework in which LLMs develop skills through a bipartite \"skill graph\" that links training data to fundamental reasoning abilities, demonstrating how compositional generalization emerges as models scale. \n\nBeyond theoretical explanations, studies have examined the role of training data and model architecture in shaping ICL.",
            "score": 0.4437833841268616,
            "section_title": "IV. EMERGENT ABILITIES AS IN-CONTEXT LEARNING",
            "char_start_offset": 47006,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2201
                },
                {
                    "start": 2204,
                    "end": 2323
                }
            ],
            "ref_mentions": [
                {
                    "start": 1332,
                    "end": 1336,
                    "matchedPaperCorpusId": "248665718"
                },
                {
                    "start": 1940,
                    "end": 1943,
                    "matchedPaperCorpusId": "258999187"
                },
                {
                    "start": 1943,
                    "end": 1946,
                    "matchedPaperCorpusId": "246823378"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "270847724",
            "title": "Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability",
            "text": "Large language models (LLMs) have emerged as powerful tools for many AI problems and exhibit remarkable in-context learning (ICL) capabilities. Compositional ability, solving unseen complex tasks that combine two or more simple tasks, is an essential reasoning ability for Artificial General Intelligence. Despite the tremendous success of LLMs, how they approach composite tasks, especially those not encountered during the pretraining phase, remains an open and largely underexplored question. In this study, we delve into the ICL capabilities of LLMs on composite tasks, with only simple tasks as in-context examples. We develop a test suite of composite tasks including linguistic and logical challenges and perform empirical studies across different LLM families. We observe that models exhibit divergent behaviors: (1) For simpler composite tasks that apply distinct mapping mechanisms to different input segments, the models demonstrate decent compositional ability, while scaling up the model enhances this ability; (2) for more complex composite tasks involving reasoning multiple steps, where each step represents one task, models typically underperform, and scaling up generally provides no improvements. We offer theoretical analysis in a simplified setting, explaining that models exhibit compositional capability when the task handles different input parts separately. We believe our work sheds new light on the capabilities of LLMs in solving composite tasks regarding the nature of the tasks and model scale. Our dataset and code are available at {\\url{https://github.com/OliverXUZY/LLM_Compose}}.",
            "score": 0.44348185938193907,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7275390625
        },
        {
            "corpus_id": "276902722",
            "title": "How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders",
            "text": "Large Language Models (LLMs) demonstrate remarkable multilingual capabilities and broad knowledge. However, the internal mechanisms underlying the development of these capabilities remain poorly understood. To investigate this, we analyze how the information encoded in LLMs' internal representations evolves during the training process. Specifically, we train sparse autoencoders at multiple checkpoints of the model and systematically compare the interpretative results across these stages. Our findings suggest that LLMs initially acquire language-specific knowledge independently, followed by cross-linguistic correspondences. Moreover, we observe that after mastering token-level knowledge, the model transitions to learning higher-level, abstract concepts, indicating the development of more conceptual understanding.",
            "score": 0.440952831375588,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95263671875
        },
        {
            "corpus_id": "259212162",
            "title": "Limits for learning with language models",
            "text": "Our results are particularly relevant to downstream tasks that require an agent to not only create fluent, creative and contextually relevant speech but also to act precisely based on the meaning of linguistic expressions and reliably recognize semantic inconsistency. For a robot that has been instructed (via conversation) to tighten every screw of a door, to never walk on an airplane wing, or to stop drilling immediately if certain conditions hold, acting appropriately requires being able to infer what do to based on the linguistic meaning of the words every, never, stop, immediately and if -and in these cases, getting things mostly right won't do, especially if lives or substantial economic loss are at risk.\n\nAn important corollary of our argument is that while it might be tempting to separate reasoning and linguistic competence (Mahowald et al., 2023), the former is in fact inextricably tied to our ability to draw inferences based on linguistic content-not just on, say, mathematical or real-world facts. This in turn suggests that approaches which attempt to patch up knowledge deficiencies for LLMs by giving them access to external models (Mialon et al., 2023) will fall short in developing reliable models of linguistic understanding because LLMs fail to grasp the notions that underlie the very way that sentences (and actions) are woven together in conversation.\n\nEmpirical studies like Chaturvedi et al. (2022) show that LLM failures to respect semantic entailment in question answering tasks follow from fundamental features of LLM training; thus while extensive training and large data sets may improve LLM results, performance will inevitably remain unstable and we should continue to expect hallucinations and reasoning errors in NLP tasks like question-answering and natural language inference.\n\n3 Language models and formal semantics with continuations",
            "score": 0.44065039152789154,
            "section_title": "Context",
            "char_start_offset": 2654,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1410,
                    "end": 1434,
                    "matchedPaperCorpusId": "254926379"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61181640625
        },
        {
            "corpus_id": "271892176",
            "title": "A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive Language Models",
            "text": "Recent studies on logical reasoning in Language Models (LMs) have sparked a debate on whether they can learn systematic reasoning principles during pre-training or merely exploit superficial patterns in the training data. This paper presents a mechanistic interpretation of syllogistic reasoning in LMs to advance the understanding of internal dynamics. Specifically, we present a methodology for circuit discovery aimed at interpreting content-independent reasoning mechanisms. Through two distinct intervention methods, we uncover a sufficient and necessary circuit involving middle-term suppression that elucidates how LMs transfer information to derive valid conclusions from premises. Furthermore, we investigate how belief biases manifest in syllogistic reasoning, finding evidence of partial contamination from additional attention heads responsible for encoding commonsense and contextualized knowledge. Finally, we explore the generalization of the discovered mechanisms across various syllogistic schemes, model sizes and architectures, finding that the identified circuit is sufficient and necessary for the schemes on which the models achieve high downstream accuracy (>60%), and that the activation patterns apply to models of different families. Overall, our findings suggest that LMs indeed learn transferable content-independent reasoning mechanisms, but that, at the same time, such mechanisms do not involve generalizable and abstract logical primitives, being susceptible to contamination by the same world knowledge acquired during pre-training.",
            "score": 0.43779406003318855,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "255941592",
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "text": "Both the \"good at language -> good at thought\" and the \"bad at thought -> bad at language\" fallacies stem from the conflation of language and thought, and both can be avoided if we distinguish between two kinds of linguistic competence: formal linguistic competence (the knowledge of rules and statistical regularities of language) and functional linguistic competence (the ability to use language in the real world, which often draws on non-linguistic capacities). Of course, language does not live in a vacuum and is fundamentally embedded and social, so the formal capacity is of limited value without being integrated in a situated context [e.g., Clark, 1996, Hudley et al., 2020, Bucholtz and Hall, 2005, Labov, 1978, Wittgenstein, 1953, Grice, 1975, Lakoff, 1972, Clark, 1992]. But even solving the more restricted problem of formal linguistic competence (e.g., what counts as a valid string of a language) is far from trivial and indeed has been a major goal of modern linguistics. \n\nOur motivation for the distinction between formal and functional linguistic competence comes from the human brain. A wealth of evidence from cognitive science and neuroscience has established that language and thought in humans are robustly dissociable: the machinery dedicated to processing language is separate from the machinery responsible for memory, reasoning, and social skills [e.g., Fedorenko and Varley, 2016a, ;Section 2]. Armed with this distinction, we evaluate contemporary LLM performance and argue that LLMs have promise as scientific models of one piece of the human cognitive toolbox-formal language processing-but fall short of modeling human thought. \n\nUltimately, what \"pure\" LLMs can learn is necessarily constrained both by the information available in their training data and by whether that information is learnable through a word prediction mechanism.",
            "score": 0.4369576944145892,
            "section_title": "Introduction",
            "char_start_offset": 4711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1661
                },
                {
                    "start": 1664,
                    "end": 1868
                }
            ],
            "ref_mentions": [
                {
                    "start": 741,
                    "end": 754,
                    "matchedPaperCorpusId": "148132585"
                },
                {
                    "start": 1383,
                    "end": 1413,
                    "matchedPaperCorpusId": "5043404"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.463134765625
        },
        {
            "corpus_id": "273098400",
            "title": "Intelligence at the Edge of Chaos",
            "text": "Thus, the ability to learn from past states may be key to the model's success in adapting to diverse problems. \n\nCertain ECA rules, such as Rule 110, are known to be computationally irreducible, meaning their behavior cannot be predicted without directly computing each step (Wolfram, 1997). However, some studies suggest that even in these systems, partial predictability can be achieved under certain conditions (Israeli & Goldenfeld, 2004). This implies that models learning more complex, nontrivial solutions can actually outperform simpler, irreducible approaches by leveraging approximate but efficient predictions. Rather than directly calculating each state, models can explore patterns and generalize from past states, potentially leading to solutions that are not only more robust across tasks but also more efficient than the irreducible solution. \n\nBroader Impact Our findings connect to a larger body of work on the emergence of intelligence in large language models (LLMs). Understanding how LLMs develop sophisticated reasoning capabilities when trained on relatively simple data could offer new insights into why and how intelligence emerges in these models. This research may help shed light on some of the open questions surrounding LLMs, particularly how their internal representations evolve and how certain training conditions lead to more transferrable reasoning abilities. \n\nIn future work, this framework can be further explored by training larger LLMs on synthetic data generated by simple rule-based systems. Incorporating measures of complexity, such as those used in this study, could provide a valuable tool for prioritizing and curating data, ensuring that models are exposed to information with the right balance of structure and randomness. This aligns with recent advances in data curation, where data quality and complexity, rather than quantity, is increasingly emphasized in improving model performance (Zhao et al., 2023;Cao et al., 2023;Liu et al., 2023). \n\nAdditionally, this work may have implications for our understanding of human intelligence, which is proposed to have evolved as a mechanism for interacting with a complex and hard-to-predict world (Euler, 2018). The idea that intelligence arises in systems operating at the \"edge of chaos\" parallels cognitive science theories suggesting that human brains function at a critical state between different dynamics (Cocchi et al., 2017;Hesse & Gross, 2014;O'Byrne & Jerbi, 2022).",
            "score": 0.4356098541699075,
            "section_title": "DISCUSSION",
            "char_start_offset": 31012,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 113,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1993
                },
                {
                    "start": 1996,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2472
                }
            ],
            "ref_mentions": [
                {
                    "start": 414,
                    "end": 442,
                    "matchedPaperCorpusId": "32218397"
                },
                {
                    "start": 2193,
                    "end": 2206,
                    "matchedPaperCorpusId": "52080756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "264591406",
            "title": "Unveiling A Core Linguistic Region in Large Language Models",
            "text": "Collectively, they form the 'multiple demand network.' (Duncan et al., 2020). This network is pivotal in supporting a myriad of cognitively demanding tasks, from logical deductions Figure 1: Based on the human brain (left) as a prototype, we have discovered a region in LLMs (right) that corresponds to linguistic competence. Furthermore, we have found that improvements in linguistic competence do not necessarily coincide with increases in knowledge levels, which may suggest the presence of a dissociated knowledge region. In the future, we will continue to explore the possibility of other functional regions. and mathematical analyses (Fedorenko et al., 2013;Amalric & Dehaene, 2019) to physical reasoning (Schwettmann et al., 2019;Pramod et al., 2021) and computer code understanding (Ivanova et al., 2020;Liu et al., 2020). On a related note, individuals diagnosed with semantic dementia, which primarily affects the anterior temporal lobes, often grapple with tasks centered on world knowledge. Their struggle remains consistent whether the information is presented through words or visual cues like images (Patterson et al., 2007). This phenomenon serves as a testament to the idea that while language and general world knowledge are closely intertwined in practical usage, they are underpinned by distinct neural circuits. \n\nThe regions within the human brain collaboratively form the foundation of human intelligence. We wonders if LLMs as large-scale artificial neural networks manifest similar functional regions phenomenon internally, akin to human brain. This paper embarks on a preliminary exploration, delving deeper into the intrinsic mechanisms of LLMs' intelligence. Through analysis and comparison of six languages, we discover a core region in LLMs corresponding to linguistic competence, which accounts for approximately 1% of the model's total parameters. Perturbations to this region consistently lead to a sharp decline in performance across 30 test languages. We observe that the linguistic core region of LLMs exhibits significant dimension dependence. In certain dimensions, perturbing a single parameter could lead to the model losing its linguistic competence. Additionally, further pretraining on LLaMA model with over 100 billion tokens do not yield performance improvements on C-Eval (Huang et al., 2023), a Chinese exam benchmark.",
            "score": 0.43484154267863284,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1332
                },
                {
                    "start": 1335,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2191
                },
                {
                    "start": 2192,
                    "end": 2365
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 76,
                    "matchedPaperCorpusId": "220975612"
                },
                {
                    "start": 640,
                    "end": 664,
                    "matchedPaperCorpusId": "2305069"
                },
                {
                    "start": 664,
                    "end": 688,
                    "matchedPaperCorpusId": "58647234"
                },
                {
                    "start": 790,
                    "end": 812,
                    "matchedPaperCorpusId": "216071897"
                },
                {
                    "start": 1115,
                    "end": 1139,
                    "matchedPaperCorpusId": "7310189"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.642578125
        },
        {
            "corpus_id": "265281451",
            "title": "Predictive Minds: LLMs As Atypical Active Inference Agents",
            "text": "There have been various attempts to conceptualize LLMs, explain \"how they actually work\", and understand them using existing frameworks from a variety of fields. \n\nOne class of conceptualization focuses on the fact that the LM training objective is to minimize predictive loss, and the fact LLMs are not embodied in a way comparable to humans, but trained on large datasets of text from the internet. Bender et al. coined the term 'stochastic parrots' and claim that text generated by an LM is not grounded in communicative intent, any model of the world, or any model of the reader's state of mind [1]. In a similar spirit, using framing from linguistics, Mahowald et al. conceptualize LLMs as models that are good at formal linguistic competence but incomplete at functional linguistic competence. According to this view, LLMs are good models of language but incomplete models of human thought, good at generating coherent, grammatical, and seemingly meaningful paragraphs of text, but failing in functional competence, which recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition [16]. \n\nThese reductionist views of LLMs were subject to considerable criticism. Mitchell and Krakauer, surveying the debate, note an opposing faction which argues that these networks truly understand language, can perform reasoning in a general way, and in a real sense understand concepts and capture important aspects of meaning [21]. Mitchell and Krakauer's overall conclusion is that cognitive science is currently inadequate for answering such questions about LLMs. \n\nOther conceptualizations of LLMs recognize that the trained model is a distinct object from the training process, and so that the nature of the training objective need not be shared by the resulting artifact. For example, based on experiments with LLMs autoregressively completing complex token sequences, Mirchandani et al. look at LLMs as general pattern machines, or general sequence modellers, driven by in-context learning [20]. Others extend the 'general sequence modeling' in the direction of 'general computation'.",
            "score": 0.4324265019018543,
            "section_title": "Background and related work 2.1 Conceptualizing LLMs",
            "char_start_offset": 1667,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 164,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1657
                },
                {
                    "start": 1660,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2182
                }
            ],
            "ref_mentions": [
                {
                    "start": 599,
                    "end": 602,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1518,
                    "end": 1522,
                    "matchedPaperCorpusId": "253107905"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.478271484375
        },
        {
            "corpus_id": "261822542",
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "text": "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.",
            "score": 0.4321832711590916,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "260899983",
            "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models",
            "text": "Linguistic knowledge includes grammatical, semantic, and pragmatic knowledge (Fromkin et al., 2018). The grammar of a natural language is its set of structural constraints on speakers' or writers' composition of clauses, phrases, and words. The term can also refer to the study of such constraints, a field that includes domains such as phonology, morphology, and syntax, often complemented by phonetics, semantics, and pragmatics. Semantic (Austin, 1975) studies the meaning of words, phrases, and sentences, focusing on general meanings rather than on what an individual speaker may want them to mean. Pragmatics (Austin, 1975) studies language use and how listeners bridge the gap between sentence meaning and the speaker's meaning. It is concerned with the relationship between semantic meaning, the context of use, and the speaker's meaning. The Linguistic Knowledge competency is embodied in almost all NLP tasks, researchers usually design specific scenarios to test the linguistic competency of LLMs. Some examples are shown in the upper group of Table 1. BLiMP (Warstadt et al., 2020) evaluates what language models (LMs) know about major grammatical phenomena. Linguistic mappings3 task aims to explore the depth of linguistic knowledge in enormous language models trained on word prediction. It aims to discover whether such knowledge is structured so as to support the use of grammatical abstractions, both morphological (past tense formation and pluralization) and syntactic (question formation, negation, and pronominalization). The minute mysteries qa4 is a reading comprehension task focusing on short crime and mystery stories where the goal is to identify the perpetrator and to explain the reasoning behind the deduction and the clues that support it. The metaphor boolean5 task presents a model with a metaphoric sentence and asks it to identify whether a second sentence is the correct interpretation of the first. The last three are selected from BIG-Bench (Srivastava et al., 2022), containing diverse task topics including linguistics.",
            "score": 0.4314925760610008,
            "section_title": "Linguistic Knowledge Competency",
            "char_start_offset": 5406,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 1070,
                    "end": 1093,
                    "matchedPaperCorpusId": "208527435"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59423828125
        },
        {
            "corpus_id": "276647333",
            "title": "Sparse Auto-Encoder Interprets Linguistic Features in Large Language Models",
            "text": "Large language models (LLMs) exhibit excellent performance in solving tasks that require different levels of linguistic competence, such as dependency parsing (Lin et al., 2022;Roy et al., 2023), reference disambiguation (Iyer et al., 2023) and metaphor interpretation (Wachowiak and Gromann, 2023;Yerukola et al., 2024;Tian et al., 2024). While their linguistic capabilities are largely attributed to the emergence of abilities from largescale pre-training and model size (Manning et al., 2020; Allen-Zhu and Li, 2023;Mahowald et al., 2024), the underlying mechanisms by which LLMs process these linguistic structures remain underexplored (Saba, 2023). Thus, we aim to interpret the linguistic mechanisms of LLMs by addressing the following question: Can we identify minimal components within LLMs that are responsible for distinct linguistic processing capabilities? \n\nPrevious attempts at interpreting the linguistic mechanisms of LLMs usually involve instructing them with expert-designed prompts, aiming to explain how these models generate particular outputs. (Yin and Neubig, 2022). Nevertheless, such behavior-based methods do not provide modelstructure-level mechanism interpretation. Most recent works turn to establish the connection between specific linguistic capabilities of LLMs and their interior structure, such as hidden states (Katz and Belinkov, 2023), attention heads (Wu et al., 2020), and activated neurons (Sajjad et al., 2022;Huang et al., 2023). However, these approaches mostly suffer from the following challenges: \n\nCoarse Interpretation Granularity. Linguistic mechanism interpretation aims to find the atomic linguistic structure in LLMs. However, even neurons are the most fine-grained native components of LLMs, they are observed to be activated by multiple different conditions, a phenomena that are termed as poly-semanticity (Yan et al., 2024). Thus, it is necessary to extract more fine-grained structures from LLMs to interpret their linguistic mechanism. \n\nInsufficient Causal Analysis.",
            "score": 0.43107454243190424,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 868
                },
                {
                    "start": 871,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1542
                },
                {
                    "start": 1545,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1993
                },
                {
                    "start": 1996,
                    "end": 2025
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 177,
                    "matchedPaperCorpusId": "256630854"
                },
                {
                    "start": 221,
                    "end": 240,
                    "matchedPaperCorpusId": "262083660"
                },
                {
                    "start": 269,
                    "end": 298,
                    "matchedPaperCorpusId": "259370783"
                },
                {
                    "start": 320,
                    "end": 338,
                    "matchedPaperCorpusId": "270514407"
                },
                {
                    "start": 519,
                    "end": 541,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 640,
                    "end": 652,
                    "matchedPaperCorpusId": "261697105"
                },
                {
                    "start": 1066,
                    "end": 1088,
                    "matchedPaperCorpusId": "247011700"
                },
                {
                    "start": 1346,
                    "end": 1371,
                    "matchedPaperCorpusId": "258841183"
                },
                {
                    "start": 1389,
                    "end": 1406,
                    "matchedPaperCorpusId": "222291617"
                },
                {
                    "start": 1430,
                    "end": 1451,
                    "matchedPaperCorpusId": "237353268"
                },
                {
                    "start": 1451,
                    "end": 1470,
                    "matchedPaperCorpusId": "262054234"
                },
                {
                    "start": 1861,
                    "end": 1879,
                    "matchedPaperCorpusId": "270737676"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66162109375
        },
        {
            "corpus_id": "273877583",
            "title": "Crystal: Illuminating LLM Abilities on Language and Code",
            "text": "In this work, we present a multi-phase LLM pretraining method designed to encode natural language and coding abilities into a single model. Through this approach, we obtain a model CRYSTAL, achieving natural language and coding performance on par with Llama 2 and Code Llama, respectively. By tracking, observing and analyzing model performance throughout the pretraining, as well as a study of an additional adaptation phase, we obtain and present insights of the interplay of language and coding ability acquisition during the model training, highlighting the importance of data curriculum design. \n\nThough we have included careful analysis on the training process, it remains challenging to verify each design choice and explain every observed phenomenon during pretraining, largely due to constraints in computational resources. We will address the following limitation and release all our training artifacts, inviting the community to collaborate in overcoming these challenges: Necessity of a Smooth Transition Between Phases. Our examination in Section 5.1 reveals a slight performance drop when transitioning from Phase 1 to Phase 2 in certain benchmarks. This observation hints at the potential importance of ensuring a smooth transition between training phases. Further investigation and validation of this hypothesis could further enhance our training methodology. \n\nImpact of Code on Enhancing Reasoning Abilities. Notably, our second phase, which incorporates 67% code data, yielded unexpectedly high scores on the MMLU, a benchmark for natural language. This outcome suggests that structured code data may also boost language capabilities. A more definitive confirmation of this hypothesis could be obtained by re-running the second phase while omitting all code data, allowing for a direct assessment of its impact on learning outcomes.",
            "score": 0.43068906576791355,
            "section_title": "Conclusion",
            "char_start_offset": 21256,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1375
                },
                {
                    "start": 1378,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1851
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "261049443",
            "title": "On the Unexpected Abilities of Large Language Models",
            "text": "An important issue that needs to be investigated is how well the skills acquired by LLMs are predictable. This question has important implications for AI safety and alignment, since the impossibility of predicting the abilities that will be developed by larger models implies that these models could acquire undesired and dangerous capabilities, without warning (Schaeffer et al., 2023). \n\nThe performance of large language models scales as a power-law with model size, dataset size, and amount of computation used for training (Kaplan et al., 2020). This implies that the overall performance (prediction error) of these systems is predictable. In other words, it implies that the overall performance that can be obtained by increasing the size of the model and/or the training time can be extrapolated based on the performance displayed by models that are smaller or less trained. However, the specific abilities that will be developed by a model of a certain size are not necessarily predictable. \n\nThe interest in this topic was raised by the publication of an influential article by Wei et al. (2022) entitled \"Emergent abilities of large language models\". The authors observe that several abilities, such as in-context learning, instruction following, step-by-step reasoning, and arithmetic skills, are not present in smaller models. They appear in larger models only. Moreover, they show that the acquisition process of these abilities in sufficiently large models is characterized by a sharp transition between a phase in which the system does not show any progress in the ability and a subsequent phase in which the system progresses. According to the authors, these two observations imply that predicting the abilities that will be acquired by scaled-up models will be impossible. In other words, they claim that the abilities that will be developed by scaled-up models cannot be extrapolated from what we know about smaller models. Wei et al. (2022) later questioned the presence of sharp transitions during the training process and showed that the occurrence of sharp or more continuous transitions in the acquisition process depends on the metric used to evaluate the performance. The occurrence of sharp transitions might thus be induced by the utilization of specific metrics.",
            "score": 0.4302732890678317,
            "section_title": "Predictability and emergence",
            "char_start_offset": 18193,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 387
                },
                {
                    "start": 390,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2192
                },
                {
                    "start": 2193,
                    "end": 2290
                }
            ],
            "ref_mentions": [
                {
                    "start": 1087,
                    "end": 1104,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1942,
                    "end": 1959,
                    "matchedPaperCorpusId": "249674500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "272397970",
            "title": "Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges",
            "text": "Delving deeper into specific cognitive abilities, Srinivasan et al. [18] proposed novel methods based on cognitive science principles to test LLMs' common sense reasoning abilities through prototype analysis and proverb understanding. These methods offer new ways to assess LLMs' cognitive capabilities in more nuanced and context-dependent tasks. Binz and Schulz [19] used tools from cognitive psychology to study GPT-3, assessing its decision-making, information search, deliberation, and causal reasoning abilities. Their approach demonstrates the potential of cognitive psychology in studying AI and demystifying how LLMs solve tasks. \n\nIn summary, Large Language Models exhibit remarkable parallels with human cognitive processes, particularly in language and sensory tasks, yet they fall short in several critical areas, such as reasoning under novel conditions and functional linguistic competence. The diverse methodologies employed to evaluate LLMs' cognitive abilities highlight both their potential and limitations as models of human cognition. As LLMs continue to evolve, they provide a valuable tool for exploring the nature of human intelligence, but their differences from human cognitive processes must be carefully considered. Future research should aim to refine these models further, improving their alignment with human cognition and addressing the gaps that currently exist. Understanding the complex interplay between LLMs and human cognitive processes will advance both AI and cognitive science, bridging the divide between machine and human intelligence.",
            "score": 0.43008360750050567,
            "section_title": "B. Methods for evaluating LLMs cognitive abilities",
            "char_start_offset": 9165,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 638
                },
                {
                    "start": 641,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1578
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 72,
                    "matchedPaperCorpusId": "261392381"
                },
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "250113371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60107421875
        },
        {
            "corpus_id": "265067168",
            "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
            "text": "The attributes and behaviors of LLMs are deeply intertwined with their training processes. LLMs undergo three primary training stages: pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). Analyzing these stages provides insight into hallucination origins in LLMs, as each stage equips the model with specific capabilities. Pre-training. Pre-training is generally considered a crucial stage for LLM to acquire knowledge and skills (Zhou et al., 2023a). Language models, during pre-training, aim to predict the next token in a sequence autoregressively. Through selfsupervised training on extensive textual corpora, the model acquires knowledge of language syntax, world knowledge, and reasoning abilities, providing a robust foundation for subsequent fine-tuning tasks. Besides, recent research (Sutskever, 2023;Del\u00e9tang et al., 2023) suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence of language models lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world. \n\nSupervised Fine-Tuning. While LLMs acquire substantial knowledge and capabilities during the pre-training stage, it's crucial to recognize that pretraining primarily optimizes for completion. Consequently, pre-trained LLMs fundamentally served as completion machines, which can lead to a misalignment between the next-word prediction objective of LLMs and the user's objective of obtaining desired responses. To bridge this gap, SFT (Zhang et al., 2023d) has been introduced, which involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, resulting in enhanced capabilities and improved controllability of LLMs. Furthermore, recent studies (Chung et al., 2022;Iyer et al., 2022) have confirmed the effectiveness of supervised fine-tuning to achieve exceptional performance on unseen tasks, showcasing their remarkable generalization abilities. \n\nReinforcement Learning from Human Feedback. While the SFT process successfully enables LLMs to follow user instructions, there is still room for them to better align with human preferences.",
            "score": 0.42970717763971567,
            "section_title": "Training Stages of Large Language Models",
            "char_start_offset": 7928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2082
                },
                {
                    "start": 2085,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2274
                }
            ],
            "ref_mentions": [
                {
                    "start": 1899,
                    "end": 1917,
                    "matchedPaperCorpusId": "255096269"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.857421875
        },
        {
            "corpus_id": "4852738",
            "title": "Modeling Coevolution between Language and Memory Capacity during Language Origin",
            "text": ". [62]) entangled lexical and syntactic knowledge, and the setting of single-individual generations made them hard to disentangle cultural and natural selections. In addition, due to the focus on the evolutions of grammatical constructions, artificial agents in many language game models (e.g. [40]) directly recruited relevant learning mechanisms to handle language materials, without manipulating their levels. This made these models unable to trace the coevolution between language and cognitive abilities during language origin, which manifests primarily in terms of level change in relevant cognitive abilities and linguistic understandability.\n\nFourth, the acquired linguistic knowledge in the expected model should not be limited to lexical items. This criterion makes sure that the emergent language in the model is distinct from lexicon-like communication systems as in some nonhuman species, and avoids showing a superficial correlation between language and memory (i.e., an increasing number of lexical items certainly requires an increase in memory capacity for storage). The outcome of the coevolution between language and memory capacity is not merely an increase in memory capacity for keeping relevant linguistic knowledge, but also a transition of linguistic knowledge to accommodate limited capacity. Such transition is collectively induced by individual learning mechanisms and acquired linguistic knowledge. It resolves the conflict between unlimited expressions and limited storing capacities, and allows language users to encode an unlimited number of meanings using a limited number of expressions. It is not evident in most animal communication systems, and cannot be observed in lexical evolution models without grammar learning mechanisms.\n\nConsidering these, we adopt the lexicon-syntax coevolution model [63,64] in our study of language-memory coevolution. The model was first designed to simulate a collective acquisition of lexical items and constituent word orders out of a holistic protolanguage in a multiagent population. In the model, artificial agents are equipped with both STM units to store exchanged meaning-utterance instances, and LTM units to record lexical and syntactic knowledge extracted from these instances. In addition, they apply domain-general abilities such as pattern extraction and sequential learning to acquire lexical and syntactic knowledge. These abilities resemble those used by language learning children [65] or early hominins [8,66]. The initial holistic language resembles the hypothesized",
            "score": 0.4286101926271846,
            "section_title": "Selecting models to simulate language-memory coevolution",
            "char_start_offset": 12978,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 2,
                    "end": 6,
                    "matchedPaperCorpusId": "6567063"
                },
                {
                    "start": 1836,
                    "end": 1839,
                    "matchedPaperCorpusId": "62696833"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6240234375
        },
        {
            "corpus_id": "264590507",
            "title": "Probing LLMs for Joint Encoding of Linguistic Categories",
            "text": "Recent advancements in natural language processing (NLP) can be attributed to the development and pretraining of large language models (LLMs) such as BERT, GPT-3, and many others (Devlin et al., 2019;Brown et al., 2020;Touvron et al., 2023). For their intended use of providing general-purpose language representations suitable for many NLP tasks, these models must efficiently capture a wide range of linguistic features within their finite capacity. Despite their success, little is known about the way in which different types of linguistic information are organized in these models. Systematically understanding how these models represent linguistic phenomena and their interaction is crucial for the development of more effective NLP methods. \n\n\u22c6 Corresponding author: giulio.starace@gmail.com. \n\nExisting research probed LLMs for their encoding of various linguistic properties such as agreement (Jawahar et al., 2019), word order and sentence structure (Tenney et al., 2018;Hewitt and Manning, 2019), co-reference (Tenney et al., 2019), semantics (Ettinger, 2020) and multilinguality (Ravishankar et al., 2019;Libovick\u00fd et al., 2020). Taking a step further, Tenney et al. (2019) and Clark et al. (2019) studied where linguistic information is encoded in LLMs by probing different layers. Their results demonstrated that a linguistic hierarchy emerges in BERT representations, with lower layers capturing local syntax and higher layers being employed in higher-level semantic and discourse tasks. However, we do not yet understand how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations. \n\nThere are many dependencies between processing different linguistic phenomena: for instance, information about a word's part of speech is likely to be employed when disambiguating its word sense. Alternatively, lower-level syntax is an important first step for semantic composition and natural language understanding tasks. In this work, we investigate how the (hierarchical) dependencies between different linguistic categories are encoded in LLMs, focusing on syntax. We ask a set of novel questions: \n\n(1) how related syntactic categories (e.g.",
            "score": 0.4282450610930568,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1728
                },
                {
                    "start": 1731,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2200
                },
                {
                    "start": 2201,
                    "end": 2233
                },
                {
                    "start": 2236,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 200,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 902,
                    "end": 924,
                    "matchedPaperCorpusId": "195477534"
                },
                {
                    "start": 960,
                    "end": 981,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 981,
                    "end": 1006,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 1021,
                    "end": 1042,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 1054,
                    "end": 1070,
                    "matchedPaperCorpusId": "199001173"
                },
                {
                    "start": 1091,
                    "end": 1117,
                    "matchedPaperCorpusId": "203690445"
                },
                {
                    "start": 1117,
                    "end": 1140,
                    "matchedPaperCorpusId": "215745575"
                },
                {
                    "start": 1165,
                    "end": 1185,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 1190,
                    "end": 1209,
                    "matchedPaperCorpusId": "184486746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62548828125
        },
        {
            "corpus_id": "258762577",
            "title": "Language Models Meet World Models: Embodied Experiences Enhance Language Models",
            "text": "While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is desirable to preserve the generality of LMs during finetuning, which facilitates generalizing the embodied knowledge across tasks rather than being tied to specific simulations. We thus further introduce the classical (EWC) for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency. Extensive experiments show our approach substantially improves base LMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs (1.3B, 6B, and 13B) enhanced by our approach match or even outperform much larger LMs (e.g., ChatGPT).",
            "score": 0.42661894765448294,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7529296875
        },
        {
            "corpus_id": "210898819",
            "title": "Criticality in Pareto Optimal Grammars?",
            "text": "tify complexity, and measuring similarity between synthetic and real texts for accuracy). Those alternative choices should be explored systematically in the future to understand which options are more informative. Our choices, however, make our results easy to interpret in physical terms. For example, improbable (unnatural) texts have high energies in any good model.\n\nThe grammars that optimally trade between accuracy (low energy) and simplicity (high entropy) conform the Pareto front (i.e., the solution) of the MOO problem. Its shape in the energy-entropy plane ( Figure 3) is linked to phase transitions [25][26][27][28][29]. According to this framework, we do not find evidence of a positive (second order) phase transition. What could such a transition imply for our system? The presence of a positive phase transition in our data would suggest the existence of a salient level of description capable of capturing a large amount of linguistic structure in relatively simple terms. For example, if a unique grammatical rule would serve to connect words together disregarding of the grammatical classes in which we have split our vocabulary. We would expect that to be the case, e.g., if a single master rule such as merge would serve to generate all the complexity of human language without further constraints arising. This does not seem to be the case. However, this does not rule out the existence of the relevant merge operation, nor does it deny its possible fundamental role. Indeed, Chomsky proposes that merge is the fundamental operation of syntax, but that it leaves the creative process of language underconstrained [51][52][53]. As a result, actual implementations (i.e., real languages) see a plethora of further complexities arising in a phenomena akin to symmetry breaking.\n\nThe presence of a negative (first order) phase transition would acknowledge several salient levels of description needed to understand human language. These salient descriptions would furthermore present an important gap separating them. This would indicate that discrete approaches would be possible to describe language without missing any detail by ignoring the intermediate possibilities. If that were the case, we would still need to analyze the emerging models and look at similarities between them to understand whether both models capture a same core phenomenology at two relevant (yet distant) scales; or",
            "score": 0.42659225293869524,
            "section_title": "Discussion",
            "char_start_offset": 33488,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 620,
                    "end": 624,
                    "matchedPaperCorpusId": "41078692"
                },
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "118748605"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1007080078125
        },
        {
            "corpus_id": "255941592",
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "text": "Ultimately, what \"pure\" LLMs can learn is necessarily constrained both by the information available in their training data and by whether that information is learnable through a word prediction mechanism. It has turned out that quite a lot of linguistic knowledge, e.g., about syntax and semantics, can be learned from language data alone [Potts, 2020, Merrill et al., 2022, Bommasani et al., 2021], in our opinion far more than most researchers in the field would have guessed 5 or 10 years ago (see Merrill et al. [2022] for an argument of how semantic information is in-principle learnable from language data, and Piantadosi and Hill [2022] for an argument that models can genuinely learn meaning). The success of these models is a major development, with far-reaching implications. But LLMs' success in developing linguistic knowledge by predicting words using massive amounts of text does not guarantee that all aspects of thought and reasoning could be learned that way (although, as we will discuss, some aspects of thought and reasoning can be learned that way provided the relevant information is typically encoded in distributional patterns over words). \n\nBy saying that LLMs do not, in and of themselves, model human thought, we are not suggesting that AI approaches which start from building LLMs will necessarily run up against hard limits. Indeed, at the end of this article, we discuss current modular approaches in which separate architectures or diverse objectives are combined. InstructGPT [Ouyang et al., 2022] and ChatGPT are examples of successes in this vein, in that they combine an LLM with Reinforcement Learning from Human Feedback (RLHF) [Christiano et al., 2017], whereby human feedback is used to iteratively adjust the trained models. In that sense, they are more than just LLMs and can learn based on more than just what is available in massive amounts of passively observed text. For our purposes here, we will use the term LLMs to refer primarily to \"pure\" language models (such as the original GPT-3) that are trained to predict held-out language tokens conditional on the immediate linguistic context, from large corpora of naturally observed language use.",
            "score": 0.4264746473540678,
            "section_title": "Introduction",
            "char_start_offset": 6375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2191
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7373046875
        },
        {
            "corpus_id": "276937763",
            "title": "PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs",
            "text": "Our experiments on downstream performance ( \u00a73), intermediate representations ( \u00a74), and model parameters ( \u00a75) allow us to examine the stability of training and find outlier runs using different methods across the model training pipeline. In this section, we analyse commonalities across the resulting metrics to identify broader characteristics of LM pre-training dynamics. \n\nLanguage modelling is largely stable. Generally, we observe LM pre-training dynamics to follow consistent trajectories. Across seeds and model sizes, downstream performance and representational efficiency consistently increase during pre-training, and training maps are linear (except for the outlier seeds). Furthermore, model scaling laws seem to hold across seeds, not only for downstream performance but also for information content and representational efficiency of model representations. \n\nSimilarly, both at the performance and representational level, we observe the effect of \"saturation\" (Michaelov and Bergen, 2023;Godey et al., 2024) in smaller models. \n\nLinguistic information is encoded in the initial learning phase (10 3 -10 4 steps). Across all experiments, metrics start moving away from the initial random baseline around step 10 3 (2B tokens circa) and reach their convergence level around step 10 4 (20B tokens circa). In this initial phase, representational shift peaks and linguistic information begins to be encoded into the models' latent representations. Through the lens of multiple metrics, we can analyse model behaviour in this phase in detail. Specifically, while the amount and the efficiency with which linguistic information is encoded in model representations have already increased substantially at step 10 3 , the model does not yet generate coherent outputs, as indicated by low performance on linguistic acceptability benchmarks like BLiMP (Gender Agreement). Simultaneously, self-consistency is low while inter-seed agreement is high, which we hypothesise to be an artefact of all models initially choosing an incorrect baseline answer. In terms of training maps, this phase corresponds to the 0 \u2192 1 transition, which occurs consistently in this initial training phase for all model sizes (except for the 410M outliers). \n\nMost improvements happen in the \"critical\" learning phase (10 4 -10 5 steps). In the range of 10 3 to 10 4 steps, most learning occurs, as measured by all of our metrics.",
            "score": 0.42556288493036853,
            "section_title": "DISCUSSION",
            "char_start_offset": 28106,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2238
                },
                {
                    "start": 2241,
                    "end": 2318
                },
                {
                    "start": 2319,
                    "end": 2411
                }
            ],
            "ref_mentions": [
                {
                    "start": 976,
                    "end": 1004,
                    "matchedPaperCorpusId": "258865474"
                },
                {
                    "start": 1004,
                    "end": 1023,
                    "matchedPaperCorpusId": "269042847"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "268187543",
            "title": "Generative Artificial Intelligence in Education: From Deceptive to Disruptive",
            "text": "The original GPT could manage basic text labeling but lacked coherence in text generation [25]. GPT-2 improved, offering higher-quality text and some instruction-following capabilities [26]. GPT-3, however, emerged as a versatile and practical LLM for various language tasks. The significant capability leap between these models is mainly due to scaling up computational power and data: GPT-3 required about 20,000 times more computation than the original GPT [27]. Although these models share similar designs, their advancements are largely attributed to breakthroughs in high-performance computing infrastructure rather than specific advancements in language technology model design. \n\nAs they scale up, LLMs exhibit new properties that their developers had not anticipated, and we are only now starting to discover them. Among these properties, few-shot learning and chain-of-thought reasoning stand out. \n\n\u2022 Few-shot learning enables a sufficiently large LLM to quickly grasp new tasks from just a few examples in a single interaction [26]. \n\n\u2022 Chain-of-thought reasoning allows the model to articulate its thought process when tackling complex tasks, similar to how the students would explain their reasoning during a math test, thereby enhancing their performance [28]. \n\nThese GPT-3 capabilities, particularly in few-shot learning and chain-of-thought reasoning, were identified post-training and several months after its widespread public deployment, respectively [29]- [32]. \n\nIn hindsight, these characteristics are partly the consequence of the LLMs' ability to \"learn\" from the information within the context of execution -all the information received from user messages and that the model has generated-like the training and fine-tuning data. \n\nFurthermore, LLMs demonstrate unforeseen skills in programming, arithmetic, correcting misconceptions, and answering exam questions across various domains, and improve as the model size scales up [30], [33]. \n\nThere is a common belief that LLMs are merely statistical predictors of the next word, limited to text-based learning and reasoning. However, recent evidence suggests that LLMs are developing internal representations of the world, enabling them to reason abstractly beyond the specific linguistic structure of texts [24].",
            "score": 0.42423285781410647,
            "section_title": "A. Discovering the Emergent Abilities of Large Language Models",
            "char_start_offset": 6779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1275
                },
                {
                    "start": 1278,
                    "end": 1483
                },
                {
                    "start": 1486,
                    "end": 1755
                },
                {
                    "start": 1758,
                    "end": 1965
                },
                {
                    "start": 1968,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2289
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 94,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "246822642"
                },
                {
                    "start": 1478,
                    "end": 1482,
                    "matchedPaperCorpusId": "265157752"
                },
                {
                    "start": 1954,
                    "end": 1958,
                    "matchedPaperCorpusId": "249674500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6474609375
        },
        {
            "corpus_id": "268876447",
            "title": "Emergent Abilities in Reduced-Scale Generative Language Models",
            "text": "What is ICL?ICL is the ability of a pre-trained model to solve tasks without task specific finetuning (Radford et al., 2019;Brown et al., 2020;Olsson et al., 2022).Many large models have shown excellent ICL capabilities (Touvron et al., 2023;Chowdhery et al., 2022).This has shifted the research community's focus towards leveraging prompts to elicit zero-shot or few-shot responses from models.In a similar vein, the technique of chain-of-thought (CoT) reasoning, as discussed in  Wei et al. (2022b), revealed that including a sequence of intermediate reasoning steps can enhance the reasoning skills of large language models.Yet, these abilities are emergent, i.e., it is primarily the larger models that exhibit them.However, recent studies question the belief that improvements in ICL result exclusively from increasing model sizes (Schaeffer et al., 2023;Du et al., 2024), suggesting that using discontinuous metrics like accuracy merely creates the illusion of emergent abilities, whereas employing continuous metrics shows gradual, predictable changes in model performance.\n\nICL in smaller language models.It has been shown that the emergent abilities observed in larger models can be effectively transferred to smaller models through imitation learning or behavior cloning, where a larger language model such as GPT-4 serves as the \"teacher\", to generate synthetic datasets with instructions and explanations which can be used to train smaller language models, referred to as \"student\" models (Taori et al., 2023;Peng et al., 2023;Mukherjee et al., 2023;Magister et al., 2023).This allows smaller models to leverage the capabilities of their larger counterparts.However, the primary drawback of such methods is that most of the knowledge acquired by the model is done in the pre-training stage and the student model copies the style of the teacher model but does not learn the reasoning capabilities employed by these large models (Gudibande et al., 2023).",
            "score": 0.42376362273304147,
            "section_title": "Related Work",
            "char_start_offset": 4046,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 12,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 266
                },
                {
                    "start": 266,
                    "end": 395
                },
                {
                    "start": 395,
                    "end": 627
                },
                {
                    "start": 627,
                    "end": 720
                },
                {
                    "start": 720,
                    "end": 1080
                },
                {
                    "start": 1082,
                    "end": 1113
                },
                {
                    "start": 1113,
                    "end": 1585
                },
                {
                    "start": 1585,
                    "end": 1670
                },
                {
                    "start": 1670,
                    "end": 1964
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 124,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1562,
                    "end": 1584,
                    "matchedPaperCorpusId": "254823156"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "273025704",
            "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime",
            "text": "We just saw how dimensionality scales with size, and now we investigate its change over time. We find that feature complexity is highly related to the LM's linguistic capabilities, assessed using the eval-harness benchmark performance, over training. Figure 3 shows the evolution of I d on the k = 1 dataset (top), where each curve is one layer, with the evolution of LM performance on the benchmark tasks (bottom), where each curve plots performance on an individual task. We observe in Figure 3 that, for all models, the evolution of representational dimensionality closely tracks a sudden transition in LM task performance. In Figure 3 (2024), who observed in BERT models a similar two-part I d transition on the training corpus; they showed that the two extrema corresponding to the dip and uptick in I d temporally coincided with the onset of higher-order linguistic capabilities. Together, results show that representational complexity can signify whether and when LMs learn linguistic structure. Crucially, we show that the phase transition exists for inputs beyond in-distribution data, which was the subject of (Chen et al., 2024), and, furthermore, beyond grammatical data (Figure G.4) as a more general property of LM processing.",
            "score": 0.4225733805389244,
            "section_title": "EVOLUTION OF REPRESENTATIONAL GEOMETRY TRACKS EMERGENT LINGUISTIC ABILITIES OVER TRAINING",
            "char_start_offset": 19171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1240
                }
            ],
            "ref_mentions": [
                {
                    "start": 1120,
                    "end": 1139,
                    "matchedPaperCorpusId": "261822542"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "271859652",
            "title": "Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives",
            "text": "Several studies have explored LLMs from the perspectives of knowledge and skill. KoLA (Yu et al., 2023) emphasizes the importance of world knowledge for LLMs and establishes a knowledgeoriented evaluation benchmark. In its approach to ability modeling, KoLA simplifies and selects from Bloom's learning theories to form four levels of knowledge-capability assessment: knowledge memorization, knowledge understanding, knowledge applying, and knowledge creating. Skill-it (Chen et al., 2024) posits that language models naturally acquire a sequence of skills from training data and formalizes the notion of a skill and an ordered set of skills in terms of associated data, differentiating this approach from traditional curriculum learning (Bengio et al., 2009), which focuses  et al., 2023) introduces a lightweight fine-tuning methodology that improves retrieval-augmented language models by enhancing both the relevance of retrieved knowledge and its effective utilization, marking a specialized form of knowledge and skill enhancement. MMLU (Hendrycks et al., 2020) serves as a benchmark designed to measure the possession of world knowledge and problem-solving abilities. Reflecting on these developments, Bengio & Hu (2023) have emphasized the integration of the world model and the inference machine in current LLMs. They suggest that to reason effectively, a robust world knowledge model and a powerful inference machine are necessary, advocating for their separation and simultaneous development to enhance reasoning capabilities.",
            "score": 0.42169805920845227,
            "section_title": "KNOWLEDGE AND SKILLS IN LLMS",
            "char_start_offset": 7101,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1537
                }
            ],
            "ref_mentions": [
                {
                    "start": 470,
                    "end": 489,
                    "matchedPaperCorpusId": "260203057"
                },
                {
                    "start": 738,
                    "end": 759,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69287109375
        },
        {
            "corpus_id": "268032136",
            "title": "Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models",
            "text": "The pursuit of multilingual capabilities, mirroring our world's linguistic diversity, is a critical research objective that paves the way for information democratization across linguistic divides.The emergence of pre-trained language models (PLMs) such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020a) has marked a significant shift towards enhanced multilingual understanding.Furthermore, large language models (LLMs), such as GPT-4 (Achiam et al., 2023) and PaLM-2 (Anil et al., 2023), have recently demonstrated more excellent multilingual capabilities in language understanding, reasoning, and generation, despite being predominantly trained in English corpora.\n\nExisting studies (Pires et al., 2019;Conneau et al., 2020b) have mainly explored how multilingual PLMs (e.g., mBERT) possess semantic alignment capabilities across languages despite the absence of multilingual parallel corpora.They have identified several critical factors that influence cross-lingual transfer, including training data (e.g., overlapped tokens) and training settings (e.g., shared parameters) (Dufter and Sch\u00fctze, 2020;Philippy et al., 2023).Nevertheless, the underlying mechanisms by which the model itself process diverse languages at the composition level continue to be an area of vigorous investigation.\n\nTo develop a deeper understanding of the multilingual capabilities of LLMs, we draw inspiration from the neurobiological underpinnings of human language faculties (Friederici, 2011;Parr et al., 2022;Khanna et al., 2024).Specific regions within the human brain, such as Broca's area and Wernicke's area have been identified to support particular language functions.To make an analogy with human's language functions, we posit that regions within the language models can be delineated into two primary components: languageagnostic regions that encompass universal knowledge and pragmatics principles, and languagespecific regions that handle language-specific vocabulary, grammar, and idiomatic expressions.Figure 1 presents such a conceptual illustration of region distribution in LLMs posited by us.",
            "score": 0.4211179579711445,
            "section_title": "Carl Sagan",
            "char_start_offset": 115,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 196,
                    "end": 393
                },
                {
                    "start": 393,
                    "end": 681
                },
                {
                    "start": 683,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1142
                },
                {
                    "start": 1142,
                    "end": 1308
                },
                {
                    "start": 1310,
                    "end": 1530
                },
                {
                    "start": 1530,
                    "end": 1674
                },
                {
                    "start": 1674,
                    "end": 2015
                },
                {
                    "start": 2015,
                    "end": 2109
                }
            ],
            "ref_mentions": [
                {
                    "start": 262,
                    "end": 283,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 294,
                    "end": 317,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 700,
                    "end": 720,
                    "matchedPaperCorpusId": "174798142"
                },
                {
                    "start": 720,
                    "end": 742,
                    "matchedPaperCorpusId": "207853017"
                },
                {
                    "start": 1093,
                    "end": 1119,
                    "matchedPaperCorpusId": "226262235"
                },
                {
                    "start": 1119,
                    "end": 1141,
                    "matchedPaperCorpusId": "258947231"
                },
                {
                    "start": 1473,
                    "end": 1491,
                    "matchedPaperCorpusId": "3224368"
                },
                {
                    "start": 1509,
                    "end": 1529,
                    "matchedPaperCorpusId": "267362650"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6357421875
        },
        {
            "corpus_id": "268691480",
            "title": "Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models Using Minimal Pairs",
            "text": "Inspired by cognitive neuroscience studies, we introduce a novel \u201cdecoding probing\u201d method that uses minimal pairs benchmark (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer. By treating the language model as the brain and its representations as \u201cneural activations\u201d, we decode grammaticality labels of minimal pairs from the intermediate layers\u2019 representations. This approach reveals: 1) Self-supervised language models capture abstract linguistic structures in intermediate layers that GloVe and RNN language models cannot learn. 2) Information about syntactic grammaticality is robustly captured through the first third layers of GPT-2 and also distributed in later layers. As sentence complexity increases, more layers are required for learning grammatical capabilities. 3) Morphological and semantics/syntax interface-related features are harder to capture than syntax. 4) For Transformer-based models, both embeddings and attentions capture grammatical features but show distinct patterns. Different attention heads exhibit similar tendencies toward various linguistic phenomena, but with varied contributions.",
            "score": 0.4194654548337735,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84423828125
        },
        {
            "corpus_id": "259138812",
            "title": "A blind spot for large language models: Supradiegetic linguistic information",
            "text": "And, how far does the apparent linguistic fluency extend? How many of the wide variety of things we think of as being encoded in language 88 can be diegetically accessible for an LLM? We both learn things individually and as a group with history; we accumulate knowledge. 89 ChatGPT is an extension of both of these functions. Many now-familiar technologies we have made, such as books, have been constrained to accumulating knowledge, but that is not necessarily the case with computation. 90 LMs seem to get a significant degree of syntactic and semantic knowledge faster than they learn similarly complete information about some characters. 91 They do learn about the characters, but more slowly. What is the shape of the diegetic space? How do we know what is near the core, and what is at the border? With more and more textual input, would the boundary expand forever? Some things you need a lot of data or processing to learn; but technically they are just as licensed. 92 As Kaushal and Mahowald put it, \"[u]nderstanding whether and how the building blocks of this meta-linguistic knowledge can emerge during self-supervised training on a word prediction task could be of interest not just in NLP, but in the cognitive sciences\" [7]. And of course that sentiment extends beyond those specific tasks. 93 cluding \"social dynamics between people\" like power differentials and biases [33]. In Ref.' [12]: \"[T]here are three time frames at which we can study behavior: here and now ; learning and development; and evolution.\" Panic as a response to technologies perceived to be changing our epistemology is nothing new: \"The printed book is destroying age-old memory habits\" [47]. And in the case of sophisticated mnemotechnology, maybe we did lose those skills (most of us, anyway). Or, we did not lose them, but we offloaded them to books, computers, etc., as part of our extended mind [48]. Specifically in the case that character-level information is not built into the vocabulary.",
            "score": 0.41722196851244336,
            "section_title": "E. Ergodicity and span",
            "char_start_offset": 67961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 1988
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1473388671875
        },
        {
            "corpus_id": "273482770",
            "title": "Leveraging Large Language Models for Enhancing Public Transit Services",
            "text": "This diverse pool of training data provides LLMs with a holistic and macroscopic understanding of various linguistic styles, themes, and contexts, thereby enabling them to tackle a broader range of tasks (Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu (2019);Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, et al. (2023)). Meanwhile, we observe a growing trend in the size of the model architecture, with parameter counts increasing into the billions, motivated by the requirement to optimally learn from increasing data sizes (Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et al. (2020)). The growth in both the volume of training data and the number of model parameters endows LLMs with emergent capabilities, which are not shown in the smaller language models, such as in-context learning, instruction following, and step-by-step reasoning (Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, et al. (2022a);Zhao et al. (2023)). \n\nTraining Process: To address the challenge of training with extensive volumes of data, the LLM training pipeline is generally divided into two primary phases: Pretraining and Fine-tuning. The pre-training phase is the more time-consuming step, which involves self-supervised learning from large-scale datasets in a distributed manner. This phase enables the model to acquire a broad, general-purpose problem-solving capability (Brown et al. (2020)). On the other hand, the fine-tuning process sharpens the pre-trained model's skills towards specific tasks. This stage leverages relatively smaller computational resources and requires less training time, as it adjusts the model to support more task-orientated applications (Zhao et al. (2023)). This structured training pipeline offers significant advantages to the end-users of LLMs. Users do not need to start the training process from scratch but can instead focus on tailoring the pre-trained model to their specific tasks.",
            "score": 0.4161162135435249,
            "section_title": "Key Characteristics",
            "char_start_offset": 6660,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2001
                }
            ],
            "ref_mentions": [
                {
                    "start": 555,
                    "end": 652,
                    "matchedPaperCorpusId": "256501098"
                },
                {
                    "start": 1451,
                    "end": 1471,
                    "matchedPaperCorpusId": "256501098"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65673828125
        },
        {
            "corpus_id": "276766647",
            "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network",
            "text": "In this work, we investigated how brain alignment in LLMs evolves throughout training, revealing different learning processes at play. We demonstrated that alignment with the human language network (LN) primarily correlates with formal linguistic competence Mahowald et al. (2024), peaking and saturating early in training. In contrast, functional linguistic competence, which involves world knowledge and reasoning, continues to grow beyond this stage. These findings suggest that the LN primarily encodes syntactic and compositional structure, in line with the literature of language neuroscience Fedorenko et al. (2024), while broader linguistic functions may rely on other cognitive systems beyond the LN. We also showed that model size is not a reliable predictor of brain alignment when controlling for feature size. Instead, alignment is determined by architectural inductive biases, token integration mechanisms, and training dynamics. Our standardized brain-scoring framework eliminates contextualization biases from previous work, ensuring more rigorous evaluations. Finally, we demonstrated that current brain alignment benchmarks are not saturated, indicating that LLMs can still be improved in modeling human language processing. Together, these findings challenge prior assumptions about how alignment emerges in LLMs and provide new insights into the relationship between artificial and biological language processing.",
            "score": 0.4157778868427594,
            "section_title": "Conclusion & Future Work",
            "char_start_offset": 24602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1433
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 280,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 599,
                    "end": 622,
                    "matchedPaperCorpusId": "269112224"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94287109375
        },
        {
            "corpus_id": "216071155",
            "title": "Lessons From Neuro-(a)-Typical Brains: Universal Multilingualism, Code-Mixing, Recombination, and Executive Functions",
            "text": "In this paper, I argue that language acquisition involves contact of idiolects (i.e., contact between individual S-learners leading to contact between different linguistic features in the mind of individual S-learners). Building on Aboh (2015b), I propose that grammars emerge through recombination: a fully automated cognitive process which allows S-learners to select linguistic features and recombine them into new syntactic objects as part of their mental hybrid grammars. Immediately observable instances of recombination are illustrated by code-mixing which appears a capacity present in all S-learners. In this regard, I have shown that both neuro-atypical and neuro-typical S-learners exhibit similar production (and arguably processing) patterns, a conclusion already reached by Perecman in the early 80s. What this paper adds to the discussion is the distinction between the role of executive functions as necessary for vocabulary selection, while recombination appears an innate capacity.\n\nBuilding on this, I further show that while recombination within the TMA domain, traditionally referred to as the INFLdomain, is immune to structural change due to strong Minimality constraints, this does not seem to be the case when it comes to the left periphery, that is, the phase level. I therefore conclude that structural variation of the type that leads to typological variation is a phase-level property. This view accounts for the fact that even though recombination appears \"free, \" its effects vary depending on the structural domain that it applies to. While the discussion here mainly focuses on syntax, one can imagine similar recombination patterns in semantics and phonology, and how these are constrained cross-linguistically.\n\nThe approach developed in this paper makes clear what core aspects of language are common to neurotypical and neuro-atypical S-learners. There has been a tendency in the literature to study neuro-atypical S-learners only from the perspective of what they \"lack\" or \"fail to exhibit.\" By focusing on what is common to both neuro-typical and neuro-atypical S-learners, this paper sheds light on the relation between fundamental aspects of language and peripheral ones, that is, what is core and undamageable versus what is peripheral",
            "score": 0.4149567671686422,
            "section_title": "CONCLUSION",
            "char_start_offset": 61594,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.492919921875
        },
        {
            "corpus_id": "232124783",
            "title": "Depth-Bounded Statistical PCFG Induction as a Model of Human Grammar Acquisition",
            "text": "There has long been a distinction within the linguistic discipline of theoretical syntax between a hypothesized model of language that is posited to exist within in the brain of each speaker of that language and the phenomenon of language as it is actually spoken and encountered in the real world. The concept of a mental model of language has been described in terms of langue (de Saussure 1916), linguistic competence (Chomsky 1965), or simply as the grammar of the language, while the details of how language is actually spoken and used have been described as parole (de Saussure 1916), linguistic performance (Chomsky 1965), or sometimes as usage. Chomsky (1965) argues that models of linguistic performance should be informed by models of linguistic competence, but that models of competence should not take performance into account: \"Linguistic theory is concerned primarily with an ideal speakerlistener, in a completely homogeneous speech-community, who knows its language perfectly and is unaffected by such grammatically irrelevant conditions as memory limitations, distractions, shifts of attention and interest, and errors\" (page 3). Within the Chomskyan generative tradition, this idea that syntactic theory should model an idealized grammar of linguistic competence (rather than one that incorporates performance) has remained dominant in the decades since (see Newmeyer 2003, for example). Others outside this tradition have criticized the Chomskyan position in part for its failure to connect idealized theories of competence to actual language usage (for example, see Pylyshyn 1973;Miller 1975;Kates 1976).\n\nThe framework for unsupervised grammar induction presented in this article is significant in that it represents a concrete discovery procedure that can produce both a competence grammar G (a PCFG in Chomsky normal form) and a corresponding formally defined performance grammar G D (another PCFG defined to be sensitive to center-embedding depth). Although PCFGs in principle allow for unlimited recursion in the form of center-embedding (Chomsky and Miller 1963), evidence from corpus studies of spoken and written language use strongly indicates that such recursion essentially never extends beyond the limits of human cognitive memory constraints (Schuler et al. 2010;Noji, Miyao, and Johnson 2016). Given a cognitive",
            "score": 0.41377455091679416,
            "section_title": "Introduction",
            "char_start_offset": 4212,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1586,
                    "end": 1600,
                    "matchedPaperCorpusId": "10569336"
                },
                {
                    "start": 1600,
                    "end": 1612,
                    "matchedPaperCorpusId": "6345319"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31787109375
        },
        {
            "corpus_id": "249017999",
            "title": "Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models",
            "text": "Large-scale language models pretrained jointly on text from many different languages (Delvin, 2019;Lample and Conneau, 2019;Lin et al., 2021) perform very well on various languages and on crosslingual transfer between them (e.g., Kondratyuk and Straka, 2019;Pasini et al., 2021). Due to this success, there has been a great deal of interest in uncovering what these models learn from the multilingual pretraining signal ( \u00a76). However, these works analyze a single model artifact: the final training checkpoint at which the model is considered to be converged. Recent work has also studied monolingual models by expanding the analysis to multiple pretraining checkpoints to see how model knowledge changes across time (Liu et al., 2021). \n\nWe analyze multilingual training checkpoints throughout the pretraining process in order to identify when multilingual models obtain their inlanguage and cross-lingual abilities. The case of multilingual language models is particularly interesting, as the model learns both to capture individual languages and to transfer between them just from unbalanced multitask language modeling for each language. \n\nSpecifically, we retrain a popular multilingual model, XLM-R (Conneau et al., 2020a), and run a suite of linguistic tasks covering 59 languages on checkpoints from across the pretraining process. 1 his suite evaluates different syntactic and semantic skills in both monolingual and cross-lingual transfer settings. While our analysis primarily focuses on the knowledge captured in model output representations over time, we also consider how the performance of internal layers changes during pretraining for a subset of tasks. \n\nOur analysis uncovers several insights into multilingual knowledge acquisition. First, while the model acquires most in-language linguistic information early on, cross-lingual transfer is learned across the entire pretraining process. Second, the order in which the model acquires linguistic information for each language is generally consistent with monolingual models: lower-level syntax is learned prior to higher-level syntax and then semantics. In comparison, the order in which the model learns to transfer linguistic information between specific languages can vary wildly.",
            "score": 0.4134188986243922,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1671
                },
                {
                    "start": 1674,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2253
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 258,
                    "matchedPaperCorpusId": "102351547"
                },
                {
                    "start": 258,
                    "end": 278,
                    "matchedPaperCorpusId": "231878544"
                },
                {
                    "start": 718,
                    "end": 736,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 1206,
                    "end": 1229,
                    "matchedPaperCorpusId": "207880568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7822265625
        },
        {
            "corpus_id": "263134958",
            "title": "A Benchmark for Learning to Translate a New Language from One Grammar Book",
            "text": "By scaling up training compute and data (Kaplan et al., 2020;Hoffmann et al., 2022b), language models have become able to perform ever-increasing numbers of complex tasks from examples in context (Brown et al., 2020;Chowdhery et al., 2022), instructions (Wei et al., 2021;Sanh et al., 2022;Ouyang et al., 2022;Chung et al., 2022), or lightweight finetuning (Li & Liang, 2021; Lester et al., 2021;Hu et al., 2021;Dettmers et al., 2023)-and competence on some of these tasks seems to emerge as a phase transition with scale (Ganguli et al., 2022;Wei et al., 2022a). There is disagreement about whether these capabilities are truly general, reflecting nascent reasoning capabilities (Wei et al., 2022b;Suzgun et al., 2023;Lewkowycz et al., 2022;Lightman et al., 2023) from a meta-learned in-context optimization algorithm (von Oswald et al., 2022;Dai et al., 2022;Ravent\u00f3s et al., 2023); or just illusory, an artifact of discontinuous metrics (Schaeffer et al., 2023) that belie a menagerie of heuristics underneath (Dziri et al., 2023). With internet-scale training data, even the latter could automate many socially and economically valuable tasks, but it would underperform on tasks that are less well-trodden. MTOB aims to help explore this tension.",
            "score": 0.4124530409078866,
            "section_title": "LARGE LANGUAGE MODELS",
            "char_start_offset": 32414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1250
                }
            ],
            "ref_mentions": [
                {
                    "start": 522,
                    "end": 544,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 544,
                    "end": 562,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 680,
                    "end": 699,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 699,
                    "end": 719,
                    "matchedPaperCorpusId": "252917648"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53125
        },
        {
            "corpus_id": "236340884",
            "title": "Ideal Words",
            "text": "The present paper makes an attempt at piecing together the various arguments and ideas about competence acquisition that we have related above. Our position is that linguistic competence is the result of cognitive processes but that it does not preclude the formal definition of an intensional semantics over incomplete models, dependent on a speaker's exposure to performance data. That is, following Partee [47], competence is not super-competence. We will explore what this means in terms of the formalisation of a model.\n\nOur hypothesis, as stated in Sect. 1, is that the acquisition of semantic (and syntactic) competence should be derivable from performance data. The formalisation of competence should have the same components as that of performance, so that performance can be seen as 'incomplete' or 'degraded' competence rather than a fully different type of linguistic object. We have seen that semantic competence can refer to various notions. One relates to the knowledge of core lexical relations [32], another to the ability to retrieve the extension of a term [46], yet another to the 'acceptable' use of a term [32,35,50]. We endeavour in this paper to find a common formalisation underlying these three notions, whilst at the same time acknowledging that they may not emerge jointly (and consequently not fail jointly). The limitations of speakers' competence that we presented in Sect. 2.2 (e.g. not knowing the extensional difference between elms and beeches) should be explicable in terms of the very nature of the performance data they were exposed to. A consequence of our approach is that Katz and Fodor's lexical relations should be discoverable from performance data rather than assumed to be innate, and they should be tightly bound to the state of the syntax-semantics interface in the learner. We will cover this in Sect. 4.3.\n\nThe currently most popular approach to learning meaning from performance data is distributional semantics (henceforth DS-for introductions to the topic, see [6,18,40]). DS is a corpus-driven technique to acquire lexical meaning, in the tradition of distributionalists such as Harris [26]. By virtue of being corpus-driven, DS is usually considered a representation of performance, to be distinguished from the type of lexical relations that might be",
            "score": 0.41234937201085364,
            "section_title": "How to Position this Paper",
            "char_start_offset": 12499,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "9860676"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "9860676"
                },
                {
                    "start": 1132,
                    "end": 1135,
                    "matchedPaperCorpusId": "142943379"
                },
                {
                    "start": 2014,
                    "end": 2017,
                    "matchedPaperCorpusId": "146120618"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.486328125
        },
        {
            "corpus_id": "12242596",
            "title": "Inter-subject variability modulates phonological advance planning in the production of adjective-noun phrases",
            "text": "Language production is a complex and generative process during which cognitive processes unfold in time from concept to articulation (Levelt, 1989). Although words represent the building blocks of sentences, it is generally considered that speakers probably do not encode one word after the other since this would result in scattered, disfluent speech. Nevertheless, it is unlikely that speakers have planned an entire sentence before they start articulating the first lexical word, since this would involve long speech breaks and memory overload. Furthermore, the amount of advance planning is probably not fixed and it may vary according to various linguistic (language, syntax, etc.) and extra-linguistic (speaker's age, speed, stress, etc.) factors. Here we aim at investigating to what extent the span of phonological encoding varies across speakers for adjective-NPs. \n\nTo do so, we first describe how speech production models deal with the question of the amount of advance planning. Then we review how non-experimental data can provide information on this question. Finally, we focus on experimental paradigms and on the contradictory results emerging from the literature. \n\nMost language production models usually agree on the main processing stages involved in word production (Dell, 1986;Levelt et al., 1999), although they do not always agree on the flow of activation across the system, nor on the way speakers plan ahead before speaking. After the activation of a pre-linguistic concept, the formulation process involves semantic and grammatical encoding, where the different thematic roles are attributed a functional role (e.g., subject, object). The second stage in the formulation processes, lexical processing, involves the selection of the lexical entries (lemmas) corresponding to the concepts (lexicalsemantic encoding) and the retrieval/encoding of the lexemes, i.e., of the corresponding phonological codes. Finally, the articulatory plans can be prepared and articulation initiated. The question of how much speakers plan ahead at these different encoding stages is essential to understanding language production mechanisms. The amount of advance planning has been addressed in particular in serial models of language planning (Levelt, 1989), where it has been proposed to be larger at the grammatical and lexical levels than at the level of phonological encoding.",
            "score": 0.4119404791034593,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2389
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 147,
                    "matchedPaperCorpusId": "2363595"
                },
                {
                    "start": 1287,
                    "end": 1299,
                    "matchedPaperCorpusId": "21975212"
                },
                {
                    "start": 1299,
                    "end": 1319,
                    "matchedPaperCorpusId": "2363595"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.154052734375
        },
        {
            "corpus_id": "220045413",
            "title": "How does BERT\u2019s attention change when you fine-tune? An analysis methodology and a case study in negation scope",
            "text": "As large-scale pre-trained language models such as BERT and ELMo have achieved high performance in a variety of natural language processing tasks (Peters et al., 2018a;Radford et al., 2018;Devlin et al., 2019), a growing body of research is devoted to understanding what linguistic properties these language models have acquired. Recent work uses probes, which are supervised models trained to predict linguistic properties including morphology (Belinkov et al., 2017), syntax (Hewitt and Manning, 2019) and semantics (Peters et al., 2018b), etc. (See Belinkov and Glass (2019) for a complete survey.) A good probing performance is considered as evidence that the language models have learned the linguistic knowledge. \n\nWhat is not yet well understood is how this encoded linguistic knowledge changes when a pretrained language model is fine-tuned for a downstream task. Peters et al. (2019) applies a supervised probe both before and after fine-tuning BERT, and suggests that fine-tuning makes the internal representation task-sensitive. But with supervised probes it can be difficult to disentangle what was learned by the probe from what was present in the internal representation (Hewitt and Liang, 2019). \n\nRecent studies have thus turned to unsupervised probes that require no additional training of the model and instead look directly at the attention mechanism, i.e., how much to care about other words when computing the next version of the current word. Clark et al. (2019) inspected pretrained transformers and found several syntactic properties encoded in an intuitive way, where the maximum attention from a dependent is on its syntactic head. But only the pretrained models were considered, not what happened to these intuitive encodings after fine-tuning to a downstream task. \n\nWe argue that if some interpretable encoding of linguistic knowledge is a good explanation of a model, rather than showing it in the pretrained model, it is more important to show it will be enhanced by fine-tuning on a task where that linguistic knowledge is necessary. If the encoding is not enhanced by such fine-tuning, then the model must be using some other mechanism to encode that linguistic knowledge.",
            "score": 0.41186893479663,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1792
                },
                {
                    "start": 1795,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 168,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 168,
                    "end": 189,
                    "matchedPaperCorpusId": "23678406"
                },
                {
                    "start": 189,
                    "end": 209,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 445,
                    "end": 468,
                    "matchedPaperCorpusId": "7100502"
                },
                {
                    "start": 477,
                    "end": 503,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 518,
                    "end": 540,
                    "matchedPaperCorpusId": "52098907"
                },
                {
                    "start": 552,
                    "end": 577,
                    "matchedPaperCorpusId": "56657817"
                },
                {
                    "start": 872,
                    "end": 892,
                    "matchedPaperCorpusId": "76666127"
                },
                {
                    "start": 1185,
                    "end": 1209,
                    "matchedPaperCorpusId": "202538609"
                },
                {
                    "start": 1465,
                    "end": 1484,
                    "matchedPaperCorpusId": "184486746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63330078125
        },
        {
            "corpus_id": "272753373",
            "title": "Making Large Language Models into World Models with Precondition and Effect Knowledge",
            "text": "Counter-intuitively, LLMs trained on sufficiently rich text sources may be induced to behave like a world model. While an LLM may not be reasoning about the underlying transition dynamics to answer a question about the application of brakes on a bike, the same LLM can be asked questions about the applicability of actions and how the world is changed by actions carried out in the real world. For example, large language models such as GPT-4, Gemini 1.5 Pro, and Claude 3.5 Sonnet can reliably answer questions about what must be true in the world for an action to be performable (also called preconditions), and what aspects of the world does one desire to affect through the execution of an action (also called effects). Precondition and effect knowledge about actions can be used to assemble a world model because they tell us whether an action can be performed and what states we will transition to if an action is performed. \n\nIn this work, we show how to induce large language models to behave like world models. As depicted in Figure 1, a world model must perform two functions. (1) It must determine action applicability. We must be able to answer the question: \"can this action be performed at this time?\" (2) If the action is applicable, a world model must be able to answer the question: \"how the world is different if it were to be performed?\" This is equivalent to asking \"what state I will be in?\" We fine-tune two LLMs, one that predicts the preconditions of an action, and another that predicts the effects of an action. We provide a means for these two models to work together, along with procedures for checking preconditions against a world state and applying effects to alter the world state, thus fully replicating the functionality of a world model. We also provide a method for using LLMs to generate synthetic data with which to fine-tune the above two models. \n\nWe conduct human-participant studies that assess the extent to which people agree that the corpus of preconditions and effects generated by our models matches their own world model understanding; human understanding of the real world is the best source of ground-truth data available.",
            "score": 0.41169049252991197,
            "section_title": "Introduction",
            "char_start_offset": 1930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 930
                },
                {
                    "start": 933,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1885
                },
                {
                    "start": 1888,
                    "end": 2172
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "275788769",
            "title": "Implicit Causality-biases in humans and LLMs as a tool for benchmarking LLM discourse capabilities",
            "text": "Outside the realm of psycholinguistics, Binz and Schulz (2023) used vignette studies from cognitive psychology to gauge the cognitive abilities of GPT-3 (Brown et al., 2020). As the abilities and potential applications of language models widen, there has been a surge in benchmarking systems that either allow for a more sophisticated model evaluation (Kiela et al., 2021) or extrapolate far beyond NLP into tasks based on general reasoning, world-knowledge or domain-specific problem-solving (Srivastava et al., 2022). This reflects a general shift towards using LLMs as general-purpose problem solvers. However, fine-grained linguistic analysis of even highly advanced models can still be useful. For instance, although GPT-3 is capable of various NLP and problem-solving tasks (Zong and Krishnamachari, 2022;Yang et al., 2022;Zong and Krishnamachari, 2023), it still exhibits undesirable linguistic behaviour (Schuster and Linzen, 2022). It should be noted that a lot of the existing research, especially in benchmarking, focuses on isolating specific phenomena without assessing the interconnectivity between linguistic tests on the one hand and their implications for broader reasoning capabilities on the other. \n\nIn this paper, we investigate whether LLMs exhibit human-like behaviour for a linguistic phenomenon called 'Implicit Causality' (IC), which is widely researched in pragmatics and has connections to analyses of LLMs' general causal reasoning ability (Hong et al., 2024). As such, IC is a cover notion for a number of properties associated with interpersonal verbs like fascinate and admire (1). Over the last fifty years, linguistic and psycholinguistic research has investigated how participants continue such sentences (with typical continuations in parentheses): \n\n(1) a. Mary fascinated Peter. (e.g., She always came up with great suggestions) b. Mary admired Peter. \n\n(e.g., He was a great dancer) \n\nIt has been shown that IC verbs display three different biases.",
            "score": 0.41124471710389743,
            "section_title": "Introduction",
            "char_start_offset": 1800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1784
                },
                {
                    "start": 1787,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1889
                },
                {
                    "start": 1892,
                    "end": 1921
                },
                {
                    "start": 1924,
                    "end": 1987
                }
            ],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 62,
                    "matchedPaperCorpusId": "250113371"
                },
                {
                    "start": 153,
                    "end": 173,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 780,
                    "end": 811,
                    "matchedPaperCorpusId": "259738553"
                },
                {
                    "start": 811,
                    "end": 829,
                    "matchedPaperCorpusId": "237485500"
                },
                {
                    "start": 829,
                    "end": 859,
                    "matchedPaperCorpusId": "259738553"
                },
                {
                    "start": 1469,
                    "end": 1488,
                    "matchedPaperCorpusId": "265150278"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3466796875
        },
        {
            "corpus_id": "213613608",
            "title": "Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches",
            "text": "Humans perform natural language inference based on a vast amount of external knowledge about language and the world. To support machines' inference ability, compiled knowledge resources become important. It is an unsolved problem to comprehensively taxonomize all of the human knowledge and reasoning required to perform general inference (Davis & Marcus, 2015). For the purpose of this survey, though, we attempt to provide an overview of three types of knowledge resources: linguistic knowledge, common knowledge, and commonsense knowledge. Although modern approaches often rely on large text corpora to implicitly incorporate external knowledge without regard to what types of knowledge are required, we think it would be helpful to distinguish different types of knowledge to facilitate better understanding of the nature of NLI tasks.\n\nLinguistic knowledge. In order to comprehend human language, machines first need linguistic knowledge, i.e., knowledge about the language. This includes an understanding of word meanings, grammar, syntax, semantics, and discourse structure. Having linguistic knowledge gives a human or machine the basic capabilities of understanding language, and is a required property of virtually any NLP system, even those not created for NLI tasks.\n\nCommon knowledge. Common knowledge refers to well-known facts about the world that are often explicitly stated, e.g., \"canine distemper is a domestic animal disease\" (Cambria, Song, Wang, & Hussain, 2011). This kind of knowledge is often referred to in human communication (Cambria et al., 2011). Some types of common knowledge may be domain-specific, e.g., the knowledge required to answer questions on an elementary-level science test (Sugawara, Yokono, & Aizawa, 2017). While domain-specific knowledge is obviously useful for domain-specific applications, much of this knowledge may not be needed for general-purpose communication with humans.\n\nCommonsense knowledge. Commonsense knowledge, on the other hand, is typically unstated, as it is considered obvious to most humans (Cambria et al., 2011), and consists of universally accepted beliefs about the world (Nunberg, 1987). Davis and Marcus (2015) demonstrate this: \"if you see a six-foot-tall person holding a two-foot-tall person in his arms, and you are told they are father and son, you do not have to ask which is which",
            "score": 0.4105056750641065,
            "section_title": "Scope of Knowledge Resources",
            "char_start_offset": 7826,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 361,
                    "matchedPaperCorpusId": "13583137"
                },
                {
                    "start": 1446,
                    "end": 1484,
                    "matchedPaperCorpusId": "18922883"
                },
                {
                    "start": 1553,
                    "end": 1575,
                    "matchedPaperCorpusId": "18922883"
                },
                {
                    "start": 1717,
                    "end": 1751,
                    "matchedPaperCorpusId": "29159687"
                },
                {
                    "start": 2059,
                    "end": 2081,
                    "matchedPaperCorpusId": "18922883"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53271484375
        },
        {
            "corpus_id": "277065878",
            "title": "Will Pre-Training Ever End? A First Step Toward Next-Generation Foundation MLLMs via Self-Improving Systematic Cognition",
            "text": "Pre-training has been a key driver of progress in artificial intelligence (AI) research [1], enabling the development of diverse large language models (LLMs) [2,3,4] and multimodal large language models (MLLMs) [5,6,7,8,9] that exhibit impressive performance across a broad spectrum of tasks. During pre-training, these models acquire diverse forms of knowledge-including factual information [10,11,12], multimodal understanding [7,13], and strong capabilities such as conversational proficiency [14,15], analytical reasoning, and generative skills [16,17]. Despite these advances, concerns have emerged that pre-training may eventually plateau due to the limited availability of high-quality real-world data [1]. In response, recent efforts have increasingly focused on inference-time compute scaling [18,19,20] and post-training optimization [21,22,23,24,25,26] to further enhance model capabilities. However, merely scaling inference-time compute or applying post-training optimization is often insufficient [27]. For example, some models can exhibit significantly worse self-improvement performance than others under identical reinforcement learning (RL) [28] training, primarily due to the initial absence of key capacities [20,29]. We hypothesize that effective model advancement requires a strong synergy between pre-training and downstream mechanisms such as inference-time compute scaling and post-training optimization. \n\nTo validate this hypothesis, we focus on multimodal pre-training for foundation MLLM construction [7,30,31,32], a critical step toward real-world understanding [33]. Prevalent approaches [30,31,32,34,35] typically rely on large-scale training with high-quality image-caption data generated by advanced MLLMs [4,13,36] to endow models with diverse multimodal knowledge and fine-grained visual perception skills (e.g., identifying detailed object attributes) [31,34,37,38].",
            "score": 0.41025226138375526,
            "section_title": "introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1429
                },
                {
                    "start": 1432,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1903
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 220,
                    "matchedPaperCorpusId": "259243718"
                },
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "256389395"
                },
                {
                    "start": 844,
                    "end": 848,
                    "matchedPaperCorpusId": "252847579"
                },
                {
                    "start": 848,
                    "end": 851,
                    "matchedPaperCorpusId": "267657805"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.689453125
        },
        {
            "corpus_id": "258865433",
            "title": "Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models",
            "text": "In this paradigm, LLMs are finetuned to learn how to follow instructions on a few tasks the model is capable of and are subsequently expected to generalize and follow instructions for novel tasks by utilizing their pre-training knowledge (residing in their parameters). \n\nHowever, it is not clear to what extent this as- (3) The evaluation on the application of acquired knowledge is estimated through the performance on the test split, after finetuning M \u03b8 on the downstream task. \n\nsumption holds in practice, giving rise to a central question: how much of parametric knowledge will get applied in downstream tasks? If the causal link between \"identifiable knowledge\" and its practical application in downstream tasks is not established (Kulmizev and Nivre, 2021), the mere presence of knowledge within a model's parameters does not necessarily guarantee its utilization in such tasks. This raises questions about the assertion of pretrained language models (PLMs) as differentiable knowledge bases (Hao et al., 2022) and their overall capabilities. For instance, as demonstrated by Qin et al. (2023), ChatGPT's performance lags behind its foundational model, GPT-3.5, in areas including commonsense and logical reasoning tasks. \n\nPrevious studies have investigated this question within linguistic domains and have demonstrated that although PLMs have the capacity to encode linguistic knowledge, they may not effectively employ it in downstream tasks. For example, McCoy et al. (2019) illustrates that PLMs employ syntactic heuristics to solve NLI even though they are able to represent proper linguistic hierarchies (Tenney et al., 2019a), even after finetuning (Merchant et al., 2020;Zhou and Srikumar, 2022). Warstadt et al. (2020) provide evidence that RoBERTa requires data inoculation or pre-training with extensive data in order to effectively utilize its hierarchical linguistic knowledge. In a more recent study, Lover-ing et al. (2021) demonstrate that the quantity of \"evidence\" presented in the finetuning dataset influences the features that PLMs rely on during the finetuning process. Specifically, the model may resort to lexical heuristics when the finetuning signal toward linguistic features is insufficient.",
            "score": 0.4101617096000319,
            "section_title": "Introduction",
            "char_start_offset": 1732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 272,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 1001,
                    "end": 1019,
                    "matchedPaperCorpusId": "249525582"
                },
                {
                    "start": 1468,
                    "end": 1487,
                    "matchedPaperCorpusId": "59599752"
                },
                {
                    "start": 1620,
                    "end": 1642,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 1666,
                    "end": 1689,
                    "matchedPaperCorpusId": "216914339"
                },
                {
                    "start": 1689,
                    "end": 1713,
                    "matchedPaperCorpusId": "235658872"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63525390625
        },
        {
            "corpus_id": "270063546",
            "title": "Phase Transitions in the Output Distribution of Large Language Models",
            "text": "In a physical system, changing parameters such as temperature can induce a phase transition: an abrupt change from one state of matter to another. Analogous phenomena have recently been observed in large language models. Typically, the task of identifying phase transitions requires human analysis and some prior understanding of the system to narrow down which low-dimensional properties to monitor and analyze. Statistical methods for the automated detection of phase transitions from data have recently been proposed within the physics community. These methods are largely system agnostic and, as shown here, can be adapted to study the behavior of large language models. In particular, we quantify distributional changes in the generated output via statistical distances, which can be efficiently estimated with access to the probability distribution over next-tokens. This versatile approach is capable of discovering new phases of behavior and unexplored transitions -- an ability that is particularly exciting in light of the rapid development of language models and their emergent capabilities.",
            "score": 0.4100735921544345,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67236328125
        },
        {
            "corpus_id": "261277016",
            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
            "text": "Our work provides evidence that there exist other capabilities or types of generalizations (e.g. non-syntactic abilities or even more fine-grained syntactic sub-abilities) that can only be learned after others, or at least only once the model reaches some particular state. Isolating these sequential dependencies is an exciting direction for future work. \n\nN-gram learning and refinement. As a further step towards understanding fine-grained learning patterns in language models, our work investigates whether simple statistical regularities can explain learning patterns such as the sudden loss changes discussed above. We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts ( \u00a76.2). High probability n-grams in particular are less likely to be \"forgotten\", suggesting that evolving model generalizations throughout pre-training have larger effects on low-probability n-grams. Combined with findings that language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; Chang and Bergen, 2022b), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; Liu et al., 2021). \n\nRobust model deployment. Our work also has implications for robust model deployment. High token frequencies and n-gram probabilities are by far the most influential predictors of early and stable learning in language models ( \u00a76.2, with marginal additional effects of context lengths and likelihoods). As language models are deployed in do-mains with highly-specific vocabulary terms (e.g. healthcare, law, and finance; Yang et al., 2024), the accurate prediction of infrequent domain-specific terms during text generation is likely to require extensive pre-training (late acquisition, likely mitigated by large pre-training datasets). Such domainspecific text generation is also likely to be unstable across models and pre-training steps (high variability, potentially more difficult to mitigate).",
            "score": 0.4097270538154639,
            "section_title": "Discussion",
            "char_start_offset": 29451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 1112,
                    "end": 1136,
                    "matchedPaperCorpusId": "238408423"
                },
                {
                    "start": 1370,
                    "end": 1387,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 1811,
                    "end": 1829,
                    "matchedPaperCorpusId": "258331833"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91845703125
        },
        {
            "corpus_id": "247920720",
            "title": "Psicolingu\u00edstica e Modelos Baseados no Uso / Psycholinguistics and Usage-based Models",
            "text": ": Linguistic theories and language processing theories have distinct study objects and, therefore, also distinct goals and research methods. In this article, we attempt to demonstrate that, despite such differences, there is room in Psycholinguistics for language processing models that are guided by notions such as communicative efficiency and speaker conversational goals, which are also held dear by usage-based linguistic models. To this end, we present two language processing models with widely varying assumptions and analyze how these reveal points of convergence and divergence in relation to different linguistic theories. In doing so, we hope to facilitate collaborations between linguists and psycholinguists that develop research within usage-based frameworks.",
            "score": 0.4096839651034413,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.354248046875
        },
        {
            "corpus_id": "258686446",
            "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
            "text": "Perhaps the most fundamental ability for a language model is to produce coherent and fluent English text, which, as we discussed above, requires not only grammatical and lexical knowledge, but also factual information and contextual reasoning. How well can language models generate text that is consistent, diverse, and meaningful? And what are the minimal requirements for a language model to achieve this ability? \n\nSo far, the evidence points to the fact that producing coherent text already requires quite a large scale: small language models (SLMs) are very limited in their performance and capabilities, especially in text generation tasks. For example, models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate any consistent text beyond a few words even after extensive training on large corpora such as the Pile [9], Common Crawl [1] or the CC-100 [31]. These models often produce incoherent, repetitive, or nonsensical sentences, and fail to maintain a clear topic or a logical structure across paragraphs [12]. This raises the question of whether the emergence of the ability to speak coherent English requires large models (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). \n\nHowever, it is currently not clear whether the inability of SLMs to produce coherent text is a result of the intrinsic complexity of natural language, or of the excessive breadth and diversity of the corpora used for training. \n\nWhen we train a model on Wikipedia, for example, we are not only teaching it how to speak English, but also how to encode and retrieve an immense amount of facts and concepts from various domains and disciplines. Could it be that SLMs are overwhelmed by the amount and variety of information they have to process and store, and that this hinders their ability to learn the core mechanisms and principles of language? \n\nThis raises the question of whether we can design a dataset that preserves the essential elements of natural language, such as grammar, vocabulary, facts, and reasoning, but that is much smaller and more refined in terms of its breadth and diversity. Such a dataset would allow us to isolate and examine the minimal requirements for a language model to generate coherent and fluent text, and to evaluate its performance and capabilities more precisely and fairly.",
            "score": 0.4091838216078068,
            "section_title": "Introduction",
            "char_start_offset": 2107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1517
                },
                {
                    "start": 1520,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1936
                },
                {
                    "start": 1939,
                    "end": 2189
                },
                {
                    "start": 2190,
                    "end": 2402
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71044921875
        },
        {
            "corpus_id": "261822542",
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "text": "Recent work in interpretability has begun to take advantage of the chronology of training in developing a better understanding of models. In some of the first papers explicitly interpreting the training process, Some studies find that specific capabilities are often learned in a particular order. In autoregressive language models, Xia et al. (2023) show that training examples tend to be learned in a consistent order independent of model size. In MLMs, Chiang et al. (2020) find that different part of speech tags are learned at different rates, while Warstadt et al. (2020b) find that linguistic inductive biases only emerge late in training. Our work likewise finds that extrinsic grammatical capabilities emerge at a consistent point in training. \n\nWhile our phase transition results mirror Murty et al. (2022)'s findings that the latent structure of autoregressive language models plateaus in its adherence to formal syntax, their work also finds the structure continues to become more tree-like long after syntacticity plateaus. Their results suggest that continued improvements in performance can still be attributed to interpretable hierarchical latent structure, which may be an inductive bias of some autoregressive model training regimes (Saphra & Lopez, 2020). \n\nAlthough Appendix I precludes the impact of thresholding effects (Schaeffer et al., 2023;Srivastava et al., 2022) on our results, the relationship between the structure onset and capabilities onset does reflect a dependency pattern similar to the checkmate-in-one task, which Srivastava et al. (2022) consider to be precipitated by smooth scaling in the ability to produce valid chess moves. Even in cases where there is no clear dependency between extrinsic capabilities, there may be internal structures like SAS that emerge smoothly, which can be interpreted as progress measures (Barak et al., 2022;Nanda et al., 2023;Merrill et al., 2023).",
            "score": 0.4089635714087517,
            "section_title": "C.2 INTERPRETING TRAINING",
            "char_start_offset": 35208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1921
                }
            ],
            "ref_mentions": [
                {
                    "start": 1251,
                    "end": 1273,
                    "matchedPaperCorpusId": "222272198"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.822265625
        },
        {
            "corpus_id": "278237785",
            "title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
            "text": "Large Language Models (LLMs), such as OpenAI's GPT models (Brown et al., 2020), Google's Gemini models (Team et al., 2024), and Meta's Llama models (Touvron et al., 2023), are typically trained on high-resource natural languages (e.g., English, Spanish, and Chinese). This focus on high-resource languages disadvantages speakers of low-resource languages, as training models for these languages are more challenging due to their inherent complexity (Team et al., 2022). Furthermore, semantic ambiguity, grammatical complexities, and contextual dependencies in natural languages can limit the capabilities of an LLM in precise logical reasoning. Since natural language often relies on implied meaning, subtle cues, and flexible syntax, models trained primarily on data using these principles * These authors contributed equally to this work. may struggle to follow strict rules needed for logical reasoning (Asher et al., 2023). \n\nTo isolate these reasoning abilities from language-specific challenges, we propose the evaluation of LLMs within a controlled setting using formal logical language. Logical languages, characterized by strict syntax and precise semantics, eliminate many of the extraneous factors present in natural languages, allowing us to focus squarely on pattern recognition and problem solving. Although prior benchmarks, such as LOGIGLUE (Luo et al., 2024), provide structured reasoning tasks, these typically rely on predefined reasoning steps, making it challenging to determine whether an LLM can autonomously identify and apply logical rules. In contrast, our benchmark, Rosetta-PL, evaluates whether LLMs can discover logical patterns within a propositional language, thereby measuring reasoning ability without relying on predefined inference steps or extraneous linguistic factors. Research on applying LLMs to logic-based problem solving is relatively scarce, and while chain-ofthought (CoT) prompting has gained popularity in natural language tasks (Wei et al., 2023), its effectiveness in logical or symbolic contexts remains largely unexplored (Creswell et al., 2022).",
            "score": 0.40891149307016605,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 927
                },
                {
                    "start": 930,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 2098
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "225524474",
            "title": "Linguistic competence and academic performance in English of grade 11 students in Sta. Teresa College",
            "text": "Lexical ambiguity arises when at least one word in a phrase has more than one meaning. Our knowledge of lexical and structural ambiguities reveals that the meaning of a linguistic expression is built both on the words it contains and its syntactic structure. To account for speakers' knowledge of grammatically, constituent structure, and relations between sentences, as well as for the limitless creativity of our linguistic competence, we concluded that the grammar must contain syntactic rules. To account for speaker's knowledge of the truth, reference, entailment and ambiguity of sentences, as well as for our ability to determine the meaning of a limitless number of expressions, we must suppose that the grammar contains semantic rules that combine the meanings of words into meaningful phrases and sentences. Our semantic rules must be sensitive not only to the meaning of individuals but to the structure in which they occur (Fromkin, Rodman, & Hyams, 2010). \n\nSyntax -Another aspect of linguistic competence relates to the nature of linguistic structure, in particular syntax, and its acquisition. The object of inquiry is the linguistic competence of individual speakers or the language and its nature, use and acquisition. This competence is acquired through interplay of the innate language faculty and the social norms of the learning environment. Faarlund and associates (2012), building on their work in the first phase of CSMN, plans to focus their research on two specific issues: (a) the transfer of structural knowledge from one generation to the next and thus through time (diachronic syntax), and (b) the role of the socio-cultural environment in the evolution of language, especially related to the question of linguistic and cultural complexity. In addition, Koll (2012) added that despite what modern eyes appear to be a spelling and grammatical nightmare such writing was and is profound, inspirational and indeed immortal. He even concluded that great writers do not become great because they blindly obey grammar and punctuation rules, they are great because of what they have to say and how they go about saying it. However, Koll (2012) was also contradicting the notion that he is entirely suggesting that developing skills in spelling, grammar and punctuation is unnecessary.",
            "score": 0.40882662551955984,
            "section_title": "International Journal of Research Studies in Education 123",
            "char_start_offset": 10290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2307
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1953125
        },
        {
            "corpus_id": "256863867",
            "title": "Moving away from lexicalism in psycho- and neuro-linguistics",
            "text": "In standard models of language production or comprehension, the elements which are retrieved from memory and combined into a syntactic structure are \u201clemmas\u201d or \u201clexical items.\u201d Such models implicitly take a \u201clexicalist\u201d approach, which assumes that lexical items store meaning, syntax, and form together, that syntactic and lexical processes are distinct, and that syntactic structure does not extend below the word level. Across the last several decades, linguistic research examining a typologically diverse set of languages has provided strong evidence against this approach. These findings suggest that syntactic processes apply both above and below the \u201cword\u201d level, and that both meaning and form are partially determined by the syntactic context. This has significant implications for psychological and neurological models of language processing as well as for the way that we understand different types of aphasia and other language disorders. As a consequence of the lexicalist assumptions of these models, many kinds of sentences that speakers produce and comprehend\u2014in a variety of languages, including English\u2014are challenging for them to account for. Here we focus on language production as a case study. In order to move away from lexicalism in psycho- and neuro-linguistics, it is not enough to simply update the syntactic representations of words or phrases; the processing algorithms involved in language production are constrained by the lexicalist representations that they operate on, and thus also need to be reimagined. We provide an overview of the arguments against lexicalism, discuss how lexicalist assumptions are represented in models of language production, and examine the types of phenomena that they struggle to account for as a consequence. We also outline what a non-lexicalist alternative might look like, as a model that does not rely on a lemma representation, but instead represents that knowledge as separate mappings between (a) meaning and syntax and (b) syntax and form, with a single integrated stage for the retrieval and assembly of syntactic structure. By moving away from lexicalist assumptions, this kind of model provides better cross-linguistic coverage and aligns better with contemporary syntactic theory.",
            "score": 0.4085469623576359,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.414306640625
        },
        {
            "corpus_id": "9588773",
            "title": "Understanding Grounded Language Learning Agents",
            "text": "Importantly, the knowledge acquired by such agents while mastering these tasks also permits the interpretation of familiar language in entirely novel surroundings, and the execution of novel instructions composed of combinations of familiar words (Chaplot et al., 2017;Hermann et al., 2017). \n\nThe potential impact of situated linguistic agents, VQA models and other grounded language learning systems is vast, as a basis for human users to interact with situated learning applications such as self-driving cars and domestic robotic tools. However, our understanding of how these agents learn and behave is limited. The challenges of interpreting the factors or reasoning behind the decisions and predictions of neural networks are well known. Indeed, a concerted body of research in both computer vision (Zeiler & Fergus, 2014;Simonyan et al., 2014;Yosinski et al., 2015) and natural language processing (Linzen et al., 2016;Strobelt et al., 2016) has focused on addressing this uncertainty. As grounded language learning agents become more prevalent, then, understanding their learning dynamics, representation and decision-making will become increasingly important, both to inform future research and to build confidence in users who interact with such models. \n\nWe therefore aim to establish a better understanding of neural network-based models of grounded language learning, noting the parallels with research in neuroscience and psychology that aims to understand human language acquisition. Extending the approach of Ritter et al. (2017), we adapt various experimental techniques initially developed by experimental psychologists (Landau et al., 1988;Markman, 1990;Hollich et al., 2000). In line with typical experiments on humans, our experimental simulations are conducted in a highly controlled environment: a simulated 3D world with a limited set of objects and properties, and corresponding unambiguous, symbolic linguistic stimuli (Figure 1). However, the simplicity and generality of our architecture and the form of the inputs to the model (continuous visual plus symbolic linguistic) make the proposed methods and approach directly applicable to VQA and other tasks that combine linguistic and visual data. Using these methods, we explore how the training environment of our agent affects its learning outcomes and speed, measure the generality and robustness of its understanding of certain fundamental linguistic concepts, and test for biases in the decisions it takes once trained.",
            "score": 0.40850194123108446,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2223
                },
                {
                    "start": 2224,
                    "end": 2501
                }
            ],
            "ref_mentions": [
                {
                    "start": 805,
                    "end": 828,
                    "matchedPaperCorpusId": "3960646"
                },
                {
                    "start": 828,
                    "end": 850,
                    "matchedPaperCorpusId": "1450294"
                },
                {
                    "start": 850,
                    "end": 872,
                    "matchedPaperCorpusId": "9591565"
                },
                {
                    "start": 1638,
                    "end": 1659,
                    "matchedPaperCorpusId": "205117480"
                },
                {
                    "start": 1659,
                    "end": 1673,
                    "matchedPaperCorpusId": "15076873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "275907001",
            "title": "Advancing Mathematical Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages",
            "text": "To address the challenge of insufficient mathematical reasoning capabilities in large language models (LLMs), various math-specific LLMs are developed. These include models that enhance performance from the pre-training stage, such as LLEMMA (Azerbayev et al., 2023), DeepSeekMath (Shao et al., 2024), InternLM-Math (Ying et al., 2024), and Qwen2-Math (Yang et al., 2024a), as well as models that improve through post-training, such as MetaMath (Yu et al., 2023), WizardMath (Luo et al., 2023), and KwaiYiiMath (Fu et al., 2023). These models generally follow a common training paradigm. During the pre-training stage, math-related corpora are filtered from extensive internet data to augment the model's mathematical knowledge. During the post-training stage, they typically utilize problem datasets and their augmented versions, such as Program-of-Thought (PoT) (Chen et al., 2022), evol-Instruct (Xu et al., 2023), and Tool-Integrated Reasoning (TIR) (Gou et al., 2023;Yin et al., 2024), to construct supervised datasets for Supervised Fine-Tuning (SFT). This enables the models to follow instructions and produce outputs in the desired format. Recently, there is a growing focus on constructing preference datasets for the solution process to perform Step-DPO (Lai et al., 2024) or online-RLHF (Dong et al., 2024). These approaches aim to obtain more accurate reasoning pathways, thereby significantly enhancing the mathematical reasoning capabilities of the models. \n\nDue to the intrinsic distinction between mathematical knowledge and general world knowledge, different strategies are required for their effective acquisition and application. The primary challenge in acquiring world knowledge lies in memorizing and understanding vast amounts of information, necessitating large corpora during the pre-training phase to enhance knowledge reserves (Roberts et al., 2020;Petroni et al., 2019;Dubey et al., 2024).",
            "score": 0.40834364011506286,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1917
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 266,
                    "matchedPaperCorpusId": "264172303"
                },
                {
                    "start": 445,
                    "end": 462,
                    "matchedPaperCorpusId": "262084051"
                },
                {
                    "start": 864,
                    "end": 883,
                    "matchedPaperCorpusId": "253801709"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.358154296875
        },
        {
            "corpus_id": "258686446",
            "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
            "text": "Natural language is rich and diverse. It is not only a system of rules and symbols, but also a way of conveying and interpreting meaning [32]. To understand and produce language, one needs not only to master the technical rules of grammar and knowledge of vocabulary, but also to have sufficient factual information and to be able to reason logically and contextually. Therefore, autoregressive language models, which are able to generate coherent English text, must have acquired some degree of these capabilities as well. For example, consider the following incomplete sentence: Jack was hungry, so he went looking for \u27e8 \u27e9 To complete this sentence in a sensible way, the language model needs to know that hunger is a state that motivates people to seek food, and that food is a category of things that can satisfy hunger. It also needs to choose a word that fits the syntactic and semantic constraints of the sentence (such as \"a snack\"), and that is plausible given the situation and the background knowledge. \n\nAn example that illustrates the need for reasoning is: \n\nLily wanted to get either a cat or a dog. Her mother didn't let her get a dog so instead she \u27e8 \u27e9 \n\nTo complete this sentence, the language model needs to invoke reasoning: it needs to apply the principle of disjunction elimination: if Lily wants either a cat or a dog, and she cannot get a dog, then she must choose a cat. It also needs to choose a words that expresses Lily's intention or action that is coherent with the tone and style of the text. Language models have been shown to exhibit a range of emergent abilities, such as summarization, arithmetic, translation, and commonsense reasoning, as they are scaled up in size and trained on diverse and large corpora [24,4,5,21]. These abilities suggest that language models are not only learning the surface patterns of language, but also acquiring some degree of semantic and logical understanding of the world and the text. However, it is not clear at what scale these abilities emerge, and how they depend on the model architecture and the data distribution. \n\nPerhaps the most fundamental ability for a language model is to produce coherent and fluent English text, which, as we discussed above, requires not only grammatical and lexical knowledge, but also factual information and contextual reasoning.",
            "score": 0.40815515993594265,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1013
                },
                {
                    "start": 1016,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2089
                },
                {
                    "start": 2092,
                    "end": 2335
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 141,
                    "matchedPaperCorpusId": "56798209"
                },
                {
                    "start": 1744,
                    "end": 1748,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1748,
                    "end": 1750,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "261682256",
            "title": "Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis",
            "text": "(2) Memorize and recall commonsense knowledge (such as the definition of entities, the relationship between entities, and the relationship between events) for understanding the language. Commonsense knowledge supports the commonsense reasoning process, to allow the model to understand the material world and human society. Cognitive psychology theory concludes such ability as Crystallized Intelligence (Ziegler et al. 2012;Cattell 1963); \n\n(3) Understand abstract relationships and make inferences. Which is the foundation of high-level cognitive processes, such as planning, problem-solving, and decision-making (Miller and Wallis 2009;Alexander 2016). Such ability is called Fluid Intelligence in cognitive psychology (Ziegler et al. 2012;Cattell 1963); \n\n(4) Understand and follow human instructions. \n\nThrough the pretraining process on the large-scale corpus, the LLMs have demonstrated impressive performance upon various linguistic tasks, such as syntactic parsing. Hence, in this paper, we focus on testing the relationship between the model hallucination with the (2) (3), and (4) levels of ability. Based on linguistic knowledge, commonsense knowledge, and relationship modeling ability, LLMs are able to model language. On this basis, models learn to complete tasks under human instruction. The composition of these fundamental abilities can cover a wide span of NLP tasks. Hence, we design tasks and recognize risk factors to probe the deficiency of model capabilities. \n\nTask Setting However, it would be rather challenging to construct a task that can simultaneously probe all the capabilities abovementioned. Hence, in this paper, we separately quantify the hallucination brought by different subtypes of model capability deficiency using a commonsense QA task, a relational reasoning task, and a counterfactual commonsense reasoning task. Figure 2 shows the tasks and corresponding prompts. \n\nCommonsense QA Task Among various kinds of commonsense knowledge, we focus on investigating the hallucination level about the definition of entities and abstract concepts. These two kinds of knowledge stand as representatives of two major kinds of commonsense knowledge, factual knowledge. As other kinds of factual knowledge and relational knowledge, such as event and event relationship knowledge share a similar learning process, the conclusion on entities/concepts and their relationships would also be applicable to other kinds of factual knowledge and relational knowledge.",
            "score": 0.4074629325172666,
            "section_title": "Model Capability Based Risk Factors Identification",
            "char_start_offset": 12072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1483
                },
                {
                    "start": 1486,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1908
                },
                {
                    "start": 1911,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2200
                },
                {
                    "start": 2201,
                    "end": 2490
                }
            ],
            "ref_mentions": [
                {
                    "start": 404,
                    "end": 425,
                    "matchedPaperCorpusId": "16455018"
                },
                {
                    "start": 425,
                    "end": 437,
                    "matchedPaperCorpusId": "143592190"
                },
                {
                    "start": 639,
                    "end": 653,
                    "matchedPaperCorpusId": "44751247"
                },
                {
                    "start": 722,
                    "end": 743,
                    "matchedPaperCorpusId": "16455018"
                },
                {
                    "start": 743,
                    "end": 756,
                    "matchedPaperCorpusId": "143592190"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5078125
        },
        {
            "corpus_id": "259251885",
            "title": "Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective",
            "text": "Large language models (Brown et al., 2020;Chowdhery et al., 2022;Hoffmann et al., 2022;Zhang et al., 2022a;Scao et al., 2022;Touvron et al., 2023) have emerged as a powerful form of intelligence. These models demonstrate numerous traits associated with both human and superhuman intelligence. They can engage in natural conversations with humans (OpenAI, 2022), learn from limited examples (Dong et al., 2022), solve complex reasoning problems (Wei et al., 2022), generate programs (Chen et al., 2021), and pass exams designed for human professionals (OpenAI, 2023). Although the capabilities of large language models have been extensively documented, our understanding of the underlying cognitive mechanisms that enable these capabilities remains limited. By consuming a huge collection of records of human behavior and knowledge, have these models managed to think and reason like humans? Or are they merely copycats? If neither is the case, what exactly is their \"model of thought\"? Providing a scientific answer to these questions is crucial for dispelling unfounded speculations about large language models and guiding their future development. \n\nIn this paper, we attempt to mathematically characterize the cognitive process of large language models. Our work is inspired by the work of Mahowald et al. (2023) who propose a distinction between formal competence (knowledge about linguistic rules and patterns) and functional competence (knowledge that enables pragmatic use of language) in evaluating large language models. To formalize this intuition, we introduce a mathematical cognitive model called the bounded pragmatic speaker (Figure 1), which is a generalized version of the Rational Speech Act model (Frank & Goodman, 2012). The bounded pragmatic speaker represents an agent that strives to communicate pragmatically but is constrained by its computational capacity. Consequently, it develops a base speaker model to effectively narrow the space of utterances to consider, and a theory-of-mind listener model to select the utterance that would trigger the desired effect in the listener's mind. The base speaker encapsulates the formal competency of the agent, whereas the theory-of-mind listener embodies its functional competency.",
            "score": 0.40713398587439753,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2248
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 42,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 444,
                    "end": 462,
                    "matchedPaperCorpusId": "238583037"
                },
                {
                    "start": 1716,
                    "end": 1739,
                    "matchedPaperCorpusId": "26797288"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7109375
        },
        {
            "corpus_id": "258999187",
            "title": "Birth of a Transformer: A Memory Viewpoint",
            "text": "As large language models (LLMs) are growing in usage and deployment, it is increasingly important to open the black box and understand how they work. A better understanding can help with interpretability of how these models make decisions, and will be crucial to improve these models and mitigate their failure cases, such as hallucinations or reasoning errors. \n\nAn important ingredient in the success of recent LLMs is their ability to learn and reason from information present in their context [5]. These \"in-context\" learning capabilities are often attributed to the transformer architecture [46], in particular its self-attention blocks, which are able to carefully select parts of the input sequence in order to infer plausible next tokens. Additionally, predictions may require \"global\" knowledge, such as syntactic rules or general facts, which may not appear in the context and thus needs to be stored in the model. \n\nIn order to better understand how transformers develop these capabilities during training, we introduce a synthetic dataset that exhibits both aspects. It consists of sequences generated from a bigram language model, but where some of the bigrams are specific to each sequence. Then, the model needs to rely on in-context learning for good prediction on the sequence-specific bigrams, while the global bigrams can be guessed from global statistics conditioned on the current token. While one-layer transformers fail to reliably predict the in-context bigrams, we find that two-layer transformers succeed by developing an induction head mechanism [14,36], namely a \"circuit\" of two attention heads that allows the transformer to predict b from a context [\u2022 \u2022 \u2022 , a, b, \u2022 \u2022 \u2022 , a], and which appears to be ubiquitous in transformer language models [36,48]. \n\nIn order to obtain a fine-grained understanding of how this in-context mechanism emerges during training, we further simplify the two-layer architecture by freezing some of the layers at random initialization, including embeddings and value matrices. This focuses our study on attention and feed-forward mechanisms, while avoiding the difficulty of learning representations, which may require complex nonlinear dynamics [13,29,40].",
            "score": 0.4058661151266508,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 361
                },
                {
                    "start": 364,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2215
                }
            ],
            "ref_mentions": [
                {
                    "start": 497,
                    "end": 500,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 596,
                    "end": 600,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1777,
                    "end": 1780,
                    "matchedPaperCorpusId": "253244237"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8837890625
        },
        {
            "corpus_id": "276408955",
            "title": "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training",
            "text": "Knowledge is a cornerstone of intelligence, shaping how humanity perceives the world, interacts with others, and navigates daily life (Choi, 2022;Chen, 2023). Recent studies (Brown et al., 2020;OpenAI, 2023;Dubey et al., 2024;DeepSeek-AI et al., 2024;Yang et al., 2024;Zhao et al., 2023;Wu et al., 2024) on Large Language Models (LLMs) have demonstrated their ability to capture factual knowledge from pre-training corpus and encapsulate it as extensive parametric knowledge, empowering their remarkable capabilities in numerous knowledgeintensive tasks (Wang et al., 2024;Cao et al., 2024), as well as in developing higher-order capabilities like reasoning (Qiao et al., 2023;Huang and Chang, Figure 1: Illustration of our findings: Phase shift from formation to optimization in the evolution of knowledge circuits, each phase characterized by distinct features at the performance, topology, and component levels. 2023). Nevertheless, these powerful models still struggle with knowledge updates, especially with regard to the dynamic nature of world knowledge that evolves after the cut-off date of the pre-training corpus (Zhang et al., 2023;Mousavi et al., 2024). Extensive efforts focus on developing advanced techniques for injecting new knowledge into LLMs (Jang et al., 2022;Jiang et al., 2024;Mecklenburg et al., 2024;Ovadia et al., 2024;Chen et al., 2024a), yet the absence of a well-defined mechanism for new knowledge acquisition in LLMs continues to hinder further progress in this area. \n\nRecent works introduce mechanistic interpretability techniques to uncover knowledge machanisms in LLMs. Allen-Zhu and Li (2024a) adopts probing methods to examine the storage and extraction of factual knowledge encoded in hidden states of language models. Kim et al. (2024) introduces the concept of knowledge entropy to examine how the integration of knowledge of LLMs evolves during the pre-training phase. However, previous works typically treat knowledge blocks as isolated components and often focus on identifying specific blocks that store particular knowledge.",
            "score": 0.4052821992588994,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1499
                },
                {
                    "start": 1502,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2070
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 146,
                    "matchedPaperCorpusId": "246828626"
                },
                {
                    "start": 573,
                    "end": 590,
                    "matchedPaperCorpusId": "257505035"
                },
                {
                    "start": 658,
                    "end": 677,
                    "matchedPaperCorpusId": "254854219"
                },
                {
                    "start": 677,
                    "end": 693,
                    "matchedPaperCorpusId": "254877753"
                },
                {
                    "start": 915,
                    "end": 919,
                    "matchedPaperCorpusId": "272423770"
                },
                {
                    "start": 1124,
                    "end": 1144,
                    "matchedPaperCorpusId": "254854219"
                },
                {
                    "start": 1263,
                    "end": 1282,
                    "matchedPaperCorpusId": "238419458"
                },
                {
                    "start": 1282,
                    "end": 1301,
                    "matchedPaperCorpusId": "267759882"
                },
                {
                    "start": 1326,
                    "end": 1346,
                    "matchedPaperCorpusId": "266162497"
                },
                {
                    "start": 1606,
                    "end": 1630,
                    "matchedPaperCorpusId": "262825178"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "270869445",
            "title": "MathCAMPS: Fine-grained Synthesis of Mathematical Problems From Human Curricula",
            "text": "Finally, we use Pythia [6] to showcase another analysis that MathCAMPS enables: understanding the learning dynamics of mathematical skills during LM training.We evaluate checkpoints of Pythia 12B on all standards, and track the performance change as the model was trained.Figure 2 shows Pythia's performance evolving during training on all 7 CC standards where the last checkpoint achieves at least 30% accuracy.Early in training, after 28k steps, Pythia performs best in a Kindergarten standard, K.OA.A.5 -\"Fluently add and subtract within 5.\".At 57k steps, its performance is best in both K.OA.A.5 (37% accuracy) and two first-grade standards, 1.OA.A.1 and 1.OA.A.2 -both standards involve simple word problems with addition and subtraction within 20.Pythia starts to become proficient at a sixth-grade standard around midway during training: 6.EE.A.1, which involves evaluating simple expressions using whole-number exponents (e.g, computing squares and cubes).These skills develop in tandem with its linguistic competence -at first, Pythia repeats questions verbatim often, but at 57k steps it already often produces responses.Overall, the high-resolution of MathCAMPS as a reasoning benchmark can support future work to deepen our understanding of how language models acquire capabilities during training, and how specific factors (such as data, or scale) contribute to their learning.",
            "score": 0.4041690511797974,
            "section_title": "Learning dynamics",
            "char_start_offset": 23350,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 545
                },
                {
                    "start": 545,
                    "end": 753
                },
                {
                    "start": 753,
                    "end": 964
                },
                {
                    "start": 964,
                    "end": 1131
                },
                {
                    "start": 1131,
                    "end": 1390
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "257921893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57421875
        },
        {
            "corpus_id": "259076231",
            "title": "Second Language Acquisition of Neural Language Models",
            "text": "The trajectory of the OVERALL scores in Figure 3 suggest that linguistic ability generally improves along with the number of epochs. There was a tendency for large improvements to emerge after dozens of epochs; in other words, the models began to acquire L2 knowledge after seeing the same examples many times, e.g., 50-100 times. Note that humans are argued to acquire a vocabulary after encountering the same word about 12 times (Nation, 2014), and of course, the lexical and syntactic acquisition is not comparable, but the observation that the L2 knowledge improves after 50-100 rounds of the corpus may be in the direction that LMs are inefficient at acquiring a new language. \n\nDifferences in grammar items: Focusing on the general trajectory shapes for each grammatical item, we observed at least four patterns: (i) spike-at-the-end (D-N AGR., IRREGULAR, S-V AGR.), (ii) flat (ARG.STR., CTRL.RAIS., ISLAND), (iii) bumpy (ANA.AGR., ELLIPSIS, NPI, QUANTI-FIERS), and (iv) mixed (FILLER-GAP, BINDING). In addition, these groups roughly mirror the linguistic categories of the grammar items (morphology, syntax, semantics, and syntax&semantics); for example, all the items in the spike-at-the-end group are morphological phenomena, while all the semantic categories (NPI, QUANTIFIERS) yielded the bumpy patterns. Note that existing studies reported that low-level (e.g., morphological) linguistic skills could be acquired earlier and vice versa (Liu et al., 2021;Blevins et al., 2022); but at least in our cognitively-inspired bilingual training scenario, we did not observe such an explicit tendency.",
            "score": 0.4039413900797021,
            "section_title": "General improvement after dozens of epochs:",
            "char_start_offset": 15261,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1604
                }
            ],
            "ref_mentions": [
                {
                    "start": 1448,
                    "end": 1466,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 1466,
                    "end": 1487,
                    "matchedPaperCorpusId": "249017999"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.495361328125
        },
        {
            "corpus_id": "225096632",
            "title": "Bifurcations and the Emergence of L2 Syntactic Structures in a Complex Dynamic System",
            "text": "Germane to the behavior of complex systems is the tendency to exhibit emergent underlying patterns-i.e., patterns in the flux-as the system self-organizes toward growing complexity. If emergence in language development is taken as the arising of linguistic structures from patterns of usage over time (MacWhinney, 2015), evidence of spontaneous pattern formation (van Geert, 2008) within the linguistic system may provide valuable insight into the process of emergence and the complexity that ensues. Indeed, at critical moments in time, complex systems experience abrupt, qualitative shifts from one discernable pattern of behavior to another (Kelso, 2009). It is at these precise moments of phase transition, or \"points of instability and turbulence where old patterns break down and new ones appear\" (Lewis, 2000, p. 39), that increasingly disordered, entropic behavior makes way for new attractor states, or \"pockets of stability\" (Hiver, 2015, p. 21) to emerge. Thus, seeking to understand how the interconnected components of the complex linguistic system converge to give rise to new patterns of behavior has become the crux of the CDST agenda (Larsen-Freeman and Cameron, 2008). \n\nIn sum, human language, in both its development and use, is now widely accepted as a complex adaptive system (Ellis and Larsen-Freeman, 2009). With this appellation, undoubtedly, come new challenges and new approaches to its study. One such challenge, of course, is to move beyond descriptions of the static phases of development, instead focusing on the transition between such phases (de Bot et al., 2013) and the ways in which patterned language behavior emerges in context. As complex systems are known to behave in distinct ways, language researchers committed to a CDST view must foreground the unique behavior of complex systems focusing on nonlinearity and the patterns in the flux that characterize language development. In what follows, we highlight one particular pattern in the flux-the bifurcation-while paying special attention to the interaction of fluency and accuracy at these unique points of transition. In doing so, we gain insight not only into the emergent patterns of development, but equally into the ways in which competition between syntactic constructions motivates such transitions.",
            "score": 0.40351386308534964,
            "section_title": "CDST and Patterns in the Flux",
            "char_start_offset": 8458,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2111
                },
                {
                    "start": 2112,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 379,
                    "matchedPaperCorpusId": "53969605"
                },
                {
                    "start": 644,
                    "end": 657,
                    "matchedPaperCorpusId": "207314"
                },
                {
                    "start": 1575,
                    "end": 1596,
                    "matchedPaperCorpusId": "64322635"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48583984375
        },
        {
            "corpus_id": "269964733",
            "title": "ChatGPT-4 as a journalist: Whose perspectives is it reproducing?",
            "text": "Once trained, LLMs are able to perform a multitude of tasks such as generating coherent and contextually relevant text, answer questions, translate languages, summarise text and others (Yang et al., 2023). \n\nDeveloped by OpenAI, ChatGPT (Generative Pre-trained Transformer) is one of the most popular LLMs which is designed to perform conversational tasks, simulating a chatbot-like experience. Rooted in the Transformers architecture, ChatGPT has revolutionised the field of NLP because of its impressive capabilities to generate coherent, diverse and contextually appropriate content across a wide range of prompts and because of its broad application from chatbots and customer support to content generation, programming help, gaming and education (Burger et al., 2023;Ray, 2023). ChatGPT operates based on human prompts, which activate a series of neurons in the model that help it determine the response to specific queries based on the patters found in the training data (Arguedas and Simon, 2023). In order to achieve their vast linguistic capabilities, Large Language Models like ChatGPT undergo a two-phase developmental process (Guo et al., 2023;Wang et al., 2021). Initially, in the pretraining phase, the model is exposed to vast amounts of text from diverse corpora, enabling it to simulate linguistic patterns, which correspond to facts about the world and may even display reasoning abilities. At this stage, the model mirrors patterns in its training data, which means it also replicates biases inherent in that input, [unsupervised learning] (Radford et al., 2019 ). Following this extensive foundational training, the model enters a fine-tuning phase (Guo et al., 2023;Zhou et al., 2023). In this phase, the model undergoes refinement on more specific tasks or datasets, frequently under the guidance of human reviewers, a process known as supervised learning. This iterative feedback process sharpens the model's responses, aligning them closer to desired outcomes and ensuring they are more controlled, relevant and safe for specific applications.",
            "score": 0.40256785614086077,
            "section_title": "Large language models and artificial sociality",
            "char_start_offset": 7882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 208,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 204,
                    "matchedPaperCorpusId": "258331833"
                },
                {
                    "start": 751,
                    "end": 772,
                    "matchedPaperCorpusId": "258011641"
                },
                {
                    "start": 772,
                    "end": 782,
                    "matchedPaperCorpusId": "258157875"
                },
                {
                    "start": 1559,
                    "end": 1582,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79052734375
        },
        {
            "corpus_id": "274965332",
            "title": "Conceptual In-Context Learning and Chain of Concepts: Solving Complex Conceptual Problems Using Large Language Models",
            "text": "Large Language Models (LLMs) have revolutionized solving everyday problems like text summarising, copy-writing and knowledge acquisition [1], [2]. LLMs can solve these general problems through the model's base knowledge, captured by its large representation power trained on amounts of open-world data. Many problems that LLMs can solve are never introduced to the LLM during training. These emergent capabilities of LLMs are enabled through specialized natural language (NL) prompting techniques like In-Context Learning (ICL) [3]- [5] and Chain of Thoughts (CoT) [6] referred to as shallow customization methods (SCM) of LLMs as they do not alter model embeddings. Therefore LLMs, with or without such SCM, are good at general problem-solving or formal linguistic abilities. Formal linguistic competency tasks are based on syntactic rules and statistical patterns that characterize a language. On the flip side, open-world data often lack specialized information like math/logic related knowhow, process information or engineering guidelines and hence, LLMs lack such knowledge too. It has been shown that LLMs often fail to solve even simple math problems [7] and have inconsistencies in their knowledge when reasoning [8]. \n\nProblem solving is a complex process that humans excel through the interplay of several aspects of their cognitive abilities [9], [10]. It is an acquired skill for humans, often requiring reasoning [11]. It is both logical and creative in nature, hence challenging for LLMs. While LLMs do well on formal linguistic competency tasks, LLM's base knowledge lacks essential problem solving mechanisms that humans possess [12], and cannot solve functional linguistic competency problems [13] well. LLMs struggle at even problems with single correct answers [14]. \n\nCognitive science theories prove concepts are key to problem solving [15]. The term concept used is as in cognitive science literature [16] meaning 'building blocks of thought'. Various problem solving mechanisms like deductive, abductive, and inductive reasoning rely on conceptual information (CI). We call such problems that can be solved with CI as complex conceptual problems (CP). Many science and engineering problems fall in this category of complex CP with often multiple correct answers.",
            "score": 0.4024935652210432,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1786
                },
                {
                    "start": 1789,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2175
                },
                {
                    "start": 2176,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 1359,
                    "end": 1363,
                    "matchedPaperCorpusId": "59436456"
                },
                {
                    "start": 1646,
                    "end": 1650,
                    "matchedPaperCorpusId": "16238486"
                },
                {
                    "start": 1924,
                    "end": 1928,
                    "matchedPaperCorpusId": "29342874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "265158068",
            "title": "In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax",
            "text": "Language models (LMs) have become increasingly important subjects of study due to their expressive power and performance at scale. When training large language models (LLMs) on massive amounts of text, surprisingly sophisticated linguistic behaviors, such as in-context learning (ICL), emerge (Brown et al., 2020;Min et al., 2022a): given only a small number of labeled training examples in the input context, LLMs can generalize to new instances of the task without weight updates. Thus, even without access to the model's weights, we We prompt language models with labeled examples px, yq train that can be explained using either robust syntactic/hierarchical features or spurious positional/linear features. We also include the input from a test example x test . We ensure the models have learned the task by evaluating on in-distribution examples px, yq test-id . Then, we observe whether models generalize syntactically or linearly on out-of-distribution examples px, yq test-ood . \n\ncan teach an LLM to perform new tasks with significantly higher-than-chance performance. This raises questions as to whether context is sufficient for LLMs to learn the underlying structure of a task, as opposed to superficial heuristics that do not generalize well. Indeed, LLMs have demonstrated counterintuitive biases in ICL settings (Pan et al., 2023;Min et al., 2022b;Webson and Pavlick, 2022), giving reason for skepticism. In this study, we ask: How robust is ICL to distribution shifts between in-context exemplars and test examples? \n\nWe investigate these questions using the test case of syntactic generalization. Accurate syntactic representations are necessary for robust language understanding. In LMs, syntax acquisition is causally associated with significant and abrupt drops in loss and improved performance on NLP tasks (Chen et al., 2024). Language is structured hierarchically, but the structure of sentences is not provided to models as part of their input. Therefore, a model could incorrectly assume that sentences have lin-arXiv:2311.07811v2",
            "score": 0.4024064736942175,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1531
                },
                {
                    "start": 1534,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2055
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 331,
                    "matchedPaperCorpusId": "240288835"
                },
                {
                    "start": 1345,
                    "end": 1363,
                    "matchedPaperCorpusId": "247155069"
                },
                {
                    "start": 1363,
                    "end": 1388,
                    "matchedPaperCorpusId": "237416760"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75439453125
        },
        {
            "corpus_id": "216080851",
            "title": "Syntactic Structure from Deep Learning",
            "text": "Going even further, not all modern linguistic theorists recognize a sharp distinction between competence and performance. Under some views of syntax, grammar is more akin to a \"toolbox of tricks\" we picked up along our evolutionary way than to the maximally elegant and powerful formal grammars of computer science (e.g., Culicover & Jackendoff 2005;Pinker & Jackendoff 2005;Goldberg 2019a). Under such views, the difference between the syntactic knowledge of DNNs and that of humans might be more one of quantity than quality: humans possess a larger and more sophisticated set of heuristics to parse sentences than DNNs do, but they do not rely on any radically different and more powerful \"narrow language faculty\" abilities. If that is the case, the behavior of DNNs might give us insights not only into online processing (\"performance\"), but also on some of the core syntactic tools that constitute human grammatical competence. We look forward to theoretical work linking modern DNNs to construction grammar and similarly \"shallow\" syntactic formalisms. \n\nConclusion. In our view, the time is ripe for more linguists to get engaged in the lines of work we sketched in this survey. On the one hand, linguists' know-how in probing grammatical knowledge can help develop the next generation of language-processing DNNs, and the success of events such as the BlackBox NLP series confirms that the deep learning community is warmly welcoming linguistic analyses of DNNs. On the other, studying what the best DNNs learn about grammar, and how they do so, can offer new insights about the nature of language and, ultimately, what is genuinely unique about the human species. For this line of work to be effective, linguists will need to be closely involved in developing relevant network architectures, training them on appropriate data, and conducting experiments that address linguists' theoretical concerns.",
            "score": 0.40237626446071123,
            "section_title": "DISCUSSION",
            "char_start_offset": 45936,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1909
                }
            ],
            "ref_mentions": [
                {
                    "start": 322,
                    "end": 350,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 350,
                    "end": 375,
                    "matchedPaperCorpusId": "1599505"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.260498046875
        },
        {
            "corpus_id": "239460340",
            "title": "Schr\u00f6dinger's tree\u2014On syntax and neural language models",
            "text": "In the last half-decade, the field of natural language processing (NLP) has undergone two major transitions: the switch to neural networks as the primary modeling paradigm and the homogenization of the training regime (pre-train, then fine-tune). Amidst this process, language models have emerged as NLP's workhorse, displaying increasingly fluent generation capabilities and proving to be an indispensable means of knowledge transfer downstream. Due to the otherwise opaque, black-box nature of such models, researchers have employed aspects of linguistic theory in order to characterize their behavior. Questions central to syntax\u2014the study of the hierarchical structure of language\u2014have factored heavily into such work, shedding invaluable insights about models' inherent biases and their ability to make human-like generalizations. In this paper, we attempt to take stock of this growing body of literature. In doing so, we observe a lack of clarity across numerous dimensions, which influences the hypotheses that researchers form, as well as the conclusions they draw from their findings. To remedy this, we urge researchers to make careful considerations when investigating coding properties, selecting representations, and evaluating via downstream tasks. Furthermore, we outline the implications of the different types of research questions exhibited in studies on syntax, as well as the inherent pitfalls of aggregate metrics. Ultimately, we hope that our discussion adds nuance to the prospect of studying language models and paves the way for a less monolithic perspective on syntax in this context.",
            "score": 0.4022229419781994,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "260308313",
            "title": "The relevance of words and the language/communication divide",
            "text": "See also Kissine (2021), who argues that the patterns of language acquisition and learning in autism present a strong challenge to constructionist theories of language development. system of syntactic knowledge (competence) has received a lot of attention and hard work. Still, the syntax/lexicon distinction as conceived here raises interesting issues, which, I think, have yet to be fully addressed. First, if words are the basic units of linguistic communication and comprehension (rather than roots), then word recognition is one of two basic but quite distinct processes in language comprehension, the other being the assignment of a syntactic structure to the incoming sequence of words. The question is, then, how do these two parts of the language decoder/module work together in utterance processing/comprehension? A second quite different sort of question is what does all this mean for language evolution? Again, there is a vast quantity of work on the possible continuities and discontinuities between animal communication systems and human linguistic communication. What the current picture indicates is that it makes good analytical sense to think about the advent of recursive syntax and of words separately, with the evolution of the latter to be viewed as arising in the crucible of communication and sociality more widely, while the former was more likely a result of internal changes to the thinking capacities of the human mind/brain.",
            "score": 0.4020170971147192,
            "section_title": "Conclusion: the two parts of the language code",
            "char_start_offset": 78299,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1454
                }
            ],
            "ref_mentions": [
                {
                    "start": 9,
                    "end": 23,
                    "matchedPaperCorpusId": "235498020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30322265625
        },
        {
            "corpus_id": "265157830",
            "title": "SAIE Framework: Support Alone Isn't Enough - Advancing LLM Training with Adversarial Remarks",
            "text": "Large Language Models (LLMs) can justify or critique their predictions through discussions with other models or humans, thereby enriching their intrinsic understanding of instances. While proactive discussions in the inference phase have been shown to boost performance, such interactions have not been extensively explored during the training phase. We hypothesize that incorporating interactive discussions into the training process can enhance the models' understanding and improve their reasoning and verbal expression abilities during inference. This work introduces the SAIE framework, which facilitates supportive and adversarial discussions between learner and partner models. The learner model receives responses from the partner, and its parameters are then updated based on this discussion. This dynamic adjustment process continues throughout the training phase, responding to the evolving outputs of the learner model. Our empirical evaluation across various tasks, including math problems, commonsense reasoning, and multi-domain knowledge, demonstrates that models fine-tuned with the SAIE framework outperform those trained with conventional fine-tuning approaches. Furthermore, our method enhances the models' reasoning capabilities, improving both individual and multi-agent inference performance.",
            "score": 0.401472929743626,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8359375
        },
        {
            "corpus_id": "267412734",
            "title": "Universal Syntactic Structures: Modeling Syntax for Various Natural Languages",
            "text": "Having the ability to create and understand complex thoughts is what fundamentally separates humans from other mammals. These discrete thoughts are often referred to as sentences. Language is the system of vocabulary and syntax that enables humans to generate and process structural connections of words. In essence, it consists of an encoding and decoding mechanism in the form of natural languages. The language process produces semantics or meaning, which gives us an ability to understand abstract or highly complex concepts. Although the word is usually considered to be a form of communication, it is better to define language as a code or cipher for generating discrete thoughts. Because we happen to be a social species, language is also utilized for communication as well. \n\nBut how does language actually work? More specifically, how does the human brain connect words to form sentences? Despite an enormous amount of effort by scientists and engineers, we have yet reached a plateau where we can undoubtedly declare machines are our equals when it comes to natural language processing (NLP). The best chess players and go players in the world are no longer humans. However, the best translators of natural languages are still humans and not machines. Machine translation (MT) has not reached a level of human translators. But if we were to apply the same process used by the language faculty, it will no longer be such an impossible task for machines to process language at the same level as humans in terms of quality and performance. Most likely, the machine will eventually exceed our linguistic capabilities in both speed and precision. Computers have already become faster and better at playing board games, making mathematical calculations, creating images, etc. It is likely only a matter of time before they conquer NLP as well. But this feat would require a comprehensive understanding of what language is and how it works. Machines are not built to compute numbers by guessing the right outcome based on data and training. Having a simple, logical process with transistors and logic gates is clearly a better approach. Similarly, finding a more optimal process for language is needed for future language models. \n\nIn this article, we propose a novel modeling technique for representing syntax. This method appears to exhibit a level of performance that is equal to humans. We suspect it could be an indication of a discovery, potentially revealing how the brain processes words to create discrete thoughts.",
            "score": 0.4007258323650216,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2232
                },
                {
                    "start": 2235,
                    "end": 2314
                },
                {
                    "start": 2315,
                    "end": 2393
                },
                {
                    "start": 2394,
                    "end": 2527
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.371337890625
        },
        {
            "corpus_id": "131775742",
            "title": "Does Scale-Free Syntactic Network Emerge in Second Language Learning?",
            "text": "In contrast, such properties, which are absent at the initial stage, will suddenly emerge during L1 acquisition. This disparity can be attributed to the fact that during L2 learning, learners can rely on the syntactic similarity between the native language-Chinese and the target language-English in this case to construct sentences. It is known that both languages are SVO languages, with some slight differences, such as the placement of attributives, adverbials and the word order of interrogatives. For language beginners, most of their productions are simple sentences with basic subject-verb-object structure like My family has six people. I have a brother. He is tall and The apple is haochi (Chinese pinyin for delicious). These simple sentences have similar structures in both English and Chinese. Therefore, as long as the basic vocabulary is mastered, early English learners can readily construct English sentences by resorting to their syntactic knowledge of Chinese. Moreover, English dependency network has much in common with Chinese dependency network (Liu and Li, 2010). In other words, positive transferring of mother language's syntax is possible, which may explain the absence of sudden emergence of syntactic complex networks in Chinese English learners. The syntactic knowledge of Chinese facilitates the mastering of English syntactic structures. Therefore, if native and target languages have similar syntactic structures, syntactic phase transition may not take place during L2 learning. This tentative conclusion, of course, needs more evidence from different languages. In addition, this lack of phase shift can be explained with the Unified Competition Model (UCM) proposed by MacWhinney (2012). There is a competition between native language and target language during L2 learning: the \"entrenched\" L1 knowledge constrains the learning of new knowledge of L2, and the degree of \"entrenchment\" depends on the extent of consolidation of the learner's L1 (Li, 2015). As L1 knowledge becomes more solid, new and different L2 structures are more difficult to acquire. Chinese English learners begin to learn L2 at around the age of nine, and at that time their L1 syntax has already been well-established.",
            "score": 0.4006249245815592,
            "section_title": "Scale-Free and Small-World Properties of the Nine Syntactic Dependency Networks",
            "char_start_offset": 36635,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 1068,
                    "end": 1086,
                    "matchedPaperCorpusId": "119989792"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26220703125
        },
        {
            "corpus_id": "271719875",
            "title": "Towards an Analysis of Discourse and Interactional Pragmatic Reasoning Capabilities of Large Language Models",
            "text": "Within the landscape of linguistic capabilities that have been studied and analyzed in Large Language Models (LLMs), a considerable amount of research has focused on phenomena on the level of morphology and syntax (Marvin and Linzen, 2018;Hu et al., 2020). Here, the community seems to have agreed on benchmarks and phenomena that an LLM should be capable of (e.g. agreement phenomena (Warstadt et al., 2020)). Various studies show that LLMs can handle a rich and diverse set of such phenomena (Chang and Bergen, 2023). Recent inquiries have expanded to investigate the proficiency of LLMs in pragmatic discourse processing (Ruis et al., 2022;?;Sieker et al., 2023). \n\nPragmatic phenomena are often utilized when arguing for or against the reasoning capabilities of LLMs, which are a requirement for grounding in dialog. However, research on pragmatic abilities in LLMs remains more scarce and less systematic. We argue that studying the pragmatic competencies of LLMs is particularly interesting as it bridges aspects of 'core-linguistic' knowledge with the communicative, functional, and contextual aspects of grounding and is still actively discussed in current research (Mahowald et al., 2024). What does it mean when models can infer mental states while struggling with implicit meaning (Chang and Bergen, 2023)? Why do Language Models tussle, especially with phenomena that break language rules, such as humor, irony, and conversational maxims (?)? \n\nTo address these questions and categorize findings effectively, capabilities related to pragmatics and grounding must be mapped out clearly and defined in relation to one another. In this work, we want to give an overview on which pragmatic abilities have been tested in LLMs so far and how these tests have been carried out. To do this, we first discuss the scope of the field of pragmatics and suggest a subdivision into discourse pragmatics and interactional pragmatics. We give a nonexhaustive overview of the phenomena of those two subdomains and the methods traditionally used to analyze them.",
            "score": 0.40030691213213376,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 666
                },
                {
                    "start": 669,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1454
                },
                {
                    "start": 1457,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2056
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 239,
                    "matchedPaperCorpusId": "52113185"
                },
                {
                    "start": 239,
                    "end": 255,
                    "matchedPaperCorpusId": "218571018"
                },
                {
                    "start": 1174,
                    "end": 1197,
                    "matchedPaperCorpusId": "268551442"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55029296875
        },
        {
            "corpus_id": "237502815",
            "title": "Connecting degree and polarity: An artificial language learning study",
            "text": "It has the following main ingredients: \n\n1. fragment of an artificial language in the form of expressions that do not belong to the language that participants are speakers of; \n\n2. training phase, where some information about the language fragment is given to the participants; \n\n3. testing phase, where it is checked what other knowledge, beside the provided, was inferred during training. \n\nOriginally designed for studies with human participants, the Artificial Language Learning framework has also been applied to neural network-based learning models (Piantadosi et al., 2012;Carcassi et al., 2019;van de Pol et al., 2021). Replacing human participants with artificial learning agents allows to examine the learning process in more detail and to make a variety of learnability statements. One important property of these experiments is that the learning agents typically come in a blank state with no prior knowledge or biases. This limits the set of linguistic questions that can be targeted by this type of experiment. \n\nThe way we use the Artificial Language Learning paradigm can be seen as middle ground between experiments with human participants and with artificial learners described above. Our approach also involves an artificial language fragment and a training procedure to introduce knowledge about some property A, but it uses a pre-trained language model (LM) (Peters et al., 2018;Devlin et al., 2019;Brown et al., 2020) as the learning agent. More technically, we extend a pre-trained LM with a set of new tokens with randomly initialized embeddings and perform fine-tuning on a carefully constructed synthetic dataset. The dataset is constructed in a way to indirectly introduce different values along property A for different new tokens. Upon fine-tuning, we measure how the training affected property B and how variation along B depends on the values of property A introduced during training. \n\nOur study will focus on English, as represented in a pre-trained LM BERT (Devlin et al., 2019). We see this as a proof of concept work that can be extended further along the cross-linguistic dimension and in application to other models. Using this set-up, we address the question of whether LMs encode a connection between degree semantics and polarity, therefore making a generalization across two different linguistic properties.",
            "score": 0.40020896920874727,
            "section_title": "Introduction",
            "char_start_offset": 2249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 41,
                    "end": 175
                },
                {
                    "start": 178,
                    "end": 277
                },
                {
                    "start": 280,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1915
                },
                {
                    "start": 1918,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2154
                },
                {
                    "start": 2155,
                    "end": 2349
                }
            ],
            "ref_mentions": [
                {
                    "start": 555,
                    "end": 580,
                    "matchedPaperCorpusId": "263546492"
                },
                {
                    "start": 580,
                    "end": 602,
                    "matchedPaperCorpusId": "203619482"
                },
                {
                    "start": 602,
                    "end": 626,
                    "matchedPaperCorpusId": "236223322"
                },
                {
                    "start": 1379,
                    "end": 1400,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1400,
                    "end": 1420,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1991,
                    "end": 2012,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.607421875
        },
        {
            "corpus_id": "56362403",
            "title": "Second Language Grammatical Proficiency and Third Language Acquisition",
            "text": "According to Bachman's (1990) and Bachman and Palmer's (1996) multi-componential model of communicative language ability, language knowledge has two general components: (1) organizational knowledge and (2) pragmatic knowledge. Organizational knowledge, defined as how individuals control language structure to produce grammatically correct utterances or sentences and texts, is further divided into grammatical knowledge and textual knowledge. Grammatical knowledge is defined as knowledge of vocabulary, syntax, and phonology/graphology. Grammatical knowledge governs \"the choice of words to express specific significations, their forms, their arrangement in utterances to express propositions, and their physical realizations, either as sounds or as written symbols\" (Bachman, 1990, p. 87). In Bachman and Palmer's (1996) view, grammatical knowledge refers to several components of linguistic form relating strictly to sentence-based phonology, graphology, vocabulary and syntax. Larsen-Freeman (1991) has also characterized grammatical knowledge along three dimensions: linguistic form, semantic meaning and pragmatic use. \"Grammatical structures not only have (morphosyntactic) form, they are also used to express meaning (semantics) in context appropriate use (pragmatics) (Larsen-Freeman, 1991, p. 252)\". According to her, these three dimensions may be viewed as independent or interconnected. \n\nAccording to Purpura, (2004), grammatical knowledge has two highly related components: grammatical form and grammatical meaning. Grammatical form refers to \"linguistic forms on the subsentential, sentential and suprasentential levels, as described in the syntactocentric approaches to language discussed previously\" (Purpura, 2004, p. 61). Grammatical form includes phonological/graphological forms, lexical forms, morphosyntactic forms, cohesive forms, information management forms, and interactional forms. \n\nGrammatical meaning refers to knowledge of the meaning associated with an utterance as the sum of its parts and how these parts are arranged in syntax (literal meaning), as well as how these parts are used to convey the speaker's intended meaning in context (intended meaning).",
            "score": 0.4000858180931418,
            "section_title": "Theoretical Framework",
            "char_start_offset": 7153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1910
                },
                {
                    "start": 1913,
                    "end": 2190
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.409423828125
        },
        {
            "corpus_id": "260671711",
            "title": "Unveiling Sustainability in Ecommerce: GPT-Powered Software for Identifying Sustainable Product Features",
            "text": "The pre-training phase of GPT (Generative Pre-trained Transformer) constitutes a pivotal step wherein the model is immersed in vast and diverse text data originating from various sources [71]. By harnessing the self-attention mechanism inherent in the transformer architecture, GPT gains the ability to comprehend the contextual relationships within the text and engage in unsupervised learning to predict subsequent words in a sentence [76]. GPT models lack true comprehension and knowledge of the world; they do not possess genuine understanding of syntax, semantics, or discourse in the way humans do. Instead, they rely on statistical associations and patterns they have learned from the vast amounts of text during training [77]. While they can generate impressive responses, especially in context, they are fundamentally different from human intelligence. As a result of the pre-training phase, GPT acquires the adaptability to be fine-tuned for specific tasks, granting it versatility in generating contextually appropriate text. The exposure to diverse textual data further enriches GPT's understanding of language and enhances its recognition of contextual nuances [78,79]. The resounding success of the pre-training phase can be attributed to GPT's remarkable capacity to assimilate knowledge from diverse sources and process vast volumes of data, laying the foundational groundwork for its exceptional language-understanding capabilities [71].",
            "score": 0.4000822763873815,
            "section_title": "The Pre-Training Phase",
            "char_start_offset": 37063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1454
                }
            ],
            "ref_mentions": [
                {
                    "start": 1178,
                    "end": 1181,
                    "matchedPaperCorpusId": "247296094"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "272524832",
            "title": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models",
            "text": "Layerwise encoding performance and layerwise representational dimensionality across layers are highly correlated, consistently across brain areas involved in linguistic processing. Table 1 shows the correlation between encoding performance and dimensionality, averaged over all voxels. The relationship is largely consistent across a variety of metrics for measuring dimensionality. \n\nFigure 1a shows the correlation between average encoding performance and normalized I d for various model sizes in from the OPT model family. The positive relationship, \u03c1 = 0.85, between normalized I d and encoding performance suggests that in trained language models, the I d of layer activations captures abstract linguistic feature complexity needed to support language comprehension. Figure 1b overlays, for OPT-1.3b, the encoding performance, I d , and next-token prediction loss computed from each layer. Observe that encoding performance peaks at layer 17, which exactly marks the sharp downwards turn in prediction loss. While Cheng et al. [12] show that layers leading up to the I d peak extract high-level features related to syntax and semantics, our results additionally indicate a shift in post-I d -peak to next-token prediction. This sharp phase transition from abstraction to prediction is observed across model sizes, but it is more gradual for Pythia (see Appendix E.1). \n\nTo further verify the existence of a phase transition, we report the inter-layer representational similarity via linear Centered Kernel Alignment [23]. Figure 1d depicts at least two phases of inter-layer similarity (lighter is more similar): the I d peak approximately marks a junction at which preceding layers are no longer similar to following layers. Results generally hold across models, see Appendix E.3. \n\nFigure 1c shows the correlation of I d with encoding performance across layers at the voxelwise level (red is better), in a single subject. With the exception of the primary auditory cortex, which processes low-level auditory information, encoding performance in brain areas thought to handle higher-level linguistic processing is well-predicted by I d across layers. Results generally hold across subjects, model families, and model sizes, see Appendix E.2. \n\nThe relationship between encoding performance and I d arises nontrivially from learning, in a way that does not simply reflect the layer position.",
            "score": 0.39968352542512753,
            "section_title": "Results",
            "char_start_offset": 8394,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1787
                },
                {
                    "start": 1790,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2248
                },
                {
                    "start": 2251,
                    "end": 2397
                }
            ],
            "ref_mentions": [
                {
                    "start": 1522,
                    "end": 1526,
                    "matchedPaperCorpusId": "141460329"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81201171875
        },
        {
            "corpus_id": "33801669",
            "title": "Models of lexical meaning",
            "text": "bounded set of hierarchically structured objects that interface in particular ways with the systems of sound and meaning. From this perspective, lexical meaning, and linguistic meaning in general, is fundamentally a mind-internal construct, insofar as the theory models a mind-internal competence. This internalist stance, however, can be more or less prominent. The work of Noam Chomsky has consistently asserted the mind-internal nature of linguistic meaning, and denied the usefulness of world-determined reference. However these considerations have little direct impact on his technical contributions, which mostly concern themselves with the combinatorial apparatus (a fully developed and original argument that addresses the notion of truth from a Minimalist perspective can instead be found in the work of Hinzen 2007). In contrast to this radically internalist stance, which minimizes the role of the relation between linguistic meaning and mind-external reality, classic model-theoretic semantics based on truth conditions is predicated of an externalist approach. The point has some significance, because semantic frameworks deriving from Montague semantics have historically evolved alongside Chomskyan models of syntax (Government-Binding and then Minimalism), often presupposing each other despite the sharply different philosophical assumptions. What is significant, and instructive, is that this contiguity has allowed formal syntax and semantics to develop as closely related subdisciplines sharing a large body of results. Beside semantic internalism, 'generative' models typically also subscribe to the view that language is a specific cognitive ability irreducible to non-linguistic ones. Linguistic knowledge, in this view, certainly interfaces with non-linguistic knowledge; but it is not subsumed by it, and in particular it is not based on mind-external properties of the body (as opposed to a strict interpretation of the thesis of embodied cognition). An influence of the human body in linguistically relevant categories can be compatible with this position, but not the idea that the mental lexicon, as a part of the theory of language, is shaped by mind-external factors. Concerning specifically lexical meaning, what deserves to be highlighted is the theoretical sharpening of the notion of lexical item, and the vast body of generalizations and predictions that has resulted from approaches as different as syntax-based lexical decomposition (in several different varieties: von",
            "score": 0.39952380254540176,
            "section_title": "Meaning and lexical meaning",
            "char_start_offset": 27470,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.126220703125
        },
        {
            "corpus_id": "268667576",
            "title": "Can 3D Vision-Language Models Truly Understand Natural Language?",
            "text": "Background of Language Characteristics.Human language conveys meaning through its flexible syntax, grammar structures, and features like voice and tones.We use five main characteristics to model human language inspired by linguist theories [6,7].Firstly, Syntax refers to varying word or phrase orders to create different sentence structures; for example, inverse sentences are commonly used in daily conversation.Secondly, Voice involves paraphrasing a sentence from active to passive voice, or vice versa, a fundamental aspect of human language.Thirdly, Modifier, such as adjectives and adverbs, are varied by humans to enhance the details in a sentence, adding richness and depth to communication.Fourthly, Accent reflects the distinct linguistic habits of English speakers from different regions, characterized by unique vocabulary and sentence structures.These regional variations, however, do not change the fundamental meaning of the communication.Finally, Tone encompasses the attitudes and emotions conveyed in a sentence, which vary across different contexts.An example is the use of questions in daily conversation, which demonstrates how tone can add layers of meaning beyond the literal interpretation of words.Some examples are shown in Fig. 4. A detailed explanation is shown in the supplementary file.\n\nFig. 4: The overall rephrase process to build our proposed evaluation suite and the abstract prompt design.We prompt gpt to paraphrase the original sentence into five different styles derived from human natural language characteristics.One example from ScanRefer [10] is shown in different styles.\n\nLLM Rephrasing for Dataset Construction.\n\nWe aim to create a dataset including the language variants defined earlier to systematically assess the language robustness of existing models across different linguist styles.Our approach involves rephrasing sentences from existing datasets: ScanQA [5] for 3D-VQA, NR3D [2] and ScanRefer [10] for 3D-VG.These rephrased sentences adhere to our variant definitions while retaining their original meaning.Formally, let D \u2208 {ScanQA, NR3D, ScanRefer} and o \u2208 {Syntax, Voice, Modifier, Accent, Tone}.\n\nWe utilize the large language model (LLM)-gpt-3.5-turbo,for this sentence rephrasing task.",
            "score": 0.3994013924113865,
            "section_title": "3D Language Robustness (3D-LR) dataset",
            "char_start_offset": 13448,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 39,
                    "end": 153
                },
                {
                    "start": 153,
                    "end": 246
                },
                {
                    "start": 246,
                    "end": 414
                },
                {
                    "start": 414,
                    "end": 547
                },
                {
                    "start": 547,
                    "end": 700
                },
                {
                    "start": 700,
                    "end": 860
                },
                {
                    "start": 860,
                    "end": 955
                },
                {
                    "start": 955,
                    "end": 1069
                },
                {
                    "start": 1069,
                    "end": 1224
                },
                {
                    "start": 1224,
                    "end": 1317
                },
                {
                    "start": 1319,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1555
                },
                {
                    "start": 1555,
                    "end": 1616
                },
                {
                    "start": 1618,
                    "end": 1658
                },
                {
                    "start": 1660,
                    "end": 1836
                },
                {
                    "start": 1836,
                    "end": 1964
                },
                {
                    "start": 1964,
                    "end": 2063
                },
                {
                    "start": 2063,
                    "end": 2155
                },
                {
                    "start": 2157,
                    "end": 2213
                },
                {
                    "start": 2213,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 243,
                    "matchedPaperCorpusId": "124655953"
                },
                {
                    "start": 1582,
                    "end": 1586,
                    "matchedPaperCorpusId": "209414687"
                },
                {
                    "start": 1910,
                    "end": 1913,
                    "matchedPaperCorpusId": "245334889"
                },
                {
                    "start": 1931,
                    "end": 1934,
                    "matchedPaperCorpusId": "221378802"
                },
                {
                    "start": 1949,
                    "end": 1953,
                    "matchedPaperCorpusId": "209414687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.267822265625
        },
        {
            "corpus_id": "270560657",
            "title": "Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts",
            "text": "Given the human-like reasoning capabilities observed in LLMs across various tasks (Zhao et al. 2023), and since our brain contains functional locations associated with distinct cognitive processes (Brett, Johnsrude, and Owen 2002;Bjaalie 2002;Gholipour et al. 2007), we ask whether there are similar regions in LLMs. Previous investigations have explored the behaviors of individual neurons indicating that a neuron can encode multiple concepts (Bolukbasi et al. 2021) while a concept can also be distributed across multiple neurons (Dalvi et al. 2019;Durrani et al. 2020;Chen et al. 2024b). Subsequent endeavors have sought to identify functional regions in LLMs, encompassing linguistic regions (Zhang et al. 2024b), factual subnetworks (Ren and Zhu 2022;Bayazit et al. 2023), and modular structures (Zhang et al. 2023;Conmy et al. 2023). These studies have investigated localized behaviors in smaller-scale language models, such as BERT and GPT-small. Building upon these foundations, our research embarks on the examination of knowledge locations in larger-size LLMs, specifically those with 7B parameters, spanning multiple knowledge domains.",
            "score": 0.39932628581271656,
            "section_title": "Analyzing Knowledge Distribution in LLMs",
            "char_start_offset": 7032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1147
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 243,
                    "matchedPaperCorpusId": "6095640"
                },
                {
                    "start": 243,
                    "end": 264,
                    "matchedPaperCorpusId": "18352507"
                },
                {
                    "start": 445,
                    "end": 467,
                    "matchedPaperCorpusId": "269655"
                },
                {
                    "start": 533,
                    "end": 552,
                    "matchedPaperCorpusId": "56895415"
                },
                {
                    "start": 552,
                    "end": 572,
                    "matchedPaperCorpusId": "6940861"
                },
                {
                    "start": 572,
                    "end": 590,
                    "matchedPaperCorpusId": "261214558"
                },
                {
                    "start": 697,
                    "end": 717,
                    "matchedPaperCorpusId": "267782481"
                },
                {
                    "start": 739,
                    "end": 757,
                    "matchedPaperCorpusId": "250562909"
                },
                {
                    "start": 821,
                    "end": 838,
                    "matchedPaperCorpusId": "258418244"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5849609375
        },
        {
            "corpus_id": "264591406",
            "title": "Unveiling A Core Linguistic Region in Large Language Models",
            "text": "The core regions of linguistic competence and their dimensional dependence have guiding significance in the pre-training and fine-tuning of large language models. To achieve superior model performance, we believe the following recommendations are particularly important: \n\nConsideration of Data Ratios during Further Pre-training: \n\n1. After pretraining, specific parameter regions of the language model are responsible for particular functions. Introducing a significant amount of knowledge that was missing during the pre-training may cause notable parameter shifts, potentially leading to a decline in model capabilities. \n\n2. For a set of fine-tuning data, consider mixing it with 5-10 times the original pre-training data before training.",
            "score": 0.39923339532207713,
            "section_title": "DISSICUSION AND FUTURE WORK",
            "char_start_offset": 22663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 270
                },
                {
                    "start": 273,
                    "end": 330
                },
                {
                    "start": 333,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 743
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80419921875
        },
        {
            "corpus_id": "267782481",
            "title": "Unveiling Linguistic Regions in Large Language Models",
            "text": "(3) Optimization of freezing core regions during further pre-training decelerates language forgetting. et al., 2023) and LLaMA 2 (Touvron et al., 2023), showcase a significant breakthrough. Thanks to unparalleled scales of model architecture and the vastness of training data, these LLMs now exhibit exceptional linguistic competence and can execute complex tasks requiring abstract knowledge (Dong et al., 2023) and reasoning (Cobbe et al., 2021). \n\nPrevious research has revealed that LLMs naturally capture cross-linguistic similarities in their representation space, facilitating zero-shot crosslingual transfer (Pires et al., 2019;Wu and Dredze, 2019;Xu et al., 2023). The model is fine-tuned on one language, enabling the acquisition of comparable capabilities in another language (Muennighoff et al., 2023;Ye et al., 2023), and exhibits the phenomenon of code-switching when generating context (Khanuja et al., 2020;Zhao et al., 2024). Attempts to improve LLMs' cross-lingual generalization abilities have been successful through parameter and information transfer learning (\u00dcst\u00fcn et al., 2020;Choenni et al., 2023), aligning languages compulsorily (Sherborne and Lapata, 2022;Shaham et al., 2024) and utilizing in-context learning techniques (Winata et al., 2021;Tanwar et al., 2023). However, a detailed investigation into the internal mechanisms of how LLMs possess crosslinguistic alignment capability remains elusive. \n\nTo delve deeper into the intrinsic mechanisms of LLMs' linguistic competence, this paper focuses on the LLMs' parameter importance and investigate the linguistic regions of LLMs based on 30 distinct languages' performance, with the purpose of figuring out the following questions: \n\nQ1: Does a core linguistic region exist within LLMs that facilitates cross-lingual alignment and generalization? By conducting further pretraining across six languages and evaluating models' parameter importance (Section 2.2), we discover a region in LLMs corresponding to the core linguistic competence, which accounts for approximately 1% of the model's total parameters.",
            "score": 0.3988682092144885,
            "section_title": "Core Linguistic Regions Distinct Monolingual Regions",
            "char_start_offset": 665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1429
                },
                {
                    "start": 1432,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 2088
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 636,
                    "matchedPaperCorpusId": "174798142"
                },
                {
                    "start": 636,
                    "end": 656,
                    "matchedPaperCorpusId": "126167342"
                },
                {
                    "start": 656,
                    "end": 672,
                    "matchedPaperCorpusId": "264306069"
                },
                {
                    "start": 787,
                    "end": 813,
                    "matchedPaperCorpusId": "253264914"
                },
                {
                    "start": 901,
                    "end": 923,
                    "matchedPaperCorpusId": "216553264"
                },
                {
                    "start": 1081,
                    "end": 1101,
                    "matchedPaperCorpusId": "216642043"
                },
                {
                    "start": 1101,
                    "end": 1122,
                    "matchedPaperCorpusId": "258919240"
                },
                {
                    "start": 1156,
                    "end": 1184,
                    "matchedPaperCorpusId": "233241079"
                },
                {
                    "start": 1271,
                    "end": 1291,
                    "matchedPaperCorpusId": "258588286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64990234375
        },
        {
            "corpus_id": "268201735",
            "title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
            "text": "To capture this grammatical structure as comprehensively as possible, especially given the challenges conventional models face in benchmarks like BLiMP (Warstadt et al., 2020), we focus on four key skills: agreement (anaphora and subjectverb relationships), licensing (negative polarity items and reflexive pronouns), managing longdistance dependencies (filler-gap constructions and cleft sentences), and navigating garden-path sentences, which contain temporary ambiguities that must be resolved for correct understanding. For example, \"the horse raced past the barn fell,\" the initial interpretation is that the horse is racing, but upon reaching \"fell,\" it becomes clear that the horse is being raced by another (unnamed) entity. \n\nSemantics, on the other hand, while more closely related to high-level cognitive understanding, within the context of LINGUISTIC KNOWL-EDGE, pertains to the meanings of individual words or lexical semantics (Geeraerts, 2009). This aspect is distinct from conceptual knowledge, which falls under other dimensions of LLM capabilities, highlighting the meaning-related understanding, such as synonymy, antonymy, and hypernymy. FORMAL KNOWLEDGE. Beyond encoding linguistic structures and word meanings, an essential aspect of language capability involves understanding formal operations among words, or wordbased reasoning. This means LLMs should be capable of recognizing relationships between words and deducing missing elements in a given pattern, such as completing analogies (e.g. \"man:woman :: king:_\"). FAC 2 E includes three types of reasoning mechanisms-deductive, inductive, and analogical reasoning-between words (Bang et al., 2023), and includes three symbol-based formal skills: numeric (dealing with numbers), logic (applying logical operations), and manipulation (altering the inputs in a rule-based manner) (Wei et al., 2022b). An example task is concatenating the last letters of a word list (\"think, machine, learning\" \u2192 \"keg\"). WORLD MODELING. To step towards cognitive capabilities, well-grounded comprehension of factual and commonsense knowledge is required.",
            "score": 0.39885903227175634,
            "section_title": "Formulation of LLMs' Capabilities",
            "char_start_offset": 7002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2111
                }
            ],
            "ref_mentions": [
                {
                    "start": 1854,
                    "end": 1872,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6533203125
        },
        {
            "corpus_id": "237491997",
            "title": "The Grammar-Learning Trajectories of Neural Language Models",
            "text": "We showed that NLMs learn English grammatical phenomena in a consistent order, and subsequently investigated the emerging trajectory. Our findings suggest that NLMs present consistent and informative trends. This finding suggests a path for studying NLMs' acquired behavior through their learning dynamics, as a useful complementary perspective to the study of final representations.\n\nFuture work will consider the impact of additional factors, architectures and learning phases that appear only later in training. We hope that this work will increase the affinity between the knowledge and methodologies employed in developmental studies, and those used for studying NLMs. Our goal is to obtain a better understanding of what makes linguistic generalization complex or simple to learn, for both humans and NLMs.",
            "score": 0.398657787598625,
            "section_title": "Summary and Conclusions",
            "char_start_offset": 31707,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "267412517",
            "title": "Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation",
            "text": "Recently, pre-trained large language models (LLMs) (Touvron et al., 2023a;b;Brown et al., 2020) have demonstrated remarkable capabilities in performing intricate reasoning tasks (Kojima et al., 2022). These tasks include problemsolving with world knowledge (Hendrycks et al., 2020;Suzgun et al., 2022), logical reasoning (Pan et al., 2023), and solving mathematical problems (Cobbe et al., 2021; Figure 1. We hypothesize that the pre-training corpus can be viewed as generated from random walks on a reasoning graph over world knowledge/concepts. With each node si representing concepts, pj can be viewed as arguments that connect them. Then we hypothesize that a language model (LM) training on such a corpus can be viewed as reasoning by a weighted aggregation of random walk paths that connect the entities in interest. PLM denote the LM distribution while PD denotes the random walk probability from the pre-training corpus. w 1 i denotes the weight assigned to the first random walk path by the LM for argument pi, and w 2 i denotes the weight assigned to the second random walk path. Hendrycks et al., 2021). These models are typically not explicitly fine-tuned to solve these tasks. Recent research (Jain et al., 2023) also suggests that the supervised fine-tuning process following pre-training only learns a wrapper on top of the already existing model capabilities, instead of learning new ones. It is intriguing to understand how next-token prediction pre-training contributes to the emergence of such reasoning capability. A better understanding of this matter can also inspire new pre-training/fine-tuning techniques to improve these important abilities of LLMs. \n\nIt is well-known that LLMs acquire emergent abilities through extensive pre-training (Wei et al., 2022a). In this paper, we focus on elucidating the emergence of reasoning ability -the capacity to draw novel conclusions from existing knowledge, which has been less studied. Many recent works also attempt to understand this phenomenon.",
            "score": 0.3980955608233405,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1675
                },
                {
                    "start": 1678,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2013
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 95,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 257,
                    "end": 281,
                    "matchedPaperCorpusId": "221516475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.849609375
        },
        {
            "corpus_id": "274776135",
            "title": "Understanding Knowledge Hijack Mechanism in In-context Learning through Associative Memory",
            "text": "In recent years, transformer-based models, such as BERT [13] and GPT [42,43,5], have achieved remarkable success in natural language processing. Especially, in-context learning (ICL) [14] has emerged as a groundbreaking capability within large language models (LLMs), enabling them to adapt to new tasks without traditional fine-tuning. Instead, these models leverage patterns from a prompt or input sequence, effectively learning \"in context\" by interpreting examples or instructions provided in real-time [33,56]. The capability for ICL [5] in language models develops gradually during a \"phase change\" observed in the early stages of training. At the same time, models acquire the ability to complete complex patterns through induction heads, suggesting that induction heads are a key mechanistic basis of ICL [40]. \n\nIn the context of ICL, Jiang et al. [25] discovered a phenomenon called context hijacking. This phenomenon occurs when altering the context disrupts fact recall, causing the model to generate incorrect outputs influenced by the in-context knowledge provided in the prompt. Similarly, ICL can exhibit oversight of in-context knowledge provided in the prompt, where the model relies solely on the knowledge acquired during pretraining (global knowledge) while ignoring the in-context knowledge. These types of knowledge hijacking are thought to arise from either discarding part of the prompt's information or overly trusting the information in the prompt. Conversely, if the model can (i) comprehensively use the information in the prompt and (ii) balance in-context knowledge and global knowledge, hijacking can be prevented. In relation to the induction head, we demonstrate that employing relative positional encoding (RPE) allows the transformer to comprehensively use the information in the prompt. Furthermore, we show that even a three-layer transformer without positional encoding can achieve comprehensive use of information. Understanding how to avoid knowledge hijacking is essential for the safe and reliable use of ICL capabilities in LLMs and is a matter of significant societal importance.",
            "score": 0.39794612230242793,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 76,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "13490401"
                },
                {
                    "start": 507,
                    "end": 511,
                    "matchedPaperCorpusId": "231632658"
                },
                {
                    "start": 857,
                    "end": 861,
                    "matchedPaperCorpusId": "270737871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "259138462",
            "title": "Large language models and (non-)linguistic recursion",
            "text": "In conclusion, we tested GPT-4 and GPT-3.5's explicit command of recursion on a series of linguistic and visual tasks. We find that GPT-4 is capable of identifying, creating, and analyzing recursive structures in the linguistic domain. The model is also capable of creating recursive art in ASCII, illustrating recursive processes with Mermaid syntax, and creating fractal graphs using the tikz package in L A T E X. GPT-4's success at producing recursion across domains suggests that it may be the first language model with emergent and general (i. e. non-domain-specific) recursive ability. In comparison, GPT-3.5's performance on the same tasks is more limited. While the older model is capable of identifying recursion and illustrating it with linguistic examples, it often fails at analyzing it and representing it visually. \n\nOne could object to our findings by noting that the GPT training data almost certainly contains linguistics textbooks (and other pedagogical and research materials) that explicitly discuss recursion. Therefore, the models' understanding of recursion can be attributed to memorization. We respond by observing that humans are not intuitively aware of recursion either and need to be explicitly instructed as well. Thus, there are no fundamental asymmetries in the prior training necessary for humans and LLMs to succeed on the tasks presented in this paper. \n\nFinally, we observe that while GPT-4's initial replies to a prompt are not always correct, the model's performance improves considerably when asked to evaluate its own output (e. g. 5, 7). These results are at odds with \u015eahin et al.'s (2020) assertion that language models \"lack the skill of iterative reasoning upon knowledge\" (p. 1241), and challenge Katzir (2023), who shows a case where reprompting does not result in an improvement and argues that \"further time and resources are of no use to ChatGPT\" (p. 5). Instead, our findings suggest that-given the right experimental set-up-GPT-4 is capable of emergent iterative reasoning, which is a prerequisite for complex problem-solving.",
            "score": 0.3976901136068807,
            "section_title": "discussion and conclusion",
            "char_start_offset": 24860,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2079
                }
            ],
            "ref_mentions": [
                {
                    "start": 1744,
                    "end": 1757,
                    "matchedPaperCorpusId": "266331665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54052734375
        },
        {
            "corpus_id": "247656607",
            "title": "Emergent Structures and Training Dynamics in Large Language Models",
            "text": "Recent advances in self-supervised learning, distributed training, and architecture improvements have enabled training massive language models (Devlin et al., 2019;Brown et al., 2020;Radford et al., 2019;Ma et al., 2020;Liu et al., 2019). As these models have grown larger, so has their performance and generalization to new tasks. Furthermore, these techniques have also shown substantial improvements in learning multilingual (Chen et al., 2020) and multimodal representations (Radford et al., 2021). These large language models (LLMs) have advanced the state of the art in fewand zero-shot tasks (Radford et al., 2019;Brown et al., 2020;Radford et al., 2021). However, the size of these models makes them difficult to evaluate, examine, and audit. What structures emerge from training these neural networks? What internal representations do these networks learn?\n\nIn part, this opacity is implicit in the models themselves. Many of the fascinating capabilities of LLMs are \"implicitly induced, not explicitly constructed\" emergent properties (Bommasani et al., 2021). Emergent properties are those that result from the structural relations and interactions between a system's components (Ablowitz, 1939;Callebaut and Rasskin-Gutman, 2005). One way of characterizing the emergence of useful properties from complexity is through self-organization, wherein complex systems come to develop ordered patterns from the interactions of their components (Gershenson et al., 2020). Interactions between the parts of a system can produce complex global behavior, for example in the collective behavior of ants, flocking in birds (Cucker and Smale, 2007), or in the brain and central nervous system (Dresp-Langley, 2020;Brown, 2013). In the context of deep learning models, qualitatively different behavior has been observed during phase transitions in model size or training steps (Steinhardt, 2022). Current research on understanding the generalization abilities of LLMs has largely focused on the degree to which they learn various linguistic features (e.g. syntax) that would support performance on diverse downstream tasks. Our goal instead is to motivate research that grounds the learning of these higher",
            "score": 0.39761688334827205,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 164,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 183,
                    "end": 204,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 220,
                    "end": 237,
                    "matchedPaperCorpusId": "84841767"
                },
                {
                    "start": 428,
                    "end": 447,
                    "matchedPaperCorpusId": "226975516"
                },
                {
                    "start": 599,
                    "end": 621,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1449,
                    "end": 1474,
                    "matchedPaperCorpusId": "81977040"
                },
                {
                    "start": 1622,
                    "end": 1646,
                    "matchedPaperCorpusId": "206590734"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65966796875
        },
        {
            "corpus_id": "270357296",
            "title": "BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense",
            "text": "The exploration of reasoning abilities in large language models, lateral thinking, and common sense reasoning has been the focus of several studies in recent years.The BRAINTEASER is a novel task in this context, requiring a unique blend of these capabilities.In the paper (Zhang et al., 2022), foundational insights into the use of knowledge graphs for self-supervision in common sense reasoning tasks are provided.This work is particularly relevant as it demonstrates how external knowledge can significantly improve an LLM's ability to solve reasoning problems.Furthering the discussion on reasoning, LatEval (Huang et al., 2024) introduces an evaluation benchmark for LLMs based on lateral thinking puzzles.This process is similar to the method in solving the BRAINTEASER, which encourages models to elaborate on their reasoning, leading to improved performance.The paper Rid-dleSense (Lin et al., 2021) offers an analysis of how LLMs handle riddle questions that require linguistic creativity and common sense knowledge.This is directly linked to our approaches, where the ability of an LLM to use analogous examples influences its problem-solving effectiveness.Also, (Dou and Peng, 2022) investigates the inherent knowledge within language models and their ability to function in zero-shot common sense question answering tasks.The integration of external knowledge is particularly relevant to tasks requiring common sense and real-world information, underscoring the importance of knowledge retrieval in the context of the BRAINTEASER.MVP-Tuning (Huang et al., 2023) introduces a novel approach to knowledge retrieval using prompt tuning.This aligns with our aim in solving BRAINTEASER to evaluate the capacity of LLMs to handle questions that necessitate an understanding of the world as humans perceive it.Lastly, ReConcile (Chen et al., 2023) and (Liang et al., 2023) both discuss the use of multiple LLMs to improve reasoning capabilities.These works highlight the potential of using a diverse set of models to solve complex tasks like the BRAINTEASER, further enriching the strategies for solving such tasks with AI systems.",
            "score": 0.397359443165493,
            "section_title": "Related Works",
            "char_start_offset": 3081,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 260
                },
                {
                    "start": 260,
                    "end": 416
                },
                {
                    "start": 416,
                    "end": 564
                },
                {
                    "start": 564,
                    "end": 711
                },
                {
                    "start": 711,
                    "end": 866
                },
                {
                    "start": 866,
                    "end": 1025
                },
                {
                    "start": 1025,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1334
                },
                {
                    "start": 1334,
                    "end": 1542
                },
                {
                    "start": 1542,
                    "end": 1645
                },
                {
                    "start": 1645,
                    "end": 1815
                },
                {
                    "start": 1815,
                    "end": 1950
                },
                {
                    "start": 1950,
                    "end": 2136
                }
            ],
            "ref_mentions": [
                {
                    "start": 1553,
                    "end": 1573,
                    "matchedPaperCorpusId": "259370811"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43505859375
        },
        {
            "corpus_id": "270562766",
            "title": "Reframing linguistic bootstrapping as joint inference using visually-grounded grammar induction models",
            "text": "Given our primary goal: to demonstrate how the interplay between syntactic and semantic acquisition can follow from joint learning, using simulated children's book data presented a sufficient environment to examine these dynamics.\n\nJoint learning works because it helps mutually constrain related hypothesis spaces, here grammar and semantic representations.These types of constraints are likely necessary for human learners who-unlike large language models-are limited in terms of memory and processing capacity as well as amount of input evidence.Even with these limitations though, we learn language and better yet, we learn representations which allow to generalize and use language in completely novel contexts.The reason for our learning efficiency and generalization abilities may lie in our effective learning strategies, which we argue are built on joint learning.\n\nEmpirical evidence suggests that semantic and syntactic processing during language comprehension or production are not separable into distinct areas of the brain, but instead represent distributed processes which overlap across a wide region referred to as the language network (Fedorenko, Blank, Siegelman, & Mineroff, 2020;Hu et al., 2022;Shain et al., 2023;Fedorenko, Ivanova, & Regev, 2024;Shain et al., 2024).Furthermore, children's lexicon and their syntactic production abilities grow side by side during language development (Bates et al., 1994;Brinchmann, Braeken, & Lyster, 2019;Frank, Braginsky, Marchman, & Yurovsky, 2021).These results all support our proposal: that language learning is joint learning across many levels of linguistic representation.The acquisition of morphemes, words, syntax, semantics, pragmatics have for the most part been considered in isolation.However, if language learning is indeed a joint inference problem across many levels of linguistic structure, then future research in the field should try to understand how learning biases or constraints within these different levels arise as a function of joint learning.For example, how does the acquisition of semantic knowledge affect the acquisition of syntax?or how does learning morpheme boundaries interplay with the acquisition of semantic knowledge?Understanding how these constraints arise and interact, we suggest should be the next key direction in language learning debates.\n\nComputational modeling is not new to the fields of language development and cognitive science.",
            "score": 0.397101001745465,
            "section_title": "Discussion",
            "char_start_offset": 49997,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 232,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 549
                },
                {
                    "start": 549,
                    "end": 716
                },
                {
                    "start": 716,
                    "end": 873
                },
                {
                    "start": 875,
                    "end": 1289
                },
                {
                    "start": 1289,
                    "end": 1510
                },
                {
                    "start": 1510,
                    "end": 1639
                },
                {
                    "start": 1639,
                    "end": 1758
                },
                {
                    "start": 1758,
                    "end": 2030
                },
                {
                    "start": 2030,
                    "end": 2123
                },
                {
                    "start": 2123,
                    "end": 2217
                },
                {
                    "start": 2217,
                    "end": 2346
                },
                {
                    "start": 2348,
                    "end": 2442
                }
            ],
            "ref_mentions": [
                {
                    "start": 1153,
                    "end": 1200,
                    "matchedPaperCorpusId": "213787994"
                },
                {
                    "start": 1200,
                    "end": 1216,
                    "matchedPaperCorpusId": "251323162"
                },
                {
                    "start": 1216,
                    "end": 1235,
                    "matchedPaperCorpusId": "259328196"
                },
                {
                    "start": 1235,
                    "end": 1269,
                    "matchedPaperCorpusId": "269112224"
                },
                {
                    "start": 1269,
                    "end": 1288,
                    "matchedPaperCorpusId": "269474005"
                },
                {
                    "start": 1408,
                    "end": 1428,
                    "matchedPaperCorpusId": "19579561"
                },
                {
                    "start": 1428,
                    "end": 1464,
                    "matchedPaperCorpusId": "205063826"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59521484375
        },
        {
            "corpus_id": "254247214",
            "title": "Event knowledge in large language models: the gap between the impossible and the unlikely",
            "text": "Distributional models like LLMs provide us with the unique opportunity to test the relationship between language and world knowledge. The fact that LLMs master selectional restrictions but not fine-grained event schemas suggests a distinction between linguistic and conceptual knowledge. The striking difference in score distributions in humans and LLMs (Figure 1B,C;  Figures S4 and S14) further highlights the fact that the way in which semantic categories are represented and combined in language models differs markedly from how they are represented and used by humans.\n\nThe dissociation between language and GEK observed in LLMs is consistent with the wealth of human evidence showing that language processing relies on mechanisms that are distinct from other cognitive capacities, such as logic and math (e.g., Amalric & Dehaene, 2016;Coetzee & Monti, 2018;Monti et al., 2007Monti et al., , 2009Monti et al., , 2012Varley et al., 2005), music perception (e.g., Basso & Capitani, 1985;Chen et al., 2021;Luria et al., 1965), gesture perception (Jouravlev et al, 2019;Pritchett et al, 2018), and social reasoning (Lecours & Joanette, 1980;Paunov et al., 2019Paunov et al., , 2022R. Varley & Siegal, 2000). Many of these capacities are important for language use in real-life situations, yet their neural processing mechanisms are distinct from the core language network (Fedorenko & Varley, 2016;Mahowald, Ivanova et al., in prep). GEK, as well as semantic knowledge more generally, might be considered somewhat of an outlier among these functions due to a tight coupling between language and semantics/pragmatics. After all, how is it possible to process language without accessing the underlying meaning? Nevertheless, evidence from brain-damaged individuals points to a dissociation between linguistic and conceptual processing (e.g., Caramazza et al., 1982;Lambon Ralph et al., 2017;Patterson et al., 2007), including an event plausibility task performed on pictures (Ivanova et al., 2021). That said, the language",
            "score": 0.39691627121831885,
            "section_title": "Linguistic and conceptual knowledge dissociate in humans",
            "char_start_offset": 55941,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 817,
                    "end": 841,
                    "matchedPaperCorpusId": "12041996"
                },
                {
                    "start": 841,
                    "end": 863,
                    "matchedPaperCorpusId": "3885543"
                },
                {
                    "start": 881,
                    "end": 901,
                    "matchedPaperCorpusId": "5624174"
                },
                {
                    "start": 921,
                    "end": 941,
                    "matchedPaperCorpusId": "8184854"
                },
                {
                    "start": 967,
                    "end": 990,
                    "matchedPaperCorpusId": "38989053"
                },
                {
                    "start": 1008,
                    "end": 1027,
                    "matchedPaperCorpusId": "143518502"
                },
                {
                    "start": 1116,
                    "end": 1142,
                    "matchedPaperCorpusId": "8176860"
                },
                {
                    "start": 1142,
                    "end": 1161,
                    "matchedPaperCorpusId": "58589867"
                },
                {
                    "start": 1182,
                    "end": 1207,
                    "matchedPaperCorpusId": "33837368"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255126953125
        },
        {
            "corpus_id": "273025796",
            "title": "An Information Theory of Compute-Optimal Size Scaling, Emergence, and Plateaus in Language Models",
            "text": "To optimally use computational resources when training language models, several recent studies have empirically investigated how model size and dataset size should scale with compute budget [14,11], finding a certain allometric rule much like in mathematical biology [28,9]. As the sizes of language models continue to increase, large improvements in performance have been observed in certain complex tasks with only a small improvement in the model's loss [29] (but see [25]). The larger language models are therefore said to exhibit emergent capabilities, a term drawn from statistical mechanics, where small changes in a macroscopic variable of the system (such as temperature) around a critical value cause an abrupt change-a phase transition or emergent behavior [5]-in its properties. More recently, there has been prevalent discourse in the AI community that further increases in language model size lead to plateauing of performance [7,24]. Although, there have been attempts to explain one or two of these empirical phenomena, a unified mathematical framework that explains all three of these empirically observed phenomena is lacking. \n\nHere we take an approach that builds on information and coding theory [20] that does so, and also predicts multiple plateaus. In particular, we draw on mathematical ideas around low-density parity check (LDPC) codes (which achieve Shannon optimality) [27,23] and random graph theory [3]. Though statistical language modeling and information theory were introduced in the same paper [26], modern connections between the two are still fairly limited, cf. [4]. \n\nTo provide simple and insightful explanations of empirical phenomena, several abstract frameworks have been proposed [2,18,21], all based on a skill-text bipartite graph that operates at a semantic level and captures key real-world properties [30]. Arora and Goyal [2] explain emergent phenomena by assuming a compute-optimal size scaling rule (Chinchilla allometry rule) [11]. Liao et al. [18] also assume compute-optimal (Chinchilla) size scaling to explain emergence. Michaud et al. [21] assume power-law scaling and that each text piece contains only one skill, which may be very different than real-world scenarios.",
            "score": 0.39679475043422685,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1604
                },
                {
                    "start": 1607,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2227
                }
            ],
            "ref_mentions": [
                {
                    "start": 471,
                    "end": 475,
                    "matchedPaperCorpusId": "258418299"
                },
                {
                    "start": 1398,
                    "end": 1402,
                    "matchedPaperCorpusId": "4358269"
                },
                {
                    "start": 1529,
                    "end": 1533,
                    "matchedPaperCorpusId": "5747983"
                },
                {
                    "start": 1600,
                    "end": 1603,
                    "matchedPaperCorpusId": "259937719"
                },
                {
                    "start": 1730,
                    "end": 1733,
                    "matchedPaperCorpusId": "257687665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66552734375
        },
        {
            "corpus_id": "263830318",
            "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition",
            "text": "Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specifically focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.",
            "score": 0.39662625871133195,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "277780778",
            "title": "Can Large Language Models Become Policy Refinement Partners? Evidence from China's Social Security Studies",
            "text": "The emergence of large language models (LLMs) marks a transformative phase in artificial intelligence. Although AI technology has evolved over six decades, its applications remained largely confined to specialized domains such as computer vision, speech recognition, game-playing systems, and autonomous vehicles. LLMs represent a paradigm shift by demonstrating human-like knowledge integration and cross-domain reasoning capabilities for the first time, significantly expanding AI's societal applicability. In March 2023, OpenAI's GPT-4 breakthrough captured global attention. The model achieved unprecedented natural language fluency, enabling seamless human-machine communication. Moreover, in dealing with complex issues, GPT-4 has demonstrated a level close to that of human experts [11]. Subsequent 2024 releases (GPT-4o and GPT-4-turbo) further advanced task performance [12]. \n\nChina's LLM development has progressed remarkably. Between 2024 and 2025, several institutions launched distinctive models: Moonshot AI's Kimi series excelled in long-context processing [13]; Baichuan Inc's models showed clinical decision-support advantages [14]; and DeepSeek's R1 model achieved internationally competitive reasoning performance [15]. These locally developed models exhibit optimized performance for Chinese language processing through specialized cultural and linguistic adaptation. \n\nThe cognitive capabilities of large language models (LLMs) primarily emerge during pretraining on vast corpora of unlabeled text data. Through this process, the models develop sophisticated representations of linguistic patterns, syntactic relationships, and semantic frameworks. To illustrate this scale, consider GPT-4's training corpus of approximately 13 trillion tokens [16]. This unprecedented volume in AI research equates to roughly 130 million standard-length books (assuming 100,000 tokens per book) -a dataset that dwarfs both conventional model training sets and the entire print collection of the U.S. Library of Congress (approximately 50 million cataloged items). The diversity of training data contributes significantly alongside data volume. Training on various content types including multiple languages, news articles, published works, research papers, technical documentation and online discussions helps language models develop cross-domain understanding. This varied training appears to support their ability to handle different kinds of tasks.",
            "score": 0.3965566588335992,
            "section_title": "A concise overview of LLM development",
            "char_start_offset": 7728,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2367
                },
                {
                    "start": 2368,
                    "end": 2457
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64794921875
        },
        {
            "corpus_id": "254094978",
            "title": "A computational model of language comprehension unites diverse perspectives",
            "text": "When we comprehend language, we convert the acoustic or visual signal that we perceive into an internal representation of meaning. This conversion is complex, and language scientists have worked for decades to understand how it functions. In studying language comprehension, the field has been marked by several sharp theoretical divides. One of these controversies concerns modularity of the component processes of comprehension, such as whether interpretation of a sentence's syntactic structure proceeds independently from interpretation of meaning and other aspects of language (1), or whether comprehension involves interaction among syntax, meaning, and so on (2). A second debate concerns the relationship between comprehension processes and the temporary memory that is necessary to execute them: does this working memory stem from a dedicated temporary storage system (3), or do language processes themselves create the temporary memories they need (4)? These longstanding debates get at core questions about the nature of human language capacities, but alternative positions have been difficult to distinguish from one another in precise ways. In PNAS, Hahn et al. ( 5) describe a theory and implemented computational model of central pieces of language comprehension, linking disparate theoretical perspectives in both language comprehension and working memory. \n\nBecause linguistic signals arrive over time, converting signal to meaning requires integrating earlier and later portions of the signal. For perceivers to understand who did what to whom in the sentence in Fig. 1A, for example, the word dog must be linked to three verbs-the nearby word jumped and the more distant chased and caught. The perceiver must also determine that the squirrel is the object of the chasing and is not doing any jumping, chasing, or catching itself. Although the example in Fig. 1A may not seem much like everyday language, these complex sentences have been a central testing ground of theories of language comprehension, for two reasons. First, sentences with embedded clauses, such as the one in Fig. 1A, have been a cornerstone of generative linguists' claims for the competence-performance distinction (6, 7), which holds that humans have expansive linguistic knowledge (competence) but more restricted comprehension performance because of constraints on their working memory capacity. Studying these difficult embedded sentences may therefore inform longstanding claims about human language knowledge and use.",
            "score": 0.39638511030994705,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2388
                },
                {
                    "start": 2389,
                    "end": 2513
                }
            ],
            "ref_mentions": [
                {
                    "start": 582,
                    "end": 585,
                    "matchedPaperCorpusId": "195719082"
                },
                {
                    "start": 666,
                    "end": 669,
                    "matchedPaperCorpusId": "15560738"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53515625
        },
        {
            "corpus_id": "73609281",
            "title": "Ellipsis and dialogue in the early acquisition of syntax",
            "text": "The function of this type of construction has already been identified for both the two-word phase (Behrens & Gut, 2005) and the earlier transition from one word phase to two word phase (Carranza et al., 1991). That is to say, the early vertical \"ellipses\" are a cooperative mode of linguistic construction and can be an observable and measurable index of social determination in the process of language acquisition, since verticality is a shared text, dialogue. \n\nTo our knowledge, no systematic investigation has been carried out in any language on the early production of vertical ellipses during the \"two-word phase\", and their part in the transition to syntactic construction. In our view this study is perfectly consistent with others seeking to identify exactly what are the constructions that enable the child to make the transition to early syntax by way of learning processes. As implied in the research already reviewed, it is assumed these processes are facilitated by a pragmatic context (e.g.: vertical construction) where linguistic advances are obtained (e.g.: the pre-ellipses) avoiding the memory load demanded by a horizontal construction of those same ellipses. At the same time, the proper use of vertical preelliptical constructions can result from short scope statistical abstractions, done on the relations of formal variants of words (e.g.: morphology) to their semantic function within weakly generalized contexts.",
            "score": 0.3963511498897636,
            "section_title": "Introduction",
            "char_start_offset": 5629,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1439
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 119,
                    "matchedPaperCorpusId": "32013565"
                },
                {
                    "start": 185,
                    "end": 208,
                    "matchedPaperCorpusId": "145571150"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.306884765625
        },
        {
            "corpus_id": "268532485",
            "title": "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models",
            "text": "In this section, we delve into three pivotal observations on agent tuning that serve as the foundation of our subsequent investigation.\n\nObservation 1.Most agent training data is entangled with both format following and general reasoning, causing a significant departure from the model's original pretraining language domain, namely, natural conversation.\n\nRecent agent tuning works (Zeng et al., 2023;Qin et al., 2023) endorse the adoption of specific formats, exemplified by ReAct (Thought-Action-Observation), for fine-tuning the language model.Moreover, it's noteworthy that action arguments are frequently presented in JSON format.Encoding both format and reasoning knowledge into the training corpus shifts the tuning process from the original chat domain, presenting it as an out-of-domain task for language models.As shown in Figure 2, we compare the training curve of formatted data and normal data.It can be clearly seen that the loss associated with formatted data descends more rapidly to a low value, while keeping content loss still high (0.54 vs 0.04), indicating that the former leads to an inadequate learning process.This phenomenon can be probably attributed to the existence of a fixed structure (ReAct, JSON), where the model quickly gets overfitted to the format itself.Consequently, it fails to grasp the underlying reasoning abilities embedded within the training data, resulting in unsatisfied performance.Observation 2. By explicitly decomposing the training data along the basic capability aspects, each loss exhibits different convergence curves, indicating varied learning speeds on the capabilities requisite for agent tasks of LLMs.\n\nInspired by (Chen et al., 2023c), we explicitly disentangle the model's capabilities into distinct components: instruction following, reasoning, retrieval, and understanding.In this context, instruction following corresponds to format generation, reasoning corresponds to the thought quality at each step, retrieval involves selecting the appropriate function name to execute the task, and the understanding encompasses the parameter inputs for the selected functions.\n\nBy visualizing the loss based on respective aspects in Figure 3, we discern that LLM tends to exhibit varying learning speeds for the capabilities essential for proficient agents.\n\nTo elaborate, retrieval and understanding emerge as relatively more manageable tasks compared to reasoning, with instruction following being the simplest in the learning process.",
            "score": 0.3960436304099155,
            "section_title": "Pilot Observations",
            "char_start_offset": 7166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 137,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 355
                },
                {
                    "start": 357,
                    "end": 548
                },
                {
                    "start": 548,
                    "end": 636
                },
                {
                    "start": 636,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 908
                },
                {
                    "start": 908,
                    "end": 1135
                },
                {
                    "start": 1135,
                    "end": 1292
                },
                {
                    "start": 1292,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1663
                },
                {
                    "start": 1665,
                    "end": 1839
                },
                {
                    "start": 1839,
                    "end": 2133
                },
                {
                    "start": 2135,
                    "end": 2314
                },
                {
                    "start": 2316,
                    "end": 2494
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6943359375
        },
        {
            "corpus_id": "276724796",
            "title": "Triple Phase Transitions: Understanding the Learning Dynamics of Large Language Models from a Neuroscience Perspective",
            "text": "Previous research linking model representations to brain activity (Yamins et al., 2014;G \u00fcc \u00b8l \u00fc & van Gerven, 2015;Schrimpf et al., 2021;Caucheteux et al., 2023) has not fully addressed such learning dynamics. While Antonello et al. (2023) reported a rise in alignment with increasing model size, our findings depict a more intricate path: an initial surge in brain alignment, a decline, and a final resurgence. This suggests that LLMs may adopt distinct computational strategies at various stages, rather than gradually acquiring brain-like language representations. Moreover, alignment with semantic regions in the temporal cortex generally rises over the course of training, implying a dynamic reorganization in how models correspond to brain activity. By capturing shifts in LLM neuron activations related to downstream tasks, we elucidated how task-specific neurons form and how much specialization they share among tasks. These findings align with previous studies showing neurons that encode particular linguistic information (Tenney et al., 2019;Dai et al., 2022;Wang et al., 2022;Gurnee et al., 2023). We observed that neurons contributing substantially to certain tasks emerge abruptly in later layers as training progresses. Moreover, the variations in neuron sharing across tasks suggest that differences in activation patterns-reflecting task-specific characteristics (e.g., required capabilities, difficulty levels, and answer formats)-are indeed discernible at the neuronal level. Taken together, our results offer a more comprehensive view of LLM neurons during the learning process, building on past work that focused solely on trained models. Further in-depth analyses of these neurons would be a fascinating avenue for future research. \n\nFinally, we extended the findings of Cheng & Antonello (2024), who demonstrated a positive correlation between the dimensionality of LLM activations in trained models and the alignment of those LLMs with the brain. Our results confirm that this correlation remains robust during training, suggesting that the observed phase transitions are tied to changes in activation dimensionality. It is crucial to note that although these metrics confirm that fundamental transformations occur in the representation space, they do not fully explain the specific underlying mechanisms.",
            "score": 0.3959881719302649,
            "section_title": "Discussion and conclusions",
            "char_start_offset": 27402,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1755
                },
                {
                    "start": 1758,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2331
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 116,
                    "matchedPaperCorpusId": "3515742"
                },
                {
                    "start": 116,
                    "end": 138,
                    "matchedPaperCorpusId": "222359195"
                },
                {
                    "start": 138,
                    "end": 162,
                    "matchedPaperCorpusId": "257309545"
                },
                {
                    "start": 1034,
                    "end": 1055,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 1055,
                    "end": 1072,
                    "matchedPaperCorpusId": "233296761"
                },
                {
                    "start": 1072,
                    "end": 1090,
                    "matchedPaperCorpusId": "253510792"
                },
                {
                    "start": 1090,
                    "end": 1110,
                    "matchedPaperCorpusId": "258437237"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8974609375
        },
        {
            "corpus_id": "269449935",
            "title": "Hallucination of Multimodal Large Language Models: A Survey",
            "text": "Before moving to multimodal large language models, it is essential to introduce the concept of large language models. Typically, LLMs encompass a range of transformer-based models that are extensively trained on vast textual datasets. Prominent examples include GPT-3 [14], PaLM [31], LLaMA [153], and . Through scaling both data volume and model capacity, LLMs demonstrate notable emergent capabilities, including In-Context Learning [14], Chain-of-Thought prompting [168] and instruction following [130], among others. \n\nThe characteristics and behaviors of LLMs are intricately linked to their training processes. LLMs typically undergo three primary training stages: pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). Below, we provide a concise overview of each stage to facilitate comprehension. \n\nPre-trianing. Pre-training serves as a fundamental phase in the learning process of LLMs [219]. During this stage, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence. By undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities. This pre-training process establishes a solid groundwork for the models to undertake subsequent fine-tuning tasks effectively. \n\nSupervised Fine-Tuning. Although pre-training equips LLMs with substantial knowledge and skills, it's important to acknowledge that its primary focus is on optimizing for completion. Consequently, pre-trained LLMs essentially function as completion machines, which may create a misalignment between the objective of predicting the next word within LLMs and the user's objective of obtaining desired responses. To address this disparity, the concept of Supervised Fine-Tuning (SFT) [204] has been introduced. SFT involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, thereby enhancing the capabilities and controllability of LLMs. \n\nReinforcement Learning from Human Feedback. Although SFT has made strides in enabling LLMs to adhere to user instructions, there remains a need for further alignment with human preferences.",
            "score": 0.3957890807559977,
            "section_title": "Large Language Models",
            "char_start_offset": 5855,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2078
                },
                {
                    "start": 2081,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 279,
                    "end": 283,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 435,
                    "end": 439,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 468,
                    "end": 473,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90673828125
        },
        {
            "corpus_id": "268374806",
            "title": "The effect of planning time on vocabulary acquisition in a task-based environment: the mediating roles of working memory and field (in)dependence",
            "text": "As an individual difference train, WM, a fundamental cognitive system, plays a pivotal role in the intricate process of vocabulary acquisition. WM refers to the cognitive capacity responsible for temporarily holding and manipulating information during complex cognitive tasks [14]. In the context of language learning, WM serves as a mental workspace where learners can actively process and store linguistic elements, including vocabulary, grammar rules, and sentence structures [15]. The limited capacity of WM necessitates efficient cognitive processes, making it a critical component in the acquisition and retention of new lexical items. \n\nWhen engaged in vocabulary learning, WM acts as a dynamic cognitive mechanism that facilitates the encoding, rehearsal, and retrieval of linguistic information [14]. Learners utilize WM to link new words to existing knowledge, form associations, and establish connections that aid in the storage and retrieval of vocabulary items during language production. The active engagement of WM in these processes not only strengthens the neural networks associated with vocabulary but also enhances the efficiency of language recall [15]. \n\nMoreover, the interplay between WM and vocabulary acquisition is particularly pronounced in TBLT settings. As learners navigate communicative tasks, WM becomes a cognitive scaffold, enabling them to manipulate linguistic elements, such as selecting appropriate vocabulary, constructing grammatically sound sentences, and adapting language use to the communicative context [11]. \n\nAs another individual difference trait, FI/D represents an individual's cognitive style characterized by the degree to which they rely on contextual cues in information processing. Those classified as field-independent (FI) tend to separate details from the surrounding context, focusing on discrete elements, while field-dependent (FD) individuals perceive information holistically, considering the broader context (16). In the realm of language learning, an individual's field (in)dependence (FI/D) can significantly impact how they approach and comprehend linguistic input. \n\nFI learners may excel in isolating and analyzing specific components of language, allowing for meticulous attention to vocabulary, grammar rules, and syntactic structures [16]. This cognitive style may lead to a heightened ability to discern nuanced meanings and relationships among words, potentially facilitating vocabulary acquisition through a more analytical and detail-oriented approach.",
            "score": 0.39578750937287227,
            "section_title": "Introduction",
            "char_start_offset": 4751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1174
                },
                {
                    "start": 1177,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1554
                },
                {
                    "start": 1557,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2133
                },
                {
                    "start": 2136,
                    "end": 2312
                },
                {
                    "start": 2313,
                    "end": 2529
                }
            ],
            "ref_mentions": [
                {
                    "start": 479,
                    "end": 483,
                    "matchedPaperCorpusId": "53775186"
                },
                {
                    "start": 1169,
                    "end": 1173,
                    "matchedPaperCorpusId": "53775186"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.463623046875
        },
        {
            "corpus_id": "261214466",
            "title": "Construction Grammar and Language Models",
            "text": "Having provided the necessary background of pre-trained language models, we now discuss the extent to which PLMs have access to constructional information. As mentioned, pre-trained language models have been shown to have access to a significant amount of linguistic information with no explicit training. That is, pre-training alone results in the emergence of linguistic knowledge. To more comprehensively address the extent to which constructional information is emergent in PLMs, we start by exploring related cognitive linguistic phenomena, specifically polysemy and concepts (or categories). \n\nPLMs, unlike embedding methods such as GloVe and word2vec, have been shown to learn a good estimation of the different senses of words during pre-training (Vuli\u0107 et al. 2020;Gar\u00ed Soler & Apidianaki 2021;Haber & Poesio 2021). In addition, significant gains have been made on the task of word sense disambiguation by the use of PLMs (Loureiro & Jorge 2019;Loureiro et al. 2022), further reinforcing this notion. \n\nOn the other hand, the differentiations among concepts in pre-trained language model representations are not clear-cut but rely on such information as syntax and sentiment (Yenicelik et al. 2020). For example, representations of the concept ARMS fall into different clusters based on sentiment as in the case of handcuffed arms (scared) and ... swooped him up into her arms (love). This alternative approach to word senses, however, is likely to be more appealing to construction grammarians and to provide a better approximation of how humans represent meaning and concepts: \"a core, tapering to a periphery\" (Croft & Cruse 2004). Nair et al. (2020) similarly argue for \"the potential utility of continuous-space representations of sense meanings\", given their finding that there is a correlation between human judgements of the relation between meanings and distances in the BERT embeddings space.",
            "score": 0.3957672896393954,
            "section_title": "The Emergence of Construction Grammar in Pre-Trained Language Models",
            "char_start_offset": 28418,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 597
                },
                {
                    "start": 600,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1911
                }
            ],
            "ref_mentions": [
                {
                    "start": 774,
                    "end": 803,
                    "matchedPaperCorpusId": "233476317"
                },
                {
                    "start": 954,
                    "end": 975,
                    "matchedPaperCorpusId": "235196103"
                },
                {
                    "start": 1184,
                    "end": 1207,
                    "matchedPaperCorpusId": "226283455"
                },
                {
                    "start": 1622,
                    "end": 1642,
                    "matchedPaperCorpusId": "7138078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7216796875
        },
        {
            "corpus_id": "254247214",
            "title": "Event knowledge in large language models: the gap between the impossible and the unlikely",
            "text": "A vital component of human intelligence is our ability to learn, store, and flexibly use rich, structured knowledge about the world. World knowledge spans different domains (from physical properties to social conventions) and covers different types of information, including knowledge of objects, agents, actions, and ideas. One important component of world knowledge is our generalized event knowledge (GEK) -templates of common events observed in the world (e.g., McRae & Matsuki, 2009). We acquire GEK both through sensorimotor experiences (i.e., from performing and observing events in the world) and through linguistic experiences (i.e., from event descriptions generated by other people). The close link between event knowledge and language behavior (e.g., Bicknell et al., 2010;Federmeier & Kutas, 1999;Kamide et al., 2003;Matsuki et al., 2011;McRae & Matsuki, 2009) raises the question to which extent GEK can be learned from linguistic input alone, as a consequence of acquiring rich statistical knowledge of word co-occurrence patterns in text.\n\nLarge language models (LLMs) allow us to test the possibility that GEK can emerge naturally from tracking co-occurrence patterns in linguistic input. State-of-the-art LLMs, trained to predict words based on their context, have achieved remarkable success across a variety of tasks, such as generating syntactically and semantically coherent paragraphs of text (Brown et al., 2020), sentiment analysis and logical inference (e.g., Devlin et al., 2018;Liu et al., 2019;Radford et al., 2019;Yang et al., 2019), closed-book QA (Roberts et al., 2020), and certain aspects of commonsense reasoning (Talmor et al., 2020;Zellers et al., 2018).\n\nStudies of world knowledge in LLMs so far have produced mixed results. On one hand, LLMs perform well on multiple linguistic tasks designed to probe world knowledge, such as the Winograd Schema Challenge (WNLI; Levesque et al., 2012), the Story Cloze Test (SWAG; Zellers et al., 2018), and the Choice of",
            "score": 0.39557604284056647,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 466,
                    "end": 488,
                    "matchedPaperCorpusId": "2616234"
                },
                {
                    "start": 763,
                    "end": 785,
                    "matchedPaperCorpusId": "1691371"
                },
                {
                    "start": 810,
                    "end": 830,
                    "matchedPaperCorpusId": "4663616"
                },
                {
                    "start": 830,
                    "end": 851,
                    "matchedPaperCorpusId": "3132645"
                },
                {
                    "start": 851,
                    "end": 872,
                    "matchedPaperCorpusId": "2616234"
                },
                {
                    "start": 1416,
                    "end": 1436,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1523,
                    "end": 1544,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1544,
                    "end": 1562,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1579,
                    "end": 1601,
                    "matchedPaperCorpusId": "211205183"
                },
                {
                    "start": 1648,
                    "end": 1669,
                    "matchedPaperCorpusId": "209515274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80126953125
        },
        {
            "corpus_id": "244729397",
            "title": "AI and the Everything in the Whole Wide World Benchmark",
            "text": "GLUE and SuperGLUE benchmarks combine linguistic competence (the ability to model a linguistic system) with general commonsense and world reasoning as if they were equivalently scoped problems. In reference to a diagnostic component of the benchmark, authors state that \"this dataset is designed to highlight common challenges, such as the use of world knowledge and logical operators, that we expect models must handle to robustly solve the tasks\" [Wang et al., 2019a, p.1]. Thus a benchmark designed to test for generalizability across different language understanding tasks comes to subsume not only the task of building up linguistic competence (e.g. logical operators) in the language in question (English) but also the ability to acquire and deploy world knowledge. It is well established that natural language understanding requires both linguistic processing and reasoning over the combination of the linguistic signal, communicative common ground, and world knowledge [Hunter et al., 2018], but while linguistic knowledge is relatively self-contained and reusable across different textual domains, world knowledge is open-ended. Conflating these two abilities in a benchmark that is easily (mis)interpreted as representing a much more general, flexible, and robust set of capabilities than it possesses, and thus inappropriately presented as comparable to the \"human ability to understand language\" [Wang et al., 2019a]. \n\nFurthermore, language understanding relies not only on linguistic competence but also world knowledge, commonsense reasoning, and the ability to model the interlocutor's state of mind [Reddy, 1979, Clark, 1996], none of which can be thoroughly tested through text-only tasks, such as GLUE.Several researchers have raised the need to establish effective physical and social grounding as part of the process of moving towards robust and effective natural language understanding, warning against text-only learning as a limited approach [Bisk et al., 2020, Zellers et al., 2021]. Bender and Koller [2020] additionally mention the tendency of machine learning researchers to misinterpret certain benchmarks as capturing the model's ability to decipher meaning in language, arguing that benchmarks need to be constructed with care if they are to show evidence of \"understanding\" as opposed to merely the ability to manipulate linguistic form sufficiently to pass the test.",
            "score": 0.395265109328253,
            "section_title": "Critical Misunderstandings of Domain Knowledge and Application Problem Space",
            "char_start_offset": 25672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1429
                },
                {
                    "start": 1432,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2399
                }
            ],
            "ref_mentions": [
                {
                    "start": 1408,
                    "end": 1428,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1616,
                    "end": 1628,
                    "matchedPaperCorpusId": "142857922"
                },
                {
                    "start": 1966,
                    "end": 1984,
                    "matchedPaperCorpusId": "216035815"
                },
                {
                    "start": 1984,
                    "end": 2007,
                    "matchedPaperCorpusId": "235097641"
                },
                {
                    "start": 2009,
                    "end": 2033,
                    "matchedPaperCorpusId": "211029226"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59912109375
        },
        {
            "corpus_id": "267412707",
            "title": "Integration of cognitive tasks into artificial general intelligence test for large models",
            "text": "LLMs have demonstrated remarkable proficiency in accomplishing both \"pretext tasks\" and \"downstream tasks\".In the pretext task, LLMs such as GPT 1,14 and BERT 36 can learn language representations from large-scale texts without the need for manual annotation through self-supervised learning methods.The foundational language knowledge acquired by LLMs demonstrated zero-shot generalization capabilities.This facilitates their broad applicability across various downstream tasks, 37 such as understanding tasks, [38][39][40] generation tasks, 41,42 and reasoning tasks, 43,44 as shown in Table 1.\n\nFurthermore, LLMs have extended their language ability to encompass cognitive capabilities, such as few-shot learning, 14 in-context learning, 45 problem solving. 46,479][50][51][52] Interestingly, the emergence of \"advanced intelligence\" is not a result of deliberate training on specific tasks but rather a natural consequence of the pre-training process using extensive amounts of textual data.\n\nInitiating training for LLMs from scratch requires significant time and computational resources.To circumvent potentially expensive and ineffective training, it is conventional to periodically evaluate the capabilities of LLMs during the training process, thereby enabling timely adaptations of training strategies.However, relying solely on language tasks fails to provide a comprehensive evaluation of the capabilities of LLMs.A lower loss in language tasks does not necessarily indicate a higher level of intelligence.There is a need to bridge the gap between language tasks testing and general intelligence evaluation, transitioning from language tests to cognitive tests, and ultimately to AGI test, as shown in  provide valuable insights into comprehending and evaluating the cognitive abilities of LLMs.For instance, a commonly employed approach in the assessment of \"working memory\" is the n-back task.\n\nThis task is like a memory game where participants judge whether a new stimulus matches one from the previous n stimuli. 53Certain cognitive tasks that were traditionally employed to assess human or animal cognition are now being utilized to evaluate the cognitive capabilities of LLMs. 54,55Some pioneering work reports that LLMs have demonstrated human-like performance. 54,56,57For instance, Theory of mind (ToM) has been applied to assess large models, revealing that GPT-4 exhibits ToM capabilities similar to human inference patterns. 48,56,58",
            "score": 0.39516564999146936,
            "section_title": "From language tests to cognitive tests",
            "char_start_offset": 5766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 596
                },
                {
                    "start": 598,
                    "end": 766
                },
                {
                    "start": 766,
                    "end": 995
                },
                {
                    "start": 997,
                    "end": 1093
                },
                {
                    "start": 1093,
                    "end": 1312
                },
                {
                    "start": 1312,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1518
                },
                {
                    "start": 1518,
                    "end": 1807
                },
                {
                    "start": 1807,
                    "end": 1907
                },
                {
                    "start": 1909,
                    "end": 2032
                },
                {
                    "start": 2032,
                    "end": 2201
                },
                {
                    "start": 2201,
                    "end": 2290
                },
                {
                    "start": 2290,
                    "end": 2458
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 149,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 159,
                    "end": 161,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 480,
                    "end": 482,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 516,
                    "end": 520,
                    "matchedPaperCorpusId": "6203757"
                },
                {
                    "start": 520,
                    "end": 524,
                    "matchedPaperCorpusId": "8310135"
                },
                {
                    "start": 543,
                    "end": 546,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 546,
                    "end": 548,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 570,
                    "end": 573,
                    "matchedPaperCorpusId": "19240019"
                },
                {
                    "start": 573,
                    "end": 575,
                    "matchedPaperCorpusId": "85504763"
                },
                {
                    "start": 717,
                    "end": 719,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 761,
                    "end": 764,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 764,
                    "end": 766,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 768,
                    "end": 772,
                    "matchedPaperCorpusId": "252668917"
                },
                {
                    "start": 772,
                    "end": 776,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 2030,
                    "end": 2032,
                    "matchedPaperCorpusId": "14432275"
                },
                {
                    "start": 2199,
                    "end": 2201,
                    "matchedPaperCorpusId": "257257414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54150390625
        },
        {
            "corpus_id": "256863867",
            "title": "Moving away from lexicalism in psycho- and neuro-linguistics",
            "text": "The rest of the paper is composed of two main sections. In the first, we discuss how lexicalist assumptions are implemented in the language production literature, especially as they relate to the \"lemma\" representation, and how the models operate over those representations. We also elaborate on the kinds of data that these models struggle to account for, given their lexicalist assumptions. The second section discusses what an alternative might look like, as a non-lexicalist model of language production. To move away from lexicalism in models of language production, it is not enough to simply update the syntactic representations; it is also necessary to reconsider the algorithms involved in language production, because they are constrained by the lexicalist representations that they operate over. Instead of relying on a lemma representation, a non-lexicalist production model can represent stored linguistic knowledge as separate mappings between meaning and syntax, and syntax and form, such that meaning, syntax, and form may not line up with each other in a 1-to-1-to-1 fashion. Such a model can also account for prosodic computations that depend on meaning, syntax, and form information. Furthermore, we suggest that cognitive control mechanisms play an important role in resolving competition between the multiple information sources that influence the linearization of speech. \n\nAs we illustrate, non-lexicalist production models generate distinct predictions for aphasia and other acquired language disorders. By moving away from lexicalist assumptions, this kind of model provides better cross-linguistic coverage and aligns better with contemporary work in syntactic theory which has observed that syntactic and morphological processes cannot be distinct, that there are no good criteria to empirically define wordhood (Haspelmath, 2017), and that representations of meaning and form do not always align. However, it is important to recognize that the experimental literature in the lemma tradition has played a crucial role in psycho-and neuro-linguistics through its recognition of abstract syntactic representations independent of meaning and form. We are in complete sympathy with those models on this point, and we preserve this insight in the non-lexicalist architecture we propose here.",
            "score": 0.3950446902935313,
            "section_title": ". Introduction",
            "char_start_offset": 6059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2171
                },
                {
                    "start": 2172,
                    "end": 2313
                }
            ],
            "ref_mentions": [
                {
                    "start": 1839,
                    "end": 1857,
                    "matchedPaperCorpusId": "62789916"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402587890625
        },
        {
            "corpus_id": "203093488",
            "title": "Predicting (variability of) context effects in language comprehension",
            "text": "When distinct or similar cues contribute to the same comprehension process, they should affect comprehension in a similar manner; by contrast, if they inform distinct comprehension processes, their effects will show up as distinct (''Differentiating grounding: do different world-language relations elicit distinct context effects?''). \n\nWhile language comprehension in visual context can clearly vary as a function of comprehender characteristics, the reviewed accounts do not explicitly integrate speaker and comprehender characteristics as a modulatory factor in language comprehension. Moreover, they are underspecified regarding how grounding and context effects influence comprehension. Much psycho-and neurolinguistic research has, however, assessed to what extent language comprehension depends on the perceived context, the body, and longterm linguistic knowledge of the comprehender (e.g., Barsalou 2008;Hasson et al. 2018;Holcomb et al. 1992;Kotz 2009;Osterhout et al. 2008 This situation-based approach would predict strong context effects in language comprehension for referential and instruction situations, with diminishing context effects when the referential context and the communicative situation are less overlapping. The approach would not be able to make finer-grained predictions such as whether within one such situation, referential relations influence language comprehension more than other world-language relations. Myachykov et al. (2014) present a framework of embodied representations and distinguish between invariant aspects of the world such as gravity (p. 446) from somewhat stable embodied aspects (e.g. of an agent's state such as frontal vision, p. 446) to less stable aspects such as an agent's interpretation of a specific environment given her goals. Myachykov et al. (2014) view these aspects as hierarchical, and differentially susceptible to learning. The framework can make predictions about context effects when comparing the relative influence of physical aspects of the world compared with aspects of the immediate context or an agent's body. To accommodate distinct language-world relations, we would, in addition, need the context part of the framework to be specified further.",
            "score": 0.39497175585591926,
            "section_title": "P3",
            "char_start_offset": 48888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 338,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 900,
                    "end": 914,
                    "matchedPaperCorpusId": "16842691"
                },
                {
                    "start": 914,
                    "end": 933,
                    "matchedPaperCorpusId": "49907851"
                },
                {
                    "start": 933,
                    "end": 953,
                    "matchedPaperCorpusId": "3135694"
                },
                {
                    "start": 953,
                    "end": 963,
                    "matchedPaperCorpusId": "29196950"
                },
                {
                    "start": 963,
                    "end": 984,
                    "matchedPaperCorpusId": "270116117"
                },
                {
                    "start": 1443,
                    "end": 1466,
                    "matchedPaperCorpusId": "738185"
                },
                {
                    "start": 1791,
                    "end": 1814,
                    "matchedPaperCorpusId": "738185"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33544921875
        },
        {
            "corpus_id": "233407633",
            "title": "Morph Call: Probing Morphosyntactic Content of Multilingual Transformers",
            "text": "In the last few years, transformer language models (Vaswani et al., 2017) have accelerated the growth in the field of NLP. The models have established new state-of-the-art results in multiple languages and even demonstrated superiority in NLU benchmarks compared to human solvers (Raffel et al., 2020;Xue et al., 2020;He et al., 2020). Their distilled versions, or so-called student models, have shown competitive performance on many NLP tasks while having fewer parameters (Tsai et al., 2019). However, many questions remain on how these models work and what they know about language. The previous research focuses on what knowledge has been learned during and after pre-training phases (Chiang et al., 2020;Rogers et al., 2020a), and how it is affected by fine-tuning (Gauthier and Levy, 2019;Peters et al., 2019;Miaschi et al., 2020;Merchant et al., 2020). Besides, a wide variety of language phenomena has been investigated including syntax (Hewitt and Manning, 2019a;Liu et al., 2019a), world knowledge (Petroni et al., 2019;Jiang et al., 2020), reasoning (van Aken et al., 2019), common sense understanding (Zhou et al., 2020;Klein and Nabi, 2019), and semantics (Ettinger, 2020). \n\nMost of these studies involve probing which measures how well linguistic knowledge can be inferred from the intermediate representations of the model. The methods range from individual neuron analysis (Dalvi et al., 2020;Durrani et al., 2020a), examination of attention mechanisms (Kovaleva et al., 2019;Vig and Belinkov, 2019), correlationbased similarity measures (Wu et al., 2020), to probing tasks accompanied by linguistic supervision (Adi et al., 2016;Conneau et al., 2018). \n\nDespite growing interest in interpreting the models, morphology has remained understudied, specifically for languages other than English.",
            "score": 0.3948387950248601,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1669
                },
                {
                    "start": 1672,
                    "end": 1809
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 301,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 770,
                    "end": 795,
                    "matchedPaperCorpusId": "202769809"
                },
                {
                    "start": 795,
                    "end": 815,
                    "matchedPaperCorpusId": "76666127"
                },
                {
                    "start": 945,
                    "end": 972,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 972,
                    "end": 990,
                    "matchedPaperCorpusId": "84841767"
                },
                {
                    "start": 1008,
                    "end": 1030,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1061,
                    "end": 1084,
                    "matchedPaperCorpusId": "202558795"
                },
                {
                    "start": 1132,
                    "end": 1153,
                    "matchedPaperCorpusId": "173188167"
                },
                {
                    "start": 1169,
                    "end": 1185,
                    "matchedPaperCorpusId": "199001173"
                },
                {
                    "start": 1470,
                    "end": 1493,
                    "matchedPaperCorpusId": "201645145"
                },
                {
                    "start": 1493,
                    "end": 1516,
                    "matchedPaperCorpusId": "184486755"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64453125
        },
        {
            "corpus_id": "25422730",
            "title": "Towards Linguistically Generalizable NLP Systems: A Workshop and Shared Task",
            "text": "This state of affairs has not gone unnoticed. On the one hand, there is work on creating targeted evaluation datasets that exhibit and are annotated for particular linguistic phenomena, in order to facilitate fine-grained analysis of the linguistic capacities of systems for tasks such as parsing, entailment, and semantic relatedness (Rimell et al., 2009;Bender et al., 2011;Marelli et al., 2014). Additionally, there is an increasing amount of work on developing methods of exposing exactly what linguistic knowledge NLP models develop (K\u00e1d\u00e1r et al., 2016;Li et al., 2015) and what linguistic information is encoded in models' produced representations (Adi et al., 2016;Ettinger et al., 2016). Our aim in organizing this work-1 During a panel at the 1st Workshop on Representation Learning for NLP (ACL 2016; https://sites. google.com/site/repl4nlp2016/) some panelists acknowledged the fact that they could probably break any NLP system with very little effort-meaning it shouldn't be hard to invent reasonable examples that would confuse the systems. \n\nshop was to build on this foundation, designing the shared task to generate data specifically created to identify the boundaries of systems' linguistic capacities, and welcoming further related research contributions to stimulate additional discussion. \n\n3 Shared Task: Build It Break It, The Language Edition \n\nTo address the issues identified above, we developed a shared task inspired by the Build It Break It Fix It Contest2 and adapted for application to NLP. The shared task proceeded in three phases: a building phase, a breaking phase, and a scoring phase: \n\n1. In the first phase, \"builders\" take a designated NLP task and develop techniques to solve it. \n\n2. In the second phase, \"breakers\", having seen the output of the builders' systems on some development data, are tasked with constructing minimal-pair test cases intended to identify the boundaries of the systems' capabilities. \n\n3. In the third phase, builders run their systems on the newly created minimal pair test set and provide their predictions for scoring. \n\nBuilders are scored based how well their systems can withstand the attacks of breakers, and breakers are scored based on how well they can identify system boundaries.",
            "score": 0.39483743074051175,
            "section_title": "Motivation: Robust NLP Systems",
            "char_start_offset": 4185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1309
                },
                {
                    "start": 1312,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2089
                },
                {
                    "start": 2092,
                    "end": 2258
                }
            ],
            "ref_mentions": [
                {
                    "start": 335,
                    "end": 356,
                    "matchedPaperCorpusId": "9357023"
                },
                {
                    "start": 356,
                    "end": 376,
                    "matchedPaperCorpusId": "17352617"
                },
                {
                    "start": 376,
                    "end": 397,
                    "matchedPaperCorpusId": "762228"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6025390625
        },
        {
            "corpus_id": "261048819",
            "title": "Analyzing Transformer Dynamics as Movement through Embedding Space",
            "text": "Transformers (Vaswani et al., 2017) which started as text sequence modelers (Devlin et al., 2019;Raffel et al., 2020) generalize to unseen tasks via zero and few shot learning (Brown et al., 2020). Transformer based instruction tuned (Ouyang et al., 2022) large language models (LLMs) exhibit intelligent abilities enabling them to be used as general purpose intelligence machines (OpenAI, 2023). The zero and few shot learning abilities also extend across modalities (Reed et al., 2022;Driess et al., 2023;Girdhar et al., 2023). Further, prompting techniques such as chain-of-thought (CoT) (Wei et al., 2022b;Kojima et al., 2023) have been developed on top of LLMs and now freeform conversations are possible. Very impressive intelligent behaviors have been demonstrated e.g., instruction based source code (Chen et al., 2021) and image generation (Ramesh et al., 2021) plus a variety of capabilities deemed as the beginnings of AGI (Bubeck et al., 2023;Wei et al., 2022a). These abilities make it appear as if Transformers can 1) understand and follow instructions, 2) think, plan, reason and explain themselves, 3) even possess a Theory of Mind (Kosinski, 2023) and 4) comprehend multiple modalities at the same time. And most recently, LLMs are being trained to act as autonomous agents, appearing to observe, plan, reason, reflect, use external tools & APIs and act (Schick et al., 2023;Yao et al., 2023;Karpas et al., 2022;Park et al., 2023) even in a multimodal space (Liang et al., 2023;Lu et al., 2023). However, as Brown et al. note: understanding precisely how few-shot [and zero-shot] learning works is an important unexplored direction for future research. In this work we attempt to answer this question and explain how intelligent behaviours arise from the underlying dynamics.",
            "score": 0.3946961894953656,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1792
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 196,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.455078125
        },
        {
            "corpus_id": "254960044",
            "title": "From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation",
            "text": "Interpretation studies demonstrate that language model representations encode rich knowledge about language and the world (Voita, Sennrich,  and Rumshisky 2020). The intense exploration of language models' inherent knowledge has been motivated by their impressive performance in NLU tasks. Interpretation studies attempt to understand what this high performance is due to, by deciphering the knowledge that is encoded inside the representations. The bulk of this interpretation work relies on probing tasks which serve to predict linguistic properties from the representations that are generated by vanilla models, before integration of any external knowledge. Success in these tasks indicates that the model's representations encode the addressed linguistic knowledge.\n\nEarly probing studies explored surface linguistic phenomena pertaining to grammar and syntax, which are directly accessible in contextualized (token-level) representations (Linzen, Dupoux, and Goldberg 2016; Hewitt and Manning 2019; Hewitt and Liang 2019). The first studies addressing semantic knowledge explored phenomena in the syntax-semantics interface such as semantic role labeling and coreference Kovaleva et al. 2019), and the symbolic reasoning potential of LM representations (Talmor et al. 2020). Lexical polysemy is more challenging to study using token-level representations since it is encoded at a higher level of abstraction than individual instances, that of word types. Representations extracted from pools of sentences allow to abstract away from individual context variation. They are more informative about words' semantic properties (Vuli\u0107 et al. 2020b), and can serve to model abstract semantic notions (e.g., intensity) (Gar\u00ed Soler and Apidianaki 2020a, 2021b). Semantic relationships like hypernymy and entailment are also usually encoded at the word type-level (e.g., cat |= animal, tulip |= flower), although they are context-dependent in the case of polysemous words. 31 We hereby present methods that have been proposed for exploring the semantic information that is encoded in contextualized representations. These include visualization and probing tasks, as well as studies that investigate the semantic properties of the words in the constructed space by relying on its geometry.",
            "score": 0.39461806025689994,
            "section_title": "Motivation",
            "char_start_offset": 69917,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1176,
                    "end": 1197,
                    "matchedPaperCorpusId": "201645145"
                },
                {
                    "start": 1627,
                    "end": 1647,
                    "matchedPaperCorpusId": "222290596"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67822265625
        },
        {
            "corpus_id": "199001173",
            "title": "What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models",
            "text": "Pre-training of NLP models with a language modeling objective has recently gained popularity as a precursor to task-specific fine-tuning. Pretrained models like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018a) have advanced the state of the art in a wide variety of tasks, suggesting that these models acquire valuable, generalizable linguistic competence during the pretraining process. However, though we have established the benefits of language model pre-training, we have yet to understand what exactly about language these models learn during that process. \n\nThis paper aims to improve our understanding of what language models (LMs) know about language, by introducing a set of diagnostics targeting a range of linguistic capacities, drawn from human psycholinguistic experiments. Because of their origin in psycholinguistics, these diagnostics have two distinct advantages: they are carefully controlled to ask targeted questions about linguistic capabilities, and they are designed to ask these questions by examining word predictions in context, which allows us to study LMs without any need for task-specific fine-tuning. \n\nBeyond these advantages, our diagnostics distinguish themselves from existing tests for LMs in two primary ways. First, these tests have been chosen specifically for their capacity to reveal insensitivities in predictive models, as evidenced by patterns that they elicit in human brain responses. Second, each of these tests targets a set of linguistic capacities that extend beyond the primarily syntactic focus seen in existing LM diagnosticswe have tests targeting commonsense/pragmatic inference, semantic roles and event knowledge, category membership, and negation. Each of our diagnostics is set up to support tests of both word prediction accuracy and sensitivity to distinctions between good and bad context completions. Although we focus on the BERT model here as an illustrative case study, these diagnostics are applicable for testing of any language model. This paper makes two main contributions. First, we introduce a new set of targeted diagnostics for assessing linguistic capacities in language models. 1 Second, we apply these tests to shed light on strengths and weaknesses of the popular BERT model.",
            "score": 0.39450226305380726,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2265
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 187,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 197,
                    "end": 219,
                    "matchedPaperCorpusId": "3626819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85791015625
        },
        {
            "corpus_id": "260681211",
            "title": "Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023)",
            "text": "Nonetheless, the fact remains that young adults who pass the LSAT are doing so without having read the trillions of sentences and structured internet data that these large language models are trained on. The difference in training regimes is stark and highlights the fundamental question: how do humans come to pass the LSAT, and other standardized tests, on comparatively so little data? Parallel questions arise in the domain of language acquisition by children, which is characterized by a relative paucity of linguistic experience. Children are exposed to at most about ten million tokens per year (Hart and Risley, 1992;Gilkerson et al., 2017), and most children have vocabularies of under one thousand words around age three, regardless of the language being learned (Bornstein et al., 2004;Fenson et al., 1994). Yet these same children produce sentences that largely obey the grammatical rules of their communities' languages (Berko, 1958;Brown, 1973;Montrul, 2004;Yang, 2006;Phillips, 2010;Slobin, 2022). Thus, a fundamental question of linguistic study is how children become fluent in their native language(s) at a young age from so little data and experience. \n\nThe disconnect between the linguistic experience (input) and the linguistic capacity (output) is what gives rise to the The Poverty of the Stimulus argument for the hypothesis that many aspects of language learning and representation are innate (Chomsky, 1959(Chomsky, , 1980;;Nowak et al., 2001;Yang, 2013). Under this hypothesis, children generalize from their limited input in specific ways, navigating a constrained space of possible natural language grammars. Consequently, they do not consider all logically possible generalizations that are consistent with their linguistic experience. Rather, the particular structure of the hypothesis space facilitates the rapid development of their linguistic capabilities. The Poverty of the Stimulus is not tied to a specific theory of language, such as Minimalism or particular variants thereof, but rather follows from the basic problem of making generalizations from experience, as we describe in the next section. \n\nThe contrast between the input to the child and the input to LLMs is striking, but Piantadosi is not concerned by this discrepancy. He makes two claims.",
            "score": 0.3941288046922131,
            "section_title": "Introduction",
            "char_start_offset": 2104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2136
                },
                {
                    "start": 2139,
                    "end": 2270
                },
                {
                    "start": 2271,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 602,
                    "end": 625,
                    "matchedPaperCorpusId": "143528292"
                },
                {
                    "start": 625,
                    "end": 648,
                    "matchedPaperCorpusId": "4447742"
                },
                {
                    "start": 773,
                    "end": 797,
                    "matchedPaperCorpusId": "31573227"
                },
                {
                    "start": 983,
                    "end": 998,
                    "matchedPaperCorpusId": "16305694"
                },
                {
                    "start": 1450,
                    "end": 1469,
                    "matchedPaperCorpusId": "11049891"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1473388671875
        },
        {
            "corpus_id": "274131140",
            "title": "On the Compatibility of Generative AI and Generative Linguistics",
            "text": "One of the central goals of theoretical linguistics is to develop a theory of language which explains how we both produce and understand language, while also explaining how it is acquired by a learner. In other words, theoretical linguistics strive to develop an algorithmic explanation for how we learn and eventually parse and generate language. A central figure whose work helped define this goal is Noam Chomsky. Many In generative linguistics, languages are represented via generative models often composed of rewrite rules which can be combined to derive grammatical sentences, a simple example of such a model in the form of a phrase-structure grammar is given in Figure 1. A grammar should only be able to generate grammatical sentences, while ungrammatical ones should not be derivable. However, there are infinitely many possible grammatical sentences in a language making the elaboration and evaluation of grammars a complex task. As an approximation we use observed sentences representing a sample of a language to help us elaborate a hypothesized generative model. 2 A generative linguistic theory then explains the process by which, given such a corpus, we should arrive at a generative grammar for the sampled language. In Figure 1, our generative theory may specify what the latent categories are, a process by which they can be assigned to words or composed with other categories. This generative theory also constitutes an algorithmic explanation for how we learn to eventually process language. \n\nThe generative theories envisioned by Chomsky and others were a priori about human's knowledge of language and how they acquire it. In the age of artificial intelligence (AI) and large language models (LLMs), new \"language learners\" have emerged. These models on the surface exhibit impressive linguistic capabilities (Srivastava et al., 2023;Mahowald et al., 2024).",
            "score": 0.3938400506006809,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1882
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5263671875
        },
        {
            "corpus_id": "270688478",
            "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
            "text": "Here, we illustrate the training processes of LLMs, which consist of at least three main steps: pretraining, which is done on the task of next token prediction, instruction fine-tuning, and preference alignment.\n\nPre-training.Pre-training for large language models (Radford et al., 2018) involves an extensive unsupervised learning phase, where the model is exposed to a large corpus of text data to learn the underlying statistical properties of natural language.This process employs the next token prediction task, where the model is conditioned on a sequence of tokens w 1:i\u22121 and trained to predict the subsequent token w i , i.e., p(y) = n i p \u03b8 (w i |w 1:i\u22121 ).By iteratively processing vast and diverse textual datasets, the model learns linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies.This process delivers what is commonly called a \"base\" model.This first model is usually improved with further specialized training, as we will see now.\n\nInstruction Fine-Tuning.Instruction fine-tuning is the part of the training process of large language models aimed at enhancing their ability to follow specific directives and perform specialized tasks (Taori et al., 2023).This phase involves further training of the base model on curated datasets that include explicit instructions paired with corresponding responses.The objective is to teach the model how to interpret and execute various types of commands or queries effectively.During instruction fine-tuning, the model is exposed to a wide range of examples that illustrate how to respond to different types of prompts, from answering questions to generating summaries and more complex task-oriented interactions.The primary purpose of instruction fine-tuning is to improve the model's usability and performance in real-world applications.By aligning the model's behavior with the specific needs and expectations of users, instruction finetuning ensures that the model can handle diverse and nuanced tasks more effectively, enhancing its overall functionality and practical utility.This pro-cess might involve using an instruction template, where specific patterns and structures for providing instructions and expected responses are standardized to facilitate consistent and effective learning.It is worth noticing that this is still achieved with traditional supervised learning, unlike the next phase we are considering.",
            "score": 0.3936219084400523,
            "section_title": "LLM Training",
            "char_start_offset": 6387,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 213,
                    "end": 226
                },
                {
                    "start": 226,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 667
                },
                {
                    "start": 667,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 911
                },
                {
                    "start": 911,
                    "end": 1002
                },
                {
                    "start": 1004,
                    "end": 1028
                },
                {
                    "start": 1028,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1373
                },
                {
                    "start": 1373,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1849
                },
                {
                    "start": 1849,
                    "end": 2092
                },
                {
                    "start": 2092,
                    "end": 2305
                },
                {
                    "start": 2305,
                    "end": 2433
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8427734375
        },
        {
            "corpus_id": "269033472",
            "title": "A Mathematical Theory for Learning Semantic Languages by Abstract Learners",
            "text": "In the recent era of natural language processing (NLP), the evolution of large language models (LLMs), such as GPT-4 [1] and Gemini [2], has greatly impacted people's daily lives.There is a growing consensus that enhancing language model performance and sample efficiency across a broad spectrum of downstream NLP tasks is closely linked to scaling up these models.This scaling involves increasing both the size of the training data and the number of model parameters, as discussed in [3], [4].The relationship between the scale of an LLM and its performance can often be quantitatively forecasted using scaling laws, as explored in [5], [6].\n\nRecent studies [1], [7], [8] further demonstrates, through various numerical examples, that LLMs manifest emergent capabilities absent in their smaller counterparts.There is a notable enhancement in system performance once a certain critical scale threshold is surpassed, exhibiting a phase transition behavior often observed in network science [9].However, as pointed out in [7], the exact mechanisms behind these emergent abilities in large language models are still not fully understood and remain a topic of active research.\n\nIn the study presented by [10], a simple explanation is offered for the phase transition phenomenon observed in LLMs based on the concept of list decoders.The approach in [10] models an LLM as a sequence-to-sequence random function over a certain token space with M possible tokens.It was shown that the expected number of erroneous sequences in the list decoder can be bounded by a constant if M \u03f5 \u0103 1, where \u03f5 is the false alarm probability.Since transformer-based LLMs with more parameters and extensive training can memorize more patterns [11], [12], they are more likely to reduce the false alarm probability \u03f5 below the percolation threshold of 1{M .However, the list decoder approach does not explain how the capabilities of language skills emerge from scaling up the size of the training data.In [13], a novel random graph approach was proposed to explain the scaling law discussed in [6].A language is modeled by a random bipartite graph with skills on one side and texts (originally termed text-pieces in [13]) on the other side.",
            "score": 0.39354884284380776,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 179,
                    "end": 365
                },
                {
                    "start": 365,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 642
                },
                {
                    "start": 644,
                    "end": 809
                },
                {
                    "start": 809,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1172
                },
                {
                    "start": 1174,
                    "end": 1329
                },
                {
                    "start": 1329,
                    "end": 1456
                },
                {
                    "start": 1456,
                    "end": 1617
                },
                {
                    "start": 1617,
                    "end": 1830
                },
                {
                    "start": 1830,
                    "end": 1975
                },
                {
                    "start": 1975,
                    "end": 2071
                },
                {
                    "start": 2071,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 490,
                    "end": 493,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1717,
                    "end": 1721,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70361328125
        },
        {
            "corpus_id": "198192195",
            "title": "A Single Paradigm for Implicit and Statistical Learning",
            "text": "In the second experiment, we investigated in greater depth the point at which participants derived explicit knowledge of the syntactic structure of the language and their use of this knowledge in order to guide their acquisition of learning the word-referent mappings in the language. Experiment 1 tested how explicit knowledge of structure could affect learning, and whether awareness, as measured by retrospective verbal reports, affected performance during the acquisition of the language structure. In Experiment 2, we determined more precisely when explicit knowledge of structure emerged during training, and how this affected learning. In Experiment 2, we asked participants to report, on a trial by trial basis, what each classification decision was based on, with response options ranging from implicit to explicit sources of knowledge. Determining whether explicit knowledge preceded or followed an enhanced ability to acquire the language addresses the relation between vocabulary and syntax acquisition, and, more broadly, the relation between implicit and statistical learning.",
            "score": 0.39277589232270804,
            "section_title": "Introduction",
            "char_start_offset": 7049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1090
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7216796875
        },
        {
            "corpus_id": "271329091",
            "title": "Thought-Like-Pro: Enhancing Reasoning of Large Language Models through Self-Driven Prolog-based Chain-of-Though",
            "text": "Recent advancements in large language models (LLMs) have positioned them as versatile tools for a variety of tasks, achieved through a structured training process that includes pre-training on large text corpora, supervised fine-tuning, and reinforcement learning from human feedback (RLHF) (Casper et al., 2023). These phases equip LLMs with extensive knowledge, the ability to generate context-specific responses, and alignment with human values. One of the key factors contributing to the perception of LLMs as a precursor to artificial general intelligence (AGI) is their scalable and explainable reasoning capabilities (Zhao et al., 2023), which have demonstrated potential to solve multiple types of real-world complex logic reasoning problems, including natural language logical reasoning and arithmetic reasoning. \n\nGeneral logic reasoning tasks continue to present substantial challenges for LLMs. These challenges primarily emerge during the LLMs' reasoning processes (Ribeiro et al., 2023;Zhou et al., 2024). At the semantic level, the generation of accurate multihop inferential pathways and the assurance of validity at each reasoning step remain significant obstacles (Creswell et al., 2023;Kazemi et al., 2023). LLMs function in a self-regressive manner at the token level, processing information sequentially from left to right, one token at a time. This method can amplify errors: an inaccurate token prediction may negatively impact all subsequent tokens, leading to escalating inaccuracies and false reasoning outcomes. Interestingly, some studies (Ye and Durrett, 2022;Saparov and He, 2023;Ribeiro et al., 2023) have observed instances where LLMs deliver correct answers despite erroneous internal reasoning processes. This phenomenon highlights a misalignment between the reasoning processes and outcomes of LLMs, suggesting a weak causal relationship and underdeveloped implicit reasoning connections within the LLMs. \n\nTo address the challenges associated with generating reasoning trajectories in LLMs, several strategies have been developed to enhance cognitive alignment and logical coherence.",
            "score": 0.39273711660877003,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1939
                },
                {
                    "start": 1942,
                    "end": 2119
                }
            ],
            "ref_mentions": [
                {
                    "start": 1182,
                    "end": 1205,
                    "matchedPaperCorpusId": "248887351"
                },
                {
                    "start": 1567,
                    "end": 1589,
                    "matchedPaperCorpusId": "252873674"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "253172266",
            "title": "Challenges for Methodological Designs and Proposed Alternative Instruction for Teaching English Grammar to Thai Deaf and Hard-of-Hearing Students",
            "text": "Definitions and scopes of grammatical competence vary among linguistics under different paradigms. Linguists under the innatism umbrella, such as Chomsky (1959( , as cited in Harmer, 2015)), view grammatical competence as innate linguistic ability, which is biologically programmed and known as Universal Grammar (UG). Chomsky believes it is innately equipped with some sort of Language Learning Device (LAD), facilitating the acquisition of a first language (L1). Grammatical competence can be manifested through observable actions (grammatical performance). Errors made by language users indicate problems in their grammatical competence. Therefore, grammatical performance reflects how competent language learners are. \n\nIn contrast to Chomsky's ideology in innatism, communication-oriented linguists such as Canale and Swain (1980) consider grammatical competence as an element of communicative competence. According to Canale and Swain's (1980) communicative competence framework, there are three distinct aspects accountable for communicative achievement: 1) grammatical competence (knowledge about vocabulary, morphemes, syntax, semantics, and phonology); 2) sociolinguistic competence (ability to use language in social interaction); and 3) strategic competence (applications of communicative strategies to successfully convey and receive messages). Later, Canale (1983) included discourse competence (skills for creating coherent/cohesive language for different situations). In conclusion, they believe that grammatical competence is the foundation of overall communication. \n\nAlternatively, Larsen-Freeman (2003) proposed the Three-Dimensional Grammar Model concept, stating that grammatical competence lies in the knowledge of grammatical form, meaning, and use. First, grammar form refers to phonological and morphosyntactic features. Second, grammar meaning indicates semantic competence. Lastly, grammar use reflects learners' pragmatic and discourse competences. These three aspects of grammatical competence interrelate with each other, creating a complex language system. This model is based on a Complexity Theory where social interactions and language uses in different situations are essential for language learners' development. It could be concluded that this definition of grammatical competence greatly refers to the complexity of language and discourse diversity.",
            "score": 0.3926642729257587,
            "section_title": "Definitions of Grammatical Competence",
            "char_start_offset": 945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1583
                },
                {
                    "start": 1586,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2249
                },
                {
                    "start": 2250,
                    "end": 2388
                }
            ],
            "ref_mentions": [
                {
                    "start": 812,
                    "end": 835,
                    "matchedPaperCorpusId": "197656439"
                },
                {
                    "start": 924,
                    "end": 949,
                    "matchedPaperCorpusId": "197656439"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.353759765625
        },
        {
            "corpus_id": "270560497",
            "title": "Grammaticality Representation in ChatGPT as Compared to Linguists and Laypeople",
            "text": "Chomsky, Roberts, and Watumull (2023) have been vocal about the inherent discrepancies between how LLMs and humans perceive and communicate.Yet, other scholars like Piantadosi (2023) hold a contrasting view, positioning LLMs as genuine reflections of human linguistic cognition.\n\nEmpirical studies have emerged as a crucial tool to answer this debate.Pioneering work by Binz and Schulz (2023) subjected GPT-3 to a battery of psychological tests, originally crafted to understand facets of human thought processes, ranging from decision-making matrices to reasoning pathways.The outcomes were intriguing, with GPT-3 not just mirroring but at times outperforming human benchmarks in specific scenarios.On a similar trajectory, Kosinski (2023) assessed the capacity of LLMs to understand and respond to false-belief scenarios, often utilized to gauge human empathy and comprehension.Here, the responses from ChatGPT echoed the patterns seen in school-going children, though subsequent research from Brunet-Gouet and colleagues ( 2023) voiced concerns about the consistency of such responses.Delving into ChatGPT's language processing abilities, Cai et al. (2023) subjected ChatGPT to a myriad of psycholinguistic experiments and showed an impressive alignment between the models and humans in language use in a majority of the tests, ranging from sounds, to syntax, all the way to dialogue.However, it's noteworthy that ChatGPT can diverge from humans in language use, for example, in word length preference for conveying lesser information (e.g., Mahowald et al., 2013).\n\nWhen examining LLM-human similarities, it's crucial to assess the extent to which ChatGPT's representations of linguistic knowledge align with those of humans.Contemporary linguistic theories often distinguish between the inherent mental systems that enable language comprehension and production, and the actual use of language-illustrated by distinctions like \"Langue vs. Parole\" from Saussure (1916) and \"Competence vs Performance\" by Chomsky (1965).Grammaticality judgement is a central method to assess linguistic representation competence.Chomsky (1986) highlighted that evidence for linguistic theorizing largely depends on \"the judgements of native speakers\".",
            "score": 0.3925151007179738,
            "section_title": "Introduction",
            "char_start_offset": 1936,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 140,
                    "end": 278
                },
                {
                    "start": 280,
                    "end": 351
                },
                {
                    "start": 351,
                    "end": 574
                },
                {
                    "start": 574,
                    "end": 700
                },
                {
                    "start": 700,
                    "end": 880
                },
                {
                    "start": 880,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1387
                },
                {
                    "start": 1387,
                    "end": 1568
                },
                {
                    "start": 1570,
                    "end": 1729
                },
                {
                    "start": 1729,
                    "end": 2022
                },
                {
                    "start": 2022,
                    "end": 2114
                },
                {
                    "start": 2114,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 182,
                    "matchedPaperCorpusId": "145006258"
                },
                {
                    "start": 370,
                    "end": 392,
                    "matchedPaperCorpusId": "250113371"
                },
                {
                    "start": 1545,
                    "end": 1567,
                    "matchedPaperCorpusId": "8877476"
                },
                {
                    "start": 2007,
                    "end": 2021,
                    "matchedPaperCorpusId": "238655569"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.324462890625
        },
        {
            "corpus_id": "246294995",
            "title": "Reasoning Like Program Executors",
            "text": "Table 1: The demonstration of five representative reasoning types. Listed are the types, the example questions, the representative dataset, and their corresponding tasks. \n\n[DOC] and [TAB] indicates the start of a passage and a semi-structured table respectively. Here we regard Question , Conclusion and Hypothesis as sentence, and Passage , Fact , Context and Premise as natural context in Figure 1. \n\nexplore whether symbolic reasoning can be internalized by language models and, especially, Can neural language models advance reasoning abilities by imitating symbolic reasoners? \n\nMotivated by this, we conceive a new pretraining paradigm, POET (Program Executor), to investigate the learnability of language models from symbolic reasoning and transferrability across distinct representation systems. As illustrated in Figure 1, with a program (e.g., SQL query) and its program context (e.g., database) as input, the model receives automatic supervision from an established program executor (e.g., MySQL) and learns to produce correct execution result. By imitating program execution procedures, we believe LMs could potentially learn the reasoning knowledge that humans adopted to create the associated program executor and tackle NL sentences with the learned reasoning capability. This reveals the key hypothesis of POET: program executors are crystallized knowledge of formal reasoning, and such knowledge can be grasped by language models and transferred to NL reasoning via pre-training. In other words, pre-training over natural language might be a contingent condition for LMs to have better reasoning capabilities over natural language. This contingency assumption of NL brings POET another great merit in data quality: while it is typically difficult to obtain large amounts of clean natural language sentences containing clear evidence of reasoning, synthesized programs can be made arbitrarily complicated but readily available on any scale, thanks to the artificial and compo-sitional nature of programming languages. These merits greatly facilitate the construction of highquality corpora, addressing most of the unresolved shortcomings in previous reasoning-enhancement pre-training. In other words, POET differs from existing pre-training paradigms relying on noisy NL data. In summary, our contribution is three-fold:",
            "score": 0.3920302102398403,
            "section_title": "Introduction",
            "char_start_offset": 1926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 170
                },
                {
                    "start": 173,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2294
                },
                {
                    "start": 2295,
                    "end": 2338
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "276766647",
            "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network",
            "text": "Large language models (LLMs) exhibit remarkable similarity to neural activity in the human language network. However, the key properties of language shaping brain-like representations, and their evolution during training as a function of different tasks remain unclear. We here benchmark 34 training checkpoints spanning 300B tokens across 8 different model sizes to analyze how brain alignment relates to linguistic competence. Specifically, we find that brain alignment tracks the development of formal linguistic competence -- i.e., knowledge of linguistic rules -- more closely than functional linguistic competence. While functional competence, which involves world knowledge and reasoning, continues to develop throughout training, its relationship with brain alignment is weaker, suggesting that the human language network primarily encodes formal linguistic structure rather than broader cognitive functions. We further show that model size is not a reliable predictor of brain alignment when controlling for feature size and find that the correlation between next-word prediction, behavioral alignment and brain alignment fades once models surpass human language proficiency. Finally, using the largest set of rigorous neural language benchmarks to date, we show that language brain alignment benchmarks remain unsaturated, highlighting opportunities for improving future models. Taken together, our findings suggest that the human language network is best modeled by formal, rather than functional, aspects of language.",
            "score": 0.39192157969005925,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82177734375
        },
        {
            "corpus_id": "15563116",
            "title": "Sources of variation in developmental language disorders: evidence from eye-tracking studies of sentence production",
            "text": "Speaking in sentences is a momentous developmental milestone that marks the beginning of a child's ability to communicate an infinite array of ideas, feelings and experiences, past and present, real or imagined, to other people. Intuitively, this seems an effortless development for most children, yet producing a fluent and meaningful utterance is a complex process. Models of skilled, adult sentence production outline at least three key processes: conceptualization, formulation and articulation [1]. Conceptualization is the stage at which speakers decide on the message to be conveyed. Formulation requires that speakers map that intended message onto word forms (lexicalization) and particular word orders (syntactic planning). Finally, the speaker must plan and make the motor movements necessary to articulate the message. The vast majority of children will begin to master this complex process in the preschool years. For those with neurodevelopmental disorders, however, language production may present lifelong challenges. Until recently though, we have known little about where in the processing chain language breaks down and how identifying points of difficulty may inform potential intervention strategies.\n\nTraditionally, psychologists and linguistics have had to rely on the product of this processing chain to infer the prerequisites needed at each stage for successful production [2]. What skilled speakers say, and importantly the timing of production elements and the errors that speakers make, have provided insights into processing mechanisms and informed computational models of how meaning is translated into the syntax of a particular language [3]. Such models have also made inroads into our understanding of how children become competent syntactic speakers. These models highlight the importance of experience-dependent and error-based learning mechanisms that are shaped by the innate (neural) architecture of the developing system [4]. For most young children, experience is socially mediated; children learn from their interactions with caregivers and very quickly can adapt their language production to their listening audience. intended outputs against expectations derived from linguistic input [4] and the social conventions shared by interlocutors [5].\n\nWhat speakers say, however, will only take us so far. Analysing speech output tells us relatively little about the process of conceptualization, for instance, how do people decide what to talk about and how do children learn to create utterances that are relevant to context and to listener need? How is the form of the",
            "score": 0.3917367742964523,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1399,
                    "end": 1402,
                    "matchedPaperCorpusId": "21975212"
                },
                {
                    "start": 1670,
                    "end": 1673,
                    "matchedPaperCorpusId": "1237448"
                },
                {
                    "start": 1961,
                    "end": 1964,
                    "matchedPaperCorpusId": "8713012"
                },
                {
                    "start": 2229,
                    "end": 2232,
                    "matchedPaperCorpusId": "8713012"
                },
                {
                    "start": 2284,
                    "end": 2287,
                    "matchedPaperCorpusId": "144643962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45263671875
        },
        {
            "corpus_id": "11002725",
            "title": "Heritage language and linguistic theory",
            "text": "Since its inception, the generative tradition within linguistic theory has concerned itself primarily with monolingual speakers in its quest for what we know when we know (a) language. The object of study, linguistic competence, or grammar, instantiates in and emerges from the brains of human speakers. Grammar cannot get loaded onto a microscope slide or set upon a scale; it gets accessed through its effects on naturally-developing speakers who employ the grammar in their native language du jour. Grammar informs and determines linguistic behavior; linguists study grammar by studying the behavior of speakers and making generalizations about the idealized state of mind of these speakers. But which speakers?\n\nThe investigation of grammar is necessarily a circuitous enterprise: we observe linguistic competence through linguistic performance, the situation-specific deployment of grammar. But extra-linguistic factors influence performance, so linguists help themselves to various domain restrictions in an attempt to limit noise in the translation from competence to performance. Chomsky (1965, p. 4) provides an early description of the obstacle to be overcome: \"The problem for the linguist, as well as for the child learning the language, is to determine from the data of performance the underlying system of rules that has been mastered by the speaker-hearer and that he puts to use in actual performance.\" Chomsky also provides an early characterization of one strategy for meeting this obstacle, focusing the linguist's attention on idealized, untainted language users: Linguistic theory is concerned primarily with an ideal speaker-listener, in a completely homogeneous speech-community, who knows its language perfectly and is unaffected by such grammatically irrelevant conditions as memory limitations, distractions, shifts of attention and interest, and errors (random or characteristic) in applying his knowledge of the language in actual performance. (Chomsky, 1965, p. 3)\n\nThe rapid ascension of formal linguistics over the intervening five decades has demonstrated the success of this focused approach to the study of language (for a similar line of discussion, see Lohndal, 2013). A great deal of progress has been made to move beyond \"grammars\" in the traditional sensecomprehensive descriptions of language-specific regularities and their exceptions-to grammar in the Chomskyan sense: the rules and processes that generate those regularities in the first place.\n\nStill, Chomsky's counsel necessarily exclude",
            "score": 0.3916245053626599,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 2189,
                    "end": 2203,
                    "matchedPaperCorpusId": "62251856"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.264404296875
        },
        {
            "corpus_id": "258381320",
            "title": "\u201cDoes this make sense?\u201d: The effect of matching guise in regional accent on grammatical acceptability judgments",
            "text": "To this point, because exemplar theory suggests that grammar is the product of experience, different exposures naturally lead to different grammars (Hay & Bresnan 2006). \n\nAlthough developed independent of each other, phonetic and syntactic exemplar theories share common features. Both theories find that linguistic knowledge results from storage of language experiences. Because it is built on experience, one's linguistic knowledge is highly individualized and continually growing and changing; there is also the potential to store additional detail as part of an exemplar, such as social information or semantic meaning. \n\nThe primary difference between the two is that phonetic exemplar models focus on classification while syntactic exemplar models focus on composition (Bod & Cochran 2007); by examining different aspects of language, however, there is a possibility of unifying the two, since they complement each other. Unification of these theories has been explored in language acquisition and use (see Bod & Cochran 2007 for more detail), as well as in research on the syntaxphonetics interface (Hay & Bresnan 2006;Squires 2013). \n\nStudies rooted in exemplar theory have varied in methodology used in examining the interaction of syntax and phonetics (see Hay & Bresnan 2006 for a corpus study; see Squires 2013 for an image-based forced-response task). The current study utilizes a previously underutilized methodology in speech perception: a syntactic acceptability survey. \n\n2.2. SYNTACTIC ACCEPTABILITY WITH AUDITORY STIMULI. Historically, acceptability judgment tasks have relied on text sentences; this practice has recently been scrutinized for its exclusionary features, such as requiring a standardized writing system (Sedarous & Namboodiripad 2020). The bleaching of stimulus presentation to solely written word -excluding presentation of a signing space or speech -also removes social information that may color linguistic processing. Since syntactic acceptability judgments are a tool to better understand structure, ignoring how these structures more commonly appear to language users -as signing or speech -has created a potential gap in the understanding of how language works. \n\nCombining an acceptability judgment task with matched guise, Remirez (2019) examined implications of exemplar theory presented by Sumner et al. (2014).",
            "score": 0.3911021588331828,
            "section_title": "Theoretical and methodological considerations.",
            "char_start_offset": 4574,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1487
                },
                {
                    "start": 1490,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2204
                },
                {
                    "start": 2207,
                    "end": 2358
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 168,
                    "matchedPaperCorpusId": "5538305"
                },
                {
                    "start": 776,
                    "end": 796,
                    "matchedPaperCorpusId": "10330042"
                },
                {
                    "start": 1014,
                    "end": 1032,
                    "matchedPaperCorpusId": "10330042"
                },
                {
                    "start": 1107,
                    "end": 1127,
                    "matchedPaperCorpusId": "5538305"
                },
                {
                    "start": 1127,
                    "end": 1140,
                    "matchedPaperCorpusId": "144957740"
                },
                {
                    "start": 1268,
                    "end": 1286,
                    "matchedPaperCorpusId": "5538305"
                },
                {
                    "start": 1739,
                    "end": 1770,
                    "matchedPaperCorpusId": "221592993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328369140625
        },
        {
            "corpus_id": "218487842",
            "title": "Emergence of Syntax Needs Minimal Supervision",
            "text": "Accordingly, probes were conceived to test if the model learned parts of speech (Saphra and Lopez, 2018), morphology (Belinkov et al., 2017;Peters et al., 2018a), or syntactic information. Tenney et al. (2019) evaluate contextualized word representations on syntactic and semantic sequence labeling tasks. Syntactic knowledge can be tested by extracting constituency trees from a network's hidden states (Peters et al., 2018b) or from its word representations (Hewitt and Manning, 2019). Other syntactic probe sets include the work of Conneau et al. (2018) and Marvin and Linzen (2018). \n\nDespite the vivid interest for the topic, no consensus seems to unfold from the experimental results. Two competing opinions emerge:  et al. (2018). \n\n\u2022 The language model training objective does not allow to learn compositional syntax from a corpus alone, no matter what amount of training data the model was exposed to. Syntax learning can only be achieved with taskspecific guidance, either as explicit supervision, or by restricting the hypothesis space to hierarchically structured models (Dyer et al., 2016;Marvin and Linzen, 2018;Chowdhury and Zamparelli, 2018;van Schijndel et al., 2019;Lake and Baroni, 2017). \n\nMoreover, some shortcomings of the above probing methods make it more difficult to come to a conclusion. Namely, it is not trivial to come up with minimal pairs of naturally occurring sentences that are equally likely. Furthermore, assigning a (slightly) higher probability to one sentence does not reflect the nature of knowledge behind a grammaticality judgment. Diagnostic classifiers may do well on a linguistic task because they learn to solve it, not because their input contains a hierarchical structure (Hewitt and Liang, 2019). In what follows, we present our assessment on how the difficulty of creating a linguistic probing data set is interconnected with the theoretical problem of learning a model of syntactic competence.",
            "score": 0.39109452689596674,
            "section_title": "Language models and syntax",
            "char_start_offset": 5770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1945
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 104,
                    "matchedPaperCorpusId": "53633715"
                },
                {
                    "start": 117,
                    "end": 140,
                    "matchedPaperCorpusId": "7100502"
                },
                {
                    "start": 140,
                    "end": 161,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 189,
                    "end": 209,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 404,
                    "end": 426,
                    "matchedPaperCorpusId": "52098907"
                },
                {
                    "start": 460,
                    "end": 486,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 535,
                    "end": 556,
                    "matchedPaperCorpusId": "24461982"
                },
                {
                    "start": 561,
                    "end": 585,
                    "matchedPaperCorpusId": "52113185"
                },
                {
                    "start": 1083,
                    "end": 1102,
                    "matchedPaperCorpusId": "1949831"
                },
                {
                    "start": 1102,
                    "end": 1126,
                    "matchedPaperCorpusId": "52113185"
                },
                {
                    "start": 1126,
                    "end": 1157,
                    "matchedPaperCorpusId": "52010508"
                },
                {
                    "start": 1157,
                    "end": 1184,
                    "matchedPaperCorpusId": "202540362"
                },
                {
                    "start": 1184,
                    "end": 1206,
                    "matchedPaperCorpusId": "46761158"
                },
                {
                    "start": 1721,
                    "end": 1745,
                    "matchedPaperCorpusId": "202538609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49951171875
        },
        {
            "corpus_id": "273811942",
            "title": "Can Language Models Learn to Skip Steps?",
            "text": "The pursuit of Artificial General Intelligence (AGI) is profoundly influenced and inspired by human intelligence [35,6]. Trained extensively on human language, language models not only excel in various tasks, but also begin to exhibit emergent human-like abilities that are not explicitly engineered into them [24]. Among these, reasoning stands out as a core human-like cognitive ability, and has demonstrated great potential in a wide range of problem solving scenarios [47,11,30,37,28,34]. Despite their advances in displaying human-like cognitive activities, huge gaps remain in how models and humans actually behave [22,46,20]. These differences bring up interesting questions regarding the exploration and development of similar capabilities between models and humans. \n\nWe aim to investigate whether the models exhibit any reasoning abilities unique to human experts, and whether they can evolve from beginners to reasoning experts. When humans learn to reason, beginners typically start with detailed, step-by-step solutions to imitate the gradual process of problem solving. As practice makes perfect, human experts not only solve problems more swiftly but also utilize shorter mental pathways, often skipping steps in their reasoning process [36]. This particular ability helps them speed up the reasoning and saves cognitive load for more challenging steps [44]. As demonstrated in Figure 1, the step-skipping behavior illustrated on the right side is commonly adopted by human experts during equation simplification. Step skipping in equation simplification. We use the specified number of steps in the input as a stimulation to induce the model to perform skipping by using fewer steps. \n\nIn this work, we are curious whether models exhibit mature human-like reasoning ability -skipping steps, and how such abilities can influence the model's reasoning behaviors. Unlike humans, models do not inherently possess the intrinsic motivation like time limit or skill maturity that naturally drives efficiency in cognitive tasks. To induce the skipping step behavior in models, we introduce a controlled training environment where models are instructed to generate reasoning sequences within a specified number of steps. Our method includes two phases: initialization and iteration. We begin with a dataset that contains complete stepwise reasoning processes for the questions. In initialization, models are first trained to solve the tasks comprehensively, adhering to the full sequence of reasoning steps.",
            "score": 0.3910887525770444,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1699
                },
                {
                    "start": 1702,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2227
                },
                {
                    "start": 2228,
                    "end": 2289
                },
                {
                    "start": 2290,
                    "end": 2384
                },
                {
                    "start": 2385,
                    "end": 2514
                }
            ],
            "ref_mentions": [
                {
                    "start": 485,
                    "end": 488,
                    "matchedPaperCorpusId": "264426101"
                },
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 1368,
                    "end": 1372,
                    "matchedPaperCorpusId": "55316636"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75927734375
        },
        {
            "corpus_id": "263831328",
            "title": "Transformers and Large Language Models for Chemistry and Drug Discovery",
            "text": "The rise of data-hungry machine learning algorithms, coupled with the increasing availability of data, has set a trend for scaling models to the maximum sizes that hardware constraints allow. As these models increase in size, they correspondingly improve in their capacity to perform the tasks they were originally trained for. This trend is particularly predictable in Their capabilities allow for a. general task solvers thanks to their flexibility and knowledge transferability [66,67] and b. agent architectures, capable of integrating virtually unlimited modalities, in the form of computational tools [68][69][70]. \n\nthe realm of language models, and it manifests in the form of what researchers refer to as scaling laws [71]. These scaling laws serve as a valuable tool for researchers, enabling them to identify performance trends and make accurate predictions about the capabilities of significantly larger models. However, the process of scaling does not simply enhance existing capabilities. As these models grow, they reach certain critical thresholds where not only their existing capabilities are enhanced, but entirely new capabilities are observed. \n\nThese new abilities are collectively referred to as emergent capabilities [72]. They represent a fascinating aspect of model scaling, as they are not present or even predictable in smaller models, but suddenly appear as the models increase in size. These emergent capabilities offer exciting potential for the future of machine learning and its application in various fields, including chemistry. \n\nWei et al. [72] demonstrated how language models below certain computing budgets display somewhat random behavior across a range of tasks. However, once a certain model size is reached, sudden and significant improvements in performance are observed. Other capabilities are observed in the form of augmented prompting strategies, such as Chain of Thought (CoT) reasoning [73]. In this approach, models are instructed to solve a task by following a step by step reasoning sequence. Another emergent capability is instruction following [74], where LMs are given a task in the form of a set of instructions to follow. \n\nInterestingly, these techniques generally have a negative impact on the performance of smaller models [73]. However a positive effect on performance is observed once models reach certain threshold sizes [72]. \n\nEmergent capabilities, therefore, enable language models to effectively tackle a variety of tasks that involve reasoning. This is achieved without explicit training and with the flexibility of a text query in natural language.",
            "score": 0.3909408543545664,
            "section_title": "On the capabilities of Large Language Models",
            "char_start_offset": 22959,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1164
                },
                {
                    "start": 1167,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1563
                },
                {
                    "start": 1566,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2180
                },
                {
                    "start": 2183,
                    "end": 2290
                },
                {
                    "start": 2291,
                    "end": 2391
                },
                {
                    "start": 2394,
                    "end": 2515
                },
                {
                    "start": 2516,
                    "end": 2620
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7568359375
        },
        {
            "corpus_id": "174798289",
            "title": "Like a Baby: Visually Situated Neural Language Acquisition",
            "text": "The theory of situated cognition postulates that a person's knowledge is inseparable from the physical or social context in which it is learned and used (Greeno and Moore, 1993). Similarly, Perceptual Symbol Systems theory holds that all of cognition, thought, language, reasoning, and memory, is grounded in perceptual features (Barsalou, 1999). Knowledge of language cannot be separated from its physical context, which allows words and sentences to be learned by grounding them in reference to objects or natural concepts on hand (see Roy and Reiter, 2005, for a review). Nor can knowledge of language be separated from its social context, where language is learned interactively through communicating with others to facilitate problem-solving. Simply put, language does not occur in a vacuum. \n\nYet, statistical language models, typically connectionist systems, are often trained in such a vacuum. Sequences of symbols, such as sentences or phrases composed of words in any language, such as English or German, are often fed into the model independently of any real-world context they might describe. In the classical language modeling framework, a model learns to predict a word based on a history of words it has seen so far. While these models learn a great deal of linguistic structure from these symbol sequences alone, acquiring the essence of basic syntax, it is highly unlikely that this approach can create models that acquire much in terms of semantics or pragmatics, which are integral to the human experience of language. How might one build neural language models that \"understand\" the semantic content held within the symbol sequences, of any language, presented to it? \n\nIn this paper, we take a small step towards a model that understands language as a human does by training a neural model jointly on corresponding linguistic and visual data. From an imagecaptioning dataset, we create a multi-lingual corpus where sentences are mapped to the real-world images they describe. We ask how adding such real-world context at training can improve language model performance.",
            "score": 0.39092319027150896,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2090
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 177,
                    "matchedPaperCorpusId": "2258750"
                },
                {
                    "start": 538,
                    "end": 558,
                    "matchedPaperCorpusId": "8106325"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76806640625
        },
        {
            "corpus_id": "269448752",
            "title": "Holmes: A Benchmark to Assess the Linguistic Competence of Language Models",
            "text": "We introduce Holmes, a new benchmark designed to assess language models (LMs) linguistic competence - their unconscious understanding of linguistic phenomena. Specifically, we use classifier-based probing to examine LMs' internal representations regarding distinct linguistic phenomena (e.g., part-of-speech tagging). As a result, we meet recent calls to disentangle LMs' linguistic competence from other cognitive abilities, such as following instructions in prompting-based evaluations. Composing Holmes, we review over 270 probing studies and include more than 200 datasets to assess syntax, morphology, semantics, reasoning, and discourse phenomena. Analyzing over 50 LMs reveals that, aligned with known trends, their linguistic competence correlates with model size. However, surprisingly, model architecture and instruction tuning also significantly influence performance, particularly in morphology and syntax. Finally, we propose FlashHolmes, a streamlined version that reduces the computation load while maintaining high-ranking precision.",
            "score": 0.39089894227076427,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8193359375
        },
        {
            "corpus_id": "49666627",
            "title": "Simultaneous Acquisition of Words and Syntax: Effects of Exposure Condition and Declarative Memory",
            "text": "Most experimental studies investigating the acquisition of artificial or natural languages focus on either vocabulary or grammar, but not both. This choice is methodologically sound, though it is perhaps also surprising, given that language learning outside the lab involves the simultaneous acquisition of multiple linguistic features. The investigation of the simultaneous learning of words and syntax is also of considerable theoretical interest. Studies that observe the acquisition of different aspects of language (e.g., Rebuschat et al., unpublished) allow us to test the role of syntactic knowledge in the acquisition of novel words (learning words in context) and the role of lexical knowledge in the acquisition of grammar (e.g., using word knowledge to parse speech streams). They also allow us to further investigate the mechanisms underpinning language learning. For example, it has been argued that the simultaneous acquisition of words and syntax could occur as a consequence of different modular processes (Pinker, 1998;Pe\u00f1a et al., 2002), or as a by-product of statistical learning mechanisms that allow for lexical item learning and the generalization of syntactic relations in speech (Plunkett and Marchman, 1993;Frost and Monaghan, 2016). \n\nA small number of studies have looked at the simultaneous acquisition of more than one linguistic feature, though these tended to be related features (e.g., Gass et al., 2003;Morgan-Short et al., 2012a,b, 2014;Grey et al., 2015;Rogers et al., 2016). \n\nFor example, Morgan-Short and colleagues have investigated the acquisition of both word order and morphosyntactic agreement in a series of experiments that employed an artificial language (Brocanto2; see, e.g., Morgan-Short, 2007;Morgan-Short et al., 2012a,b, 2014). In the current study, we extend this important line of inquiry by directly examining whether adult learners can acquire novel words and syntax simultaneously.",
            "score": 0.3908534054751986,
            "section_title": "Simultaneous Learning of Words and Syntax",
            "char_start_offset": 2421,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1258
                },
                {
                    "start": 1261,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1938
                }
            ],
            "ref_mentions": [
                {
                    "start": 1022,
                    "end": 1036,
                    "matchedPaperCorpusId": "170084385"
                },
                {
                    "start": 1036,
                    "end": 1054,
                    "matchedPaperCorpusId": "3835729"
                },
                {
                    "start": 1203,
                    "end": 1232,
                    "matchedPaperCorpusId": "17126604"
                },
                {
                    "start": 1232,
                    "end": 1257,
                    "matchedPaperCorpusId": "8811802"
                },
                {
                    "start": 1418,
                    "end": 1436,
                    "matchedPaperCorpusId": "143362542"
                },
                {
                    "start": 1436,
                    "end": 1462,
                    "matchedPaperCorpusId": "15889374"
                },
                {
                    "start": 1471,
                    "end": 1489,
                    "matchedPaperCorpusId": "143971198"
                },
                {
                    "start": 1743,
                    "end": 1769,
                    "matchedPaperCorpusId": "15889374"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.460693359375
        },
        {
            "corpus_id": "235525947",
            "title": "An Integrated Approach to Pragmatic Competence: Its Framework and Properties",
            "text": "If this hunch is on the right track, IMPC can function as a comprehensive model for the current reconstruction of pragmatic competence, covering both thinking and communicative activities. This will be of importance to the investigation of the acquisition, assessment, and instruction of pragmatic competence. However, more empirical investigation is needed to prove the detailed assumption in the future. \n\n3. The internal language is abbreviated as \"I-language,\" in which \"I\" refers to internal, individual, or intensional. In detail, I-language is \"a biological object internal to an individual, and considered in intension (i.e., the actual procedure of generation)\" (Chomsky, 2015, p. 92). In addition, the initial state of the language faculty, with the aid of primary linguistic data, develops to the full-fledged stable state-internalized language. Accordingly, I-language represents an internalized linguistic knowledge or linguistic system (cf. Mao, 2020;Mao et al., 2020). Originally, to clarify a systematic ambiguity of \"grammar,\" which refers to both the actual language internal to the person and the theory of that language constructed by the linguist, Chomsky (1986) creates I-language for the person's actual language, retaining the term \"grammar\" for the linguist's theory. 4. The operative procedure is a little bit different from that of Chierchia et al.'s (2005) Semantic Core Model, which follows the phase theory and states that semantic and pragmatic processing takes place in tandem, and implicatures are factored in recursively, in parallel with truth conditions, challenging Grice's proposal (i.e., it first retrieves the semantics of a whole root clause and then processes the implicatures associated with it in a strictly modular way). The different reasoning procedures are not relevant here and are thus put aside for the moment.",
            "score": 0.3908534054751986,
            "section_title": "Conclusion",
            "char_start_offset": 56300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 405
                },
                {
                    "start": 408,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 965,
                    "end": 982,
                    "matchedPaperCorpusId": "203720055"
                },
                {
                    "start": 1359,
                    "end": 1384,
                    "matchedPaperCorpusId": "112549867"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31591796875
        },
        {
            "corpus_id": "273811942",
            "title": "Can Language Models Learn to Skip Steps?",
            "text": "In initialization, models are first trained to solve the tasks comprehensively, adhering to the full sequence of reasoning steps. In Figure 1, the illustration on the left demonstrates how models are trained to follow a specified number of steps. Then in the iteration phase, the models are prompted to produce shorter answers based on the original training data (Figure 1 right). We then select the shorter reasoning paths that still achieve correct answers and mix them with the full-step reasoning paths. This expanded dataset is used to train a new model to have advanced step-skipping capabilities. Each iteration refines the model's ability to identify how steps can be skipped without sacrificing accuracy. Finally, we fine-tune the models using these iteratively generated datasets, including data instances that demonstrate successful step-skipping during each iteration. \n\nWe conduct experiments with three different reasoning datasets, each characterized by clear internal reasoning steps, to evaluate model behaviors. Empirical results demonstrate that models exhibit and develop the ability of skipping steps in our framework -not only solving tasks effectively but also actively omitting steps to enhance efficiency. Further analysis of model behaviors indicate that these skipped reasoning paths act as beneficial enhancements rather than mere biased shortcuts, as evidenced by their maintenance or even improvement of out-of-distribution (OOD) performance across various tasks. To the best of our knowledge, this work is the first investigation into the humanlike ability of step-skipping in language models, providing empirical evidence that models can indeed skip steps. These preliminary findings provide a fresh perspective on easy-to-hard generalizationtraining models on simpler data comprising both comprehensive and skipped reasoning steps can enhance their ability to generalize to more complex scenarios. \u00b6",
            "score": 0.39076916709031384,
            "section_title": "Introduction",
            "char_start_offset": 2400,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 1932
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78466796875
        },
        {
            "corpus_id": "261822542",
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "text": "At the onset of this stage (which we call the structure onset), SAS spikes. After the spike, the model starts handling complex linguistic phenomena correctly, as signaled by a break in BLiMP score (which we call the capabilities onset). Although the functional complexity of the model declines for the rest of training, it increases between these breaks. \u2022 We introduce a regularizer to examine the causal role of SAS (defined in Section 2.2) and use it to show that SAS is necessary for handling complex linguistic phenomena (Section 4.2) and that SAS competes with an alternative strategy that exhibits its own break in the loss curve, which we call the alternative strategy onset. \u2022 Section 4.3 shows that briefly suppressing SAS improves model quality and accelerates convergence. \n\nSuppressing past the alternative strategy onset damages performance and blocks SAS long-term, suggesting this phase transition terminates a critical learning period. BERT first learns to focus on syntactic neighbors with specialized attention heads, and then exhibits grammatical capabilities in its MLM objective. The former (internal) and the latter (external) model behaviors both emerge abruptly, at moments we respectively call the structure onset (\u25b2) and capabilities onset ( ) (quantified as described in Section 2.3). We separately visualize three runs with different seeds, noting that these seeds differ in the stability of Unlabeled Attachment Score (UAS; see Section 2.1) after the structure onset, but uniformly show that SAS emerges almost entirely in a brief window of time. We show (a) MLM loss, with 95% confidence intervals across samples bynonparametric bootstrapping; (b) internal grammar structure, measured by UAS on the parse induced by the attention distributions; and (c) external grammar capabilities, measured by average BLiMP accuracy with 95% confidence intervals across tasks by nonparametric bootstrapping.",
            "score": 0.39068232981712503,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 784
                },
                {
                    "start": 787,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1924
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "256863867",
            "title": "Moving away from lexicalism in psycho- and neuro-linguistics",
            "text": "We agree with production models which assume lexical items as treelets with much internal structure, such that stored linguistic knowledge can include large complexes of syntactic structure (Kempen and Huijbers, 1983;Vosse and Kempen, 2000;Ferreira, 2013;Matchin and Hickok, 2020). However, where these models typically assume as a fundamental property of the language system that each treelet has their own meaning and form, a nonlexicalist model like the one shown here allows more flexibility about how stored meaning, syntax, and form align, and does not require an additional process of lexicalization. In the nonlexicalist approach, there can be symmetrical \"triadic\" mappings, but this is not a necessary or central component of the language system. More broadly, non-lexicalist models that assume no \"lexical\" representations independent of meaning, syntax, and form, differ from neuroanatomical models that posit a distinct brain region or neural mechanism associated with \"lexical nodes.\" For example, Wilson et al. (2018)'s model proposes that there is an area of the brain [the dorsal lip of the superior temporal sulcus (STS)] which is associated with lexical nodes, and that this region is spatially and functionally distinct from \"higher level syntax. \" In our view, no such distinction is possible.",
            "score": 0.39027363863366965,
            "section_title": "FIGURE",
            "char_start_offset": 68413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1314
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 217,
                    "matchedPaperCorpusId": "248853069"
                },
                {
                    "start": 217,
                    "end": 240,
                    "matchedPaperCorpusId": "14445746"
                },
                {
                    "start": 240,
                    "end": 255,
                    "matchedPaperCorpusId": "60978472"
                },
                {
                    "start": 1012,
                    "end": 1032,
                    "matchedPaperCorpusId": "4063796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2646484375
        },
        {
            "corpus_id": "265051054",
            "title": "Syntax-Guided Transformers: Elevating Compositional Generalization and Grounding in Multimodal Environments",
            "text": "Compositional Generalization refers to the ability of an intelligent agent to generalize its understanding of the underlying structure of a problem, especially when it is faced with novel compositions of the previously seen building blocks or components (Chomsky, 1957;Montague, 1970). It is fundamental for models to be able to extrapolate from their training environment to novel situations, a common occurrence in real-world applications. Hupkes et al. (2020) categorizes compositional generalization capabilities into five categories, systematicity, substitutivity, localism & globalism, and overgeneralization. These abilities are crucial for models to achieve strong performance on tasks that require reasoning and understanding of hierarchical structures, such as natural language understanding, object classification, and robotics. \n\nHumans understand new compositions of previously observed concepts and simpler constructs. \n\nInput Command: pull the small blue object that is inside of the small green box and in the same row as the red circle while zigzagging. Action sequence: turn left, turn left, walk, turn right, walk, turn right, walk, pull Here, an agent is provided with a command. Its objective is to generate/execute a series of predefined actions to fulfill the task within the given environment. \n\nOn the other hand, despite remarkable progress in the field of Artificial Intelligence, even state-of-theart language models demonstrate limitations in this aspect (Lake and Baroni, 2018;Thomas McCoy et al., 2020;Shaw et al., 2021). Especially, they often fail to effectively generalize in the reasoning depth, which involves handling multi-turn reasoning about entities and their properties in the world or even the co-occurrence of unseen spatial relations (Wu et al., 2021). These limitations indicate a crucial need for innovative approaches to address these issues. \n\nIn this research, our objective is to exploit the syntactic structure of language to enhance compositional generalization. Our focus is mainly on the multimodal problem setting that entangles vision and language. In this unique setting, compositional linguistic descriptions must be accurately grounded in the environment to devise coherent action plans or achieve specific goals. An illustrative example of this scenario is shown in Figure 1. \n\nThe motivation behind leveraging syntax in our approach stems from the inherent structure and compositionality of natural language.",
            "score": 0.38962225286113134,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 839
                },
                {
                    "start": 842,
                    "end": 932
                },
                {
                    "start": 935,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1317
                },
                {
                    "start": 1320,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1890
                },
                {
                    "start": 1893,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2273
                },
                {
                    "start": 2274,
                    "end": 2336
                },
                {
                    "start": 2339,
                    "end": 2470
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 284,
                    "matchedPaperCorpusId": "201904046"
                },
                {
                    "start": 442,
                    "end": 462,
                    "matchedPaperCorpusId": "211259383"
                },
                {
                    "start": 1507,
                    "end": 1533,
                    "matchedPaperCorpusId": "59599752"
                },
                {
                    "start": 1533,
                    "end": 1551,
                    "matchedPaperCorpusId": "225066984"
                },
                {
                    "start": 1779,
                    "end": 1796,
                    "matchedPaperCorpusId": "237572003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57958984375
        },
        {
            "corpus_id": "123744963",
            "title": "Why Brain Oscillations Are Improving Our Understanding of Language",
            "text": "As also discussed extensively by Poeppel (e.g., Poeppel and Embick, 2005), current neurolinguistic research suffers from two crucial shortcomings. On the one hand, it relies on broad distinctions between components of language (e.g., the syntactic rules of grammar vs. the meanings of lexical representations), which actually involve multiple neural components, computations and representations. On the other hand, the core elements of linguistic theory (e.g., syntactic operations) do not map onto core neurobiological elements (neurons, nodes of Ranvier, etc.). It is consequently urgent for any neurolinguistic research to formulate a model of language in computational terms that can be processed by specific parts of the brain in real-time. For instance, we can decompose syntax into its constituent operations (MERGE, Labeling, Search;Adger, 2019) and representations [lexical and categorial features, such as N(oun) and A(djective); Adger and Svenonius, 2011]. Which seem generic enough to potentially make contact with certain neurobiological information processing frameworks. To take only the most commonly discussed cases, MERGE in its current formulation (Chomsky et al., Forthcoming) involves adding objects to a workspace, while Labeling involves attributing to a constructed set within a workspace a particular categorial identity. \n\nDecomposing language into a specific pattern of ''coupling'' between different oscillations (whereby one feature of an oscillation, such as its phase, has its firing pattern synchronized with a feature of a distinct oscillation, such as its amplitude) appears feasible. Importantly, this approach satisfactorily accounts for core facets of language according to major linguistic theories, in particular, generative theories.",
            "score": 0.38910005052921137,
            "section_title": "HOW: OSCILLATIONS AND THE LINGUISTIC BRAIN",
            "char_start_offset": 3781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1773
                }
            ],
            "ref_mentions": [
                {
                    "start": 940,
                    "end": 966,
                    "matchedPaperCorpusId": "58494057"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.282958984375
        },
        {
            "corpus_id": "264590507",
            "title": "Probing LLMs for Joint Encoding of Linguistic Categories",
            "text": "Large Language Models (LLMs) exhibit impressive performance on a range of NLP tasks, due to the general-purpose linguistic knowledge acquired during pretraining. Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing. Yet, little is known about how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations. In this paper, we propose a framework for testing the joint encoding of linguistic categories in LLMs. Focusing on syntax, we find evidence of joint encoding both at the same (related part-of-speech (POS) classes) and different (POS classes and related syntactic dependency relations) levels of linguistic hierarchy. Our cross-lingual experiments show that the same patterns hold across languages in multilingual LLMs.",
            "score": 0.388804166726843,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "264172920",
            "title": "The Quo Vadis of the Relationship between Language and Large Language Models",
            "text": "On the contrary, LLMs seem to apply the rules selectively (Figure 2), if indeed they make any particular rule inferences at all. It is thus possible that, in stark contrast to human cognition, when an LLM produces as output an answer that seems correct and coherent, this is not the fruit of the application of some \"learned\" language rule, but the inconsistent reproduction of chunks that form part of the training data (Dentella et al. 2023). Thus, as Figure 2 shows, the ChatGPT ability to detect attraction errors does not merely fail at times; it comes with errors that fall outside the domain of errors a human would produce, and this is a difference in quality. While a human could have a momentary lapse of judgment that could make an attraction error go undetected, in this specific context, a human would never produce as a justification of their judgment an explanation that suggests that the sentence is correct because the verb 'was' is in past tense and agrees (\"agree\" according to ChatGPT in Figure 2) with the subject 'sounds'. Such production errors cast some doubt on the claim that LLMs possess formal linguistic competence that detects attraction errors and encompasses linguistic knowledge of morphology, phonology, and syntax. \n\nThe framing of the object is important. If LLMs are meant to represent human language, but their tweaking of the language that constitutes their training data transforms the input into an output riddled with errors that classify it as distinctly non-human, there is a mismatch between what is allegedly being represented and what is actually being produced, that needs to be registered. In sum, LLMs can stitch together sequences of words, based on probabilistic information about how certain words pattern with others in the training data, without grasping the meaning of their outputs: they are stochastic parrots (Bender et al. 2021).",
            "score": 0.3887435065811796,
            "section_title": "Defining the object",
            "char_start_offset": 8054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1889
                }
            ],
            "ref_mentions": [
                {
                    "start": 1868,
                    "end": 1888,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2166748046875
        },
        {
            "corpus_id": "263608555",
            "title": "Language Models as Knowledge Bases for Visual Word Sense Disambiguation",
            "text": "Language models as Knowledge Bases is a novel paradigm that harnesses the implicit knowledge stored in neural weights of Language Models (LMs) similar to how explicit knowledge of Knowledge Graphs (KGs) has served related applications [5,6]. Recent works probe characteristics of different LLM-stored knowledge senses, such as commonsense [14], factual [15] and temporal [16]. Model scale has revealed unprecedented LM capabilities related to knowledge, such as different types of reasoning; there are open research questions related to arithmetic [17], symbolic [18], commonsense [19] and causal reasoning [20] in large LMs, and whether such models merely overfit large amounts of data or are genuinely capable of reasoning similar to humans [21]. More recently, the LM-as-KB paradigm has been favored by the VL community to enhance popular VL tasks [7,8,9,10,11,12], as in the case of the current paper. \n\nPrompting Language Models Traditionally, LMs are pre-trained on large amounts of text corpora and then fine-tuned on smaller labelled ones to address longstanding challenges in NLP. Scaling-up to several billion parameters (Large Language Models -LLMs) enables emergent model capabilities, which can be communicated via prompting [22]. Different prompting strategies have been proposed, such as zero-shot prompting, where the task is explicitly described in natural language, and few-shot prompting, where few exemplars are provided to the LM to guide answer generation [23]. More specifically, in-context learning (ICL) is a novel fewshot paradigm where few sample demonstrations from the dataset are used to retrieve related knowledge stored in the LLM without updating any parameters [24]. This technique is inspired from the way humans learn from analogy [25] and has been successfully applied ever since to serve several NLP tasks [26,27,28,29,30]. Another human-inspired prompting technique is Chain-of-Thought (CoT), where the LM is encouraged by the prompt phrasing to generate rationales together with the predicted answer [31,32,33]. Apart from the aforementioned discrete prompt strategies, soft prompting sacrifices human-interpretable templates to achieve more advanced knowledge retrieval [34].",
            "score": 0.3885680236662889,
            "section_title": "Related work",
            "char_start_offset": 3518,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 371,
                    "end": 375,
                    "matchedPaperCorpusId": "235669861"
                },
                {
                    "start": 548,
                    "end": 552,
                    "matchedPaperCorpusId": "173188048"
                },
                {
                    "start": 854,
                    "end": 856,
                    "matchedPaperCorpusId": "252917791"
                },
                {
                    "start": 1478,
                    "end": 1482,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1767,
                    "end": 1771,
                    "matchedPaperCorpusId": "9814700"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "273901561",
            "title": "On Functional Competence of LLMs for Linguistic Disambiguation",
            "text": "With the emergence of Transformer models such as (Devlin et al., 2018;Liu et al., 2019), and the rise in computation power to process massive amounts of text, Large Language Models (LLMs) have gained human-like capabilities. Researchers report these models, such as (Team et al., 2023;Jiang et al., 2023;OpenAI, 2022;Achiam et al., 2023;Touvron et al., 2023), perform well on a vast array of natural language processing tasks (Akter et al., 2023), for example, on Knowledge-based QA, Reasoning and Machine Translation, even though the models have not been purposely trained to perform these tasks. This raises hopes for the linguistic community that the long-standing problem of WSD would benefit from the LLM's superlative language and reasoning power (Senel et al., 2022). Some research shed light on the inherent notion of sense in LLMs (Wiedemann et al., 2019). \n\nSeveral studies report that a closely associated task, machine translation, has benefited from these models. For example, (Lee et al., 2023) reports that LLMs display some capabilities that go beyond the literal translation of words, which is much needed when handling idiomatic expressions. LLMs are also being explored for tasks that require reasoning and planning (Zhao et al., 2024), (Savarimuthu et al., 2024), and augur some emerging abilities (Wei et al., 2022). However, many researchers report that much is still lacking in the reasoning power of LLMs (Li et al., 2024), (Kassner et al., 2023), (Liu et al., 2023), (Hao et al., 2023), (Sap et al., 2022), (Ji et al., 2023). \n\nWith these deficiencies in mind, researchers have proposed many methods for improving the reasoning power of LLMs. (Wu et al., 2024) proposes an evaluation framework for measuring LLM's reasoning capabilities. (Hao et al., 2023) proposes a reasoning framework by priming LLMs with prompting.",
            "score": 0.3878327852543235,
            "section_title": "Large Language Models",
            "char_start_offset": 12451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1550
                },
                {
                    "start": 1553,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 990,
                    "end": 1008,
                    "matchedPaperCorpusId": "264426508"
                },
                {
                    "start": 1448,
                    "end": 1470,
                    "matchedPaperCorpusId": "258840940"
                },
                {
                    "start": 1492,
                    "end": 1510,
                    "matchedPaperCorpusId": "258865812"
                },
                {
                    "start": 1512,
                    "end": 1530,
                    "matchedPaperCorpusId": "253098632"
                },
                {
                    "start": 1532,
                    "end": 1549,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 1668,
                    "end": 1685,
                    "matchedPaperCorpusId": "259341893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494384765625
        },
        {
            "corpus_id": "273901562",
            "title": "Does Large Language Model Contain Task-Specific Neurons?",
            "text": "Large Language Models (LLMs) have gained widespread attention due to their powerful capabilities (Zhao et al., 2023b). Based on the unsupervised pre-training followed by instruction finetuning (IFT) paradigm (Brown et al., 2020), LLMs have developed the ability to comprehensively handle various types of natural language processing (NLP) tasks. The advantage of this paradigm is that a single model deployment can perform multiple tasks, showcasing the versatility and efficiency of LLMs in NLP applications (Yuan et al., 2024). \n\nHowever, there are significant differences in the knowledge and abilities required for different tasks by LLMs. For example, sentiment analysis focuses on adjectives and adverbs (Benamara et al., 2007), text classification emphasizes domain-specific terminology (Avancini et al., 2006), while natural language inference prioritizes the relationship between premise and conclusion sentences (Camburu et al., 2018). This suggests that the processing patterns of the same LLM may vary across different tasks, highlighting the necessity to investigate the underlying mechanisms of task-specific processing in LLM. Task-specific processing can be traced by examining the neurons activated during the inference of specific tasks. Therefore, the two critical questions emerge: Are there neurons within the same LLM that handle specific tasks?, and if so, how does the model manage and differentiate between various types of tasks? \n\nIn fact, neuroscience has discovered that different brain areas control distinct behavioral abilities (Bari and Robbins, 2013). For instance, while higher cognitive functions such as learning, reasoning, decision-making and creativity are primarily controlled by the frontal lobe, the neurons involved in these processes remain distinct (Collins and Koechlin, 2012). Inspired by these findings, we hypothesize that although LLMs utilize a unified structure and parameters, the neurons engaged in different tasks may vary significantly. Motivated by this hypothesis, this paper investigates the existence of task-specific neurons in LLMs, specifically examining whether the neurons activated by different tasks exhibit distinct patterns. As shown in Figure 1 upper, different task-specific neurons in the same LLM are marked with different colors.",
            "score": 0.3877696330668944,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1455
                },
                {
                    "start": 1458,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 228,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 509,
                    "end": 528,
                    "matchedPaperCorpusId": "259096157"
                },
                {
                    "start": 710,
                    "end": 733,
                    "matchedPaperCorpusId": "5336999"
                },
                {
                    "start": 794,
                    "end": 817,
                    "matchedPaperCorpusId": "12008094"
                },
                {
                    "start": 922,
                    "end": 944,
                    "matchedPaperCorpusId": "54040953"
                },
                {
                    "start": 1560,
                    "end": 1584,
                    "matchedPaperCorpusId": "28470669"
                },
                {
                    "start": 1795,
                    "end": 1823,
                    "matchedPaperCorpusId": "364639"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "216562282",
            "title": "PuzzLing Machines: A Challenge on Learning From Small Data",
            "text": "Meta-linguistics is defined by Chomsky (1976) as \"the knowledge of the characteristics and structures of language\" as realised on the level of phonology, morphology, syntax and semantics. Any English speaker would likely have the linguistic capacity to produce the word undo when asked \"What is the opposite of do?\" Only a speaker with some level of meta-linguistic awareness, however, would further be able to reflect on the structure of the word they have produced: to identify unas a unit that serves to negate words, to spot its similarity in function to other units like disand de-. He/she would also be aware that unis not interchangeable with disand de-, since it attaches to the front of verbs and adjectives but not to nouns. \n\nMeta-linguistic awareness is especially useful (and often improved) in the process of learning a new language, as it allows the learner to compare and contrast the structure and characteristics of the new language to those that he/she is already familiar with. It is desirable that systems for natural language processing possess meta-linguistic awareness, too, as that could hugely improve their crosslingual generalizability, a problem that remains open after being approached from various engineering perspectives, often with little recourse to linguistics. However, measuring the meta-linguistic awareness of a system is not trivial. Existing probing techniques are mostly designed to measure how well neural models capture specific linguistic phenomena, e.g., whether a specific layer of an English language model can capture that undo is negative, instead of testing for meta-linguistic awareness. Our challenge takes a step further and tests whether the model can apply the underlying morphological processes, e.g. of verbal negation through prefixing. In addition, our challenge spans a wide-range of language families and covers a variety of linguistic phenomena (see \u00a73.1), that qualifies it as a favorable testbed for measuring meta-linguistic awareness. \n\nLet us demonstrate how meta-linguistic reasoning skills are used to solve the \"Chickasaw puzzle\" given in Table 1.",
            "score": 0.387579084673743,
            "section_title": "Meta-linguistics",
            "char_start_offset": 3379,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 2002
                },
                {
                    "start": 2005,
                    "end": 2119
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.300537109375
        },
        {
            "corpus_id": "271161843",
            "title": "Large Models of What? Mistaking Engineering Achievements for Human Linguistic Agency",
            "text": "In this paper we argue that key, often sensational and misleading, claims regarding linguistic capabilities of Large Language Models (LLMs) are based on at least two unfounded assumptions; the assumption of language completeness and the assumption of data completeness. Language completeness assumes that a distinct and complete thing such as `a natural language' exists, the essential characteristics of which can be effectively and comprehensively modelled by an LLM. The assumption of data completeness relies on the belief that a language can be quantified and wholly captured by data. Work within the enactive approach to cognitive science makes clear that, rather than a distinct and complete thing, language is a means or way of acting. Languaging is not the kind of thing that can admit of a complete or comprehensive modelling. From an enactive perspective we identify three key characteristics of enacted language; embodiment, participation, and precariousness, that are absent in LLMs, and likely incompatible in principle with current architectures. We argue that these absences imply that LLMs are not now and cannot in their present form be linguistic agents the way humans are. We illustrate the point in particular through the phenomenon of `algospeak', a recently described pattern of high stakes human language activity in heavily controlled online environments. On the basis of these points, we conclude that sensational and misleading claims about LLM agency and capabilities emerge from a deep misconception of both what human language is and what LLMs are.",
            "score": 0.38744171984742026,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18994140625
        },
        {
            "corpus_id": "276724796",
            "title": "Triple Phase Transitions: Understanding the Learning Dynamics of Large Language Models from a Neuroscience Perspective",
            "text": "Large language models (LLMs) often exhibit abrupt emergent behavior, whereby new abilities arise at certain points during their training. This phenomenon, commonly referred to as a ''phase transition'', remains poorly understood. In this study, we conduct an integrative analysis of such phase transitions by examining three interconnected perspectives: the similarity between LLMs and the human brain, the internal states of LLMs, and downstream task performance. We propose a novel interpretation for the learning dynamics of LLMs that vary in both training data and architecture, revealing that three phase transitions commonly emerge across these models during training: (1) alignment with the entire brain surges as LLMs begin adhering to task instructions Brain Alignment and Instruction Following, (2) unexpectedly, LLMs diverge from the brain during a period in which downstream task accuracy temporarily stagnates Brain Detachment and Stagnation, and (3) alignment with the brain reoccurs as LLMs become capable of solving the downstream tasks Brain Realignment and Consolidation. These findings illuminate the underlying mechanisms of phase transitions in LLMs, while opening new avenues for interdisciplinary research bridging AI and neuroscience.",
            "score": 0.38704866882156796,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97021484375
        },
        {
            "corpus_id": "266933633",
            "title": "LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization",
            "text": "In this section, we provide an overview of how LINGUALCHEMY captures linguistic constraints and explain the intuition behind it. We also discuss in detail how we align model representations with the linguistic vectors. \n\n3.1 Does Multilingual LMs capture Linguistic Constraints? \n\nWe define the linguistic knowledge as a vector gathered from URIEL vector (Littell et al., 2017). We chose three distinct linguistic knowledge from the database, namely 'syntax_knn', 'syntax_average' 1 , and 'geo' features. The choice of 'syntax_knn' and 'syntax_average' is motivated by the typological nature of syntax. Syntax in languages varies widely; hence, by using aggregate measures like averages and knearest neighbors (kNN), we can capture a more general representation of syntactic features across languages. Note that in our experiments, we excluded phonological features and language family attributes from our analysis as they are less relevant to textual data and have limited granularity for understanding linguistic variations. \n\nSyntax Features These feature vectors denote a typological feature that is adapted from several sources including World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013), Syntactic Structures of World Languages (Collins, 2010), and short prose descriptions on typological features in Ethnologue (Lewis, 2009).Syntax vectors capture information about the syntactic properties of languages, derived from large-scale typological databases that document the structural and semantic variation across different languages. These syntax features in URIEL are utilized to represent languages in vector form that allows the analysis 1 In this work, we chose the 'knn' and 'average syntax features. These include consensus values (like averages) and predicted values (such as kNN regressions based on phylogenetic or geographical neighbors) and comparison of languages based on their syntactic properties. \n\nGeographical Features On the other hand, geographical features represent languages in terms of their geographical properties. The inclusion of \"geo\" features aims to capture geographical attributes of languages. This feature expresses geographical location with a fixed number of dimensions that each represents the \"great circle\" distance-from the language in question to a fixed point on the Earth's surface.",
            "score": 0.3869261179687273,
            "section_title": "Unseen Languages Adaptation with LINGUALCHEMY",
            "char_start_offset": 7099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 218
                },
                {
                    "start": 221,
                    "end": 278
                },
                {
                    "start": 281,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1026
                },
                {
                    "start": 1029,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1938
                },
                {
                    "start": 1941,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 377,
                    "matchedPaperCorpusId": "17625727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.222412109375
        },
        {
            "corpus_id": "273821157",
            "title": "The LLM Language Network: A Neuroscientific Approach for Identifying Causally Task-Relevant Units",
            "text": "Large language models (LLMs) exhibit remarkable capabilities on not just language tasks, but also various tasks that are not linguistic in nature, such as logical reasoning and social inference. In the human brain, neuroscience has identified a core language system that selectively and causally supports language processing. We here ask whether similar specialization for language emerges in LLMs. We identify language-selective units within 18 popular LLMs, using the same localization approach that is used in neuroscience. We then establish the causal role of these units by demonstrating that ablating LLM language-selective units -- but not random units -- leads to drastic deficits in language tasks. Correspondingly, language-selective LLM units are more aligned to brain recordings from the human language system than random units. Finally, we investigate whether our localization method extends to other cognitive domains: while we find specialized networks in some LLMs for reasoning and social capabilities, there are substantial differences among models. These findings provide functional and causal evidence for specialization in large language models, and highlight parallels with the functional organization in the brain.",
            "score": 0.3869161053408935,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.853515625
        },
        {
            "corpus_id": "266818340",
            "title": "On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS)",
            "text": "Pre-trained LLMs are models already trained on large datasets to understand and generate language, acquiring a broad linguistic knowledge base. Fine-tuning involves further training pre-trained LLMs on a smaller, task-specific dataset, thereby adjusting the neural network weights for particular applications. This process is mathematically represented in Equation 5. \n\nHere, \u03b8 fine-tuned are the model parameters after fine-tuning, \u03b8 pre-trained are the parameters obtained from pre-training, \u03b7 is the learning rate, and \u2207 \u03b8 L(\u03b8, D task ) denotes the gradient of the loss function L with respect to the parameters \u03b8 on the task-specific dataset D task . \n\nComplementing the fine-tuning approach is in-context learning, an alternative strategy that is particularly characteristic of models like the GPT series. This method diverges from fine-tuning by enabling the model to adapt its responses based on immediate context or prompts without necessitating further training. The efficacy of in-context learning is a direct consequence of the comprehensive pretraining phase, where models are exposed to diverse textual datasets, thereby acquiring a nuanced understanding of language and context. Given a context C, the model generates text T that is contextually relevant, as shown in Equation 6. Here, P (T |C) is the probability of generating text T given the context C, and P (t i |t <i , C) is the probability of generating the i-th token t i given the preceding tokens t <i and the context C. These diverse model types and training methodologies under the umbrella of LLMs showcase the flexibility and adaptability of language models in handling a wide range of complex tasks. Figure 1 illustrates the comparative capabilities of different LLMs across various competency domains, such as Writing (evaluating text generation quality), Roleplay (assessing conversational interaction), Reasoning (logical problem-solving), Math (numerical problem-solving), Coding (programming language understanding and generation), Extraction (information retrieval from text), STEM (proficiency in scientific and technical contexts), and Humanities (engagement with arts, history, and social sciences content).",
            "score": 0.38657282078701094,
            "section_title": "Background Large Language Models",
            "char_start_offset": 9172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 367
                },
                {
                    "start": 370,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 2195
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "268691480",
            "title": "Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models Using Minimal Pairs",
            "text": "In essence, across models, performance thresholds, and the relationship between complexity and depth, all observations support specific hypotheses: the semantics-syntax interface and morphology present greater learning challenges for language models than pure syntax.This consensus aligns with previous studies suggesting that semantics is harder to learn than syntax (Tenney et al., 2019).Intriguingly, our results also align with neurolinguistic studies utilizing neural LMs to study the hierarchical mechanism of language processing.We speculate that the additional layers we observe for these pairs of sentences may be linked to the increased persistence of predictive repre-sentations for semantic information as reported by Caucheteux et al. (2023), suggesting that semantic processing in the human brain is more long-term, high-level, than the short-term, shallow syntactic representation.Attention's behavior Our findings on attention mechanisms in GPT-2 XL raise intriguing questions about their role and functionality.The absence of a clear incremental trend in attention, unlike hidden states, suggests that each attention layer in GPT-2 XL might be capturing unique linguistic information without necessarily building upon previous layers.The relatively low performance of attention heads in capturing morphology, especially given the supervised nature of our probing, might indicate that attention in GPT-2 XL isn't as adept at discerning morphological nuances.\n\nThe variability in performance among different attention heads may suggest an inherent specialization within the model.For instance, certain heads, like No.8 and No.16, consistently outperforming others could indicate that these heads have specialized in capturing more general or prevalent linguistic features.This could be a result of the training process, where frequent patterns in the data are more likely to be captured and optimized by specific heads.",
            "score": 0.38653964050177575,
            "section_title": "Morphology and semantics harder than syntax",
            "char_start_offset": 22636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 536
                },
                {
                    "start": 536,
                    "end": 896
                },
                {
                    "start": 896,
                    "end": 1028
                },
                {
                    "start": 1028,
                    "end": 1251
                },
                {
                    "start": 1251,
                    "end": 1474
                },
                {
                    "start": 1476,
                    "end": 1595
                },
                {
                    "start": 1595,
                    "end": 1787
                },
                {
                    "start": 1787,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 368,
                    "end": 389,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 730,
                    "end": 754,
                    "matchedPaperCorpusId": "257309545"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47021484375
        },
        {
            "corpus_id": "131775742",
            "title": "Does Scale-Free Syntactic Network Emerge in Second Language Learning?",
            "text": "The development of L1 syntax is non-linear: at a certain age, the growth may be explosive, which is what is meant by emergence. Resorting to empirical data and complex network approach, Corominas-Murtra et al. (2009) found, for the first time, that at the age of 24 months, two important complex network properties, i.e., the scale-free property and the small-world property, emerge in L1 syntactic networks, signaling a sharp transition from a presyntactic organization to a scale-free and small-world syntactic network. Therefore, the emergence of syntax, which concerns the change of word-word relations, signals the rapid shift from chaotic word clusters to well-organized sentences. From the perspective of complex network, the emergence of syntax means the shift from a non-scale-free and non-small-world language structure to a scale-free and small-world language network. \n\nAs for L2 syntactic learning, various studies have attempted to find out whether there is such an abrupt shift in L2 learning, but failed to reach consensus on this issue. L2 learning is viewed as a dynamic and complex process teeming with emergences (Larsen-Freeman, 1997;Ellis, 1998), and several theories are suggested to explain this process. Chaos/Complexity Theory, for instance, may uncover the patterns underlying complex and dynamic language system (Holland, 1998;Larsen-Freeman and Cameron, 2008). Dynamic Systems Theory (DST) views learner language as a continually changing system, takes into consideration all variables concerning learning, and tries to model this system mathematically (e.g., de Bot et al., 2007). In L2 development, emergences, or, sudden shifts, which reflect system restructuring, have already been observed by empirical studies. For example, Sato (1988) investigated the emergence of complex syntactic structures in L2 development of two Vietnamese learners of English, especially the coordination, the subordination, and the complement structures.",
            "score": 0.3864891252653905,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1965
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 216,
                    "matchedPaperCorpusId": "37341097"
                },
                {
                    "start": 1133,
                    "end": 1155,
                    "matchedPaperCorpusId": "10145358"
                },
                {
                    "start": 1155,
                    "end": 1167,
                    "matchedPaperCorpusId": "15955007"
                },
                {
                    "start": 1589,
                    "end": 1609,
                    "matchedPaperCorpusId": "33567516"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69482421875
        },
        {
            "corpus_id": "277701208",
            "title": "What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure",
            "text": "Although large language models (LLMs) are generally trained on next-word prediction tasks, these systems have exhibited impressive performance in the generation of seemingly human-like, coherent texts (e.g. ChatGPT (OpenAI 2022)). Yet, we do not know how these models work and what they learn from their data. Like other types of deep neural networks, LLMs are opaque and suffer from the black-box problem: during training, LLMs learn a highly complex function with distributed representations and we cannot simply \"look inside\" to determine how they work (Burrell 2016;Creel 2020). Because of this, it is difficult to evaluate their potential linguistic and cognitive capacities (Shanahan 2023). \n\nThe field of explainable AI (XAI) aims to solve the problem of opacity by developing explanations through mathematical techniques that show how or why a network makes a certain decision (Zednik 2021). One specific question that has been asked is whether LLMs learn any kind of rules or algorithms, akin to symbolic rules in classical systems (Olah et al. 2020;Pavlick 2023). Uncovering such rules would help explain how AI systems work, facilitate the evaluation of potential cognitive and linguistic capacities, and potentially allow for interventions in the model internals to update the system's predictions. \n\nIn the particular case of LLMs, it has been suggested that they do not just perform next-word prediction based on surface statistics, but that they learn to follow symbolic rules (Pavlick 2023), develop linguistic competence (Mahowald et al. 2024), or even acquire a kind of knowledge (Meng et al. 2022;Yildirim and Paul 2024). Indeed, with respect to syntax, LLMs seem to successfully represent grammatical rules, and apply formal linguistic structure (Linzen and Baroni 2021;Mahowald et al. 2024). Less attention has been paid to semantics, but some very recent studies claim to have identified representations of world models (Li et al. 2023), implicit meaning (Hase et al. 2021), and facts (De Cao, Aziz, and Titov 2021;Meng et al. 2022).",
            "score": 0.3864128727362808,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1310
                },
                {
                    "start": 1313,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 2055
                }
            ],
            "ref_mentions": [
                {
                    "start": 556,
                    "end": 570,
                    "matchedPaperCorpusId": "61330970"
                },
                {
                    "start": 570,
                    "end": 580,
                    "matchedPaperCorpusId": "209483350"
                },
                {
                    "start": 885,
                    "end": 898,
                    "matchedPaperCorpusId": "195820497"
                },
                {
                    "start": 1041,
                    "end": 1059,
                    "matchedPaperCorpusId": "215930358"
                },
                {
                    "start": 1059,
                    "end": 1072,
                    "matchedPaperCorpusId": "259065119"
                },
                {
                    "start": 1492,
                    "end": 1506,
                    "matchedPaperCorpusId": "259065119"
                },
                {
                    "start": 1538,
                    "end": 1560,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 1598,
                    "end": 1616,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1616,
                    "end": 1639,
                    "matchedPaperCorpusId": "263780995"
                },
                {
                    "start": 1766,
                    "end": 1790,
                    "matchedPaperCorpusId": "216080851"
                },
                {
                    "start": 1790,
                    "end": 1811,
                    "matchedPaperCorpusId": "268551442"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71875
        },
        {
            "corpus_id": "242758331",
            "title": "On Semantic Cognition, Inductive Generalization, and Language Models",
            "text": "As LMs grow larger, so does their capacity to retrieve memorized facts about the world (birds can fly), leading to the development of the the paradigm known as 'LMs as Knowledge Bases'. This paradigm uses LMs to perform 'commonsense reasoning' by simply querying pre-trained LMs with prompts that elicit word predictions corresponding to the retrieved fact (Petroni et al. 2019), or by fine-tuning and evaluating LMs on knowledge bases (Bosselut et al. 2019). This paradigm exclusively focuses on what aspects of world knowledge are accessible through pre-training. Such an inquiry sheds important light on the access to long-term semantic memory as it emerges from predicting words in context. I hope to extend this line of research by focusing on how pre-trained LMs use semantic knowledge to process and generalize novel information, and to what extent their behavior aligns to that in humans.\n\nInspired by research in cognitive science, my thesis ties in the influential work of Rogers and McClelland (2004, R&M, henceforth). R&M present a connectionist account of inductive reasoning, where they describe a feed-forward network that performs inductive projections of novel properties, and displays patterns comparable to inductions in children across multiple ages (Carey 1985). Despite this connection, my thesis pursues a line of research independent to that of R&M as it exclusively relies on representations learned by models from the statistics contained in language corpora. It therefore targets the role played by language-more specifically, the pre-training of LMs to predict words in context-in facilitating the learning of semantic knowledge as opposed to the localist representations of R&M, who make no such commitment. More recently, Sinha et al. (2019) introduced the CLUTRR benchmark to study a different kind of inductive reasoning-one that is rooted in formal logic-in LMs on synthetic kinship information expressed as language. Unlike CLUTRR, the inductive reasoning capacities considered in my thesis make graded distinctions between generalizations across two different concepts -i.e., generalization of a property to robin may differ from that to penguins. Such distinctions are not considered in CLUTRR, which instead focuses The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22",
            "score": 0.38621042439797126,
            "section_title": "Related Work",
            "char_start_offset": 1469,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 357,
                    "end": 378,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1752,
                    "end": 1771,
                    "matchedPaperCorpusId": "198184456"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.775390625
        },
        {
            "corpus_id": "277780989",
            "title": "Domain-Adaptive Continued Pre-Training of Small Language Models",
            "text": "Recent advances in large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but their substantial computational requirements present significant barriers to broader adoption and experimentation. Training state-of-the-art models from scratch requires massive datasets and computational resources, making them inaccessible to many researchers and organizations. This challenge is particularly relevant in specialized domains like education, where domain-specific knowledge is crucial, but resource constraints are common. \n\nContinued pre-training offers a promising alternative to full-scale training, allowing existing pre-trained models to be further trained on domain-specific data. This approach enables models to acquire specialized knowledge while leveraging their existing capabilities, potentially offering a more efficient path to high-performance domain-specific language models. In this paper, I explore the efficacy of continued pre-training for adapting small language models (125M parameters) to educational content. My investigation focuses on several key questions: how effectively continued pre-training can improve small language models' performance on educational domain tasks; what the optimal token volumes and training methodologies are for efficient model adaptation; what trade-offs emerge between general capabilities and domain specialization; and how catastrophic forgetting can be mitigated during continued training. \n\nMy contributions include a detailed analysis of continued pre-training effectiveness on small language models for educational domain adaptation. I present an incremental training approach demonstrating performance improvements from 400M to 1B tokens, along with empirical evidence of domain-specific trade-offs in benchmark performance. I also provide practical strategies for memory-efficient training that enable continued pre-training with limited computational resources, and offer quantitative evaluation across multiple benchmarks to provide insights into domain adaptation effects. \n\nThis research provides valuable insights for researchers and practitioners seeking to develop domain-specialized language models without the computational requirements of training from scratch. My findings suggest that continued pre-training can be a viable and efficient approach for adapting language models to specific domains, even with modest computational resources. Language model development typically follows a progression from pre-training to specialized adaptation. Pre-training is the initial phase where a model learns general language patterns from a large and diverse corpus, establishing a broad foundation in language understanding. From this foundation, practitioners have two primary paths for specialization: Fine-tuning adapts a pre-trained model to a specific task (e.g., question-answering, translation) using supervised learning on labeled task data.",
            "score": 0.38569248046445814,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 2070
                },
                {
                    "start": 2073,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2445
                },
                {
                    "start": 2446,
                    "end": 2549
                },
                {
                    "start": 2550,
                    "end": 2722
                },
                {
                    "start": 2723,
                    "end": 2947
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78662109375
        },
        {
            "corpus_id": "273661991",
            "title": "Abrupt Learning in Transformers: A Case Study on Matrix Completion",
            "text": "Large Language Models (LLMs) have revolutionized the field of natural language processing (NLP). However, there are still gaps in our understanding of these models, leading to challenges in controlling their behavior. As a pertinent example, the training of these models appears to demonstrate sudden improvements in metrics correlated with various capabilities [8], prompting questions about whether learning of a given capability can be predicted by tracking predefined progress measures and why such sudden changes occur. If undesirable capabilities can suddenly 'emerge' (despite any explicit supervision for them) [16], such sudden changes can be a challenge for AI regulation [21]. \n\nTo better understand such sudden changes during model training, this work investigates training BERT [12] on the classical mathematical task of low-rank matrix completion (LRMC) [6]. Making an analogy with masked language modeling (MLM), where sudden learning of syntactical structures was recently demonstrated [8], we argue matrix completion captures the core aspect of this learning problem (Fig. 1): given some relevant context (observed tokens), fill the missing elements (masked tokens). Specifically, we assume access to a matrix with some fraction of its entries missing, and would like to complete the missing entries of this matrix assuming the ground truth matrix is low-rank. We find that despite being a simplified abstraction of MLM, this setting already demonstrates a sharp decrease in loss as the model undergoes training (Fig. 1 (B)), preceded by a loss plateau for a significant number of training steps (akin to Chen et al. [8]). The simplicity of our setting further affords us interpretability, as we find that the point of sudden drop coincides with a precise change in how the model solves the task-we call this change an algorithmic transition. Specifically, we show that the pre-transition model simply copies the input (predicting 0 at masked positions), while the post-transition model accurately predicts missing values at masked positions. To perform the latter, distinctive changes occur in the model's attention heads during the period of sudden drop, wherein the model learns to identify relevant positional information to combine various elements in the input matrix and compute missing entries for matrix completion.",
            "score": 0.3856871831954551,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 687
                },
                {
                    "start": 690,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2341
                }
            ],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 365,
                    "matchedPaperCorpusId": "261822542"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 791,
                    "end": 795,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 868,
                    "end": 871,
                    "matchedPaperCorpusId": "8061516"
                },
                {
                    "start": 1002,
                    "end": 1005,
                    "matchedPaperCorpusId": "261822542"
                },
                {
                    "start": 1634,
                    "end": 1637,
                    "matchedPaperCorpusId": "261822542"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85986328125
        },
        {
            "corpus_id": "255595663",
            "title": "Counteracts: Testing Stereotypical Representation in Pre-trained Language Models",
            "text": "Pre-trained language models (PLMs) have gained a lot of attention due to their strong performance on natural language understanding tasks. Various kinds of knowledge have been encoded implicitly in the parameters of PLMs through large corpus training, allowing PLMs to succeed on downstream tasks, such as factual knowledge (Petroni et al. 2019;Rogers, Kovaleva, and Rumshisky 2021), commensense knowledge (Da et al. 2021), relational knowledge (Safavi and Koutra 2021), and linguistic knowledge (Peters et al. 2018;Goldberg 2019;Tenney et al. 2019). Along with knowledge, PLMs also learn human stereotypes contained in the training corpus, resulting in fairness issues that can benefit one group over another. Although the embedded knowledge can be altered by finetuning with large corpus, some tasks that have insufficient data may suffer from the internal stereotypical knowledge of out-of-box PLMs. Therefore, it is important to understand how to mitigate the implicit stereotypical knowledge within PLMs. \n\nRepetitive observed experience and actions contribute to the formulation of human semantic memory (Quillian 1967;Smith and Estes 1978), and such memory also includes stereotypes. An effective strategy to overcome the spontaneous stereotypical knowledge in semantic memory is using counterexamples (Finnegan, Oakhill, and Garnham 2015). For instance, one may refer a beautician as a female, and change such judgement by thinking \"a beautician can be a male\". By introducing counterexamples, human learn and update the semantic memory about beautician. Following examples of human semantic memory, we are interested in examining to what extent do PLMs process the counterexamples to overcome the internal stereotypical knowledge. In this paper, we add nine different types of knowledge to base gender stereotypical knowledge and evaluate the ability of using counterexamples on seven PLMs with different designs and model sizes. Unlike probing factual knowledge from PLMs, we expect evenly distributed gender preference instead of one true fact. If we treat what PLMs have already learnt as \"facts\" and counterexamples as \"fake information\", we also examine the robustness of PLMs in processing and retaining \"fake information\".",
            "score": 0.38563370909407724,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 345,
                    "matchedPaperCorpusId": "28072126"
                },
                {
                    "start": 345,
                    "end": 382,
                    "matchedPaperCorpusId": "211532403"
                },
                {
                    "start": 406,
                    "end": 422,
                    "matchedPaperCorpusId": "231698886"
                },
                {
                    "start": 496,
                    "end": 516,
                    "matchedPaperCorpusId": "237940293"
                },
                {
                    "start": 1309,
                    "end": 1346,
                    "matchedPaperCorpusId": "13780782"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.587890625
        },
        {
            "corpus_id": "270847724",
            "title": "Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability",
            "text": "We briefly summarize related work and provide a detailed discussion in Appendix A. \n\nLLMs are often Transformer-based (Vaswani et al., 2017) equipped with the enormous size of parameters and pretrained on vast training data. One key property that makes such LLM successful is called scaling law: Increasing the scale of language models (pretraining data scale, model parameters) can lead to better performance in downstream tasks. One ability that emerges as the model scale increases is in-context learning (ICL) (Brown et al., 2020). Given a sequence of labeled examples and a testing example (combined as a prompt), the model can construct new predictors for testing examples without further parameter updates Dong et al. (2022). Scaling law was first proposed by Kaplan et al. (2020) and then followed up by Hoffmann et al. (2022), emphasizing both the scale of models and training data. Recent works show LLMs with larger scales have distinct behaviors compared to smaller language models (Wei et al., 2023b;Shi et al., 2023b). This work investigates experiments and analyses how LLM can exhibit compositional ability in ICL. \n\nSolving complex tasks and reasoning is an active problem in the AI community Huang & Chang (2022). There is a line of empirical works investigating the compositional ability in linguistic fashion (Kim & Linzen, 2020;Levy et al., 2022;An et al., 2023a;b). LLMs are capable of learning abstract reasoning (e.g., grammar) to perform new tasks when finetuned or given suitable in-context examples. In our work, we include linguistic experiments as part of our testing suite, illustrating LLMs' compositional ability. Ye et al. (2023); Berglund et al. (2023); Dziri et al. (2023) show LLMs will struggle to solve tasks requiring reasoning. Berglund et al. (2023) studies that LLMs trained on \"A is B\" fail to learn \"B is A\". In our work, we conduct similar experiments showing LLMs will fail on composite if different steps of logical rules are mixed.",
            "score": 0.38538045910762503,
            "section_title": "Related Work",
            "char_start_offset": 5163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 85,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 514,
                    "end": 534,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1013,
                    "end": 1031,
                    "matchedPaperCorpusId": "270123533"
                },
                {
                    "start": 1329,
                    "end": 1349,
                    "matchedPaperCorpusId": "222290851"
                },
                {
                    "start": 1367,
                    "end": 1384,
                    "matchedPaperCorpusId": "257102348"
                },
                {
                    "start": 1646,
                    "end": 1662,
                    "matchedPaperCorpusId": "256826793"
                },
                {
                    "start": 1688,
                    "end": 1707,
                    "matchedPaperCorpusId": "258967391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63330078125
        },
        {
            "corpus_id": "269448752",
            "title": "Holmes: A Benchmark to Assess the Linguistic Competence of Language Models",
            "text": "We define the linguistic competence of LMs as their ability to understand a diversity of linguistic phenomena. Specifically, we focus on five phenomena types: morphology, the structure of words; syntax, the structure of sentences; semantics, the meaning of words; reasoning, the use of words in logical deduction and other related phenomena like negation or speculation; discourse, the context in text like rhetorical structure. Following Mahowald et al. (2024), we categorize these phenomena types into two groups: morphology and syntax are formal phenomena, which include understanding grammatical rules and statistical patterns, while functional ones (semantics, reasoning, and discourse) focus on practical abilities like interpreting text sentiment or detecting the existence of speculation. \n\nDatasets We define a dataset as text examples and labels covering a specific aspect of a linguistic phenomenon, like words and their POS tags. Typically, these labels are unambiguous, enabling us to assess the specific aspect under test in isolation. \n\nProbes Using probes, we empirically assess the linguistic competence of LMs regarding the featured linguistic phenomena in Holmes. We design probing tasks using the widely recognized classifier-based probing method (Tenney et al., 2019a;Hewitt and Manning, 2019;Belinkov, 2022) also known as diagnostic classifiers (Veldhoen et al., 2016;Giulianelli et al., 2018). Running such a probing task involves training a probe (linear model) using the specific dataset to test a distinct aspect of a linguistic phenomenon in isolation. To do this, we encode the text examples of a dataset with a given LM and use them to train the probe regarding the specific labels representing the tested linguistic phenomenon. The probe's performance is then used to approximate the LM's understanding of the specific phenomenon. A higher score indicates that LMs capture patterns relevant to this phenomenon internally, which in turn enhances the accuracy (Tenney et al., 2019b).",
            "score": 0.38531056884467624,
            "section_title": "Linguistic Phenomena",
            "char_start_offset": 7950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 439,
                    "end": 461,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 1267,
                    "end": 1289,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 1289,
                    "end": 1314,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 1314,
                    "end": 1329,
                    "matchedPaperCorpusId": "236924832"
                },
                {
                    "start": 1367,
                    "end": 1390,
                    "matchedPaperCorpusId": "16018322"
                },
                {
                    "start": 1390,
                    "end": 1415,
                    "matchedPaperCorpusId": "52090220"
                },
                {
                    "start": 1988,
                    "end": 2010,
                    "matchedPaperCorpusId": "108300988"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.822265625
        },
        {
            "corpus_id": "273901551",
            "title": "Working Memory Identifies Reasoning Limits in Language Models",
            "text": "We observe that despite the use of advanced prompting techniques, LLMs occasionally struggle to fully integrate extensive world knowledge or to derive and apply essential logical patterns effectively, particularly in overcoming typical errors. Additionally, these models face challenges in the temporary retention and logical manipulation of information, which are critical for successfully applying both knowledge and reasoning in task completion. \n\nTwo distinct trends emerge across the task categories. First, the effectiveness of advanced prompts varies with the model size. Smaller models, such as text-curie-002, and those with fewer parameters, often show decreased accuracy with advanced prompts compared to basic answer-only prompts. In contrast, larger models, beginning with textdavinci-001, exhibit significant performance improvements with advanced prompting strategies. This pattern aligns with findings from Wei et al. (2022b); Suzgun et al. (2023), suggesting that emergent capabilities from complex prompting strategies are more pronounced in larger models, while smaller models may not benefit as substantially, potentially due to distractions caused by intermediate reasoning steps. \n\nA second trend underscores the persistent gap in reasoning capabilities between LLMs and humans. Despite advancements, as depicted in Figure 3 and supported by Suzgun et al. (2023), LLMs have not consistently surpassed human intelligence, even with the latest and most sophisticated models like d-2. This observation raises a critical question: What intrinsic factors limit the cognitive abilities of LLMs as reasoners? Addressing this question is essential for understanding the reasoning capabilities of LLMs and how they compare to human cognition.",
            "score": 0.3850298642413678,
            "section_title": "Models",
            "char_start_offset": 8415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1755
                }
            ],
            "ref_mentions": [
                {
                    "start": 923,
                    "end": 941,
                    "matchedPaperCorpusId": "249674500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56201171875
        },
        {
            "corpus_id": "273901561",
            "title": "On Functional Competence of LLMs for Linguistic Disambiguation",
            "text": "Here is a bag filled with popcorn. There is no chocolate in the bag. The bag is made of transparent plastic, so you can see what is inside. Yet, the label on the bag says 'chocolate' and not 'popcorn.' Sam finds the bag. She had never seen the bag before. Sam reads the label. \n\nGPT3.5 was prompted with predicting the following: She believes that the bag is full of __, The machine got the answer right in the original task (chocolate), but not in the altered version. Given LLM's excellent linguistic ability and yetunproven performance on reasoning at the human level, researchers are apt to classify the LLM capabilities into two: formal and functional competencies. This motivation comes from observing brain activities. The language network in the human brain is quite distinct from the day-to-day reasoning center as revealed in fMRI scans (Mahowald et al., 2024). In other words, linguistic abilities should be separately considered from the world knowledge.",
            "score": 0.3847377028822835,
            "section_title": "Altered task:",
            "char_start_offset": 5309,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 276
                },
                {
                    "start": 279,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 966
                }
            ],
            "ref_mentions": [
                {
                    "start": 847,
                    "end": 870,
                    "matchedPaperCorpusId": "268551442"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2919921875
        },
        {
            "corpus_id": "221006230",
            "title": "Compositional Networks Enable Systematic Generalization for Grounded Language Understanding",
            "text": "One of the defining characteristics of human languages is that they are productive. We can combine together concepts in novel ways to express ideas that have never been thought of before. This is for a good reason: as children, we observe very little of our world before we must speak to others, meaning that even mundane language is novel and not just parroting back something already expressed for us. Similarly, even with massive data collection efforts, deep models can only have an opportunity to observe a small subset of the possible utterances and worlds. This problem becomes especially acute when those models must drive the behavior of a robot, because misunderstanding a command may pose a serious safety hazard.\n\nRecently, there have been a number of attempts to probe the understanding of deep networks trained to perform linguistic tasks. Lake and Baroni (2018) point out that generalization to novel compositions of concepts is rather limited. This is not a matter of the amount of data available; for example, McCoy et al. (2019) find that even networks with the same test set performance can have very different generalization abilities. More recently, Ruis et al. (2020) released gSCAN for testing the generalization abilities of grounded language understanding. In gSCAN, an agent must follow a natural-language command in a 2D environment. Commands of specific types are systematically held out; for example, no command with a particular adjective-noun combination appears in the training set. When the test set distribution is similar to the training set, performance is phenomenal: 97% of commands are executed correctly. Yet, when combinations are missing from the training set, such as holding out an adjective-noun pair like \"yellow squares\", only 24% to 55% of commands are executed correctly.\n\nGuided by the notion that compositionality is the central feature of human languages which deep networks are failing to internalize, we construct a compositional network to guide the behavior of agents. Given a command, a command-specific network is assembled from previously-trained modules. Modules are automatically discovered in the training set without any annotation. The network structure that combines those modules is derived from the linguistic structure of the command. In \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Visual feature CNN Blue Cylinder RNN show the parse tree, as produced by a constituency parser. This tree is the source of the structure found within the composition",
            "score": 0.38457712032482,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1171,
                    "end": 1189,
                    "matchedPaperCorpusId": "212658007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56494140625
        },
        {
            "corpus_id": "247656607",
            "title": "Emergent Structures and Training Dynamics in Large Language Models",
            "text": "In this paper, we provide an overview of research on network structure, linguistic feature learning, their training dynamics, and explainability research that aims to connect network structure and function. In doing so, we highlight gaps in the literature and opportunities for future research, both in each individual research area and as a broad proposal for grounding research in understanding large language models. We highlight a few areas of future research as particularly important given the gaps in the current literature. For the study of how, and whether, linguistic structures are learned by language models, more work is needed to understand the training dynamics of this learning across a variety of model scales and architectures. More fundamentally, there is disagreement about what it means for a model to \"encode\" linguistic structures such as syntax, particularly in a multilingual setting.\n\nMore broadly, nascent work on the selforganization of neurons and subnetwork structures that emerge during training time has largely not been applied to LLMs, or neural networks in NLP more generally. Research in Computer Vision has shown the existence of emergent functional units with functions that are semantically meaningful to humans. In the context of LLMs, such structures may provide a basis for understanding the nature of linguistic features that LLMs purportedly learn, especially when comparing the development of each during training time. Additional research is needed to not only determine whether such structures emerge in LLMs, but also to apply and ex-tend the literature on self-organization in complex systems. This research can also be used for explainability. Currently, assessment of the quality of interpretations of the information flow in neural models is not straightforward. Identification of modular and emergent structures within networks may be viewed as a way of moving away from the binary definition of faithfulness as postulated by Jacovi and Goldberg (2020). Evidence for the existence of structures aligning with human perception of language, if found, can help to enable separate consideration of plausibility from a human perspective, as proposed in the same study. More broadly, we propose grounding the study of LLMs properties in the analysis of the self-organization of weights and neurons into emergent structures.",
            "score": 0.38418699776386744,
            "section_title": "Conclusion and Future Directions",
            "char_start_offset": 34780,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1979,
                    "end": 2005,
                    "matchedPaperCorpusId": "215416110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72705078125
        },
        {
            "corpus_id": "251135127",
            "title": "An Interpretability Evaluation Benchmark for Pre-trained Language Models",
            "text": "Pre-trained LMs such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) have achieved significant gains in predictive accuracy on a variety of NLP tasks (Wang et al., 2018). Many studies have proved that pre-trained LMs have learned amounts of knowledge from the massive text corpora (i.e., their training data), such as linguistic knowledge (Tenney et al., 2019;Jawahar et al., 2019) and factual knowledge (Petroni et al., 2019;P\u00f6rner et al., 2019). Such learned knowledge has enhanced representations and capabilities of LMs, e.g., the abilities of reasoning (Brown et al., 2020) and computation (Polu and Sutskever, 2020). However, some works show that pre-trained LMs have not captured adequate knowledge and are insufficient in some aspects. Some studies find that BERT has not learned some syntactic structures and can not perform well on syntax-aware data (Wang et al., 2019;Min et al., 2020). Some works state that pre-trained LMs have a poor grasp of reasoning over factual knowledge and commonsense (P\u00f6rner et al., 2019;Marcus and Davis, 2020). Meanwhile, some researchers prove that pre-trained LMs have a poor performance on mathematical problem solving, even on simple problems (Hendrycks et al., 2021;Cobbe et al., 2021). Consequently, what kind of knowledge is learned and to what degree it is learned by pre-trained LMs are still unclear. Meanwhile, there is a lack of datasets for comprehensively evaluating model capabilities. \n\nOn the other hand, interpreting the decisionmechanism of a pre-trained LM which can help us understand the reason behind its success and its limitations has attracted lots of attention (Rethmeier et al., 2020;Meng et al., 2022;Mor Geva, 2022). With input saliency methods (Smilkov et al., 2017;Sundararajan et al., 2017), Ding and Koehn (2021) use the most influential tokens in the context as the rationale and evaluate interpretability from the perspective of grammar.",
            "score": 0.38417862913254386,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1454
                },
                {
                    "start": 1457,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1927
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 50,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 163,
                    "end": 182,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 352,
                    "end": 373,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 417,
                    "end": 439,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 873,
                    "end": 892,
                    "matchedPaperCorpusId": "202577372"
                },
                {
                    "start": 892,
                    "end": 909,
                    "matchedPaperCorpusId": "216553149"
                },
                {
                    "start": 1642,
                    "end": 1666,
                    "matchedPaperCorpusId": "208527431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60107421875
        },
        {
            "corpus_id": "262199094",
            "title": "Editorial: New ideas in language sciences: linguistics",
            "text": "Their study is notable for studying language skills in hearing-impaired populations and for highlighting the importance of studying diverse languages. \n\nA continuing trend toward empirically testing \"how language works\" outside the traditional healthy young adult and native speaker of English populations is evident. Although neurolinguistic approaches based on experimental data were not submitted to this Research Topic, the perspective study by Alekseeva et al. argues for neurocognitive experimentation in linguistic research to provide \"reality\" and ways to falsify linguistic concepts that have canonically been assumed to exist on a purely theoretical, or logical, basis. The authors build the case around the inflectional zero-morpheme, which is a null constituent found across different languages. It is hard to be empirically grounded as, by definition, it does not have any overt physical and measurable expression. Besides reviewing different theoretical viewpoints about the zero-morpheme, which are motivated by idiosyncrasies observed across languages, the authors propose an interesting experimental electrophysiological research program that could help shed light on the neurocognitive reality of this specific morphological aspect of language. \n\nWe conclude that studying diverse languages across populations reveals that the human language processor is flexible. Even if grammatical knowledge may govern human language processing through universal constraints, it seems that it readily adapts to language-specific characteristics. All in all, it emerges that research in linguistics should span various languages and diverse populations such that we could establish, with greater certainty, possibly universal processing constraints while defining the limits of linguistic flexibility.",
            "score": 0.38362571106203225,
            "section_title": "body",
            "char_start_offset": 4907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 153,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1805
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.139404296875
        },
        {
            "corpus_id": "265043674",
            "title": "Perturbed examples reveal invariances shared by language models",
            "text": "To systematically measure these shared invariances across various perturbations, we introduce an invariant sample generation approach (see Section 3) and propose novel metrics: Hard-SCoPE and Soft-SCoPE (see Figure 1). These metrics assess shared invariances independently of the agreement between model predictions, i.e., Hard-SCoPE and Soft-SCoPE can be high even if two models consistently disagree in their predictions, as long as they remain invariant to the same perturbations. Such scenarios present an interesting opportunity for investigation, as they highlight how significant design changes, like finetuning, may not necessarily alter the features a model treats invariant (and irrelevant), despite changing its specific predictions. \n\nWhile evaluating shared invariance is important, not all invariances are created equal: some may be desirable (e.g., invariance to synonym substitution for content moderation) while others may be undesirable (e.g., invariance to word order of image captioning). We enable the evaluation of specific shared invariances via interpretable input perturbations designed to target a specific linguistic capability (e.g., Synonym-Invariance, Typo-Invariance). A linguistic capability evaluates a model's competence on a particular aspect of knowledge and understanding required to solve an NLP task by validating its input-output behavior under the corresponding scenario. For instance, the linguistic capability 'Synonym-Invariance' evaluates whether a sentiment analysis model changes its prediction if the positive verb is replaced by its synonym. Hence, the generated perturbations along specific linguistic capabilities enable us to measure shared model invariances along different linguistic capabilities. \n\nWe demonstrate our proposed framework's utility in deriving novel insights on how changes in models such as distillation and increase/decrease in size affect shared invariances along multiple well-defined linguistic capabilities. We also show how our framework can be used to compare how invariances along different linguistic capabilities evolve over the course of pre-training for a particular model.",
            "score": 0.3835873674733329,
            "section_title": "Introduction",
            "char_start_offset": 1757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1751
                },
                {
                    "start": 1754,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2156
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6884765625
        },
        {
            "corpus_id": "269899510",
            "title": "Rethinking ChatGPT's Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs' Prompting",
            "text": "ChatGPT has emerged as the most popular AI application, with a vast user base.The success of GPT models can be attributed to the scaling of transformer-based neural networks and the extensive pre-training data, as explored in previous studies [Radford et al., 2019;Brown et al., 2020].The scope of this paper is directed towards Large Language Models (LLMs) that are sufficiently large to acquire world knowledge, commonsense, and the linguistic capabilities required to attain high performance on benchmarks such as GLUE [Wang et al., 2019].\n\nAlthough LLMs are commonly perceived as generalpurpose language intelligence models, the practice often diverges from employing a singular, all-encompassing model for every task.Instead, the deployment frequently entails developing a suite of specialized models tailored to specific tasks.This specialization is facilitated through the introduction of task-specific channels, modifying the model's structure or its pre-trained parameters to better suit the nuances of individual tasks.This highlights a departure from the ideal of a universal, one-size-fits-all model, while the broad capabilities of LLMs suggest they could serve as jack-of-all-trades in language processing.This trend towards creating taskspecific models may stem from the tradition of evaluating linguistic intelligence through a variety of distinct tasks and benchmarks [Wang et al., 2019], with researchers striving to excel in these tasks independently to set new benchmarks.In this paper, we delve into the mechanisms behind prevalent deployment paradigms including AR-LLMs' prompting, which underpins ChatGPT's operation, and highlight several critical observations: 1) Models tailored with optimized task-specific channels often suffer from issues related to task customizability, transparency, and user-level complexity during deployment, affecting their overall usability; 2) Anticipated to mimic human-like intelligence, they often exhibit slow thinking through shortcuts [Kahneman, 2011]; 3) They frequently fall short in showcasing advanced cognitive behaviors, which we contend are vital for convincing users of the models' intelligence.",
            "score": 0.38356999506354045,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 78,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 542
                },
                {
                    "start": 544,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 833
                },
                {
                    "start": 833,
                    "end": 1029
                },
                {
                    "start": 1029,
                    "end": 1220
                },
                {
                    "start": 1220,
                    "end": 1492
                },
                {
                    "start": 1492,
                    "end": 2163
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 265,
                    "matchedPaperCorpusId": "247155069"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56494140625
        },
        {
            "corpus_id": "273026167",
            "title": "Unveiling Language Skills via Path-Level Circuit Discovery",
            "text": "For a long time, two conjectures about language skills have been increasingly recognized. They are: \n\n1. Stratification: Simple language skills reside in shallow layers, whereas complex language skills are found in deeper layers. 2. Inclusiveness: Complex language skills are formed on top of simpler language skills. For Stratification, many works have already discovered its traces. For instance, mechanism interpretability studies [25,9] have found that circuits for simple skills (such as syntax) and complex skills (such as semantics) are almost spread across all layers, but there is a clear concentration: syntax are more concentrated in shallow layers while semantics are more concentrated in deep layers. As for Inclusiveness, most existing work has found some components with simple skill features in complex skills, such as the salient induction heads found in ICL tasks, hence conjecturing that the ICL skill includes the Induction skill. However, so far, there has been no quantitative experimental evidence to prove these 2 conjectures. For instance, which specific layers do simple skills reside in? Which specific paths in simple skills are encompassed by complex skills? Our work further confirms these hypotheses via quantitative discoveries.",
            "score": 0.3832814172581209,
            "section_title": "Discovery of Language Skills",
            "char_start_offset": 19085,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 99
                },
                {
                    "start": 102,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1260
                }
            ],
            "ref_mentions": [
                {
                    "start": 438,
                    "end": 440,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 1344,
                    "end": 1347,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 1368,
                    "end": 1371,
                    "matchedPaperCorpusId": "227275068"
                },
                {
                    "start": 1398,
                    "end": 1401,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 1414,
                    "end": 1417,
                    "matchedPaperCorpusId": "253244237"
                },
                {
                    "start": 1458,
                    "end": 1461,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 1511,
                    "end": 1514,
                    "matchedPaperCorpusId": "258418244"
                },
                {
                    "start": 1558,
                    "end": 1561,
                    "matchedPaperCorpusId": "253244237"
                },
                {
                    "start": 1611,
                    "end": 1614,
                    "matchedPaperCorpusId": "990233"
                },
                {
                    "start": 1645,
                    "end": 1649,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 1649,
                    "end": 1652,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 1666,
                    "end": 1668,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 1674,
                    "end": 1677,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 1764,
                    "end": 1766,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 1771,
                    "end": 1774,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 1776,
                    "end": 1779,
                    "matchedPaperCorpusId": "258418244"
                },
                {
                    "start": 1783,
                    "end": 1787,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 1787,
                    "end": 1789,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 1791,
                    "end": 1795,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 1795,
                    "end": 1798,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 1800,
                    "end": 1804,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 1839,
                    "end": 1842,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 1863,
                    "end": 1866,
                    "matchedPaperCorpusId": "227275068"
                },
                {
                    "start": 1931,
                    "end": 1934,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 1954,
                    "end": 1956,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 1958,
                    "end": 1962,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 1966,
                    "end": 1970,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 1970,
                    "end": 1973,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 1975,
                    "end": 1979,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 2049,
                    "end": 2052,
                    "matchedPaperCorpusId": "258418244"
                },
                {
                    "start": 2056,
                    "end": 2059,
                    "matchedPaperCorpusId": "258418244"
                },
                {
                    "start": 2066,
                    "end": 2068,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 2070,
                    "end": 2073,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 2077,
                    "end": 2080,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 2084,
                    "end": 2088,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 2092,
                    "end": 2096,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 2096,
                    "end": 2099,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 2101,
                    "end": 2105,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 2138,
                    "end": 2140,
                    "matchedPaperCorpusId": "258418244"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.744140625
        },
        {
            "corpus_id": "269921725",
            "title": "Asymptotic theory of in-context learning by linear attention",
            "text": "Since their introduction by Vaswani et al. in 2017 [1], Transformers have become a cornerstone of modern artificial intelligence (AI).Originally designed for sequence modeling tasks, such as language modeling and machine translation, Transformers achieve state-of-the art performance across many domains, even those that are not inherently sequential [2].Most strikingly, they underpin the breakthroughs achieved by large language models such as BERT [3], LLaMA [4], and the GPT series [5][6][7][8].\n\nThe technological advancements enabled by Transformers have inspired a substantial body of research aimed at understanding their working principles.One key observation is that language models gain new behaviors and skills as their number of parameters and the size of their training datasets grow [7,[9][10][11].A particularly important emergent skill is in-context learning (ICL), which describes the model's ability to learn and execute tasks based on the context provided within the input itself, without the need for explicit prior training on those specific tasks.To give an example from natural language processing, a pretrained large language model might be able to successfully translate English to Italian after being prompted with a few example translations, even if it has not been specifically pretrained on that translation task [7].ICL enables language models to perform new, specialized tasks without retraining, which is arguably a key reason for their general-purpose abilities.\n\nDespite many recent studies on understanding ICL, important questions about how and when ICL emerges in large language models are still mostly open.Large language models are trained (or pretrained) with a next token prediction objective.How do the different algorithmic and hyperparameter choices that go into the pretraining procedure affect ICL performance?What algorithms do Transformers implement for ICL?How many pretraining examples are required for ICL to emerge?How many examples should be provided within the input for the model to be able to solve an in-context task?How diverse should the tasks in the training dataset be for in-context learning of truly new tasks not observed in the training dataset?\n\nIn this paper, we address these questions by investigating the ICL capabilities of a linear attention module for linear regression tasks.",
            "score": 0.3832020681152716,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 355
                },
                {
                    "start": 355,
                    "end": 499
                },
                {
                    "start": 501,
                    "end": 649
                },
                {
                    "start": 649,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 1070
                },
                {
                    "start": 1070,
                    "end": 1347
                },
                {
                    "start": 1347,
                    "end": 1496
                },
                {
                    "start": 1498,
                    "end": 1646
                },
                {
                    "start": 1646,
                    "end": 1735
                },
                {
                    "start": 1735,
                    "end": 1857
                },
                {
                    "start": 1857,
                    "end": 1907
                },
                {
                    "start": 1907,
                    "end": 1968
                },
                {
                    "start": 1968,
                    "end": 2075
                },
                {
                    "start": 2075,
                    "end": 2211
                },
                {
                    "start": 2213,
                    "end": 2350
                }
            ],
            "ref_mentions": [
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 492,
                    "end": 495,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 798,
                    "end": 801,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 801,
                    "end": 804,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 808,
                    "end": 812,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1343,
                    "end": 1346,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "273215261",
            "title": "The Impact and Acceptance of Large Language Models in Healthcare: A Perspective from China",
            "text": "Advancements in large language models (LLMs) have fundamentally redefined our understanding of machine learning capabilities, particularly concerning the emergence of novel abilities and reasoning strategies. However, it is imperative to underscore their inherent limitations. i. Emergent Abilities and Scaling Effects: Recent advancements in LLMs have been characterized by the development of emergent abilities -capabilities that are not observable in smaller models but become apparent as the model size increases [11]. For instance, research has revealed that large-scale models have developed multistep reasoning abilities [12]. Such findings underscore the inherent unpredictability of emergent abilities, making it critical to investigate their origins, properties, and potential implications for future research in Natural Language Processing (NLP). ii. Chain-of-Thought Prompting: A Method for Eliciting Reasoning: The practice of chain-ofthought prompting has been identified as a potent tool for enhancing the performance of LLMs in multistep reasoning tasks [12]. This technique, which simulates the human reasoning process, represents an intriguing new frontier in our ongoing attempts to optimize LLMs' performance. This raises compelling questions about how far we can push these reasoning abilities with further model scale-ups and how different prompting methods could potentially expand the scope of tasks LLMs can accomplish. iii. Limitations and Interpretations of LLMs: Despite these promising advances, it is vital to acknowledge the limitations of LLMs. Studies have found that LLMs, including models like GPT, often stumble with certain types of questions and are not inherently designed to pass tests based on mathematical, semantic, and ethical principles [13]. Therefore, viewing LLMs as an emerging form of general artificial intelligence remains speculative at this stage. \n\nIn response to these limitations, there have been calls for the implementation of strategies to optimize the effective utilization of LLMs. Recommendations include developing a system of checks and balances to moderate AI's impact, improving our understanding of their behaviour, and fostering the responsible use of AI technology [14]. Emphasizing the importance of responsible AI stewardship, these suggestions aim to maximize the benefits of LLMs while mitigating their potential drawbacks. \n\nIn conclusion, state-of-the-art research in LLMs presents a vibrant field marked by significant achievements and unexplored potential.",
            "score": 0.3831910721718086,
            "section_title": "The state of the art in large language models",
            "char_start_offset": 30668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1901
                },
                {
                    "start": 1904,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2240
                },
                {
                    "start": 2241,
                    "end": 2397
                },
                {
                    "start": 2400,
                    "end": 2534
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "271430405",
            "title": "Modelling constructivist language acquisition through syntactico-semantic pattern finding",
            "text": "In particular, we provide a fully operational model of how a communicatively adequate linguistic system can be captured in the form of a collection of learned form-meaning mappings.These mappings can cover syntactico-semantic patterns of variable extent and degree of abstraction.The emerged constructions provide a unique insight into the compositional and non-compositional aspects of the learned language, as a consequence of the pattern-finding processes implemented by the repair strategies.Through these pattern finding processes, non-compositional pairings between aspects of observed form and meaning are included in constructions, while compositional aspects are generalized over through the use of variable slots.Like constructions, grammatical categories also emerge during the language acquisition process.In the spirit of radical construction grammar [4], categories are construction-specific and functionally motivated.They are conceived as fine-grained abstractions over observed syntacticosemantic usage patterns, and are captured in the form of a dynamic and adaptive categorial network.\n\nFinally, the methodology and experiments presented in this paper pave the way for learning computationally tractable, large-scale, usage-based grammars that facilitate both language comprehension and production.The proposed learning algorithm supports online, interactive, incremental, transparent and data-efficient learning.The learner builds up its human-interpretable inventory of constructions and categories through the application of transparent syntacticosemantic generalization processes.Already after a single observation, the fragment of linguistic knowledge acquired by the learner can be successfully used for language comprehension and production.As more and more utterance-meaning pairs are observed, the linguistic knowledge of the learner quickly expands and becomes better fit for achieving their communication goals.As a result of the dynamics of rewarding successful construction applications and punishing competing ones, the grammar of the learner remains ever-adaptive to any changes in the task or environment.Owing to their online, interactive, incremental, transparent and data-efficient nature, the proposed mechanisms for learning computational construction grammars that facilitate both language comprehension and production can serve as an excellent basis for implementing the language acquisition ability of autonomous agents.",
            "score": 0.3830058926468957,
            "section_title": "Discussion and conclusion",
            "char_start_offset": 68262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 181,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 496
                },
                {
                    "start": 496,
                    "end": 723
                },
                {
                    "start": 723,
                    "end": 818
                },
                {
                    "start": 818,
                    "end": 933
                },
                {
                    "start": 933,
                    "end": 1104
                },
                {
                    "start": 1106,
                    "end": 1317
                },
                {
                    "start": 1317,
                    "end": 1432
                },
                {
                    "start": 1432,
                    "end": 1603
                },
                {
                    "start": 1603,
                    "end": 1767
                },
                {
                    "start": 1767,
                    "end": 1941
                },
                {
                    "start": 1941,
                    "end": 2140
                },
                {
                    "start": 2140,
                    "end": 2463
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7353515625
        },
        {
            "corpus_id": "235539723",
            "title": "A model of forming polylingual speech culture and communicative competence as a condition for teachers\u2019 training",
            "text": "Organizational competence is broken down into two parts, grammatical competence and textual competence. Grammatical competence comprises knowledge of vocabulary, syntax, morphology and phonology/graphology. Textual competence involves 'the knowledge of the conventions for joining utterances together to form a text, which is essentially a unit of language -spoken or written -consisting of two or more utterances or sentences' (Bachman, 1990). \n\nPragmatic competence is composed of two sub-parts: illocutionary competence and sociolinguistic competence. Bachman lists four language functions as part of illocutionary competence: ideational (the way we convey meanings and experiences), manipulative (using language in an instrumental way to achieve ends), heuristic (the use of language to discover new things about our world and solving problems), and the imaginative function (using language beyond the 'here and now' (e.g. for humour or fantasy). \n\nThe second part of pragmatic competence is sociolinguistic competence. Sociolinguistic competence is sensitivity to the context where language is used, ensuring that language is appropriate to the person or the situation. This may entail sensitivity to differences in local geographical dialect, sensitivity to differences in register. Sociolinguistic competence also refers to sensitivity to speaking in a native-like or natural way. \n\nFrench linguist S. Savignon defined communicative competence as \"the ability to function in a real communication environment, that is, in a dynamic exchange of information, where linguistic competence must adapt to the reception of extensive information (both linguistic and paralinguistic) from one or more interlocutors\" (Savignon, 2002). \n\nThe model of communicative competence, offered by Savignon is presented in the form of \"inverted pyramid\". This model includes grammatical, discourse, strategic and sociocultural competences. At the same time, the language learner gradually expands his / her communicative competence through practice and experience in an ever-increasing range of communicative contexts and events. \n\nThe phenomenon of communicative competence of a specialist has been studied by such Russian scientists as T.V. Beloglazova, L.K. Salnaya and others.",
            "score": 0.3830000137652568,
            "section_title": "RESULTS",
            "char_start_offset": 22510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1387
                },
                {
                    "start": 1390,
                    "end": 1730
                },
                {
                    "start": 1733,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2114
                },
                {
                    "start": 2117,
                    "end": 2265
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3251953125
        },
        {
            "corpus_id": "248862979",
            "title": "LogiGAN: Learning Logical Reasoning via Adversarial Pre-training",
            "text": "Other tasks requiring specialized reasoning skills can be considered either as (i) providing sufficient premises but requiring specific ways of premise extraction to draw conclusions, such as multi-hop (Yang et al., 2018b) or hybrid (Chen et al., 2020) reasoning; or (ii) requires external knowledge, such as commonsense (Sap et al., 2020) or numerical (Dua et al., 2019) knowledge, as premises to draw conclusions, hence could also be interpreted by the two forms of logical reasoning. Following this analysis on the relation between logic and reasoning, Logic ability will be an essential foundation for a broad scope of reasoning, and should be prioritized in improving PLMs' reasoning abilities3 . \n\nConventional pre-training via randomized Masked Language Modeling (MLM) and auxiliary tasks are generally developed upon Firth (1957)'s distributional hypothesis of semantics -\"a word is characterized by the company it keeps.\" Under this paradigm, models efficiently learn to capture grammatical structures and contextualized semantics. However, since logical consistency is beyond correctness on a linguistic level, it is less obvious how MLM could help with logical reasoning abilities. Do models harvest logic ability for free from MLM? Or is that something that needs further learning beyond language acquisition? Motivated by these questions, we propose an unsupervised pretraining method aiming at enhancing the logical reasoning ability of PLMs: we automatically identify occurrences of logical reasoning phenomena in large corpus via detection heuristics, and then require PLMs to predict the masked-out logical statements made in the original context (Section 3). For example, in the case \"Bob recently made up his mind to lose weight. Therefore, [MASK]\", the prediction goal is the masked original statement \"he decides to go on a diet\". \n\nHowever, statements different from the original statement could also be logically consistent with respect to a given context. For example, \"he decides to exercise from today on\" is also a reasonable inference in the case above. Since Generators trained merely from recovering original statements are not encouraged to explore the possibilities of other reasonable statements, their overall learning effectiveness of logic could potentially be degraded. Therefore, it makes sense to provide additional feedback based on the degree of logical consistency between statements predicted beforehand and the original context -we realize this much resembles humans' reflective thinking process.",
            "score": 0.3828935997378297,
            "section_title": "Introduction",
            "char_start_offset": 1603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1851
                },
                {
                    "start": 1854,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2306
                },
                {
                    "start": 2307,
                    "end": 2540
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 222,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 233,
                    "end": 252,
                    "matchedPaperCorpusId": "215785913"
                },
                {
                    "start": 353,
                    "end": 371,
                    "matchedPaperCorpusId": "67855846"
                },
                {
                    "start": 825,
                    "end": 837,
                    "matchedPaperCorpusId": "208093066"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46044921875
        },
        {
            "corpus_id": "265157449",
            "title": "Are Large Language Model Temporally Grounded?",
            "text": "In this study, we examined to what extent large language models (LLMs) are temporally grounded: given a textual narrative describing a series of events, we propose a framework to probe their temporal reasoning capabilities. We consider a series of tasks including commonsense knowledge about the events, ordering events along a timeline, and satisfaction of temporal constraints (which ensures the self-consistency of the LLM's internal world model). Through a systematic evaluation, we find that state-of-the-art models such as GPT-4 still fall short of human performance and even small-scale, specialised models. Additionally, we observed that increasing the model scale or the number of incontext examples does not necessarily result in meaningful gains. Instruction-tuning and chain-ofthought prompting are more beneficial for accuracy and self-consistency but yield diminishing returns with scale. To explain these patterns, we highlight the limitations of pre-training and instruction tuning in achieving satisfactory temporal grounding and suggest potential future directions. These include exploring new training paradigms, such as leveraging action and perception in physical or simulated environments.",
            "score": 0.3827839393543341,
            "section_title": "Conclusions",
            "char_start_offset": 27621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1211
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77197265625
        },
        {
            "corpus_id": "267642928",
            "title": "Does word knowledge account for the effect of world knowledge on pronoun interpretation?",
            "text": "Existing research suggests that as the size and training data of language models increases, so does their performance. A future, truly massive language model may be able to capture all of the variance in responses which here is explained by world knowledge. However, current language models are already psychologically implausible as models of human cognition. Children are estimated to be exposed to around 3-11 million words per year, for a total of 30-110 million words by the time they reach adult-like linguistic competence at age 10 (Hart & Risley, 1992;Hosseini et al., 2022). By contrast, GPT-3the model used in our analysishas been exposed to more than 200 billion words: ~2000 times that of a 10 year old (Warstadt & Bowman, 2022). While larger and better-trained models may be able to tell us more about what is learnable in principle from distributional information, evidence that this is a possible mechanism for human language comprehension will need to come from more developmentally plausible models. \n\nFinally, the results suggest that non-linguistic information and reasoning abilities exert influence on a core language comprehension process: reference assignment. Comprehenders were able to use a wide variety of physical knowledge to compare the plausibility of events while resolving pronouns. What resources underlie the rapid deployment of this physical knowledge during language comprehension? Battaglia et al. (2013) propose that humans are equipped with an intuitive physics engine (IPE), which they can use to simulate hypothetical situations and predict their outcomes. Previous research has tested this claim on non-linguistic stimuli, but future work should examine whether the IPE can also explain physical inferences during language comprehension. Similarly, Barsalou (1999) proposes that language comprehension involves relating linguistic information to multimodal perceptual symbols grounded in sensorimotor experience. Activation of embodied perceptual symbols provides an intuitively plausible hypothesis about how world knowledge can be leveraged so efficiently to influence language interpretation (Zwaan, 2016). However, more work is needed to test whether sensorimotor processes are causally involved in comprehension more generally (Ostarek & Bottini, 2021), and in knowledge-driven inference specifically. \n\nUnderstanding language necessarily involves connecting words to the world around us.",
            "score": 0.38263670500700614,
            "section_title": "General discussion",
            "char_start_offset": 77336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2349
                },
                {
                    "start": 2352,
                    "end": 2436
                }
            ],
            "ref_mentions": [
                {
                    "start": 539,
                    "end": 560,
                    "matchedPaperCorpusId": "143528292"
                },
                {
                    "start": 715,
                    "end": 740,
                    "matchedPaperCorpusId": "251623202"
                },
                {
                    "start": 1419,
                    "end": 1442,
                    "matchedPaperCorpusId": "1596551"
                },
                {
                    "start": 2138,
                    "end": 2151,
                    "matchedPaperCorpusId": "4580541"
                },
                {
                    "start": 2275,
                    "end": 2300,
                    "matchedPaperCorpusId": "231720302"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.474609375
        },
        {
            "corpus_id": "259076231",
            "title": "Second Language Acquisition of Neural Language Models",
            "text": "We observed that, for example, feeding L1-L2 translation pairs into LMs slowed down their L2 grammar acquisition, compared to only feeding L2 monolingual texts every two epochs. \n\nIn our main experiments, we conducted exploratory analyses of the effects of L1 training on L2 grammar acquisition (Section 4). We gained three main insights. First, L1 knowledge promotes better linguistic generalization in L2 (Section 4.1). Second, different L1s incur different generalizations in L2 (Section 4.2). More specifically, Japanese and Russian are far behind French and German, which is consistent with the human-defined difficulty levels of language transfer (Chiswick and Miller, 2004). Third, L1 pretraining gives different effects on different types of grammar items (Section 4.3). In particular, morphological and syntactic items get larger gains than semantic and syntax&semantic items. \n\nIn more detail, we analyzed the process of L2 acquisition (Section 5). We investigated how L2 knowledge acquisition progresses (Section 5.1) and found that L2 knowledge acquisition does not progress so much until seeing the whole dataset many times (e.g., 50-100 times), implying their data inefficiency. Furthermore, we also observed the L1 knowledge degrade during L2 acquisition; this motivates us to balance the source-target linguistic knowledge during language transfer. \n\n2 Second language acquisition of LMs Overview: We are interested in how L1 knowledge affects the linguistic generalization of LMs in L2. Figure 1 shows an overview of the experimental procedure. First, in our L1 acquisition simulation, we train LMs on a monolingual corpus of a specific language. Second, in our L2 acquisition simulation, we additionally train the pretrained LMs with a corpus including L2 texts (English). Finally, we evaluate the grammatical judgment ability of the LMs in the L2 (English) using BLiMP (Warstadt et al., 2020).",
            "score": 0.3826288718275746,
            "section_title": "Introduction",
            "char_start_offset": 1885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 180,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 885
                },
                {
                    "start": 888,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1364
                },
                {
                    "start": 1367,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1912
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67236328125
        },
        {
            "corpus_id": "252531660",
            "title": "Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts",
            "text": "Large Language Models (LMs) pretrained on a vast amounts of corpora have shown surprising, even emergent, capabilities of solving various downstream tasks through prompts (instructions) [5,16,6,25,23]. Previous work has specifically shown LMs can perform unseen tasks through multitask fine-tuning on various downstream tasks with prompts [19,22,20,14]. A 540B LM [6] has even shown the capability to act as the \"brain\" for actual robots, helping them perform different tasks in the real-world [1]. As LMs are trained to become more aligned with human values, perform real-world tasks, and are endowed with responsibilities that may result in real-world consequences, it is more-so important to ensure that LMs actually do what they are instructed to do. \n\nIn this work, we test the capabilities of Language Models (LMs) on truly following the given instructions (prompts) by conducting a case study with negated instructions; that is, telling the LM NOT to do something as shown by an example in Figure 1. Prior work [8,21] has shown that LMs (as well as other large pretrained models in different modalities such as DALLE-2 [17]) have a hard time understanding negated prompts and perform the task as if provided with the original prompt. For example, if we prompt DALLE-2 this prompt: \"Do not generate a monkey holding a banana\", it will generate an image with a monkey holding a banana. We hypothesize this is due to the syntax of  the negated prompts being rarely seen or out-of-distribution during initial pretraining or during the adaptation phase. \n\nWe aim to answer four main questions in this work. (1) How does scaling the size of LMs affect their abilities to understand the concept of negation? (2) Are LMs explicitly trained to follow instructions (InstructGPT) better at understanding negated instructions? (3) Can In-Context Learning (ICL) or Fine-tuning (FT) help mitigate this problem?",
            "score": 0.38262444868749923,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1555
                },
                {
                    "start": 1558,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1903
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 189,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1018,
                    "end": 1021,
                    "matchedPaperCorpusId": "199001173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "276557765",
            "title": "Contextualizing Search Queries In-Context Learning for Conversational Rewriting with LLMs",
            "text": "Large Language Models (LLMs) have emerged as transformative technologies in Natural Language Processing (NLP), demonstrating remarkable capabilities across a wide spectrum of tasks, including text generation, machine translation, and complex reasoning [14]. The generalization capabilities of LLMs, especially in multi-capability scenarios, are a subject of ongoing research [15]. These models, typically based on the Transformer architecture [16], are pre-trained on massive text corpora, enabling them to acquire extensive world knowledge and linguistic proficiency. The groundbreaking work on Transformer networks [16] laid the foundation for modern LLMs, introducing the attention mechanism as a core building block and demonstrating its effectiveness in capturing long-range dependencies in text. \n\nEarly LLMs, such as GPT [17], showcased the potential of generative pre-training for improving language understanding. Pre-trained models like EventBERT demonstrate the effectiveness of this approach for tasks like event correlation reasoning [11]. GPT and its successors, like GPT-3 [18], demonstrated impressive few-shot learning abilities, enabling them to perform novel tasks with only a few examples provided in the input prompt. Visual in-context learning has been shown to be effective for VLMs [3]. [18] highlighted the in-context learning paradigm, where LLMs adapt to new tasks without explicit gradient updates, simply by conditioning on input-output demonstrations. Scaling laws for neural language models have further revealed that model performance consistently improves with increasing model size, dataset size, and computational resources, suggesting a path towards even more powerful LLMs by scaling up training [19]. Research also explores training specialized VLMs, such as for medical applications, using techniques like abnormal-aware feedback [20]. Furthermore, understanding and improving long-context reasoning in VLMs is an important direction [21]. \n\nBeyond model architecture and scale, prompting strategies have become increasingly important for eliciting desired behaviors from LLMs. Chain-of-Thought (CoT) prompting, introduced by [22], is a notable example, demonstrating that providing step-by-step reasoning examples in prompts can unlock and enhance the reasoning capabilities of LLMs on complex tasks.",
            "score": 0.38230348970801564,
            "section_title": "B. Large Language Models",
            "char_start_offset": 7589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1978
                },
                {
                    "start": 1981,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2340
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "275405666"
                },
                {
                    "start": 443,
                    "end": 447,
                    "matchedPaperCorpusId": "243936778"
                },
                {
                    "start": 617,
                    "end": 621,
                    "matchedPaperCorpusId": "243936778"
                },
                {
                    "start": 1047,
                    "end": 1051,
                    "matchedPaperCorpusId": "238744494"
                },
                {
                    "start": 1088,
                    "end": 1092,
                    "matchedPaperCorpusId": "248987346"
                },
                {
                    "start": 1306,
                    "end": 1309,
                    "matchedPaperCorpusId": "267750174"
                },
                {
                    "start": 1311,
                    "end": 1315,
                    "matchedPaperCorpusId": "248987346"
                },
                {
                    "start": 1733,
                    "end": 1737,
                    "matchedPaperCorpusId": "210861095"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89990234375
        },
        {
            "corpus_id": "273186633",
            "title": "Revisiting the Superficial Alignment Hypothesis",
            "text": "The Superficial Alignment Hypothesis posits that almost all of a language model's abilities and knowledge are learned during pre-training, while post-training is about giving a model the right style and format. We re-examine these claims by empirically studying the scaling behavior of post-training with increasing finetuning examples and evaluating them using objective task-specific standardized benchmarks. Through experiments with the Llama-3, Mistral, and Llama-2 model families of multiple sizes, we observe that, similar to the pre-training scaling laws, post-training task performance scales as a power law against the number of finetuning examples. This power law relationship holds across a broad array of capabilities, including mathematical reasoning, coding, instruction following, and multihop-reasoning. In addition, for tasks like math and multihop reasoning, we observe that a handful of examples merely align the model stylistically but do not saturate performance on the benchmarks. Model performance is instead correlated with its reasoning ability and it improves significantly with more examples, illustrating the need for holistic evaluation programs leveraging objective benchmarks in addition to measurement of alignment to human preferences. We also observe that language models are not necessarily limited to using knowledge learned during pre-training. With appropriate post-training, a model's ability to integrate new knowledge greatly improves on downstream tasks like multihop question-answering. Taken together, these results shed new light on the Superficial Alignment Hypothesis, suggesting that it is, at best, an over-simplification.",
            "score": 0.3822884136400235,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "258959069",
            "title": "Syntax and Semantics Meet in the \u201cMiddle\u201d: Probing the Syntax-Semantics Interface of LMs Through Agentivity",
            "text": "and pragmatics, they do not investigate the interaction between the meanings associated with syntactic forms and those of individual lexical items.\n\nThus, we not only need to evaluate syntax and utilization of semantic knowledge, but we also need to understand how interactions of meaning at different linguistic levels-i.e. morphological, lexical, phrasal-may alter model behavior. Exploring phenomena within the syntax-semantics interface is a compelling approach as it gives us access to specific aspects of semantics while allowing precise control over syntactic form between levels.\n\nIn this work, we probe the syntax-semantics interface of several language models, focusing on the semantic notion of agentivity. We do this by prompting models to label nouns in isolation or in context as either agents or patients from a curated test set of noun-verb-adverb combinations that dis-(to) this author easily). Regardless, given that the agent and patient roles as defined by Dowty (1991) are prototypes on a scale, this author in the recipient reading is closer to the patient role. play the alternation shown in example (1). We then compare the performance of LMs to both human judgements and corpus statistics.\n\nProbing for LMs for their knowledge of agentivity in syntactic constructions as in (1) and (2) is a particularly insightful case study as it allows us to explore three interconnected questions in a highly controlled syntactic setting: I. Do models display sensitivity to aspects of word-level semantics independent of syntactic context, and is such sensitivity aligned with human judgements? ( \u00a73.1) II. Can models employ lexical semantics to determine the appropriate semantics of a sentence where the syntax is ambiguous between readings (as in 1)? ( \u00a73.2) III. Can models determine the semantics of a sentence from syntax, disregarding lexical semantics when necessary (as in 2)? ( \u00a73.3)\n\nAdditionally, the relatively infrequent pairings of semantic function and syntactic form of sentences such as (1b) are also interesting from a learnability and acquisition perspective for both LMs and humans. How both come to process and acquire exceptions to a general \"rule\" has been a topic of debate since early connectionist models (Rumelhart and McClelland, 1986). Hence",
            "score": 0.38227067649155194,
            "section_title": "Introduction",
            "char_start_offset": 3961,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.544921875
        },
        {
            "corpus_id": "267412655",
            "title": "A Survey to Recent Progress Towards Understanding In-Context Learning",
            "text": "How the skill learning ability emerges during pretraining. The emergence of the skill learning ability can be partially attributed to the skewed rank-frequency distribution of pre-training corpora (Chan et al., 2022), and Reddy (2023) highlights the role of the induction head (Olsson et al., 2022), a particular attention head which explicitly searches for a prior occurrence of the current token in-context and copying the suffix as prediction. Moreover, the function class-based analysis (Raventos et al., 2023) illustrates that the transition from skill recognition to skill learning only happens given diverse enough tasks in pre-training corpora. It is interesting to explore how these factors collaboratively influence the emergence of skill learning. Extending existing findings to other capabilities of LLMs. \n\nDespite the very original ICL being for classification tasks, more capabilities are observed, e.g., step-by-step reasoning ability (Wei et al., 2022b) for reasoning and selfcorrection (Ganguli et al., 2023). A critical question is how we can extend the understanding frameworks introduced in this paper, particularly the data generation perspective, to more complicated LLMs' capabilities. Some pioneering research has been done; Prystawski & Goodman (2023) extends the Bayesian inference framework to understand the effectiveness of the CoT prompt. Kadavath et al. (2022) focuses on the self-evaluation prompt showing that LLMs can accurately examine the correctness of their statements. We believe the introduced data generation perspective and two main understanding frameworks on ICL serve us the milestone to explore more intrinsic capacities of LLMs.",
            "score": 0.3822653032076066,
            "section_title": "Future Directions",
            "char_start_offset": 30208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 59,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 817
                },
                {
                    "start": 820,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1676
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 216,
                    "matchedPaperCorpusId": "248665718"
                },
                {
                    "start": 491,
                    "end": 514,
                    "matchedPaperCorpusId": "259261789"
                },
                {
                    "start": 951,
                    "end": 970,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6103515625
        },
        {
            "corpus_id": "270845487",
            "title": "Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization",
            "text": "With the rapid advancement of Large Language Models (LLMs), research interest has increasingly centered on their reasoning capabilities, particularly in solving complex questions. While many studies have assessed the general reasoning capabilities of LLMs (Wei et al., 2022a;Qin et al., 2023;Srivastava et al., 2023), the specific aspect of how these models recall and then utilize factual knowl- edge during reasoning has not been thoroughly explored. Some research (Dziri et al., 2023;Press et al., 2023;Wang et al., 2024) concentrate on straightforward reasoning tasks such as combining and comparing simple biographical facts to investigate the implicit reasoning skills of LLMs. However, real-world questions often demand more intricate reasoning processes that cannot be easily broken down into simple factual units. For instance, as presented in Figure 1, to answer \"Why does ReLU training take less time than sigmoid or tanh training?\", one must not only recall what an activation function is but also compare the characteristics of activation functions and understand the causal relationship between gradients and training speed. This type of reasoning requires drawing conclusions beyond simply aggregating facts. \n\nTo analyze the reasoning ability of LLMs in solving real-world questions, we propose a deconstruction of complex questions into a graph structure. In this structure, each node is represented by a question that signifies a specific level of knowledge. We adopt Webb's Depth of Knowledge (Webb, 1997(Webb, , 1999(Webb, , 2002)), which assesses both the content and the depth of understanding required. Webb's Depth of Knowledge categorizes questions into three levels: mere recall of information (D 1 ), application of knowledge (D 2 ), and strategic thinking (D 3 ). \n\nThe transition from shallower to deeper nodes involves applying the knowledge gained from shallower nodes and performing reasoning to tackle harder problems. This approach emphasizes the gradual accumulation and integration of knowledge to address real-world problems effectively. \n\nWe introduce the resulting DEPTHQA, a collection of deconstructed questions and answers derived from human-written, scientific D 3 questions in the TutorEval dataset (Chevalier et al., 2024).",
            "score": 0.381936967242924,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1791
                },
                {
                    "start": 1794,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2074
                },
                {
                    "start": 2077,
                    "end": 2268
                }
            ],
            "ref_mentions": [
                {
                    "start": 275,
                    "end": 292,
                    "matchedPaperCorpusId": "256827430"
                },
                {
                    "start": 487,
                    "end": 506,
                    "matchedPaperCorpusId": "252762102"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6591796875
        },
        {
            "corpus_id": "277398866",
            "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
            "text": "The core principles of LLMs are fundamentally rooted in the Transformer architecture [26]. This architecture has revolutionized the field of natural language processing by introducing several key features that enhance model performance and efficiency. LLMs leverage a two-phase training approach that includes pre-training on large-scale text data followed by fine-tuning on specific tasks. This strategy allows the models to learn rich linguistic knowledge and contextual information, making them highly versatile and powerful for a wide range of applications. \n\n\u2022 Self-Attention Mechanism: One of the most significant innovations in the Transformer architecture is the self-attention mechanism. This mechanism captures long-range dependencies and contextual information by computing the similarity between each element in the input sequence and all other elements. Unlike traditional sequential processing methods, which process tokens one at a time, the self-attention mechanism allows the model to focus on different parts of the input simultaneously. This parallel processing capability improves both training and inference efficiency, enabling the model to handle larger datasets and more complex tasks [26]. \n\nThe self-attention mechanism ensures that the model can dynamically weigh the importance of different words in a sequence based on their relevance to the current context, leading to more accurate and coherent text generation. \n\n\u2022 Simultaneous Processing: Another crucial advantage of the Transformer architecture is its ability to process all elements of the input sequence simultaneously. This is a departure from the sequential nature of earlier models like RNNs and LSTMs, which process tokens in a stepby-step manner. The simultaneous processing enabled by Transformers not only accelerates the training process but also allows the model to maintain a global view of the entire sequence, enhancing its ability to capture complex dependencies and interactions between words. \n\n\u2022 Pre-Training Phase: During the pre-training phase, the model is exposed to a vast corpus of text data, learning to predict the next word in a sentence. This unsupervised learning process enables the model to acquire a deep understanding of syntax, semantics, and world knowledge. By learning from diverse and extensive text data, the model builds a comprehensive language representation that can be applied to various downstream tasks [28]. The pre-training phase is crucial as it equips the model with a broad base of knowledge that can be fine-tuned for specific applications.",
            "score": 0.38180665414069753,
            "section_title": "4) CORE PRINCIPLES OF LLMs",
            "char_start_offset": 16536,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1442
                },
                {
                    "start": 1445,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1994
                },
                {
                    "start": 1997,
                    "end": 2150
                },
                {
                    "start": 2151,
                    "end": 2278
                },
                {
                    "start": 2279,
                    "end": 2439
                },
                {
                    "start": 2440,
                    "end": 2577
                }
            ],
            "ref_mentions": [
                {
                    "start": 2434,
                    "end": 2438,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "233289478",
            "title": "Probing Across Time: What Does RoBERTa Know and When?",
            "text": "Learning dynamics Early work (McClelland and Rumelhart, 1986) observed the dynamics in a feedforward neural network to assess the cognitive plausibility of a connectionist model. They found staged learning in past tense acquisition, similar to humans. More recently, Saphra and Lopez (2019) studied linguistic and topic learning dynamics in hidden states of an LSTM language model. They found that syntactic information is encoded at an early training stage, which is consistent with our finding despite the difference in training objective and network architecture. Our work, instead, uses a rich set of probes to examine more diverse aspects of language and analyzes training iterations.\n\nConcurrent work (Chiang et al., 2020) is the closest work to ours and uses probes to investigate the learning dynamics as well. However, they find that linguistic knowledge and factual knowledge do not generally improve as pretraining proceeds, we find that factual and commonsense knowledge do ( \u00a73); we attribute such difference to our more systematic choice of probes -not only adding two more categories (i.e. commonsense and reasoning) but also more tasks in the linguistic and factual knowledge categories. For example, we found that the factual knowledge probed using SQuAD and ConceptNet data still increases as the pretraining progresses. However, Chiang et al. (2020) only used a subset of T-REx, which plateaus quickly according to our experiments. Sharing their concern of how data affects pretraining, we empirically investigate how the domain of pretraining corpus affects the dynamics of different types of knowledge.\n\nZhang et al. (2020) investigate masked language models trained on corpora of varying sizes in a domain. They experiment with linguistic probes and show that 90% of the improvement in syntactic probing performance can be achieved with a pretraining corpus of only about 10M words. In contrast, the probing performance in commonsense knowledge suffers from small training data. Different from the final models obtained in their work, we consider the entire pretraining trajectories. Nevertheless, our findings by varying corpus size ( \u00a74) are consistent with their conclusion and additionally we find that adding more data in diverse domains can improve both types of knowledge. Their results on the relation between the corpus size and probing performance also support our finding that linguistic knowledge is generally easy to learn",
            "score": 0.3817846813615747,
            "section_title": "Related Work and Further Discussion",
            "char_start_offset": 21352,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "210898819",
            "title": "Criticality in Pareto Optimal Grammars?",
            "text": "indicate that discrete approaches would be possible to describe language without missing any detail by ignoring the intermediate possibilities. If that were the case, we would still need to analyze the emerging models and look at similarities between them to understand whether both models capture a same core phenomenology at two relevant (yet distant) scales; or whether each model focuses on a specific, complementary aspect that the other description has no saying about. Some elements in Figure 3b are compatible with this kind of phase transition.\n\nHowever, the disposition of several Pareto optimal grammars along a seemingly straight line rather suggests the existence of a special kind of critical phenomenon [28,37,[46][47][48]. Criticality is a worst-case scenario in terms of description. It implies that there is no trivial model, nor couple of models, nor relatively small collection that can capture the whole of linguistic phenomenology at any level. A degenerate number of descriptions is simultaneously necessary, and elements trivial in a level can become cornerstones of another. Also, potentially, constraints imposed by a linguistic domain (e.g., phonology) can penetrate all the way and alter the operating rules of other domains (e.g., syntax or semantics). We can list examples of how this happens in several tongues (such as the case of determiners \"a\" and 'an' in English mentioned above). The kind of criticality suggested by our results would indicate that such intrusions are the norm rather than the exception. Note that this opportunistic view of grammar appears compatible with Christiansen's thesis that language evolved, as an interface, to make itself useful to our species, necessarily exploiting all kinds of hacks along its way [9].\n\nZipf's law is a notable distribution in linguistics [54,55]. It states that the n-th most abundant word in a text shows up with a frequency that is inversely proportional to that word's rank (i.e., n). The presence of this distribution in linguistic corpora has been linked to an optimal balance between communicative tensions [54,56,57]. It has also been proved mathematically that Zipf's law is an unavoidable feature of open-ended evolving systems [58]. Languages and linguistic creativity are candidates",
            "score": 0.38161054646185427,
            "section_title": "Discussion",
            "char_start_offset": 35536,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 722,
                    "end": 725,
                    "matchedPaperCorpusId": "703231"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "7867675"
                },
                {
                    "start": 729,
                    "end": 733,
                    "matchedPaperCorpusId": "5867379"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "24680904"
                },
                {
                    "start": 1767,
                    "end": 1770,
                    "matchedPaperCorpusId": "13365087"
                },
                {
                    "start": 1829,
                    "end": 1832,
                    "matchedPaperCorpusId": "17300076"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.102294921875
        },
        {
            "corpus_id": "271270400",
            "title": "Identifying Smart Contract Security Issues in Code Snippets from Stack Overflow",
            "text": "LLMs typically undergo pre-training and fine-tuning during their training process.In the pre-training phase, the model is exposed to extensive text data to acquire linguistic knowledge that includes grammar, context, and semantics [57].Following completion of pretraining, the model often undergoes fine-tuning to tailor its capabilities to specific tasks, e.g., code generation and summarization [55].The fine-tuning stage generally employs supervised learning [14], utilizing labeled data for additional training to adjust model parameters and align with the requirements of the targeted tasks.\n\nInference refers to the process in which a trained model generates output based on input data [4].This typically occurs when the model has completed training and is prepared to process realworld data.This step is crucial to applying the model to practical problems and tasks.LLMs suffer from instability (i.e., the responses generated each time are different) and hallucinations (i.e., the responses generated contain things that have not appeared in context) during inference due to sensitivity to adversarial samples [68], overfitting [29], and complex context.",
            "score": 0.3815332626207112,
            "section_title": "Pre-training, Fine-tuning and Inference",
            "char_start_offset": 10315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 82,
                    "end": 236
                },
                {
                    "start": 236,
                    "end": 402
                },
                {
                    "start": 402,
                    "end": 596
                },
                {
                    "start": 598,
                    "end": 696
                },
                {
                    "start": 696,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 873
                },
                {
                    "start": 873,
                    "end": 1161
                }
            ],
            "ref_mentions": [
                {
                    "start": 397,
                    "end": 401,
                    "matchedPaperCorpusId": "153312532"
                },
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "1281211"
                },
                {
                    "start": 1117,
                    "end": 1121,
                    "matchedPaperCorpusId": "204943093"
                },
                {
                    "start": 1135,
                    "end": 1139,
                    "matchedPaperCorpusId": "12440383"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "267523425",
            "title": "Six Fallacies in Substituting Large Language Models for Human Participants",
            "text": "For instance, in linguistic processing, layers of a language model can be probed to understand how they internally represent different syntactic and semantic structures; early layers tend to represent low-level syntactic features (e.g., part-of-speech tags, which are labels assigned to each word to indicate their grammatical function, such as noun, verb, and preposition), while later layers encode more complex semantic relationships 111 . This helps map specific nodes or attention heads to linguistic tasks, shedding light on how human language processing might work 17 . \n\nHowever, while probing techniques offer insights into the internal representations of language models, they come with limitations. Perhaps the most critical is the lack of causality: high performance in classification or decoding may not reflect what the model uses functionally for its primary tasks, but the probe's capacity to extract information (e.g., superficial correlations). Conversely, just because a feature is not captured by the probing classifier does not mean it is not encoded somewhere in the model. Thus, experimental techniques are needed to bring stronger evidence by manipulating model inputs and model architecture 110 . \n\nOne key experimental method involves manipulating the input data fed into language models-analogous to controlled rearing in animal studies-to observe how different training conditions affect model behavior and performance. Just as newborn chicks' visual experiences can be manipulated (e.g., slow or fast object motion) to reveal the core learning algorithms that support object perception, input manipulations in language models help assess which specific types of input are necessary for learning. For example, by removing instances of specific constructions from the input corpus, like AANN phrases (Article + Adjective + Numeral + Noun; \"a beautiful five days\"), one can ask whether the model is able to infer such structures from related ones. A recent study shows that exposure to simple noun phrases (\"a few days\")-but not counterfactual versions of the AANN construction, like ANAN (e.g., \"a five beautiful days\") and NAAN (e.g., \"five beautiful a days\")-provides scaffolding for generalization across linguistic constructions 112 , akin to how structured sensory data helps newborn animals learn complex visual tasks.",
            "score": 0.3814802052328762,
            "section_title": "Using language models to model cognitive processes",
            "char_start_offset": 51863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1221
                },
                {
                    "start": 1224,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 572,
                    "end": 574,
                    "matchedPaperCorpusId": "219315567"
                },
                {
                    "start": 1216,
                    "end": 1219,
                    "matchedPaperCorpusId": "236924832"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "245987756",
            "title": "The Neural Correlates of Semantic and Grammatical Encoding During Sentence Production in a Second Language: Evidence From an fMRI Study Using Structural Priming",
            "text": "Speaking is an automatic yet highly complex process. According to one widely cited model of speech production (Figure 1), it involves the generation of a preverbal message (conceptualization), translating it into a grammatical linguistic form (formulation), and articulating the phonetic plan (articulation) (Levelt, 1989). During conceptualization, semantic encoding occurs, which converts a non-verbal mental representation of the entity to be expressed (reference) into a semantic structure suitable for expression (sense) (Menenti et al., 2012a). Therefore, sense is the interface between conceptualization and formulation. Moreover, the formulation process involves grammatical encoding (Bock and Levelt, 1994), whereby syntax, the rules used to construct sentences (in specific languages) (Chomsky, 1957), is computed. Importantly, grammatical encoding is \"no more accessible to conscious experience than the corresponding comprehension\"  (Levelt, 1989;Bock and Levelt, 1994), and (Menenti et al., 2012a). Boxes represent processing components, and ellipses represent products and inputs of the processing components. (Bock and Levelt, 1994), and thus is a highly automatized process that may be linked to subconscious semantic encoding or the conceptualization process. \n\nWhile speech production is automatic, speaking a second language (L2) is not as easy as speaking a first language (L1). Among the processing steps in speech production (Figure 1), it is not clear at what point the difficulty emerges in L2. Previous studies suggest that syntactic (grammatical) processing acts as a bottleneck for L2 learners. While L2 lexico-semantic processing gradually becomes native-like with higher proficiency (Hahne, 2001;Hahne and Friederici, 2001;Stein et al., 2009), reaching a native-like level for grammatical processing is difficult (Ullman, 2001;Clahsen and Felser, 2006c,b). For example, in a self-paced word by word reading experiment, L2 learners did not show any difference in reading time between number agreement and number disagreement sentences (e.g., * 1 . The bridge to the island were about ten miles away) while native English speakers did (Jiang, 2004).",
            "score": 0.3813830933954582,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1276
                },
                {
                    "start": 1279,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2176
                }
            ],
            "ref_mentions": [
                {
                    "start": 526,
                    "end": 549,
                    "matchedPaperCorpusId": "688666"
                },
                {
                    "start": 692,
                    "end": 715,
                    "matchedPaperCorpusId": "29581886"
                },
                {
                    "start": 959,
                    "end": 981,
                    "matchedPaperCorpusId": "29581886"
                },
                {
                    "start": 987,
                    "end": 1010,
                    "matchedPaperCorpusId": "688666"
                },
                {
                    "start": 1124,
                    "end": 1147,
                    "matchedPaperCorpusId": "29581886"
                },
                {
                    "start": 1712,
                    "end": 1725,
                    "matchedPaperCorpusId": "23877291"
                },
                {
                    "start": 1725,
                    "end": 1752,
                    "matchedPaperCorpusId": "60130674"
                },
                {
                    "start": 1752,
                    "end": 1771,
                    "matchedPaperCorpusId": "43076140"
                },
                {
                    "start": 1842,
                    "end": 1856,
                    "matchedPaperCorpusId": "1237458"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39599609375
        },
        {
            "corpus_id": "267061230",
            "title": "Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?",
            "text": "Through such experience, human-like intuition develops within LLMs. Some may claim that there is a fundamental difference between intuition and language expressing intuition, or between intuition gained from the \"real\" world and whatever gained from linguistic texts. This claim seems to build a wall separating language and the \"real\" world unnecessarily and unjustifiably. To any individual, the world is made up of sensory-motor, linguistic, and other experiences, which are closely entangled and often mirroring each other (Pavlik, 2023). Word meanings may emerge from use patterns (Durt et al., 2023), so a proximate function such as next-token prediction may give rise to an ultimate function such as intuition (Mollo & Milli\u00e8re, 2023). One may gain intuition from any or all of these modalities (see \"grounding\" later). \n\nFurthermore, human intuition may result from grasping underlying statistical patterns and structures of the world in various respects in an implicit way, due to repeated experiences (Hasher & Zacks, 1979;Reber, 1989). Likewise, repeated training of LLMs with a large amount of data facilitates the (implicit) capturing of statistical patterns and structures of the data within LLMs (Durt et al., 2023;Mollo & Milli\u00e8re, 2023;Zhang et al., 2023). For example, Dasgupta et al. (2022) and Saparov & He (2022) showed that LLMs demonstrated content effects in reasoning similar to humans. Trott et al. (2023) showed that LLMs could capture human intuition exhibited in false-belief experiments. And so on. \n\nOne characteristic of human intuition is that one often makes quick \"inferences\" without (consciously) considering all possible alternatives. In other words, one often automatically (unconsciously) filters out what is (perceived to be) unlikely or less likely and thus focuses on the most probable inference only (Dreyfus & Dreyfus, 1986).",
            "score": 0.380972647565576,
            "section_title": "What Can LLMs Capture?",
            "char_start_offset": 10387,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1528
                },
                {
                    "start": 1531,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1870
                }
            ],
            "ref_mentions": [
                {
                    "start": 586,
                    "end": 605,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 1011,
                    "end": 1033,
                    "matchedPaperCorpusId": "144922474"
                },
                {
                    "start": 1211,
                    "end": 1230,
                    "matchedPaperCorpusId": "2105042"
                },
                {
                    "start": 1253,
                    "end": 1272,
                    "matchedPaperCorpusId": "29729283"
                },
                {
                    "start": 1314,
                    "end": 1333,
                    "matchedPaperCorpusId": "252693237"
                },
                {
                    "start": 1412,
                    "end": 1431,
                    "matchedPaperCorpusId": "29729283"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40283203125
        },
        {
            "corpus_id": "268201735",
            "title": "FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
            "text": "Besides, motivated by building LLM-based AI assistants, other studies propose highly curated benchmarks with instancelevel fine-grained annotations, such as difficulty and reasoning skills (Mialon et al., 2023;Ye et al., 2024), for holistic evaluation of LLMs. \n\nHowever, the existing studies, assessing effectiveness across various tasks, provided limited insight into the models' true capabilities, as it only indicates their overall performance on specific datasets, without revealing the fine-grained capabilities acquired or their proficiency levels within the multiple capabilities involved. For instance, in the context of generative question answering, a model adept at extracting information but struggling to form a coherent understanding may exhibit similar overall performance to another model with profound understanding insights but difficulties in articulating accurate responses. \n\nWe argue that a fine-grained understanding of LLMs' capabilities can not only accurately unveil their inherent limitations, but also help us to better identify why one model outperforms the other In this paper, we propose FAC 2 E, a fine-grained capability evaluation framework for LLMs. Specifically, FAC 2 E dissociates the language-related and cognition-related capabilities of LLMs and organizing them into four distinct axes: LINGUISTIC KNOWLEDGE, FORMAL KNOWLEDGE, WORLD MODELING, and SOCIAL MODELING. This categorization is grounded in neuroscience evidence manifesting that language processing and cognitive processes, like memory and reasoning, operate differently in the brain. Drawing from this insight, we adapt a range of existing benchmarks into a unified question-answering format. We then develop specific instructions for each capability, allowing FAC 2 E to evaluate LLMs through a method known as few-shot instruction-following. \n\nFurthermore, we break down the application of a specific capability into three sub-steps: knowledge recall, knowledge utilization, and problem-solving, by iteratively drawing out the model's intermediate reasoning. After evaluating each sub-step, FAC 2 E can reveal the quality of knowledge encoded in the model, and effectiveness in applying relevant knowledge to solve practical problems, offering a more comprehensive evaluation than a single performance metric could. \n\nOur findings reveal a notable gap in capabilities between open-source and proprietary models, especially for cognition-related capabilities. Additionally, we found that many models have difficulties in applying knowledge effectively.",
            "score": 0.3809273030478564,
            "section_title": "Introduction",
            "char_start_offset": 1372,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 263,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1845
                },
                {
                    "start": 1848,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2319
                },
                {
                    "start": 2322,
                    "end": 2462
                },
                {
                    "start": 2463,
                    "end": 2555
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 226,
                    "matchedPaperCorpusId": "259991144"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78369140625
        },
        {
            "corpus_id": "274141509",
            "title": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models",
            "text": "The capabilities and limitations of Large Language Models have been sketched out in great detail in recent years, providing an intriguing yet conflicting picture. On the one hand, LLMs demonstrate a general ability to solve problems. On the other hand, they show surprising reasoning gaps when compared to humans, casting doubt on the robustness of their generalisation strategies. The sheer volume of data used in the design of LLMs has precluded us from applying the method traditionally used to measure generalisation: train-test set separation. To overcome this, we study what kind of generalisation strategies LLMs employ when performing reasoning tasks by investigating the pretraining data they rely on. For two models of different sizes (7B and 35B) and 2.5B of their pretraining tokens, we identify what documents influence the model outputs for three simple mathematical reasoning tasks and contrast this to the data that are influential for answering factual questions. We find that, while the models rely on mostly distinct sets of data for each factual question, a document often has a similar influence across different reasoning questions within the same task, indicating the presence of procedural knowledge. We further find that the answers to factual questions often show up in the most influential data. However, for reasoning questions the answers usually do not show up as highly influential, nor do the answers to the intermediate reasoning steps. When we characterise the top ranked documents for the reasoning questions qualitatively, we confirm that the influential documents often contain procedural knowledge, like demonstrating how to obtain a solution using formulae or code. Our findings indicate that the approach to reasoning the models use is unlike retrieval, and more like a generalisable strategy that synthesises procedural knowledge from documents doing a similar form of reasoning.",
            "score": 0.38089619036960126,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8662109375
        },
        {
            "corpus_id": "218487842",
            "title": "Emergence of Syntax Needs Minimal Supervision",
            "text": "If syntax is an autonomous module of linguistic capacity, the rules and principles that govern it are formulated independently of meaning. However, a corpus is a product of language use or performance. \n\nSyntax constitutes only a subset of the rules that generate such a product; the others include communicative needs and pragmatics. Just as meaning is uncorrelated with grammaticality, corpus frequency is only remotely correlated with human grammaticality judgment (Newmeyer, 2003). \n\nLanguage models learn a probability distribution over sequences of words. The training objective is not designed to distinguish grammatical from agrammatical, but to predict language use. While Linzen et al. (2016) found a correlation between the perplexity of RNN language models and their syntactic knowledge, subsequent studies (Bernardy and Lappin, 2017;Gulordava et al., 2018) recognized that this result could have been achieved by encoding lexical semantic information, such as argument typicality. E.g. \"in 'dogs (...) bark', an RNN might get the right agreement by encoding information about what typically barks\" (Gulordava et al., 2018). \n\nSeveral papers revealed the tendency of deep neural networks to fixate on surface cues and heuristics instead of \"deep\" generalization in solving NLP tasks (Levy et al., 2015;Niven and Kao, 2019). In particular, McCoy et al. (2019) identify three types of syntactic heuristics that get in the way of meaningful generalization in language models. \n\nFinally, it is difficult to build a natural language data set without semantic cues. Results from the syntax-semantics interface research show that lexical semantic properties account for part of syntactic realization (Levin and Rappaport Hovav, 2005). \n\n3 What is syntax a generalization of? \n\nWe have seen in section 2 that previous works on the linguistic capacity of neural language models concentrate on compositionality, the key to creative use of language. However, this creativity is not present in language models: they are bound by the type of the data they are exposed to in learning. \n\nWe suggest that it is still possible to learn syntactic generalization from a corpus, but not with likelihood maximization.",
            "score": 0.3808913883420778,
            "section_title": "Competence or performance, or why syntax drowns in the corpus",
            "char_start_offset": 7781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 201
                },
                {
                    "start": 204,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1136
                },
                {
                    "start": 1139,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1739
                },
                {
                    "start": 1742,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2082
                },
                {
                    "start": 2085,
                    "end": 2208
                }
            ],
            "ref_mentions": [
                {
                    "start": 468,
                    "end": 484,
                    "matchedPaperCorpusId": "196181887"
                },
                {
                    "start": 682,
                    "end": 702,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 846,
                    "end": 869,
                    "matchedPaperCorpusId": "4460159"
                },
                {
                    "start": 1111,
                    "end": 1135,
                    "matchedPaperCorpusId": "4460159"
                },
                {
                    "start": 1295,
                    "end": 1314,
                    "matchedPaperCorpusId": "747342"
                },
                {
                    "start": 1314,
                    "end": 1334,
                    "matchedPaperCorpusId": "196181887"
                },
                {
                    "start": 1351,
                    "end": 1370,
                    "matchedPaperCorpusId": "59599752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61328125
        },
        {
            "corpus_id": "253098841",
            "title": "DALL-E 2 Fails to Reliably Capture Common Syntactic Processes",
            "text": "Much recent research has aimed to show that some aspects of human language acquisition can be reduced to representations that are present in other species or, more recently, in AI systems. 7] AI programs can generate results that show some understanding of grammatical rules into correctly translating some aspects of 'an elephant is behind a tree' (vs. 'a tree is behind an elephant'). 6 However, more complex structure poses challenges. 28 Passives, double object constructions, and coordinated structures are early acquired by humans, but their parsing is problematic for systems like DALL\u2022E 2. \n\nHere, in the first systematic test of DALL\u2022E 2 across a wide range of linguistic phenomena, we find consistent evidence that compositionality, one of the core properties of language discussed from Frege and Descartes to contemporary linguistics, is lacking. \n\nDespite recent celebration of its linguistic abilities, DALL\u2022E 2 generally performs at or near chance on syntax to semantics mappings that are routinely mastered by young children. The prompt-to-image translation of longer sentences often produces an outcome in which only some lexical items are drawn for interpretation, without a faithful compositional representation of their parts. \n\nWhat is needed is not just linguistic input, but some mapping of linguistic structures onto some independent conceptual system, which has its own sets of compositional and inference rules, along with notions like truth/false, and faction/fiction, and cognitive models of the world [29][30] ; synthesizing text alone is not necessarily in direct service to building a realistic model of human syntax and semantics. 31 3] For example, a recent account 34 claims that language models represent 'conceptual role' meanings because these can be inferred from relationships between internal representational states. Our results shows that such representations, to the extent that they exist at all, do not suffice. 6] The very notion of 'conceptual role' presupposes a mapping to conceptual structures, and cannot be inferred purely from co-occurrence statistics of words within texts; current approaches do not seem to address this (i.e., what are the conceptual roles, and how do we know that common roles will be converged on and not some possible and computable role that does not exist for human language?).",
            "score": 0.3805295568005125,
            "section_title": "Discussion",
            "char_start_offset": 14270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 597
                },
                {
                    "start": 600,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2353
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.254638671875
        },
        {
            "corpus_id": "276742394",
            "title": "Layered Insights: Generalizable Analysis of Authorial Style by Leveraging All Transformer Layers",
            "text": "As mentioned, research showed that during the pre-training phase of transformer models, their layers acquire different linguistic capabilities. In this section, we look at this phenomenon in our trained MULTILUAR model to inspect whether the training process on authorship attribution task leads to different layers capturing different linguistic phenomena. In particular, we use the Holmes benchmark (Waldis et al., 2024), which consists of over 200 datasets and different probing methods to evaluate the linguistic competence in transformers, including syntax, morphology, semantics, and discourse at different layers. We run the probing method of this library over paraphrase-distilroberta, the base transformer of our model, quantifying the linguistic competence of its initial embedding (layer 0) and all its 6 layers. We then compare it to MULTI-LUAR model that we trained on the Reddit dataset. \n\nFigure 2 visualizes the linguistic competence of all layers for both models. As for RoBERTa model (Figure 2.b), we observe that for all four linguistic phenomena, layer 3 has the strongest representation rather than the final layer (a 0.5 score in Morphology and around 0.7 on Semantics, Discourse, and Syntax). While layer 6 is better at capturing semantics and morphology than layer 1 (0.6 and 0.38 compared to 0.5 and 0.3, respectively), it is worse at discourse and syntax. Since the authorship attribution task relies on comparing text similarity under different linguistic aspects rather than only semantics, these insights support the need to consider all transformer layers when solving the authorship attribution task. Furthermore, we can see that all layers are stronger at syntax, discourse, and semantics compared to morphology. Now, in comparison, looking at Figure 2.a that presents our MULTILUAR model's layers linguistic capacity, we can observe the following. The competence of all layers is relatively stronger for morphology and syntax compared to discourse and semantics. This is an indication that syntax and morphology are more important for the task of authorship attribution than semantics and discourse, at least on the Reddit dataset that the model is trained on. Furthermore, we can see that different layers become specialized in specific linguistic phenomena.",
            "score": 0.38030614214274305,
            "section_title": "Learned Linguistic Representations",
            "char_start_offset": 15807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2193
                },
                {
                    "start": 2194,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 401,
                    "end": 422,
                    "matchedPaperCorpusId": "269448752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66796875
        },
        {
            "corpus_id": "259089351",
            "title": "Language acquisition: do children and language models follow similar learning stages?",
            "text": "The present study fuels a long-lasting debate on the acquisition of language. While \"empiricists\" argue that language can be acquired with a statistical approach (Clark, 2002;Kolodny et al., 2015;Chater and Christiansen, 2018;McCauley and Christiansen, 2019), \"nativists\" maintain that this ability depends on a core and innate operation, spe-cific to humans (Chomsky, 1959(Chomsky, , 1971)). The present study shows how modern language models may contribute to resolving this debate, by systematically studying which components of a model (e.g. architecture) or properties of the training data (e.g., frequency of sentence structures) contribute to shape the trajectory of language acquisition. Claims about an innate Universal Grammar could be understood as an inductive bias of a language model, implemented in its architecture and dynamics, which tightly constrains learning trajectories across models. If this bias is hierarchical (rather than linear) then this could lead to learning trajectories that follow the structure of the syntactic tree, consistently with the hypothesis of three linguistic stages presented by Friedmann et al. (2021) in humans and what we find in this study in language models. Indeed, neural language models have been previously shown to have a weak inductive bias towards hierarchical processing (Mc-Coy et al., 2020;Kharitonov and Chaabouni, 2020), which can partially explain our results. \n\nThis result echos the recent observation that syntactic trees spontaneously emerge in the middle layers of neural language models (Hewitt and Manning, 2019). Together, these elements thus suggest that modern neural networks provide fruitful models of language acquisition and could reconcile or settle the confronting theories of language acquisition (Warstadt and Bowman, 2022).",
            "score": 0.380215385975854,
            "section_title": "Fueling the debate between nativism versus empiricism",
            "char_start_offset": 22353,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1806
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 196,
                    "matchedPaperCorpusId": "813498"
                },
                {
                    "start": 226,
                    "end": 258,
                    "matchedPaperCorpusId": "58632948"
                },
                {
                    "start": 1557,
                    "end": 1583,
                    "matchedPaperCorpusId": "106402715"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7509765625
        },
        {
            "corpus_id": "257557392",
            "title": "ROSE: A Neurocomputational Architecture for Syntax",
            "text": "A comprehensive model of natural language processing in the brain must accommodate four components: representations, operations, structures and encoding. It further requires a principled account of how these different components mechanistically, and causally, relate to each another. While previous models have isolated regions of interest for structure-building and lexical access, and have utilized specific neural recording measures to expose possible signatures of syntax, many gaps remain with respect to bridging distinct scales of analysis that map onto these four components. By expanding existing accounts of how neural oscillations can index various linguistic processes, this article proposes a neurocomputational architecture for syntax, termed the ROSE model (Representation, Operation, Structure, Encoding). Under ROSE, the basic data structures of syntax are atomic features, types of mental representations (R), and are coded at the single-unit and ensemble level. Elementary computations (O) that transform these units into manipulable objects accessible to subsequent structure-building levels are coded via high frequency broadband \u03b3 activity. Low frequency synchronization and cross-frequency coupling code for recursive categorial inferences (S). Distinct forms of low frequency coupling and phase-amplitude coupling (\u03b4-\u03b8 coupling via pSTS-IFG; \u03b8-\u03b3 coupling via IFG to conceptual hubs in lateral and ventral temporal cortex) then encode these structures onto distinct workspaces (E). Causally connecting R to O is spike-phase/LFP coupling; connecting O to S is phase-amplitude coupling; connecting S to E is a system of frontotemporal traveling oscillations; connecting E back to lower levels is low-frequency phase resetting of spike-LFP coupling. This compositional neural code has important implications for algorithmic accounts, since it makes concrete predictions for the appropriate level of study for psycholinguistic parsing models. ROSE is reliant on neurophysiologically plausible mechanisms, is supported at all four levels by a range of recent empirical research, and provides an anatomically precise and falsifiable grounding for the basic property of natural language syntax: hierarchical, recursive structure-building.",
            "score": 0.38007280746886696,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63818359375
        },
        {
            "corpus_id": "269448752",
            "title": "Holmes: A Benchmark to Assess the Linguistic Competence of Language Models",
            "text": "While scaling model size to 12B (Pythia) and 70B (Llama-2) allows for improved accuracy and lower deviations, decoder LMs do not match the encoder performance, even up to 700 times bigger. iv) More parameters improve LMs' linguistic competence. We discuss how the number of parameters influences the linguistic competence of LMs. Given the variety of LMs of differ-ent sizes, we focus on the Pythia (decoder-only) and T5 (encoder-decoder) families. From Figure 12, we observe for both Pythia and T5 that the linguistic competence scales with model size, and it is particularly pronounced after exceeding 0.5B (Pythia) and 1.0B (T5) parameters. Again, model architecture is crucial, as T5 LMs (encoderdecoder) exhibit a clearly higher mean winning rate of 40 \u2212 70% than Pythia (decoder-only) ones with mwr of 20 \u2212 60%. Further, we found formal phenomena evolving differently with increased model size than functional ones. Specifically, morphology and syntax start at a lower level, with an apparent performance jump after 0.5B (Pythia) and 1.0B (T5) parameters, followed by slow but steady growth. Differently, semantics, reasoning, and discourse start at a higher mwr, followed by a continuous improvement as the model size grows. From these results, we assume that more parameters enable language models to better approximate simple word co-occurrences in nearby contexts. While handling formal phenomena like word dependencies, they struggle with more distant and complex co-occurrences, such as rhetorical relations.",
            "score": 0.3800130809004514,
            "section_title": "Holmes Results",
            "char_start_offset": 30237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1520
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "270847724",
            "title": "Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability",
            "text": "Recent studies have made significant contributions to the understanding of this area. Dziri et al. (2023) formulate compositional tasks as computation graphs to quantify each task's complexity level. Power et al. (2022) show that models may develop generalization capabilities when trained extensively, beyond the point of overfitting, highlighting a phenomenon known as \"grokking\". An et al. (2023b) examines how LLMs acquire abstract reasoning and achieve compositional generalization in a linguistic context through ICL by testing LLMs on tasks that involve translating a formal language with a custom grammar. Although these studies offer insight, how LLMs compose tasks together is still not fully understood, especially in the ICL setting. Moreover, the absence of a solid theoretical framework in these discussions needs to be investigated concerning the underlying mechanisms of such behaviors. \n\nInspired by these seminal works, we further evaluate LLMs on a series of compositional tasks through ICL. The models were presented with examples of simple tasks and then asked to tackle composite tasks that they had not encountered during pretraining or in-context learning. We observe various behaviors: (1) for some composite tasks, the models showed a reasonable level of compositional skill, a capability that improved with larger model sizes; (2) for more complex composite tasks requiring sequential reasoning, the model struggle, and increasing the model size typically did not lead to better performance. \n\nOur key intuition is that if the simple tasks that form a composite task can be easily separated into subtasks based on the inputs (e.g., performed separately on different parts of the input sentence), the model is more likely to complete such a composite task successfully (we call it \"a separable composite task\"). The performance of the model depends on how it connects and uses the information given for each part of the task. To clarify this insight, we present theoretical analyses in a simplified setting and provide key insights into conditions needed for success in such separable composite tasks. \n\nOur contributions are twofold. Empirically, we introduce a variety of composite tasks from both the linguistic and logical domains to explore how the nature of these tasks influences the compositional performance of LLMs through ICL.",
            "score": 0.37975991028825656,
            "section_title": "Introduction",
            "char_start_offset": 2092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2127
                },
                {
                    "start": 2130,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 105,
                    "matchedPaperCorpusId": "258967391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.779296875
        },
        {
            "corpus_id": "276724796",
            "title": "Triple Phase Transitions: Understanding the Learning Dynamics of Large Language Models from a Neuroscience Perspective",
            "text": "It is crucial to note that although these metrics confirm that fundamental transformations occur in the representation space, they do not fully explain the specific underlying mechanisms. By examining how these transformations relate to both the brain and downstream tasks, as in this study, a more comprehensive and nuanced understanding of the underlying processes can be achieved. \n\nIn summary, this study has elucidated the existence of three distinct phase transitions during LLM training, as evidenced by three perspectives: alignment with the human brain, shifts in internal representations pertinent to downstream tasks, and task performance. We interpreted these transitions as indispensable internal transformations that enable the model to acquire downstream task capabilities. This study is the first to show that changes in brain alignment, internal representation, and model performance advance in tandem by leveraging the human brain-the only known system (aside from LLMs) capable of processing complex language. These findings highlight the critical importance of examining multiple signals to gain a comprehensive understanding of LLM learning dynamics, including emergent phenomena. Furthermore, they suggest the potential to harness human brain activity in the pursuit of explainable and human-aligned language models. 2014. doi: 10.1073/pnas.1403112111. URL https://www.pnas.org/doi/abs/10.1073/pnas. 1403112111. \n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.",
            "score": 0.37955109953353117,
            "section_title": "Discussion and conclusions",
            "char_start_offset": 29546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1433
                },
                {
                    "start": 1436,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1659
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90478515625
        },
        {
            "corpus_id": "277244499",
            "title": "Integrating Personality into Digital Humans: A Review of LLM-Driven Approaches for Virtual Reality",
            "text": "The field of natural language processing (NLP) has undergone rapid transformations since the introduction of the Transformer architecture [81]. This breakthrough has fundamentally reshaped NLP, leading to the proliferation of numerous language models, particularly decoder-based architectures, which have revolutionized text generation and paved the way for large language models (LLMs). These advanced neural networks redefine the boundaries of machine understanding and language generation, leveraging extensive datasets comprising billions or even trillions of words from diverse domains to maximize linguistic coverage and generalization [4]. \n\nThe unprecedented scale of LLMs, characterized by vast numbers of parameters, enables them to capture intricate linguistic patterns, including semantic relationships, syntactic structures, and long-range dependencies [77,4]. This capacity allows them to perform complex language-related tasks, such as advanced reasoning and contextual understanding. In particular, LLMs demonstrate emergent abilities, performing tasks that are not explicitly programmed but naturally arise from their scale and the complexity of their training process [85,37]. \n\nThe development of large language models generally involves two primary stages: pre-training and fine-tuning. The pre-training phase involves exposing the model to extensive volumes of unlabeled general knowledge text data through a language modeling task, most commonly framed as the next token prediction in a sequence [4]. Then, the resulting model passes through the finetuning phase, which refines the model's capabilities to perform a more specific set of tasks. This process leverages the pre-trained weights, which encode general knowledge about language, morphology, and syntax, and adjusts them using labeled datasets in a supervised learning framework [16]. This step, commonly referred to as Supervised Fine-Tuning (SFT), aligns the model's performance with the requirements of particular applications, significantly enhancing its ability to tackle domain-specific tasks [52]. \n\nWhile fine-tuning significantly enhances a model's domainspecific performance, it often requires access to a large amount of labeled data, which presents a substantial barrier, especially when dealing with specialized domains.",
            "score": 0.3794178375541567,
            "section_title": "LARGE LANGUAGE MODELS (LLMS)",
            "char_start_offset": 8638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 646
                },
                {
                    "start": 649,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2085
                },
                {
                    "start": 2088,
                    "end": 2314
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 142,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 642,
                    "end": 645,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 870,
                    "end": 872,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1186,
                    "end": 1190,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1518,
                    "end": 1521,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1860,
                    "end": 1864,
                    "matchedPaperCorpusId": "263830318"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84716796875
        },
        {
            "corpus_id": "5222302",
            "title": "Deterministic Parsing of Syntactic Non-fluencies",
            "text": "This raises the general question of how children can acquire a language in the face of unrestrained non-fluency. How can a language learner sort out the grammatical from the ungrammatical strings? (The non-fluencies of speech are of course but one aspect of the degeneracy of input that makes language acquisition a puzzle.) The self-correction system I have described suggests that many non-fluent strings can be resolved with little detailed linguistic knowledge. \n\nAs Table 1 shows, about a quarter of the editing signals result in expunction of only non-linguistic material. This requires only an ability to distinguish linguistic from nonlinguistic stuff, and it introduces the idea that edit signals signal an expunction site. Almost a third are resolved by the Surface Copying rule, which can be viewed simply as an instance of the general non-linguistic rule that multiple instances of the same thing count as a single instance. The category copying rules are generalizations of simple copying, applied to a knowledge of linguistic categories, Making the transition from surface copies to category copies is aided by the fact that there is considerable overlap in coverage, defining a path of expanding generalization. Thus at the earliest stages of learning, only the simplest, non-linguistic self-correction rules would come into play, and gradually the more syntactically integrated would be acquired. \n\nContrast this self-correction system to an approach that handles non-fluencies by some general problem solving routines, for example Granger (1982), who proposes reasoning from what a speaker might be expected to say. Besides the obvious inefficiencies of general problem solving approaches, it is worth giving special emphasis to the problem with learnability. A general problem solving approach depends crucially on evaluating the likelihood of possible deviations from the norms. But a language learner has by definition only partial and possibly incorrect knowledge of the syntax, and is therefore unable to consistently identify deviations from the grammatical system. With the editing system I describe, the learner need not have the ability to recognize deviations from grammatical norms, but merely the non-linguistic ability to recognize copies of the same thing. \n\nGeneration. Thus far, I have considered the selfcorrection component from the standpoint of parsing. However, it is clear that the origins are in the process of generation.",
            "score": 0.379349479624976,
            "section_title": "Discussion",
            "char_start_offset": 22667,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 465
                },
                {
                    "start": 468,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2287
                },
                {
                    "start": 2290,
                    "end": 2301
                },
                {
                    "start": 2302,
                    "end": 2390
                },
                {
                    "start": 2391,
                    "end": 2462
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.310302734375
        },
        {
            "corpus_id": "7260548",
            "title": "On the need for Embodied and Dis-Embodied Cognition",
            "text": "I propose that our concepts are encoded in at least two types of semantic representations: one type employing embodied sensorimotor representations associated with our experience of the world go beyond the information contained within specific modalities. Amodal representations provide a means of gathering and integrating information from different modalities as well as transferring information between distinct sensorimotor codes. Because linguistic representations have the design features outlined above, they can also carry out this function (Carruthers, 2002). \n\nI propose that when an individual acquires a natural language, she acquires a representational system that is different in some important respects from the multimodal, context-sensitive embodied symbol systems that exist independently of language. The acquisition of natural language, in other words, enhances and extends her representational abilities by giving her access to a context-free and arbitrary symbol system. This symbol system is independent of, and yet interacts with, other embodied symbols. \n\nThis proposal requires a revisionist conception of linguistic competence. Standard theories of linguistic competence are thoroughly amodal. Linguists have identified structural regularities at several levels of analysis, including phonology, morphology, syntax, and to some degree logical form or semantics. Knowledge relating to these levels is thought to be contained with language-specific functional modules (Fodor, 1983) and is generally thought to be couched in amodal codes. Comprehension involves translating perceptual information into these codes and production involves translating information in these codes into motor representations. The revisionist approach taken in this essay is that the process of achieving competence in a specific natural language involves acquiring the ability to generate appropriate simulations of linguistic experience. To be successful, these simulations must comport with the structural regularities at the different levels of analysis. They will not, however, depend on knowledge contained with an amodal symbol system. Three points about this revisionist proposal are especially important. The first is that it is neutral with respect to the issue of the degree to which linguistic competence is innate or learned. This proposal has to do with the format of the representations associated with this competence and not how it is acquired. The second is that, despite superficial appearances, this is not an inner speech view. The claim is that linguistic competence is contained within a system for generating perceptual symbols.",
            "score": 0.379314965109323,
            "section_title": "TheoreTical influences",
            "char_start_offset": 42350,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1077
                },
                {
                    "start": 1080,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2214
                },
                {
                    "start": 2215,
                    "end": 2339
                },
                {
                    "start": 2340,
                    "end": 2462
                },
                {
                    "start": 2463,
                    "end": 2549
                },
                {
                    "start": 2550,
                    "end": 2653
                }
            ],
            "ref_mentions": [
                {
                    "start": 549,
                    "end": 567,
                    "matchedPaperCorpusId": "7737830"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4609375
        },
        {
            "corpus_id": "276617735",
            "title": "Negation-Induced Forgetting in LLMs",
            "text": "Nguyen, 2024;Suri et al., 2024;Schubert et al., 2024). If LLMs also exhibit NIF, this could indicate that memory biases can potentially emerge from their training routine, without the need to explicitly integrate human-like reasoning mechanisms and controlled processes such as active inhibition. \n\nBy adapting Zang et al.'s methodology and comparing the performance of different conversational AI models to human results, this study contributes to ongoing debates about whether LLMs replicate human cognitive biases and how memory-related phenomena are encoded in large-scale language models. Understanding whether NIF occurs in LLMs can inform both theoretical models of linguistic cognition and practical considerations for AI deployment. \n\nIf LLMs display negation-induced forgetting effects, this may be attributed to inherent characteristics of LLMs, such as the attention mechanisms in the transformer architecture, which might distribute focus differently for information appearing in the context of negation, reducing its prominence in subsequent processing. Additionally, the statistical patterns learned during training could inherently weight negated information differently, leading to apparent \"forgetting\".",
            "score": 0.3792625837072991,
            "section_title": "Introduction",
            "char_start_offset": 2218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1221
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 31,
                    "matchedPaperCorpusId": "258556944"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2724609375
        },
        {
            "corpus_id": "270285708",
            "title": "Benchmark Data Contamination of Large Language Models: A Survey",
            "text": "Emergent abilities manifest primarily in three ways: in-context learning [33], instruction following [127], and step-by-step reasoning [113].In-context learning abilities were first observed in GPT-3 [15] The instruction following ability enables the model to comprehend and execute tasks based on directives provided directly within the input prompt.For example, when provided with a prompt that includes specific instructions, such as \"summarize the following text\" or \"translate the following sentences into French, \" the model can understand and act upon these instructions, leveraging its pre-existing knowledge base and generalization capabilities [153].\n\nStep-by-step reasoning refers to the ability of models to break down complex problems or queries into smaller steps, processing each one sequentially to arrive at a final answer.This is often accomplished through chain-of-thought (CoT) prompting strategy [155], in this approach, the model sequentially processes each step, building upon prior steps to construct a comprehensive answer, and research has shown that chain-of-thought prompts can bring performance gains [155].\n\nThe abilities mentioned above enable LLMs to exhibit strong performance but also face several issues.A frequently-reported problem is the generation of so-called hallucinations [8], where LLMs generate text that superficially appears to be correct but is actually inaccurate.This problem is difficult to resolve completely, although it can be mitigated through alignment tuning strategies [109].While LLMs have learned general language patterns, they underperform in specialized domains, such as medicine or engineering.This may be related to catastrophic forgetting [76] or a scarcity of relevant training data.Furthermore, enabling LLMs to quickly learn the latest knowledge by updating weights remains an unresolved challenge [163].",
            "score": 0.3791682091067085,
            "section_title": "Large Language Models",
            "char_start_offset": 9047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 141,
                    "end": 351
                },
                {
                    "start": 351,
                    "end": 660
                },
                {
                    "start": 662,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 1136
                },
                {
                    "start": 1138,
                    "end": 1239
                },
                {
                    "start": 1239,
                    "end": 1413
                },
                {
                    "start": 1413,
                    "end": 1533
                },
                {
                    "start": 1533,
                    "end": 1658
                },
                {
                    "start": 1658,
                    "end": 1750
                },
                {
                    "start": 1750,
                    "end": 1873
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 106,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 135,
                    "end": 140,
                    "matchedPaperCorpusId": "258048648"
                },
                {
                    "start": 200,
                    "end": 204,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 654,
                    "end": 659,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 917,
                    "end": 922,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1130,
                    "end": 1135,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1315,
                    "end": 1318,
                    "matchedPaperCorpusId": "256662612"
                },
                {
                    "start": 1527,
                    "end": 1532,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1705,
                    "end": 1709,
                    "matchedPaperCorpusId": "22910766"
                },
                {
                    "start": 1867,
                    "end": 1872,
                    "matchedPaperCorpusId": "258833129"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "233034499",
            "title": "Grounding Language Processing: The Added Value of Specifying Linguistic/Compositional Representations and Processes",
            "text": "The present paper takes the stance that the combined specification of different models (linguistic representations and computational implementation) offers added value over specifying only one of these. Perhaps linguistic and non-linguistic representational formats are best viewed as complementary levels in linking cognition to behavior (see Smolensky, 2001, p. 324), even if the link is not straightforward. Connectionist models receive vector representations consisting of zeros and ones, associate them with other vector representations, and following such associative learning, output vector activation values. The representations implicated during processing are not stipulated but emerge (at hidden layers and via connection weights). \n\nGranted, the output vectors can be associated with a symbolic compositional interpretation. But associating output vectors with a limited set of linguistic representations means that we see the output of grounded comprehension in linguistic notation but not how representations that we can reason about are incrementally grounded. With symbolic linguistic representations, we can reason about how a sentence interpretation is derived, what meaning is decoded, and how it is linked to representations of visual perception and action. Tabor (2009) argued that symbolic and connectionist approaches are incompatible (in line with Fodor and Pylyshyn (1988) but that this conflict can be resolved by appealing to non-linear dynamic approaches. In that spirit, Venhuizen et al. (2018)  How can we concretely make progress? Only a small set of empirical findings on language grounding has been modeled computationally and scaling models can be a challenge. One possibility is to complement computational implementations by relating the process of constructing a grounded compositional interpretation (made explicit in linguistic representations) to steps in a computational model. Figure 1 illustrates how the Coordinated Interplay account can be related to -and complemented by -computational modeling (pink font). Specifying linguistic representations and processes could lay the foundation for predicting grounding effects broad coverage (e.g., for a wide range of world-language relations), something that would constitute a substantial psycholinguistic contribution to the computational modeling of language grounding.",
            "score": 0.3790435799614043,
            "section_title": "Why specify grounded linguistic representations incrementally?",
            "char_start_offset": 31603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2361
                }
            ],
            "ref_mentions": [
                {
                    "start": 1278,
                    "end": 1290,
                    "matchedPaperCorpusId": "10507249"
                },
                {
                    "start": 1372,
                    "end": 1397,
                    "matchedPaperCorpusId": "29043627"
                },
                {
                    "start": 1500,
                    "end": 1523,
                    "matchedPaperCorpusId": "149473798"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.493408203125
        },
        {
            "corpus_id": "225262777",
            "title": "The State Space of Artificial Intelligence",
            "text": "All this suggests the following: syntax is 'almost sufficient' to produce the linguistic behavior that corresponds to the behavior of speakers who are truly semantically grounded. Although syntax is not completely sufficient for semantics, syntax is 'almost sufficient' in the sense that it is sufficient for all practical purposes (but still insufficient from a strict Searlean point of view). This means that, in effect, a syntactic machine can be indistinguishable in its translation performance from a human speaker. \n\nIn the light of the above considerations, it follows that DL translation systems do not appear to have any semantic grounding. Nevertheless, these systems acquire rule knowledge and meaning in an FRS sense through self-learning. This alone is remarkable. The crucial step, however, is yet to come. How can it even be possible that a mere symbol based FRS becomes effectively indistinguishable (regarding language behavior) from a truly referential semantics? Should it be possible that, when it comes to semantics, we can do with a pure FRS alone and dismiss any appeal to reference? On the face of it, this amazing possibility seems to be suggested by the translation capabilities of systems such as Google Translate and DeepL. But on closer inspection, it is not. The systems do indeed go beyond a pure FRS. The text corpora used in learning allow for a kind of indirect reference-to-the-world. They were created by human speakers who dispose of a semantics in a full referential sense. Hence, Google Translate and DeepL have no direct but an indirect grounding. They refer to the world indirectly. Regularities that can be extracted from text corpora comparisons go beyond grammatical regularities, they provide world regularities as the texts deal with worldly circumstances. This allows to extract a decent amount of structural information about the world. 8 ll of this shows that semantic grounding in the sense of reference, even if in an indirect and tricky way as in our foregoing example, but most decisively in the direct variant of causal contact with the world, is of utmost importance to acquire meaning. It is thus of utmost importance for intelligence, whether artificial or natural. Grounding in the sense of causal reference is a crucial dimension of the AI state space. Note that postulating this dimension means no commitment to any particular theory of meaning or mental representation.",
            "score": 0.37892032273220233,
            "section_title": "The Chinese Room and the Dimension of Causal Grounding",
            "char_start_offset": 34759,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2222
                },
                {
                    "start": 2223,
                    "end": 2311
                },
                {
                    "start": 2312,
                    "end": 2430
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.361083984375
        },
        {
            "corpus_id": "27228816",
            "title": "Grounded Language Learning in a Simulated 3D World",
            "text": "Before it can exhibit any lexical knowledge, the agent must learn various skills and capacities that are independent of the specifics of any particular language instruction. These include an awareness of objects as distinct from floors or walls; some capacity to sense ways in which those objects differ; and the ability to both look and move in the same direction. In addition, the agent must infer that solving the solution to tasks is always contingent on both visual and linguistic input, without any prior programming or explicit teaching of the importance of inter-modal interaction. Given the complexity of this learning challenge, it is perhaps unsurprising that the agent requires thousands of training episodes before evidence of word learning emerges. \n\nTo establish the importance of this 'pre-linguistic' learning, we compared the speed of vocabulary acquisition in agents with different degrees of prior knowledge. The training set consisted of instructions (and corresponding environments) from the twenty shape terms banana, cherries, cow, flower, fork, fridge, hammer, jug, knife, pig, pincer, plant, saxophone, shoe, spoon, tennis-racket, tomato, tree, wine-glass and zebra. The agent with most prior knowledge was trained in advance (in a single room setting with two objects) on the remaining twenty shapes from the full environment. The agent with minimal prior knowledge was trained only on the two terms ball and tv. Both regimes of advanced training were stopped once the agent reached an average reward of 9.5/10 across 1,000 episodes. The agent with no prior knowledge began learning directly on the training set. \n\nThe comparison presented in Figure 4 demonstrates that much of the initial learning in an agent trained from scratch involves acquiring visual and motor, rather than expressly linguistic, capabilities. An agent already knowing two words (and therefore exhibiting rudimentary motor and visual skills) learned new words at a notably faster rate than an agent trained from scratch. Moreover, the speed of word learning appeared to accelerate as more words were learned. This shows that the acquisition of new words is supported not only by general-purpose motor-skills and perception, but also existing lexical or semantic knowledge. In other words, the agent is able to bootstrap its existing semantic knowledge to enable the acquisition of new semantic knowledge.",
            "score": 0.3788650405649915,
            "section_title": "Word learning speed experiment",
            "char_start_offset": 18200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1639
                },
                {
                    "start": 1642,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2272
                },
                {
                    "start": 2273,
                    "end": 2404
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51806640625
        },
        {
            "corpus_id": "270522276",
            "title": "Scale matters: Large language models with billions (rather than millions) of parameters better match neural representations of natural language",
            "text": "Combined with vast training text, these models can encode a rich array of linguistic structures-ranging from low-level morphological and syntactic operations to high-level contextual meaning-in a high-dimensional embedding space. Recent work has argued that the \"size\" of these models-the number of learnable parameters-is critical, as some linguistic competencies only emerge in larger models with more parameters (Bommasani et al., 2021;Kaplan et al., 2020;Manning et al., 2020;Sutton, 2019;Zhang et al., 2021). For instance, in-context learning (Liu et al., 2021;Xie et al., 2021) involves a model acquiring the ability to carry out a task for which it was not initially trained, based on a few-shot examples provided by the prompt. This capability is present in the bigger GPT-3 (Brown et al., 2020) but not in the smaller GPT-2, despite both models having similar architectures. This observation suggests that simply scaling up models produces more human-like language processing. Such an observation is intriguing. While building and training LLMs with billions to trillions of parameters is an impressive engineering achievement, such artificial neural networks are tiny compared to cortical neural networks. In the human brain, each cubic millimeter of cortex contains a remarkable number of about 150 million synapses, and the language network can cover a few centimeters of the cortex (Cantlon & Piantadosi, 2024). Thus, scaling could be a property that the human brain, similar to LLMs, can utilize to enhance performance. \n\nOur study focuses on one crucial question: what is the relationship between the size of an LLM and how well it can predict linguistic information encoded in the brain? In this study, we define \"model size\" as the number of all trainable parameters in the model. In addition to size, we also consider the model's expressivity: its capacity to predict the statistics of natural language. Perplexity measures expressivity by evaluating the average level of surprise or uncertainty the model attributes to a sequence of words. Larger models possess a greater capacity for expressing linguistic structure, which tends to yield lower (better) perplexity scores (Radford et al., 2019).",
            "score": 0.378840145805414,
            "section_title": "Introduction",
            "char_start_offset": 1934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2214
                }
            ],
            "ref_mentions": [
                {
                    "start": 493,
                    "end": 512,
                    "matchedPaperCorpusId": "231991101"
                },
                {
                    "start": 783,
                    "end": 803,
                    "matchedPaperCorpusId": "268917993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61669921875
        },
        {
            "corpus_id": "53246965",
            "title": "Aligning Developmental and Processing Accounts of Implicit and Statistical Learning",
            "text": "suggest that priming serves as an important function in improving communication between interlocutors). Structural priming has also been influential in shaping our understanding of what children's early syntactic knowledge is like and, more recently, to study the transition between the two: how children's syntactic knowledge develops to become adult-like. To add to this, priming has also been used to explore the types of learning mechanisms that might be involved in this process (Peter, Chang, Pine, Blything, & Rowland, 2015). Thus, it is clear that not only does structural priming have much to contribute to both the adult and child language literature, but it too is important for the statistical learning and implicit learning community who share the goal of wanting to better understand how syntactic knowledge is built.\n\nRelatedly, there have been a number of attempts to model syntax acquisition. However, these models fail to include psychological and computational features that McCauley and Christiansen (2014) argue are important for a plausible model of this process. For instance, the authors propose that a model of syntax acquisition should (a) process input on-line in a word-by-word manner, as opposed to learning entire utterances; (b) learn by calculating statistics that are tied to backward transitional probabilities as opposed to using only simple distributional information; and (c) be trained using naturalistic linguistic input as opposed to input that is artificial or lacks the properties of real language. Thus, models that do not model development incrementally (e.g., Bannard, Lieven, & Tomasello, 2009), focus only on simple distributional information (e.g., Redington et al., 1998), or are not fed input with the structure of real natural languages (e.g., Howell & Becker, 2001, in which the model is trained on a 390-word language comprising twoand three-word sentences) are not fully able to capture the process by which children develop adult-like syntactic knowledge.\n\nPerhaps more important, none of them provides an explanation that scales up to adult language use, incorporating an implicit learning explanation of why we see structural priming effects in both children and adults. Conversely, a number of models have been developed to capture structural priming effects (e.g.",
            "score": 0.3788233886182768,
            "section_title": "Introduction",
            "char_start_offset": 13735,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 484,
                    "end": 531,
                    "matchedPaperCorpusId": "43394269"
                },
                {
                    "start": 994,
                    "end": 1026,
                    "matchedPaperCorpusId": "10606317"
                },
                {
                    "start": 1605,
                    "end": 1639,
                    "matchedPaperCorpusId": "6227768"
                },
                {
                    "start": 1697,
                    "end": 1720,
                    "matchedPaperCorpusId": "2596605"
                },
                {
                    "start": 1795,
                    "end": 1816,
                    "matchedPaperCorpusId": "189719"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.468505859375
        },
        {
            "corpus_id": "235525947",
            "title": "An Integrated Approach to Pragmatic Competence: Its Framework and Properties",
            "text": "Therefore, the linguistically semantic computations reveal that the previous syntactic computations are off the track. In this case, based on the interaction between syntactic and semantic submodules, language users are able to utilize the uninterpretability of (1a) and (1b) to recognize their syntactic ungrammaticality. Thus, internal pragmatic competence is expressed in the form of \"thinking capacity which accounts for the basic property of human language\" (Berwick & Chomsky, 2016, p. 11). \n\nFurthermore, when language users are thinking about certain specific pragmatic communicative issues in authentic or imagined contexts, the relevant linguistic and sociocultural pragmatic knowledge needed in the inference of pragmatic meanings is immediately activated from interlocutors' background knowledge stored in their brains, such as encyclopedic knowledge, and input into the pragmatic submodule for ongoing pragmatic inferences. Therefore, reasoning via interactions between syntactic, semantic, and pragmatic submodules denotes how human thought proceeds during language use. To cite one concise example of how pragmatic competence unrolls in this situation: When an American lady received an email from one of her foreign friends, the salutation puzzled her, because it was \"Dear Madman\" rather than \"Dear Madam.\" However, she soon calmed down. Her settlement of the embarrassment would be advanced based on serial interactions among different submodules. First, the semantic submodule interprets phrase structures transferred from syntactic submodule as \"vicious greeting\" rather than \"respectful saluting.\" However, given her friend is a nonnative speaker with poor English, the real pragmatic intention reasoned out in pragmatic submodule could not be a personal attack but a \"beautiful mistake,\" because the encyclopedic knowledge selected for pragmatic inference shows that non-native speakers always make typos or other mistakes. In this way, if thought unfolds truly as sketched above among a series of interactions of submodules at the abstract thinking level, the trajectory of internal pragmatic competence could be clearer now.",
            "score": 0.37870940618003696,
            "section_title": "A Modular Approach to Pragmatic Competence",
            "char_start_offset": 37073,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2148
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4375
        },
        {
            "corpus_id": "265728514",
            "title": "Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation",
            "text": "Exploring what linguistic phenomena neural models learn Many tests have been used to probe how well neural models learn different linguistic phenomena. Linzen et al. (2016) use \"number agreement in English subject-verb dependencies\" to show that LSTMs learn about syntax-sensitive dependencies. In addition to syntax (Shi et al., 2016), researchers have used other labeling tasks to investigate whether neural machine translation (NMT) models learn different linguistic phenomena (Belinkov et al., 2017a,b;Dalvi et al., 2017;Marvin and Koehn, 2018). Recently, Poliak et al. (2018a) used recast NLI datasets to investigate semantics captured by NMT encoders. \n\nTargeted Tests for Natural Language Understanding We follow a long line of work focused on building datasets to test how well NLU systems perform distinct types of semantic reasoning. FraCaS uses a limited number of sentencepairs to test whether systems understand semantic phenomena, e.g. generalized quantifiers, temporal references, and (nominal) anaphora (Cooper et al., 1996). FraCas cannot be used to train neural models -it includes just roughly 300 highquality instances manually created by linguists. MacCartney (2009) created the FraCaS textual inference test suite by automatically \"convert [ing] each FraCaS question into a declarative hypothesis.\" Levesque et al. (2012)'s Winograd Schema Challenge forces a model to choose between two possible answers for a question based on a sentence describing an event. \n\nRecent benchmarks test whether NLI models handle adjective-noun composition (Pavlick and Callison-Burch, 2016), other types of composition (Dasgupta et al., 2018), paraphrastic inference, anaphora resolution, and semantic proto-roles (White et al., 2017). Concurrently, Conneau et al. (2018)'s benchmark can be used to probe whether sentence representations capture many linguistic properties. It includes syntactic and surface form tests but does not focus on as a wide range of semantic phenomena as in the DNC.",
            "score": 0.3784741227872228,
            "section_title": "Related Work",
            "char_start_offset": 28874,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1481
                },
                {
                    "start": 1484,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1997
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 172,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 317,
                    "end": 335,
                    "matchedPaperCorpusId": "7197724"
                },
                {
                    "start": 506,
                    "end": 525,
                    "matchedPaperCorpusId": "37605145"
                },
                {
                    "start": 525,
                    "end": 548,
                    "matchedPaperCorpusId": "51869843"
                },
                {
                    "start": 560,
                    "end": 581,
                    "matchedPaperCorpusId": "13888490"
                },
                {
                    "start": 1321,
                    "end": 1343,
                    "matchedPaperCorpusId": "15710851"
                },
                {
                    "start": 1560,
                    "end": 1594,
                    "matchedPaperCorpusId": "115505"
                },
                {
                    "start": 1718,
                    "end": 1738,
                    "matchedPaperCorpusId": "12756520"
                },
                {
                    "start": 1754,
                    "end": 1775,
                    "matchedPaperCorpusId": "24461982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.457763671875
        },
        {
            "corpus_id": "15376353",
            "title": "Automata Networks for Memory Loss Effects in the Formation of Linguistic Conventions",
            "text": "Contrarily to the extended view on language evolution [3,7] which proposes a gradual transition (through successive stages) between a \"protolanguage\", a modern language minus syntax, and modern languages, recent works have been suggested the absence of intermediate stages. For instance, in [5] it is suggested the appearance of phase transitions (scaling relations close to the Zipf 's law ) in the emergence of vocabularies under least effort constraints. This work attempts to give new theoretical insights to the absence of intermediate stages in the evolution of language. The startpoint is to develop a mathematical approach to a crucial question: how a population of language users can reach agreement on a linguistic convention? [10,11,2,12]. Surprisingly, language users collectively reach shared languages without any kind of central control or \"telepathy\" influencing the formation of language, and only from local conversations between few participants. The solution is based on two opposite alignment preferences, which guide the behavior of language users by the selection of the words that give the highest chance of communicative success and the removal of the words that imply failures during communication [12]. These procedures can be understand as part of the lateral inhibition strategy [10]. If each convention is associated to a score measuring its amount of success, the score will decrease in the case of unsuccessful communicative interactions, and the convention will be less used. In consequence, the outcome of alignment strategies is the self-organization of agreement: the successful words will be more common, the individuals will align their own languages and there will be an increasing of the chance of successful interactions. \n\nTo describe the appearance of sharp transitions in language formation, it is adopted an extremely simple model of (working) memory [1], understood as a temporal finite memory involved in on-line tasks and, specially, in language production and comprehension. At each time step, language users simply \"loss\" part of their word-memories. What is more, it is hypothesized that the features of language (in particular, the consensus on a linguistic convention) emerge drastically at some critical memory loss capacity [6]. \n\nThe view of this work is based on a automata networks model [8,14]. Automata networks are attractive models for systems that exhibit self-organization.",
            "score": 0.378267679247005,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1762
                },
                {
                    "start": 1765,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2283
                },
                {
                    "start": 2286,
                    "end": 2353
                },
                {
                    "start": 2354,
                    "end": 2437
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 59,
                    "matchedPaperCorpusId": "8496536"
                },
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "6717688"
                },
                {
                    "start": 737,
                    "end": 741,
                    "matchedPaperCorpusId": "6726135"
                },
                {
                    "start": 746,
                    "end": 749,
                    "matchedPaperCorpusId": "8474668"
                },
                {
                    "start": 1224,
                    "end": 1228,
                    "matchedPaperCorpusId": "8474668"
                },
                {
                    "start": 1308,
                    "end": 1312,
                    "matchedPaperCorpusId": "6726135"
                },
                {
                    "start": 1896,
                    "end": 1899,
                    "matchedPaperCorpusId": "142763675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2425537109375
        },
        {
            "corpus_id": "265157830",
            "title": "SAIE Framework: Support Alone Isn't Enough - Advancing LLM Training with Adversarial Remarks",
            "text": "LLMs have demonstrated remarkable capabilities in language comprehension and generation across a wide range of tasks (Brown et al., 2020;Chung et al., 2022;OpenAI, 2023). A pivotal development in this domain is their capability to predict through discussions with either other models or humans. For example, LLMs can explain reasons for their predictions in response to counterarguments, persuade others, or correct their own mistakes through multi-turn discussions (Kaneko et al., 2023). This Only the learner model undergoes parameter updates based on these interactions. Question in this example is: 'Tom decides to renovate a house. There are 3 bedrooms and each bedroom takes 4 hours to renovate. The kitchen takes 50% longer than each bedroom. The living room took twice as much time as everything else combined. How long did everything take?' performance of LLMs, particularly in tasks that demand complex reasoning processes (Talebirad and Nadiri, 2023). \n\nRecent studies have underscored the impact of proactive discussion during the inference phase in amplifying the efficacy of LLMs (Liang et al., 2023;Xiong et al., 2023;Chen et al., 2023;Madaan et al., 2023;Du et al., 2023). Through these discussions, models refine their problem-solving approaches, thereby improving task performance. Nevertheless, such enhancements largely polish the models' capabilities after their foundational learning has been set. This raises a crucial question: Is enhancing LLMs solely during the inference stage sufficient for developing reasoning and criti-cal thinking abilities? \n\nThe integration of interactive discussions during the training phase emerges as a novel area of research. Studies by Welleck et al. (2022) and Paul et al. (2023) have explored the generator-corrector framework, wherein feedback boosts model generation performance. This approach, focusing on correcting errors using intermediate representations like equations or logical expressions, may constrain the models' adaptability across diverse tasks. Moreover, an excessive emphasis on error correction could inadvertently encourage the generator model to rely on memorizing feedback rather than developing a profound understanding.",
            "score": 0.37812352642341274,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1573
                },
                {
                    "start": 1576,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2202
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 137,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.619140625
        },
        {
            "corpus_id": "277740884",
            "title": "Linguistic Interpretability of Transformer-based Language Models: a systematic review",
            "text": "In this survey we have presented a series of research work that attempt to discover how linguistic knowledge may be present inside modern, state-of-the-art PLMs based on the Transformer architecture. We have presented their overall conclusions in an organized manner, from the perspective of different traditional linguistic disciplines such as syntax or lexico-semantics, across several languages with different typologies. Overall, we have been able to report many instances of well-encoded linguistic phenomena inside these models, but also of systematic fails of some other phenomena at the same time. The provided conclusions on linguistic competence of these models, although contradicting at times, seem to point nevertheless in the direction of an architecture which is able to generalize certain aspects of human language rules, roughly corresponding to the same ones seen on traditional linguistics, which it seems to deduce simply from the pre-training texts -of different quality-it has seen. It achieves this linguistic proficiency, however, in its own terms -i.e. \n\nvia statistically-based methods-and with some errors along the way. \n\nIt is unclear as which future tendencies there will be in the study of linguistic interpretability in PLMs. The rise of increasingly larger autoregressive models may mean the appeareance of more prompting-like studies in the near future, whereas probing and related techniques might become increasingly phased out due to the amount of computation needed to deploy them in LLMs [185]. This is already happening in some degree in our observed list of papers, as many of the analysis we have found are done in BERT-based models and omit more modern ones. Perhaps ever-larger PLMs may become increasingly unaffordable to pre-train at some point and the industry (particularly small and medium-sized enterprises) may shift to smaller-sized models. This could render our analyzed interpretability techniques still suitable in this hypothetical scenario.",
            "score": 0.37741868159342135,
            "section_title": "Conclusions",
            "char_start_offset": 87619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1077
                },
                {
                    "start": 1080,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1997
                }
            ],
            "ref_mentions": [
                {
                    "start": 1527,
                    "end": 1532,
                    "matchedPaperCorpusId": "261530292"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42822265625
        },
        {
            "corpus_id": "264398751",
            "title": "Language models and psychological sciences",
            "text": "Importantly, due to their size and complexity, the behaviour of LLMs cannot be predicted in advance by looking at the architecture and training corpus and must instead be empirically explored. The procedure required for evaluating LLM is like that used by cognitive psychologists to study the human mind and consists in testing LLMs with tasks that are believed to tap on specific cognitive functions. \n\nAs stated earlier, the advent of LLMs has sparked a robust debate within the AI community, centring on the question of whether machines possess the capability to genuinely comprehend natural language, thereby capturing the interplay of both physical and social contexts encapsulated within linguistic expression. The implications of this debate extend beyond practical applications, delving into the realm of psychological cognition. This is because LLMs, as elucidated in the subsequent discussion, exhibit a remarkable proficiency in simulating reasoning abilities that have traditionally been regarded as distinctly human. \n\nRecently, cognitive psychologists have introduced a novel evaluation methodology for LLMs. This approach involves treating LLMs as active participants within a psychological experiment, thereby facilitating a comprehensive assessment of their cognitive capabilities. Cognitive psychologists believe that this approach offers different advantages over existing evaluation protocols which are not driven by a cognitive model. The use of psychological-inspired tests to scrutinize LLMs' performance serves a multifaceted purpose. These tests aim to uncover underlying cognitive biases and different problem-solving approaches and methodologies that extend beyond the confines of conventional performance-based analyses, which have been the focal point of previous investigations. By demystifying how LLMs solve challenging reasoning problems, psychological experiments can provide a deeper understanding of their full complexity. \n\nHerein, we introduce the preliminary l findings arising from an investigation conducted over the last 6 months (late 2022 -June 2023), centred on the assessment of the reasoning abilities of LLMs using evaluation protocols initially formulated for human assessment.",
            "score": 0.3773392469285206,
            "section_title": "Psychological assessment of LLMs",
            "char_start_offset": 15408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1029
                },
                {
                    "start": 1032,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1958
                },
                {
                    "start": 1961,
                    "end": 2226
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61669921875
        },
        {
            "corpus_id": "250429802",
            "title": "Disentangling Hierarchical and Sequential Computations during Sentence Processing",
            "text": "Third, while the local effect was traced back to auditory cortices (30,80), the global one is distributed across the superior temporal sulcus, inferior frontal gyrus, dorsolateral prefrontal, intraparietal, anterior, and posterior cingulate cortices (77). Taken together, these findings show the co-existence of two distinct neural mechanisms involved in the processing of sequence patterns, with very different properties and which are sensitive to regularities at different time scales (34,35). \n\nIn contrast to the processing of auditory stimuli, the results from sentence processing differ substantially. For all three constructions, we found no transition-based effect, and only the structural effect was significant (Figure 4). This suggests that during language processing, structurebased computations dominate over transition-based computations, which is opposite to the case of non-linguistic stimuli. Specifically, in the local-global paradigm with sequences of auditory tones, transition effects are easily detectable, while structural effects can be fragile and get reduced, for instance, when subjects are distracted. For language processing, this is the opposite: while the structural effect is large and easily detectable, the transition effect is barely detectable or non-existent. Thus, once sequence items enter into the language system, as is the case here for features of number and animacy, sentence-level computations are entirely dominated by structure-sensitive processes, and largely robust to low-level transition effects. \n\nB. Distinct mechanisms underlying long-range agreement in humans and in language models. Recent studies on neural language models show that structure-sensitive neural mechanisms dedicated to the processing of long-range agreements naturally emerge in the models during training (36). Specifically, it was found that long-range agreements are processed in the models by a small neural circuit, composed of a few specialized units. The core of this neural circuit contains two types of units, termed 'Syntax units' and 'Long-range number units'. The syntax units are sensitive to the latent structure of the sentence, and they convey this information to the number units, which in turn carry the grammatical information of the main subject up to the verb.",
            "score": 0.3772772284292165,
            "section_title": "Discussion",
            "char_start_offset": 38633,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1548
                },
                {
                    "start": 1551,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 71,
                    "matchedPaperCorpusId": "17268842"
                },
                {
                    "start": 71,
                    "end": 74,
                    "matchedPaperCorpusId": "11103368"
                },
                {
                    "start": 250,
                    "end": 254,
                    "matchedPaperCorpusId": "9156042"
                },
                {
                    "start": 488,
                    "end": 492,
                    "matchedPaperCorpusId": "1302541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.603515625
        },
        {
            "corpus_id": "278327159",
            "title": "R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation",
            "text": "\"Setting goals is the first step in turning the invisible into the visible.\" \n\n-Tony Robbins Reasoning, the systematic process of synthesizing knowledge to solve novel problems, lies at the heart of intelligence. Yet, as foundation models grow increasingly sophisticated, existing benchmarks fail to comprehensively assess their complex reasoning capabilities. As shown in the above quote, before equipping foundation models with reasoning skills, we should first define goals for them by establishing a reliable evaluation to assess their reasoning capabilities. \n\nAs noted in (Kahneman, 2011) and (Wei et al., 2022), realizing system-I, a.k.a., quick and intuitive thinking and system-II, a.k.a., slow and deliberate reasoning raises distinct requirements on foundation models. Similarly, assessing quick thinking and complex reasoning requires substantially different assessment methods. On the one hand, evaluating system-I needs to evaluate the knowledge and memory, which requires collecting various daily conversations and knowledge-based questions e.g., concept and common sense questions. On the other hand, evaluating system-II requires evaluating complex reasoning skills. It requires gathering a diverse range of reasoning questions, such as analytical and deductive ones, which is more chal- lenging to collect and filter than the former. In this paper, we focus on building a reliable complex reasoning benchmark for both large language models (LLMs) and multimodal large language models (MLLMs). \n\nHow can we design an ideal assessment for complex reasoning? We believe following four properties are critical. \n\n\u2022 Comprehensiveness. Evaluating the intelligence of foundation models is akin to evaluating human intelligence. We cannot focus on just one aspect, such as mathematics. A comprehensive evaluation is essential. \n\n\u2022 Difficulty. A meaningful evaluation should exhibit the capability to effectively discriminate between the performance of different models and provide valuable insights for guiding model improvement. At present, foundation models are developing rapidly, and some simple benchmarks have been saturated and cannot provide guidance and discrimination for advanced models. \n\n\u2022 Multimodality. We live in a multimodal world, constantly processing various visual and linguistic signals. \n\nTherefore, an ideal benchmark should be designed to assess both LLMs and MLLMs. \n\n\u2022 Multilingualism. We believe that performing complex reasoning is more challenging than understanding multiple languages.",
            "score": 0.3772540588844676,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 79,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1624
                },
                {
                    "start": 1627,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2208
                },
                {
                    "start": 2211,
                    "end": 2227
                },
                {
                    "start": 2228,
                    "end": 2319
                },
                {
                    "start": 2322,
                    "end": 2401
                },
                {
                    "start": 2404,
                    "end": 2422
                },
                {
                    "start": 2423,
                    "end": 2526
                }
            ],
            "ref_mentions": [
                {
                    "start": 599,
                    "end": 617,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44970703125
        },
        {
            "corpus_id": "264306063",
            "title": "An Emulator for Fine-Tuning Large Language Models using Small Language Models",
            "text": "Widely used instruction-following large language models (LLMs) typically follow a two-stage training procedure, with a stage of unsupervised pre-training on a large, diverse dataset followed by supervised finetuning on a much smaller, carefully curated dataset (Raffel et al., 2020;Chung et al., 2022). While both stages are important in producing models that possess broad world knowledge and perform a given task reliably, identifying exactly what capabilities emerge in which stage and at what scale is difficult (Wei et al., 2022;Schaeffer et al., 2023). For example, pre-trained models typically require careful prompting in order to perform a task; after fine-tuning for instruction following, they typically do not. Evaluation of the extent to which the core capability of 'instruction following' is learned during pre-training vs. during fine-tuning is thus seriously complicated by the choice of this prompt. To enable more direct attribution of capabilities to a stage of training, we introduce a principled technique for emulating the result of combining the capabilities gained from pre-training and fine-tuning at different model scales; see Figure 1. This technique, which we call emulated fine-tuning (EFT), enables: a) direct study of the capabilities that change as only one stage is scaled up or down; b) the practical benefit of approximating the result of fine-tuning a large model without the associated computational expense; and c) the ability to modify the fine-tuning objective (e.g., the tradeoff between helpfulness and harmlessness) at test time, without additional training. \n\nEmulated fine-tuning is based on a simple factorization of the logits of a fine-tuned language model into a) the base log probabilities of a pre-trained base model and b) the 'behavior delta', or the difference between the log probabilities of a base model and fine-tuned model. This delta is a compact representation of the behavior change learned in fine-tuning and can be justified through either a reinforcement learning (Rafailov et al., 2023) or Bayesian inference (Korbak et al., 2022) framework.",
            "score": 0.37717659178945334,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2109
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 282,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "258378235",
            "title": "Modelling Language Acquisition through Syntactico-Semantic Pattern Finding",
            "text": "The scientific contribution of the methodology and experiments presented in this paper is twofold. On the one hand, they provide computational evidence for the cognitive plausibility of constructivist theories of language acquisition. These theories, as most prominently put forward by Tomasello (2003), attribute the ability of children to acquire language to two main cognitive capacities: intention reading and pattern finding. Intention reading deals with reconstructing the intended meaning of observed utterances, while pattern finding implements generalisation processes that distil these reconstructed utterance-meaning pairs into abstract schemata embodying the linguistic knowledge of a language user. These schemata can then be used to fulfil the communicative function of language through the comprehension and production of natural language expressions. The methodology introduced in this paper presents a mechanistic model of the pattern finding capacity. Based on utterances paired with a representation of their meaning, the learning algorithm gradually builds up an inventory of concrete to abstract form-meaning mappings, called constructions, along with a network of emergent grammatical categories that captures how the constructions of the grammar can combine to collaboratively comprehend and produce utterances. The experiments show that a small number of general learning operators, which become active if an utterance cannot be successfully processed by the grammar learnt so far, effectively leads to learning dynamics that are similar to those described in the psycholinguistic literature (Pine and Lieven, 1997;Tomasello, 2003;Ambridge and Lieven, 2015). In the first phase of the learning process, the learner acquires holistic mappings between utterances and their meaning representation. Soon after that, holophrase constructions are generalised to item-based constructions that integrate a variable slot. At the same time, this generalisation process leads to the emergence of slot-filling constructions, here called lexical constructions. Along with the item-based and lexical constructions, a network of grammatical categories emerges, capturing the distribution of construction slots and their observed fillers. In a third phase, more abstract itembased constructions emerge, with an increasingly large number of variable slots. In the final phase of the learning process, most constructions have already been acquired and most remaining impasses can be solved by adding new links to the categorial network. The learning dynamics are influenced by the degree of entrenchment of constructions.",
            "score": 0.3770818612161002,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 26459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2246
                },
                {
                    "start": 2247,
                    "end": 2363
                },
                {
                    "start": 2364,
                    "end": 2542
                },
                {
                    "start": 2543,
                    "end": 2627
                }
            ],
            "ref_mentions": [
                {
                    "start": 1616,
                    "end": 1639,
                    "matchedPaperCorpusId": "145808085"
                },
                {
                    "start": 1655,
                    "end": 1681,
                    "matchedPaperCorpusId": "60893172"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87255859375
        }
    ],
    "quotes": {
        "cost": 0.16791900000000004,
        "quotes": [
            {
                "idx": 0,
                "key": "[258378235 | Doumen et al. | 2023 | Citations: 15]",
                "snippets": "The experiments show that a small number of general learning operators, which become active if an utterance cannot be successfully processed by the grammar learnt so far, effectively leads to learning dynamics that are similar to those described in the psycholinguistic literature (Pine et al., 1997)Tomasello, 2003;(Ambridge et al., 2015). In the first phase of the learning process, the learner acquires holistic mappings between utterances and their meaning representation. Soon after that, holophrase constructions are generalised to item-based constructions that integrate a variable slot. At the same time, this generalisation process leads to the emergence of slot-filling constructions, here called lexical constructions. Along with the item-based and lexical constructions, a network of grammatical categories emerges, capturing the distribution of construction slots and their observed fillers. In a third phase, more abstract itembased constructions emerge, with an increasingly large number of variable slots. In the final phase of the learning process, most constructions have already been acquired and most remaining impasses can be solved by adding new links to the categorial network. The learning dynamics are influenced by the degree of entrenchment of constructions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Discussion and Conclusion",
                        "pdf_hash": "",
                        "start": 1335,
                        "end": 2627,
                        "sentence_offsets": [
                            {
                                "start": 1335,
                                "end": 1682
                            },
                            {
                                "start": 1683,
                                "end": 1818
                            },
                            {
                                "start": 1819,
                                "end": 1936
                            },
                            {
                                "start": 1937,
                                "end": 2071
                            },
                            {
                                "start": 2072,
                                "end": 2246
                            },
                            {
                                "start": 2247,
                                "end": 2363
                            },
                            {
                                "start": 2364,
                                "end": 2542
                            },
                            {
                                "start": 2543,
                                "end": 2627
                            }
                        ],
                        "ref_mentions": [
                            "145808085",
                            "60893172"
                        ],
                        "quote": "The experiments show that a small number of general learning operators, which become active if an utterance cannot be successfully processed by the grammar learnt so far, effectively leads to learning dynamics that are similar to those described in the psycholinguistic literature (Pine et al., 1997)Tomasello, 2003;(Ambridge et al., 2015). In the first phase of the learning process, the learner acquires holistic mappings between utterances and their meaning representation. Soon after that, holophrase constructions are generalised to item-based constructions that integrate a variable slot. At the same time, this generalisation process leads to the emergence of slot-filling constructions, here called lexical constructions. Along with the item-based and lexical constructions, a network of grammatical categories emerges, capturing the distribution of construction slots and their observed fillers. In a third phase, more abstract itembased constructions emerge, with an increasingly large number of variable slots. In the final phase of the learning process, most constructions have already been acquired and most remaining impasses can be solved by adding new links to the categorial network. The learning dynamics are influenced by the degree of entrenchment of constructions."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[261049443 | Nolfi | 2023 | Citations: 11]",
                "snippets": "The interest in this topic was raised by the publication of an influential article by (Wei et al., 2022) entitled \"Emergent abilities of large language models\". The authors observe that several abilities, such as in-context learning, instruction following, step-by-step reasoning, and arithmetic skills, are not present in smaller models. They appear in larger models only. Moreover, they show that the acquisition process of these abilities in sufficiently large models is characterized by a sharp transition between a phase in which the system does not show any progress in the ability and a subsequent phase in which the system progresses. According to the authors, these two observations imply that predicting the abilities that will be acquired by scaled-up models will be impossible. In other words, they claim that the abilities that will be developed by scaled-up models cannot be extrapolated from what we know about smaller models. (Wei et al., 2022) later questioned the presence of sharp transitions during the training process and showed that the occurrence of sharp or more continuous transitions in the acquisition process depends on the metric used to evaluate the performance. The occurrence of sharp transitions might thus be induced by the utilization of specific metrics.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249674500 | Wei et al. | 2022 | Citations: 2516]": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models."
                },
                "metadata": [
                    {
                        "section_title": "Predictability and emergence",
                        "pdf_hash": "",
                        "start": 1001,
                        "end": 2290,
                        "sentence_offsets": [
                            {
                                "start": 1001,
                                "end": 1160
                            },
                            {
                                "start": 1161,
                                "end": 1338
                            },
                            {
                                "start": 1339,
                                "end": 1373
                            },
                            {
                                "start": 1374,
                                "end": 1642
                            },
                            {
                                "start": 1643,
                                "end": 1789
                            },
                            {
                                "start": 1790,
                                "end": 1941
                            },
                            {
                                "start": 1942,
                                "end": 2192
                            },
                            {
                                "start": 2193,
                                "end": 2290
                            }
                        ],
                        "ref_mentions": [
                            "249674500",
                            "249674500"
                        ],
                        "quote": "The interest in this topic was raised by the publication of an influential article by (Wei et al., 2022) entitled \"Emergent abilities of large language models\". The authors observe that several abilities, such as in-context learning, instruction following, step-by-step reasoning, and arithmetic skills, are not present in smaller models. They appear in larger models only. Moreover, they show that the acquisition process of these abilities in sufficiently large models is characterized by a sharp transition between a phase in which the system does not show any progress in the ability and a subsequent phase in which the system progresses. According to the authors, these two observations imply that predicting the abilities that will be acquired by scaled-up models will be impossible. In other words, they claim that the abilities that will be developed by scaled-up models cannot be extrapolated from what we know about smaller models. (Wei et al., 2022) later questioned the presence of sharp transitions during the training process and showed that the occurrence of sharp or more continuous transitions in the acquisition process depends on the metric used to evaluate the performance. The occurrence of sharp transitions might thus be induced by the utilization of specific metrics."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[261277016 | Chang et al. | 2023 | Citations: 13]",
                "snippets": "In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)...language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[233289478 | Liu et al. | 2021 | Citations: 87]": "Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.",
                    "[222140842 | Chiang et al. | 2020 | Citations: 42]": "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL.",
                    "[238408423 | Chang et al. | 2021 | Citations: 40]": "We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words\u2019 ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 292,
                        "end": 460,
                        "sentence_offsets": [
                            {
                                "start": 292,
                                "end": 461
                            }
                        ],
                        "ref_mentions": [
                            "222140842",
                            "233289478"
                        ],
                        "quote": "In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)"
                    },
                    {
                        "section_title": "Discussion",
                        "pdf_hash": "",
                        "start": 991,
                        "end": 1389,
                        "sentence_offsets": [
                            {
                                "start": 963,
                                "end": 1338
                            },
                            {
                                "start": 1339,
                                "end": 1388
                            }
                        ],
                        "ref_mentions": [
                            "238408423",
                            "233289478"
                        ],
                        "quote": "language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021)."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[261822542 | Chen et al. | 2023 | Citations: 74]",
                "snippets": "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process...While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022).\n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes. After the spike, the model starts handling complex linguistic phenomena correctly, as signaled by a break in BLiMP score (which we call the capabilities onset).\n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 235,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process"
                    },
                    {
                        "quote": "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022).\n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes. After the spike, the model starts handling complex linguistic phenomena correctly, as signaled by a break in BLiMP score (which we call the capabilities onset).\n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[264935245 | Quirke et al. | 2023 | Citations: 5]",
                "snippets": "Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020)Wei et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[222140842 | Chiang et al. | 2020 | Citations: 42]": "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 985,
                        "end": 1255,
                        "sentence_offsets": [
                            {
                                "start": 985,
                                "end": 1255
                            }
                        ],
                        "ref_mentions": [
                            "222140842"
                        ],
                        "quote": "Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020)Wei et al., 2022)."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[265067168 | Huang et al. | 2023 | Citations: 903]",
                "snippets": "Pre-training is generally considered a crucial stage for LLM to acquire knowledge and skills (Zhou et al., 2023a). Language models, during pre-training, aim to predict the next token in a sequence autoregressively. Through selfsupervised training on extensive textual corpora, the model acquires knowledge of language syntax, world knowledge, and reasoning abilities, providing a robust foundation for subsequent fine-tuning tasks. Besides, recent research (Sutskever, 2023;Del\u00e9tang et al., 2023) suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence of language models lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Training Stages of Large Language Models",
                        "pdf_hash": "",
                        "start": 383,
                        "end": 1193,
                        "sentence_offsets": [
                            {
                                "start": 383,
                                "end": 497
                            },
                            {
                                "start": 498,
                                "end": 597
                            },
                            {
                                "start": 598,
                                "end": 814
                            },
                            {
                                "start": 815,
                                "end": 980
                            },
                            {
                                "start": 981,
                                "end": 1079
                            },
                            {
                                "start": 1080,
                                "end": 1193
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Pre-training is generally considered a crucial stage for LLM to acquire knowledge and skills (Zhou et al., 2023a). Language models, during pre-training, aim to predict the next token in a sequence autoregressively. Through selfsupervised training on extensive textual corpora, the model acquires knowledge of language syntax, world knowledge, and reasoning abilities, providing a robust foundation for subsequent fine-tuning tasks. Besides, recent research (Sutskever, 2023;Del\u00e9tang et al., 2023) suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence of language models lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[269449935 | Bai et al. | 2024 | Citations: 197]",
                "snippets": "During pre-training, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence. By undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "During pre-training, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence. By undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[270688478 | Cuconasu et al. | 2024 | Citations: 2]",
                "snippets": "Pre-training.Pre-training for large language models (Radford et al., 2018) involves an extensive unsupervised learning phase, where the model is exposed to a large corpus of text data to learn the underlying statistical properties of natural language.This process employs the next token prediction task, where the model is conditioned on a sequence of tokens w 1:i\u22121 and trained to predict the subsequent token w i , i.e., p(y) = n i p \u03b8 (w i |w 1:i\u22121 ).By iteratively processing vast and diverse textual datasets, the model learns linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies.This process delivers what is commonly called a \"base\" model.This first model is usually improved with further specialized training, as we will see now.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "LLM Training",
                        "pdf_hash": "",
                        "start": 213,
                        "end": 1002,
                        "sentence_offsets": [
                            {
                                "start": 213,
                                "end": 226
                            },
                            {
                                "start": 226,
                                "end": 464
                            },
                            {
                                "start": 464,
                                "end": 667
                            },
                            {
                                "start": 667,
                                "end": 850
                            },
                            {
                                "start": 850,
                                "end": 911
                            },
                            {
                                "start": 911,
                                "end": 1002
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Pre-training.Pre-training for large language models (Radford et al., 2018) involves an extensive unsupervised learning phase, where the model is exposed to a large corpus of text data to learn the underlying statistical properties of natural language.This process employs the next token prediction task, where the model is conditioned on a sequence of tokens w 1:i\u22121 and trained to predict the subsequent token w i , i.e., p(y) = n i p \u03b8 (w i |w 1:i\u22121 ).By iteratively processing vast and diverse textual datasets, the model learns linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies.This process delivers what is commonly called a \"base\" model.This first model is usually improved with further specialized training, as we will see now."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[273025704 | Lee et al. | 2024 | Citations: 4]",
                "snippets": "Phase transitions during LM training have been found for some, but not all, aspects of language learning. Negative evidence includes that LM circuits involved in linguistic subtasks are stable (Tigges et al., 2024) and gradually reinforced (Weber et al., 2024) over training. Positive evidence for learning phase transitions includes that the ID of BERT's final [CLS] representation tracks sudden syntax acquisition and drops in training loss (Chen et al., 2023), with similar observations on Transformers trained on formal languages (Lubana et al., 2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[261822542 | Chen et al. | 2023 | Citations: 74]": "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.",
                    "[270370801 | Weber et al. | 2024 | Citations: 4]": "The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes. In this paper, we present an alternative approach, concentrating on the quality of LM processing, with a focus on their language abilities. To this end, we construct 'linguistic task spaces' -- representations of an LM's language conceptualisation -- that shed light on the connections LMs draw between language phenomena. Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call 'similarity probing'. To disentangle the learning signals of linguistic phenomena, we further introduce a method called 'fine-tuning via gradient differentials' (FTGD). We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure. Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks. The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs."
                },
                "metadata": [
                    {
                        "section_title": "Language model training dynamics",
                        "pdf_hash": "",
                        "start": 655,
                        "end": 1211,
                        "sentence_offsets": [
                            {
                                "start": 655,
                                "end": 760
                            },
                            {
                                "start": 761,
                                "end": 930
                            },
                            {
                                "start": 931,
                                "end": 1211
                            }
                        ],
                        "ref_mentions": [
                            "270370801",
                            "261822542"
                        ],
                        "quote": "Phase transitions during LM training have been found for some, but not all, aspects of language learning. Negative evidence includes that LM circuits involved in linguistic subtasks are stable (Tigges et al., 2024) and gradually reinforced (Weber et al., 2024) over training. Positive evidence for learning phase transitions includes that the ID of BERT's final [CLS] representation tracks sudden syntax acquisition and drops in training loss (Chen et al., 2023), with similar observations on Transformers trained on formal languages (Lubana et al., 2024)."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[273661991 | Gopalani et al. | 2024 | Citations: 4]",
                "snippets": "Recent analysis on the training dynamics of Transformers has unveiled an interesting characteristic: the training loss plateaus for a significant number of training steps, and then suddenly (and sharply) drops to near--optimal values.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recent analysis on the training dynamics of Transformers has unveiled an interesting characteristic: the training loss plateaus for a significant number of training steps, and then suddenly (and sharply) drops to near--optimal values.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[274776135 | Wang et al. | 2024 | Citations: 0]",
                "snippets": "The capability for ICL [5] in language models develops gradually during a \"phase change\" observed in the early stages of training. At the same time, models acquire the ability to complete complex patterns through induction heads, suggesting that induction heads are a key mechanistic basis of ICL [40].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 516,
                        "end": 818,
                        "sentence_offsets": [
                            {
                                "start": 516,
                                "end": 646
                            },
                            {
                                "start": 647,
                                "end": 818
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The capability for ICL [5] in language models develops gradually during a \"phase change\" observed in the early stages of training. At the same time, models acquire the ability to complete complex patterns through induction heads, suggesting that induction heads are a key mechanistic basis of ICL [40]."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[276724796 | Nakagi et al. | 2025 | Citations: 0]",
                "snippets": "We propose a novel interpretation for the learning dynamics of LLMs that vary in both training data and architecture, revealing that three phase transitions commonly emerge across these models during training: (1) alignment with the entire brain surges as LLMs begin adhering to task instructions Brain Alignment and Instruction Following, (2) unexpectedly, LLMs diverge from the brain during a period in which downstream task accuracy temporarily stagnates Brain Detachment and Stagnation, and (3) alignment with the brain reoccurs as LLMs become capable of solving the downstream tasks Brain Realignment and Consolidation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 465,
                        "end": 1089,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We propose a novel interpretation for the learning dynamics of LLMs that vary in both training data and architecture, revealing that three phase transitions commonly emerge across these models during training: (1) alignment with the entire brain surges as LLMs begin adhering to task instructions Brain Alignment and Instruction Following, (2) unexpectedly, LLMs diverge from the brain during a period in which downstream task accuracy temporarily stagnates Brain Detachment and Stagnation, and (3) alignment with the brain reoccurs as LLMs become capable of solving the downstream tasks Brain Realignment and Consolidation."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[276766647 | AlKhamissi et al. | 2025 | Citations: 3]",
                "snippets": "We demonstrated that alignment with the human language network (LN) primarily correlates with formal linguistic competence (Mahowald et al., 2024), peaking and saturating early in training. In contrast, functional linguistic competence, which involves world knowledge and reasoning, continues to grow beyond this stage. These findings suggest that the LN primarily encodes syntactic and compositional structure, in line with the literature of language neuroscience (Fedorenko et al., 2024), while broader linguistic functions may rely on other cognitive systems beyond the LN.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion & Future Work",
                        "pdf_hash": "",
                        "start": 135,
                        "end": 709,
                        "sentence_offsets": [
                            {
                                "start": 135,
                                "end": 323
                            },
                            {
                                "start": 324,
                                "end": 453
                            },
                            {
                                "start": 454,
                                "end": 709
                            }
                        ],
                        "ref_mentions": [
                            "268551442",
                            "269112224"
                        ],
                        "quote": "We demonstrated that alignment with the human language network (LN) primarily correlates with formal linguistic competence (Mahowald et al., 2024), peaking and saturating early in training. In contrast, functional linguistic competence, which involves world knowledge and reasoning, continues to grow beyond this stage. These findings suggest that the LN primarily encodes syntactic and compositional structure, in line with the literature of language neuroscience (Fedorenko et al., 2024), while broader linguistic functions may rely on other cognitive systems beyond the LN."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[276902722 | Inaba et al. | 2025 | Citations: 1]",
                "snippets": "Our findings suggest that LLMs initially acquire language-specific knowledge independently, followed by cross-linguistic correspondences. Moreover, we observe that after mastering token-level knowledge, the model transitions to learning higher-level, abstract concepts, indicating the development of more conceptual understanding.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 493,
                        "end": 823,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Our findings suggest that LLMs initially acquire language-specific knowledge independently, followed by cross-linguistic correspondences. Moreover, we observe that after mastering token-level knowledge, the model transitions to learning higher-level, abstract concepts, indicating the development of more conceptual understanding."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[276903421 | Berti et al. | 2025 | Citations: 6]",
                "snippets": "Some studies frame ICL as a form of Bayesian inference over the latent space of language [94], while others view it as a compositional recombination of linguistic structures [32]. Mechanistically, ICL has been linked to neural architectures, such as induction heads [24]59] and functional modules that emerge naturally during training (Bietti et al., 2023)(Irie et al., 2022)81,84].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258999187 | Bietti et al. | 2023 | Citations: 96]": "Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an\"induction head\"mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties.",
                    "[246823378 | Irie et al. | 2022 | Citations: 44]": "Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the 1960s, no prior work has effectively studied the operations of NNs in such a form, presumably due to prohibitive time and space complexities and impractical model sizes, all of them growing linearly with the number of training patterns which may get very large. However, this dual formulation offers a possibility of directly visualising how an NN makes use of training patterns at test time, by examining the corresponding attention weights. We conduct experiments on small scale supervised image classification tasks in single-task, multi-task, and continual learning settings, as well as language modelling, and discuss potentials and limits of this view for better understanding and interpreting how NNs exploit training patterns. Our code is public."
                },
                "metadata": [
                    {
                        "section_title": "IV. EMERGENT ABILITIES AS IN-CONTEXT LEARNING",
                        "pdf_hash": "",
                        "start": 1605,
                        "end": 1953,
                        "sentence_offsets": [
                            {
                                "start": 1605,
                                "end": 1784
                            },
                            {
                                "start": 1785,
                                "end": 1953
                            }
                        ],
                        "ref_mentions": [
                            "258999187",
                            "246823378"
                        ],
                        "quote": "Some studies frame ICL as a form of Bayesian inference over the latent space of language [94], while others view it as a compositional recombination of linguistic structures [32]. Mechanistically, ICL has been linked to neural architectures, such as induction heads [24]59] and functional modules that emerge naturally during training (Bietti et al., 2023)(Irie et al., 2022)81,84]."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[277398866 | Zhang et al. | 2025 | Citations: 0]",
                "snippets": "During the pre-training phase, the model is exposed to a vast corpus of text data, learning to predict the next word in a sentence. This unsupervised learning process enables the model to acquire a deep understanding of syntax, semantics, and world knowledge. By learning from diverse and extensive text data, the model builds a comprehensive language representation that can be applied to various downstream tasks (Radford et al., 2019).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "4) CORE PRINCIPLES OF LLMs",
                        "pdf_hash": "",
                        "start": 2019,
                        "end": 2439,
                        "sentence_offsets": [
                            {
                                "start": 1997,
                                "end": 2150
                            },
                            {
                                "start": 2151,
                                "end": 2278
                            },
                            {
                                "start": 2279,
                                "end": 2439
                            }
                        ],
                        "ref_mentions": [
                            "160025533"
                        ],
                        "quote": "During the pre-training phase, the model is exposed to a vast corpus of text data, learning to predict the next word in a sentence. This unsupervised learning process enables the model to acquire a deep understanding of syntax, semantics, and world knowledge. By learning from diverse and extensive text data, the model builds a comprehensive language representation that can be applied to various downstream tasks (Radford et al., 2019)."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.02193,
        "cot": "To organize the quotes effectively for answering the user query about how linguistic capabilities emerge during language model training and the phase transitions observed, I'll create a structured approach with distinct dimensions.\n\nThe user query specifically asks about:\n1. How distinct linguistic capabilities (syntax, world knowledge, reasoning) emerge\n2. The different phases during language model training\n3. Characteristic phase transitions observed\n\nI'll organize this into the following dimensions:\n\nIntroduction/Background: This section will provide foundational context about language model training and the general concept of capability emergence. This helps frame the more specific information to follow. This should be in synthesis format to provide a coherent overview.\n\nGeneral Training Dynamics and Phase Transitions: This will synthesize information about the general learning dynamics and phase transitions observed during language model training. Synthesis format is appropriate to explain these complex patterns coherently.\n\nEmergence of Syntactic Capabilities: This specifically addresses when and how syntax abilities emerge during training. Synthesis format is best to explain the sequential nature of this process.\n\nEmergence of World Knowledge: This focuses on when world knowledge is acquired during training relative to other capabilities. Synthesis format allows for explaining the timing and process.\n\nEmergence of Reasoning Abilities: This addresses when reasoning capabilities emerge. Synthesis format helps explain this complex capability's development.\n\nOther Emergent Capabilities: This section will cover additional capabilities not specifically mentioned in the query but relevant to the broader picture of language model training phases. List format works well here to enumerate these distinct capabilities.\n\nMechanistic Explanations for Phase Transitions: This dimension will synthesize information about why these phase transitions occur from a mechanistic perspective. Synthesis format helps explain the underlying mechanisms.\n\nEach dimension directly addresses components of the user query, and the organization follows a logical flow from general background to specific capabilities and then to explanatory mechanisms.",
        "plan": {
            "Introduction: Language Model Training and Capability Emergence (synthesis)": [
                5,
                6,
                7,
                15
            ],
            "General Training Dynamics and Phase Transitions (synthesis)": [
                1,
                3,
                4,
                8,
                9,
                11
            ],
            "Emergence of Syntactic Capabilities (synthesis)": [
                0,
                2,
                3,
                12,
                13
            ],
            "Emergence of World Knowledge (synthesis)": [
                2,
                5,
                6,
                12
            ],
            "Emergence of Reasoning Abilities (synthesis)": [
                2,
                5,
                6,
                12
            ],
            "Other Emergent Capabilities (list)": [
                1,
                10,
                13,
                14
            ],
            "Mechanistic Explanations for Phase Transitions (synthesis)": [
                3,
                8,
                10,
                14
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction: Language Model Training and Capability Emergence",
                "tldr": "Language model pre-training involves autoregressive next-token prediction on vast text corpora, enabling models to acquire linguistic capabilities. Through this self-supervised learning process, models progressively develop syntax understanding, world knowledge, and reasoning abilities. (4 sources)",
                "text": "\nLarge language models (LLMs) develop their linguistic capabilities through an extensive pre-training process that follows a consistent pattern across different model architectures. This pre-training phase involves autoregressive prediction, where models learn to predict the next token in a sequence based on previous tokens <Paper corpusId=\"265067168\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"269449935\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper>. The process is fundamentally self-supervised, with models learning from vast and diverse textual corpora without requiring explicit labeling <Paper corpusId=\"270688478\" paperTitle=\"(Cuconasu et al., 2024)\" isShortName></Paper>.\n\nThe mathematical formulation of this learning objective is typically expressed as predicting the probability distribution of the next token given previous tokens, represented as p(wi|w1:i-1) <Paper corpusId=\"270688478\" paperTitle=\"(Cuconasu et al., 2024)\" isShortName></Paper>. This seemingly simple objective of next-token prediction enables models to capture complex linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies <Paper corpusId=\"270688478\" paperTitle=\"(Cuconasu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277398866\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>.\n\nRecent research suggests that the process of predicting subsequent words functions as a form of lossless compression of significant information <Paper corpusId=\"265067168\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. As models improve their prediction accuracy, they demonstrate a deeper understanding of language, which translates to a more nuanced comprehension of the world <Paper corpusId=\"265067168\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. This pre-training phase results in what is commonly referred to as a \"base\" model, which serves as a foundation for further specialized training <Paper corpusId=\"270688478\" paperTitle=\"(Cuconasu et al., 2024)\" isShortName></Paper>.\n\nThe capabilities that emerge during pre-training\u2014understanding of syntax, acquisition of world knowledge, and development of reasoning abilities\u2014provide a robust foundation for downstream tasks <Paper corpusId=\"277398866\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>. This comprehensive language representation enables models to be applied to a wide range of applications after pre-training <Paper corpusId=\"277398866\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "Pre-training is generally considered a crucial stage for LLM to acquire knowledge and skills (Zhou et al., 2023a). Language models, during pre-training, aim to predict the next token in a sequence autoregressively. Through selfsupervised training on extensive textual corpora, the model acquires knowledge of language syntax, world knowledge, and reasoning abilities, providing a robust foundation for subsequent fine-tuning tasks. Besides, recent research (Sutskever, 2023;Del\u00e9tang et al., 2023) suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence of language models lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world."
                        ],
                        "paper": {
                            "corpus_id": 265067168,
                            "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                            "authors": [
                                {
                                    "authorId": "2265930173",
                                    "name": "Lei Huang"
                                },
                                {
                                    "authorId": "2248673673",
                                    "name": "Weijiang Yu"
                                },
                                {
                                    "authorId": "2265878959",
                                    "name": "Weitao Ma"
                                },
                                {
                                    "authorId": "2208739098",
                                    "name": "Weihong Zhong"
                                },
                                {
                                    "authorId": "51056532",
                                    "name": "Zhangyin Feng"
                                },
                                {
                                    "authorId": "2256768984",
                                    "name": "Haotian Wang"
                                },
                                {
                                    "authorId": "1500384901",
                                    "name": "Qianglong Chen"
                                },
                                {
                                    "authorId": "2247980601",
                                    "name": "Weihua Peng"
                                },
                                {
                                    "authorId": "2674998",
                                    "name": "Xiaocheng Feng"
                                },
                                {
                                    "authorId": "2257004102",
                                    "name": "Bing Qin"
                                },
                                {
                                    "authorId": "2238862997",
                                    "name": "Ting Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "ACM Trans. Inf. Syst.",
                            "n_citations": 903
                        },
                        "score": 0.857421875
                    },
                    {
                        "id": "(Bai et al., 2024)",
                        "snippets": [
                            "During pre-training, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence. By undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities."
                        ],
                        "paper": {
                            "corpus_id": 269449935,
                            "title": "Hallucination of Multimodal Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2237427303",
                                    "name": "Zechen Bai"
                                },
                                {
                                    "authorId": "2299164401",
                                    "name": "Pichao Wang"
                                },
                                {
                                    "authorId": "39102205",
                                    "name": "Tianjun Xiao"
                                },
                                {
                                    "authorId": "2264160722",
                                    "name": "Tong He"
                                },
                                {
                                    "authorId": "2008848359",
                                    "name": "Zongbo Han"
                                },
                                {
                                    "authorId": "2298907819",
                                    "name": "Zheng Zhang"
                                },
                                {
                                    "authorId": "2269732179",
                                    "name": "Mike Zheng Shou"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 197
                        },
                        "score": 0.90673828125
                    },
                    {
                        "id": "(Cuconasu et al., 2024)",
                        "snippets": [
                            "Pre-training.Pre-training for large language models (Radford et al., 2018) involves an extensive unsupervised learning phase, where the model is exposed to a large corpus of text data to learn the underlying statistical properties of natural language.This process employs the next token prediction task, where the model is conditioned on a sequence of tokens w 1:i\u22121 and trained to predict the subsequent token w i , i.e., p(y) = n i p \u03b8 (w i |w 1:i\u22121 ).By iteratively processing vast and diverse textual datasets, the model learns linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies.This process delivers what is commonly called a \"base\" model.This first model is usually improved with further specialized training, as we will see now."
                        ],
                        "paper": {
                            "corpus_id": 270688478,
                            "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
                            "authors": [
                                {
                                    "authorId": "2281641042",
                                    "name": "Florin Cuconasu"
                                },
                                {
                                    "authorId": "120709579",
                                    "name": "Giovanni Trappolini"
                                },
                                {
                                    "authorId": "2281641625",
                                    "name": "Nicola Tonellotto"
                                },
                                {
                                    "authorId": "2260653973",
                                    "name": "Fabrizio Silvestri"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.8427734375
                    },
                    {
                        "id": "(Zhang et al., 2025)",
                        "snippets": [
                            "During the pre-training phase, the model is exposed to a vast corpus of text data, learning to predict the next word in a sentence. This unsupervised learning process enables the model to acquire a deep understanding of syntax, semantics, and world knowledge. By learning from diverse and extensive text data, the model builds a comprehensive language representation that can be applied to various downstream tasks (Radford et al., 2019)."
                        ],
                        "paper": {
                            "corpus_id": 277398866,
                            "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
                            "authors": [
                                {
                                    "authorId": "2326763710",
                                    "name": "Dingzong Zhang"
                                },
                                {
                                    "authorId": "2345374431",
                                    "name": "Devi Listiyani"
                                },
                                {
                                    "authorId": "2302811064",
                                    "name": "Priyanka Singh"
                                },
                                {
                                    "authorId": "2309099465",
                                    "name": "Manoranjan Mohanty"
                                }
                            ],
                            "year": 2025,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.89404296875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "General Training Dynamics and Phase Transitions",
                "tldr": "Language model training exhibits non-uniform capability development characterized by distinct phase transitions rather than gradual improvement. These transitions manifest as abrupt performance improvements in specific linguistic abilities at different points during training. (9 sources)",
                "text": "\nThe training trajectory of large language models (LLMs) reveals intriguing dynamics that contradict the notion of uniform capability development. While overall training loss often appears to improve smoothly, research has shown that the acquisition of different capabilities occurs at distinct phases during training <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This non-uniform development is characterized by phase transitions\u2014sudden improvements in specific abilities after periods of apparent stagnation <Paper corpusId=\"261049443\" paperTitle=\"(Nolfi, 2023)\" isShortName></Paper> <Paper corpusId=\"249674500\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>.\n\nA particularly striking pattern observed during training is what researchers call \"breakthroughs,\" \"emergence,\" or \"phase transitions,\" where training loss plateaus for significant periods before experiencing precipitous drops <Paper corpusId=\"273661991\" paperTitle=\"(Gopalani et al., 2024)\" isShortName></Paper>. These transitions are not merely artifacts of measurement but reflect fundamental shifts in model capabilities <Paper corpusId=\"261049443\" paperTitle=\"(Nolfi, 2023)\" isShortName></Paper>. Behavioral analysis confirms that different linguistic abilities\u2014such as predicting parts of speech and individual tokens\u2014are acquired during independent phase transitions with varying slopes and inflection points <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>.\n\nRecent research has identified specific transition patterns during model training. Chen et al. observed a two-stage acquisition process for grammatical capabilities in masked language models: first, a \"structure onset\" marked by the emergence of syntactic attention structure (SAS), followed by a \"capabilities onset\" where the model demonstrates improved performance on grammatical tasks <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This finding demonstrates how analyzing the evolution of interpretable features throughout training provides deeper insights into emergent model behavior.\n\nWhile some research suggests that certain linguistic processes develop gradually rather than through sharp transitions <Paper corpusId=\"273025704\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270370801\" paperTitle=\"(Weber et al., 2024)\" isShortName></Paper>, a novel interpretation by Nakagi et al. proposes three common phase transitions across different LLM architectures: (1) an initial surge in brain alignment as models begin following instructions, (2) a period of brain detachment coinciding with downstream task performance stagnation, and (3) a realignment phase as models consolidate their capabilities <Paper corpusId=\"276724796\" paperTitle=\"(Nakagi et al., 2025)\" isShortName></Paper>.\n\nThe existence of these emergent capabilities and phase transitions has significant implications for model scaling. As noted by Wei et al., certain abilities appear only in sufficiently large models and through sharp transitions, making it challenging to predict which capabilities will emerge in scaled-up versions based solely on observations of smaller models <Paper corpusId=\"261049443\" paperTitle=\"(Nolfi, 2023)\" isShortName></Paper> <Paper corpusId=\"249674500\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process",
                            "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022).\n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes. After the spike, the model starts handling complex linguistic phenomena correctly, as signaled by a break in BLiMP score (which we call the capabilities onset).\n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset."
                        ],
                        "paper": {
                            "corpus_id": 261822542,
                            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                            "authors": [
                                {
                                    "authorId": "13336152",
                                    "name": "Angelica Chen"
                                },
                                {
                                    "authorId": "2240524527",
                                    "name": "Ravid Schwartz-Ziv"
                                },
                                {
                                    "authorId": "1979489",
                                    "name": "Kyunghyun Cho"
                                },
                                {
                                    "authorId": "2240527814",
                                    "name": "Matthew L. Leavitt"
                                },
                                {
                                    "authorId": "2362960",
                                    "name": "Naomi Saphra"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 74
                        },
                        "score": 0.9501953125
                    },
                    {
                        "id": "(Nolfi, 2023)",
                        "snippets": [
                            "The interest in this topic was raised by the publication of an influential article by (Wei et al., 2022) entitled \"Emergent abilities of large language models\". The authors observe that several abilities, such as in-context learning, instruction following, step-by-step reasoning, and arithmetic skills, are not present in smaller models. They appear in larger models only. Moreover, they show that the acquisition process of these abilities in sufficiently large models is characterized by a sharp transition between a phase in which the system does not show any progress in the ability and a subsequent phase in which the system progresses. According to the authors, these two observations imply that predicting the abilities that will be acquired by scaled-up models will be impossible. In other words, they claim that the abilities that will be developed by scaled-up models cannot be extrapolated from what we know about smaller models. (Wei et al., 2022) later questioned the presence of sharp transitions during the training process and showed that the occurrence of sharp or more continuous transitions in the acquisition process depends on the metric used to evaluate the performance. The occurrence of sharp transitions might thus be induced by the utilization of specific metrics."
                        ],
                        "paper": {
                            "corpus_id": 261049443,
                            "title": "On the Unexpected Abilities of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "3015062",
                                    "name": "S. Nolfi"
                                }
                            ],
                            "year": 2023,
                            "venue": "Adaptive Behavior",
                            "n_citations": 11
                        },
                        "score": 0.88623046875
                    },
                    {
                        "id": "(Wei et al., 2022)",
                        "snippets": [
                            "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models."
                        ],
                        "paper": {
                            "corpus_id": 249674500,
                            "title": "Emergent Abilities of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "150272855",
                                    "name": "Rishi Bommasani"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "2368067",
                                    "name": "Barret Zoph"
                                },
                                {
                                    "authorId": "148016269",
                                    "name": "Sebastian Borgeaud"
                                },
                                {
                                    "authorId": "1755465",
                                    "name": "Dani Yogatama"
                                },
                                {
                                    "authorId": "40377863",
                                    "name": "Maarten Bosma"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                },
                                {
                                    "authorId": "1680617",
                                    "name": "Donald Metzler"
                                },
                                {
                                    "authorId": "2226805",
                                    "name": "Ed H. Chi"
                                },
                                {
                                    "authorId": "2117567142",
                                    "name": "Tatsunori Hashimoto"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "2075292388",
                                    "name": "P. Liang"
                                },
                                {
                                    "authorId": "48448318",
                                    "name": "J. Dean"
                                },
                                {
                                    "authorId": "26958176",
                                    "name": "W. Fedus"
                                }
                            ],
                            "year": 2022,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 2516
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gopalani et al., 2024)",
                        "snippets": [
                            "Recent analysis on the training dynamics of Transformers has unveiled an interesting characteristic: the training loss plateaus for a significant number of training steps, and then suddenly (and sharply) drops to near--optimal values."
                        ],
                        "paper": {
                            "corpus_id": 273661991,
                            "title": "Abrupt Learning in Transformers: A Case Study on Matrix Completion",
                            "authors": [
                                {
                                    "authorId": "2124445996",
                                    "name": "Pulkit Gopalani"
                                },
                                {
                                    "authorId": "35573359",
                                    "name": "Ekdeep Singh Lubana"
                                },
                                {
                                    "authorId": "2293775823",
                                    "name": "Wei Hu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 4
                        },
                        "score": 0.85986328125
                    },
                    {
                        "id": "(Quirke et al., 2023)",
                        "snippets": [
                            "Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020)Wei et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 264935245,
                            "title": "Training Dynamics of Contextual N-Grams in Language Models",
                            "authors": [
                                {
                                    "authorId": "2243240285",
                                    "name": "Lucia Quirke"
                                },
                                {
                                    "authorId": "2047549746",
                                    "name": "Lovis Heindrich"
                                },
                                {
                                    "authorId": "2056771333",
                                    "name": "Wes Gurnee"
                                },
                                {
                                    "authorId": "2051128902",
                                    "name": "Neel Nanda"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.83984375
                    },
                    {
                        "id": "(Chiang et al., 2020)",
                        "snippets": [
                            "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."
                        ],
                        "paper": {
                            "corpus_id": 222140842,
                            "title": "Pretrained Language Model Embryology: The Birth of ALBERT",
                            "authors": [
                                {
                                    "authorId": "1992777064",
                                    "name": "Cheng-Han Chiang"
                                },
                                {
                                    "authorId": "2210669195",
                                    "name": "Sung-Feng Huang"
                                },
                                {
                                    "authorId": "1706104",
                                    "name": "Hung-yi Lee"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 42
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2024)",
                        "snippets": [
                            "Phase transitions during LM training have been found for some, but not all, aspects of language learning. Negative evidence includes that LM circuits involved in linguistic subtasks are stable (Tigges et al., 2024) and gradually reinforced (Weber et al., 2024) over training. Positive evidence for learning phase transitions includes that the ID of BERT's final [CLS] representation tracks sudden syntax acquisition and drops in training loss (Chen et al., 2023), with similar observations on Transformers trained on formal languages (Lubana et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 273025704,
                            "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime",
                            "authors": [
                                {
                                    "authorId": "2323997802",
                                    "name": "Jin Hwa Lee"
                                },
                                {
                                    "authorId": "2187058673",
                                    "name": "Thomas Jiralerspong"
                                },
                                {
                                    "authorId": "2324060912",
                                    "name": "Lei Yu"
                                },
                                {
                                    "authorId": "1865800402",
                                    "name": "Y. Bengio"
                                },
                                {
                                    "authorId": "2323783941",
                                    "name": "Emily Cheng"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Weber et al., 2024)",
                        "snippets": [
                            "The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes. In this paper, we present an alternative approach, concentrating on the quality of LM processing, with a focus on their language abilities. To this end, we construct 'linguistic task spaces' -- representations of an LM's language conceptualisation -- that shed light on the connections LMs draw between language phenomena. Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call 'similarity probing'. To disentangle the learning signals of linguistic phenomena, we further introduce a method called 'fine-tuning via gradient differentials' (FTGD). We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure. Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks. The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs."
                        ],
                        "paper": {
                            "corpus_id": 270370801,
                            "title": "Interpretability of Language Models via Task Spaces",
                            "authors": [
                                {
                                    "authorId": "2261081927",
                                    "name": "Lucas Weber"
                                },
                                {
                                    "authorId": "51247075",
                                    "name": "Jaap Jumelet"
                                },
                                {
                                    "authorId": "2173646563",
                                    "name": "E. Bruni"
                                },
                                {
                                    "authorId": "3449411",
                                    "name": "Dieuwke Hupkes"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 4
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nakagi et al., 2025)",
                        "snippets": [
                            "We propose a novel interpretation for the learning dynamics of LLMs that vary in both training data and architecture, revealing that three phase transitions commonly emerge across these models during training: (1) alignment with the entire brain surges as LLMs begin adhering to task instructions Brain Alignment and Instruction Following, (2) unexpectedly, LLMs diverge from the brain during a period in which downstream task accuracy temporarily stagnates Brain Detachment and Stagnation, and (3) alignment with the brain reoccurs as LLMs become capable of solving the downstream tasks Brain Realignment and Consolidation."
                        ],
                        "paper": {
                            "corpus_id": 276724796,
                            "title": "Triple Phase Transitions: Understanding the Learning Dynamics of Large Language Models from a Neuroscience Perspective",
                            "authors": [
                                {
                                    "authorId": "2283312221",
                                    "name": "Yuko Nakagi"
                                },
                                {
                                    "authorId": "2348095768",
                                    "name": "Keigo Tada"
                                },
                                {
                                    "authorId": "2348095652",
                                    "name": "Sota Yoshino"
                                },
                                {
                                    "authorId": "2346109244",
                                    "name": "Shinji Nishimoto"
                                },
                                {
                                    "authorId": "2346108815",
                                    "name": "Yu Takagi"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.97021484375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Emergence of Syntactic Capabilities",
                "tldr": "Syntactic capabilities emerge early in language model training through distinct phases, beginning with holistic mappings that evolve into increasingly abstract constructions with variable slots. This early acquisition of syntax precedes the development of world knowledge and reasoning abilities. (7 sources)",
                "text": "\nThe development of syntactic capabilities in language models follows a well-defined progression through multiple phases. Initially, models acquire holistic mappings between utterances and their meanings, which quickly generalize to item-based constructions with variable slots <Paper corpusId=\"258378235\" paperTitle=\"(Doumen et al., 2023)\" isShortName></Paper>. This early phase also sees the emergence of slot-filling constructions and grammatical categories that capture the distribution of construction slots and their fillers. As training progresses, more abstract constructions emerge with an increasing number of variable slots, eventually stabilizing as the model refines its categorical network <Paper corpusId=\"258378235\" paperTitle=\"(Doumen et al., 2023)\" isShortName></Paper>.\n\nResearch consistently shows that syntactic rules are learned early in the training process, well before the acquisition of world knowledge and reasoning capabilities <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. This pattern holds across different model architectures, with studies showing that early training stages focus primarily on n-gram learning, gradually refining to accommodate longer contexts and more nuanced linguistic capabilities <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>.\n\nChen et al. identified a critical \"structure onset\" during training, marked by a dramatic increase in syntactic attention structure (SAS), followed by a \"capabilities onset\" where models demonstrate improved performance on grammatical tasks <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. These transitions coincide with precipitous drops in masked language modeling loss and represent distinct phases in the acquisition of grammatical knowledge.\n\nThe relationship between syntactic capabilities and brain alignment provides further insight into this developmental trajectory. Research demonstrates that alignment with the human language network primarily correlates with formal linguistic competence, which peaks and saturates early in training <Paper corpusId=\"276766647\" paperTitle=\"(AlKhamissi et al., 2025)\" isShortName></Paper>. This suggests that syntactic and compositional structures are foundational capabilities that stabilize before more complex linguistic functions develop.\n\nIn multilingual contexts, language models first acquire language-specific syntactic knowledge independently before developing cross-linguistic correspondences <Paper corpusId=\"276902722\" paperTitle=\"(Inaba et al., 2025)\" isShortName></Paper>. This sequential process reflects a progression from mastering token-level knowledge to learning higher-level abstract concepts, indicating an evolution toward more conceptual understanding as training advances <Paper corpusId=\"276902722\" paperTitle=\"(Inaba et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Doumen et al., 2023)",
                        "snippets": [
                            "The experiments show that a small number of general learning operators, which become active if an utterance cannot be successfully processed by the grammar learnt so far, effectively leads to learning dynamics that are similar to those described in the psycholinguistic literature (Pine et al., 1997)Tomasello, 2003;(Ambridge et al., 2015). In the first phase of the learning process, the learner acquires holistic mappings between utterances and their meaning representation. Soon after that, holophrase constructions are generalised to item-based constructions that integrate a variable slot. At the same time, this generalisation process leads to the emergence of slot-filling constructions, here called lexical constructions. Along with the item-based and lexical constructions, a network of grammatical categories emerges, capturing the distribution of construction slots and their observed fillers. In a third phase, more abstract itembased constructions emerge, with an increasingly large number of variable slots. In the final phase of the learning process, most constructions have already been acquired and most remaining impasses can be solved by adding new links to the categorial network. The learning dynamics are influenced by the degree of entrenchment of constructions."
                        ],
                        "paper": {
                            "corpus_id": 258378235,
                            "title": "Modelling Language Acquisition through Syntactico-Semantic Pattern Finding",
                            "authors": [
                                {
                                    "authorId": "2187454476",
                                    "name": "Jonas Doumen"
                                },
                                {
                                    "authorId": "2972424",
                                    "name": "Katrien Beuls"
                                },
                                {
                                    "authorId": "150266649",
                                    "name": "Paul Van Eecke"
                                }
                            ],
                            "year": 2023,
                            "venue": "Findings",
                            "n_citations": 15
                        },
                        "score": 0.87255859375
                    },
                    {
                        "id": "(Chang et al., 2023)",
                        "snippets": [
                            "In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)",
                            "language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 261277016,
                            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                            "authors": [
                                {
                                    "authorId": "2087001989",
                                    "name": "Tyler A. Chang"
                                },
                                {
                                    "authorId": "144035504",
                                    "name": "Z. Tu"
                                },
                                {
                                    "authorId": "24316216",
                                    "name": "B. Bergen"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 13
                        },
                        "score": 0.91845703125
                    },
                    {
                        "id": "(Liu et al., 2021)",
                        "snippets": [
                            "Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster."
                        ],
                        "paper": {
                            "corpus_id": 233289478,
                            "title": "Probing Across Time: What Does RoBERTa Know and When?",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Leo Z. Liu"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "11348687",
                                    "name": "Jungo Kasai"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 87
                        },
                        "score": 0.92138671875
                    },
                    {
                        "id": "(Chang et al., 2021)",
                        "snippets": [
                            "We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words\u2019 ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models."
                        ],
                        "paper": {
                            "corpus_id": 238408423,
                            "title": "Word Acquisition in Neural Language Models",
                            "authors": [
                                {
                                    "authorId": "2087001989",
                                    "name": "Tyler A. Chang"
                                },
                                {
                                    "authorId": "24316216",
                                    "name": "B. Bergen"
                                }
                            ],
                            "year": 2021,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 40
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process",
                            "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022).\n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes. After the spike, the model starts handling complex linguistic phenomena correctly, as signaled by a break in BLiMP score (which we call the capabilities onset).\n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset."
                        ],
                        "paper": {
                            "corpus_id": 261822542,
                            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                            "authors": [
                                {
                                    "authorId": "13336152",
                                    "name": "Angelica Chen"
                                },
                                {
                                    "authorId": "2240524527",
                                    "name": "Ravid Schwartz-Ziv"
                                },
                                {
                                    "authorId": "1979489",
                                    "name": "Kyunghyun Cho"
                                },
                                {
                                    "authorId": "2240527814",
                                    "name": "Matthew L. Leavitt"
                                },
                                {
                                    "authorId": "2362960",
                                    "name": "Naomi Saphra"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 74
                        },
                        "score": 0.9501953125
                    },
                    {
                        "id": "(AlKhamissi et al., 2025)",
                        "snippets": [
                            "We demonstrated that alignment with the human language network (LN) primarily correlates with formal linguistic competence (Mahowald et al., 2024), peaking and saturating early in training. In contrast, functional linguistic competence, which involves world knowledge and reasoning, continues to grow beyond this stage. These findings suggest that the LN primarily encodes syntactic and compositional structure, in line with the literature of language neuroscience (Fedorenko et al., 2024), while broader linguistic functions may rely on other cognitive systems beyond the LN."
                        ],
                        "paper": {
                            "corpus_id": 276766647,
                            "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network",
                            "authors": [
                                {
                                    "authorId": "2006905770",
                                    "name": "Badr AlKhamissi"
                                },
                                {
                                    "authorId": "1780803442",
                                    "name": "Greta Tuckute"
                                },
                                {
                                    "authorId": "2329164095",
                                    "name": "Yingtian Tang"
                                },
                                {
                                    "authorId": "2092548838",
                                    "name": "T. Binhuraib"
                                },
                                {
                                    "authorId": "2691021",
                                    "name": "Antoine Bosselut"
                                },
                                {
                                    "authorId": "8551292",
                                    "name": "Martin Schrimpf"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.94287109375
                    },
                    {
                        "id": "(Inaba et al., 2025)",
                        "snippets": [
                            "Our findings suggest that LLMs initially acquire language-specific knowledge independently, followed by cross-linguistic correspondences. Moreover, we observe that after mastering token-level knowledge, the model transitions to learning higher-level, abstract concepts, indicating the development of more conceptual understanding."
                        ],
                        "paper": {
                            "corpus_id": 276902722,
                            "title": "How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders",
                            "authors": [
                                {
                                    "authorId": "2342412237",
                                    "name": "Tatsuro Inaba"
                                },
                                {
                                    "authorId": "2261744024",
                                    "name": "Kentaro Inui"
                                },
                                {
                                    "authorId": "2302320083",
                                    "name": "Yusuke Miyao"
                                },
                                {
                                    "authorId": "50856622",
                                    "name": "Yohei Oseki"
                                },
                                {
                                    "authorId": "2266692",
                                    "name": "Benjamin Heinzerling"
                                },
                                {
                                    "authorId": "2310230516",
                                    "name": "Yu Takagi"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.95263671875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Emergence of World Knowledge",
                "tldr": "World knowledge emerges later in language model training than syntactic capabilities, demonstrating a progressive acquisition pattern from basic linguistic structures to factual information. This knowledge acquisition is more domain-sensitive and less stable than syntactic learning, highlighting a developmental sequence where world understanding builds upon established linguistic foundations. (6 sources)",
                "text": "\nFollowing the early acquisition of syntactic capabilities, language models develop world knowledge as training progresses, establishing a clear developmental sequence. Multiple studies have confirmed that while syntactic rules are learned early in pre-training, world knowledge emerges later and with less stability <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>. This developmental pattern reflects a transition from basic n-gram learning to more nuanced linguistic capabilities that incorporate factual information about the world <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>.\n\nThe acquisition of world knowledge appears to be more domain-sensitive than syntactic learning, with factual and commonsense knowledge emerging more slowly and showing greater variation across domains <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. This suggests that world knowledge acquisition requires more extensive exposure to varied textual contexts compared to the relatively uniform development of syntactic abilities.\n\nInterestingly, research has shown that world knowledge does not necessarily improve linearly as pre-training proceeds <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>. This non-linear development indicates that having more pre-training steps does not automatically provide a model with more comprehensive knowledge, highlighting the complex nature of knowledge acquisition in language models.\n\nThe autoregressive prediction mechanism during pre-training plays a crucial role in enabling models to develop world knowledge <Paper corpusId=\"265067168\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"269449935\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper>. Through self-supervised training on vast textual corpora, models progressively develop the ability to accurately predict the probability distribution for upcoming words, which indicates a deeper understanding of the world and its factual relationships <Paper corpusId=\"265067168\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\nFrom a neuroscientific perspective, the relationship between world knowledge acquisition and brain alignment provides additional insights. While alignment with the human language network primarily correlates with formal linguistic competence and saturates early in training, functional linguistic competence\u2014which encompasses world knowledge and reasoning\u2014continues to develop beyond this stage <Paper corpusId=\"276766647\" paperTitle=\"(AlKhamissi et al., 2025)\" isShortName></Paper>. This suggests that world knowledge acquisition may rely on cognitive systems beyond those responsible for syntactic processing, reflecting the multifaceted nature of linguistic development in both humans and language models.",
                "citations": [
                    {
                        "id": "(Chang et al., 2023)",
                        "snippets": [
                            "In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)",
                            "language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 261277016,
                            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                            "authors": [
                                {
                                    "authorId": "2087001989",
                                    "name": "Tyler A. Chang"
                                },
                                {
                                    "authorId": "144035504",
                                    "name": "Z. Tu"
                                },
                                {
                                    "authorId": "24316216",
                                    "name": "B. Bergen"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 13
                        },
                        "score": 0.91845703125
                    },
                    {
                        "id": "(Liu et al., 2021)",
                        "snippets": [
                            "Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster."
                        ],
                        "paper": {
                            "corpus_id": 233289478,
                            "title": "Probing Across Time: What Does RoBERTa Know and When?",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Leo Z. Liu"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "11348687",
                                    "name": "Jungo Kasai"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 87
                        },
                        "score": 0.92138671875
                    },
                    {
                        "id": "(Chiang et al., 2020)",
                        "snippets": [
                            "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."
                        ],
                        "paper": {
                            "corpus_id": 222140842,
                            "title": "Pretrained Language Model Embryology: The Birth of ALBERT",
                            "authors": [
                                {
                                    "authorId": "1992777064",
                                    "name": "Cheng-Han Chiang"
                                },
                                {
                                    "authorId": "2210669195",
                                    "name": "Sung-Feng Huang"
                                },
                                {
                                    "authorId": "1706104",
                                    "name": "Hung-yi Lee"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 42
                        },
                        "score": 0
                    },
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "Pre-training is generally considered a crucial stage for LLM to acquire knowledge and skills (Zhou et al., 2023a). Language models, during pre-training, aim to predict the next token in a sequence autoregressively. Through selfsupervised training on extensive textual corpora, the model acquires knowledge of language syntax, world knowledge, and reasoning abilities, providing a robust foundation for subsequent fine-tuning tasks. Besides, recent research (Sutskever, 2023;Del\u00e9tang et al., 2023) suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence of language models lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world."
                        ],
                        "paper": {
                            "corpus_id": 265067168,
                            "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                            "authors": [
                                {
                                    "authorId": "2265930173",
                                    "name": "Lei Huang"
                                },
                                {
                                    "authorId": "2248673673",
                                    "name": "Weijiang Yu"
                                },
                                {
                                    "authorId": "2265878959",
                                    "name": "Weitao Ma"
                                },
                                {
                                    "authorId": "2208739098",
                                    "name": "Weihong Zhong"
                                },
                                {
                                    "authorId": "51056532",
                                    "name": "Zhangyin Feng"
                                },
                                {
                                    "authorId": "2256768984",
                                    "name": "Haotian Wang"
                                },
                                {
                                    "authorId": "1500384901",
                                    "name": "Qianglong Chen"
                                },
                                {
                                    "authorId": "2247980601",
                                    "name": "Weihua Peng"
                                },
                                {
                                    "authorId": "2674998",
                                    "name": "Xiaocheng Feng"
                                },
                                {
                                    "authorId": "2257004102",
                                    "name": "Bing Qin"
                                },
                                {
                                    "authorId": "2238862997",
                                    "name": "Ting Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "ACM Trans. Inf. Syst.",
                            "n_citations": 903
                        },
                        "score": 0.857421875
                    },
                    {
                        "id": "(Bai et al., 2024)",
                        "snippets": [
                            "During pre-training, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence. By undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities."
                        ],
                        "paper": {
                            "corpus_id": 269449935,
                            "title": "Hallucination of Multimodal Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2237427303",
                                    "name": "Zechen Bai"
                                },
                                {
                                    "authorId": "2299164401",
                                    "name": "Pichao Wang"
                                },
                                {
                                    "authorId": "39102205",
                                    "name": "Tianjun Xiao"
                                },
                                {
                                    "authorId": "2264160722",
                                    "name": "Tong He"
                                },
                                {
                                    "authorId": "2008848359",
                                    "name": "Zongbo Han"
                                },
                                {
                                    "authorId": "2298907819",
                                    "name": "Zheng Zhang"
                                },
                                {
                                    "authorId": "2269732179",
                                    "name": "Mike Zheng Shou"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 197
                        },
                        "score": 0.90673828125
                    },
                    {
                        "id": "(AlKhamissi et al., 2025)",
                        "snippets": [
                            "We demonstrated that alignment with the human language network (LN) primarily correlates with formal linguistic competence (Mahowald et al., 2024), peaking and saturating early in training. In contrast, functional linguistic competence, which involves world knowledge and reasoning, continues to grow beyond this stage. These findings suggest that the LN primarily encodes syntactic and compositional structure, in line with the literature of language neuroscience (Fedorenko et al., 2024), while broader linguistic functions may rely on other cognitive systems beyond the LN."
                        ],
                        "paper": {
                            "corpus_id": 276766647,
                            "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network",
                            "authors": [
                                {
                                    "authorId": "2006905770",
                                    "name": "Badr AlKhamissi"
                                },
                                {
                                    "authorId": "1780803442",
                                    "name": "Greta Tuckute"
                                },
                                {
                                    "authorId": "2329164095",
                                    "name": "Yingtian Tang"
                                },
                                {
                                    "authorId": "2092548838",
                                    "name": "T. Binhuraib"
                                },
                                {
                                    "authorId": "2691021",
                                    "name": "Antoine Bosselut"
                                },
                                {
                                    "authorId": "8551292",
                                    "name": "Martin Schrimpf"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.94287109375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Emergence of Reasoning Abilities",
                "tldr": "Reasoning capabilities emerge later in language model training, after syntactic understanding and world knowledge acquisition. This hierarchical development suggests reasoning depends on foundational linguistic skills and factual knowledge, with models progressing from simple n-gram prediction to complex inference abilities. (7 sources)",
                "text": "\nAmong the linguistic capabilities that emerge during language model training, reasoning abilities represent the most advanced phase of development. Research consistently demonstrates that reasoning emerges later in the training process, following the acquisition of syntactic rules and world knowledge <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>. This sequential development suggests a hierarchical dependency where reasoning capabilities build upon the foundation of syntactic understanding and factual knowledge.\n\nUnlike syntactic capabilities that emerge early and stabilize quickly, reasoning abilities demonstrate a more complex developmental trajectory. Studies have found that reasoning abilities are \"not stably acquired\" during training, indicating a more fragile and potentially domain-dependent acquisition process <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. This instability contrasts with the more robust development of syntactic capabilities, highlighting the increased complexity involved in mastering reasoning tasks.\n\nThe autoregressive prediction mechanism appears fundamental to the emergence of reasoning capabilities. Through next-token prediction on extensive textual corpora, models progressively develop the ability to make logical inferences and draw conclusions based on contextual information <Paper corpusId=\"265067168\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"269449935\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper>. This self-supervised learning process enables models to transition from simple n-gram learning early in pre-training to more sophisticated reasoning in later stages <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>.\n\nFrom a neuroscientific perspective, the development of reasoning abilities corresponds to what researchers term \"functional linguistic competence,\" which continues to improve even after formal linguistic competence (primarily syntax) has plateaued <Paper corpusId=\"276766647\" paperTitle=\"(AlKhamissi et al., 2025)\" isShortName></Paper>. This suggests that reasoning capabilities may engage cognitive systems beyond those responsible for basic language processing, reflecting a more distributed neural architecture for advanced linguistic functions.\n\nThe progressive refinement of language models' predictive capabilities\u2014from unigram frequencies to bigram probabilities and eventually to more nuanced predictions\u2014provides insight into how reasoning abilities might emerge <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>. As models accumulate syntactic knowledge and world information, they gradually develop the capacity to make predictions that require multi-step logical inference, demonstrating the emergent nature of reasoning in language model training.",
                "citations": [
                    {
                        "id": "(Chang et al., 2023)",
                        "snippets": [
                            "In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)",
                            "language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 261277016,
                            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                            "authors": [
                                {
                                    "authorId": "2087001989",
                                    "name": "Tyler A. Chang"
                                },
                                {
                                    "authorId": "144035504",
                                    "name": "Z. Tu"
                                },
                                {
                                    "authorId": "24316216",
                                    "name": "B. Bergen"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 13
                        },
                        "score": 0.91845703125
                    },
                    {
                        "id": "(Liu et al., 2021)",
                        "snippets": [
                            "Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster."
                        ],
                        "paper": {
                            "corpus_id": 233289478,
                            "title": "Probing Across Time: What Does RoBERTa Know and When?",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Leo Z. Liu"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "11348687",
                                    "name": "Jungo Kasai"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 87
                        },
                        "score": 0.92138671875
                    },
                    {
                        "id": "(Chiang et al., 2020)",
                        "snippets": [
                            "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."
                        ],
                        "paper": {
                            "corpus_id": 222140842,
                            "title": "Pretrained Language Model Embryology: The Birth of ALBERT",
                            "authors": [
                                {
                                    "authorId": "1992777064",
                                    "name": "Cheng-Han Chiang"
                                },
                                {
                                    "authorId": "2210669195",
                                    "name": "Sung-Feng Huang"
                                },
                                {
                                    "authorId": "1706104",
                                    "name": "Hung-yi Lee"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 42
                        },
                        "score": 0
                    },
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "Pre-training is generally considered a crucial stage for LLM to acquire knowledge and skills (Zhou et al., 2023a). Language models, during pre-training, aim to predict the next token in a sequence autoregressively. Through selfsupervised training on extensive textual corpora, the model acquires knowledge of language syntax, world knowledge, and reasoning abilities, providing a robust foundation for subsequent fine-tuning tasks. Besides, recent research (Sutskever, 2023;Del\u00e9tang et al., 2023) suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence of language models lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world."
                        ],
                        "paper": {
                            "corpus_id": 265067168,
                            "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                            "authors": [
                                {
                                    "authorId": "2265930173",
                                    "name": "Lei Huang"
                                },
                                {
                                    "authorId": "2248673673",
                                    "name": "Weijiang Yu"
                                },
                                {
                                    "authorId": "2265878959",
                                    "name": "Weitao Ma"
                                },
                                {
                                    "authorId": "2208739098",
                                    "name": "Weihong Zhong"
                                },
                                {
                                    "authorId": "51056532",
                                    "name": "Zhangyin Feng"
                                },
                                {
                                    "authorId": "2256768984",
                                    "name": "Haotian Wang"
                                },
                                {
                                    "authorId": "1500384901",
                                    "name": "Qianglong Chen"
                                },
                                {
                                    "authorId": "2247980601",
                                    "name": "Weihua Peng"
                                },
                                {
                                    "authorId": "2674998",
                                    "name": "Xiaocheng Feng"
                                },
                                {
                                    "authorId": "2257004102",
                                    "name": "Bing Qin"
                                },
                                {
                                    "authorId": "2238862997",
                                    "name": "Ting Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "ACM Trans. Inf. Syst.",
                            "n_citations": 903
                        },
                        "score": 0.857421875
                    },
                    {
                        "id": "(Bai et al., 2024)",
                        "snippets": [
                            "During pre-training, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence. By undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities."
                        ],
                        "paper": {
                            "corpus_id": 269449935,
                            "title": "Hallucination of Multimodal Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2237427303",
                                    "name": "Zechen Bai"
                                },
                                {
                                    "authorId": "2299164401",
                                    "name": "Pichao Wang"
                                },
                                {
                                    "authorId": "39102205",
                                    "name": "Tianjun Xiao"
                                },
                                {
                                    "authorId": "2264160722",
                                    "name": "Tong He"
                                },
                                {
                                    "authorId": "2008848359",
                                    "name": "Zongbo Han"
                                },
                                {
                                    "authorId": "2298907819",
                                    "name": "Zheng Zhang"
                                },
                                {
                                    "authorId": "2269732179",
                                    "name": "Mike Zheng Shou"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 197
                        },
                        "score": 0.90673828125
                    },
                    {
                        "id": "(Chang et al., 2021)",
                        "snippets": [
                            "We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words\u2019 ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models."
                        ],
                        "paper": {
                            "corpus_id": 238408423,
                            "title": "Word Acquisition in Neural Language Models",
                            "authors": [
                                {
                                    "authorId": "2087001989",
                                    "name": "Tyler A. Chang"
                                },
                                {
                                    "authorId": "24316216",
                                    "name": "B. Bergen"
                                }
                            ],
                            "year": 2021,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 40
                        },
                        "score": 0
                    },
                    {
                        "id": "(AlKhamissi et al., 2025)",
                        "snippets": [
                            "We demonstrated that alignment with the human language network (LN) primarily correlates with formal linguistic competence (Mahowald et al., 2024), peaking and saturating early in training. In contrast, functional linguistic competence, which involves world knowledge and reasoning, continues to grow beyond this stage. These findings suggest that the LN primarily encodes syntactic and compositional structure, in line with the literature of language neuroscience (Fedorenko et al., 2024), while broader linguistic functions may rely on other cognitive systems beyond the LN."
                        ],
                        "paper": {
                            "corpus_id": 276766647,
                            "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network",
                            "authors": [
                                {
                                    "authorId": "2006905770",
                                    "name": "Badr AlKhamissi"
                                },
                                {
                                    "authorId": "1780803442",
                                    "name": "Greta Tuckute"
                                },
                                {
                                    "authorId": "2329164095",
                                    "name": "Yingtian Tang"
                                },
                                {
                                    "authorId": "2092548838",
                                    "name": "T. Binhuraib"
                                },
                                {
                                    "authorId": "2691021",
                                    "name": "Antoine Bosselut"
                                },
                                {
                                    "authorId": "8551292",
                                    "name": "Martin Schrimpf"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.94287109375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Other Emergent Capabilities",
                "tldr": "Beyond syntax, world knowledge, and reasoning, language models develop several additional capabilities that emerge at different stages of training. These include in-context learning, instruction following, cross-linguistic correspondences, and pattern completion abilities that appear through distinct phase transitions. (7 sources)",
                "text": "\n- **In-context learning (ICL)**: The ability to learn from examples provided in the context window emerges during a \"phase change\" in the early stages of training. This capability is mechanistically linked to the development of \"induction heads\" in transformer models that enable them to detect and leverage patterns within the prompt <Paper corpusId=\"274776135\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. ICL has been interpreted as a form of Bayesian inference over latent space or as compositional recombination of linguistic structures <Paper corpusId=\"276903421\" paperTitle=\"(Berti et al., 2025)\" isShortName></Paper> <Paper corpusId=\"258999187\" paperTitle=\"(Bietti et al., 2023)\" isShortName></Paper> <Paper corpusId=\"246823378\" paperTitle=\"(Irie et al., 2022)\" isShortName></Paper>.\n\n- **Instruction following**: The ability to understand and follow natural language instructions emerges in larger models through sharp transitions between phases where the system shows no progress and subsequent phases where capabilities rapidly develop <Paper corpusId=\"261049443\" paperTitle=\"(Nolfi, 2023)\" isShortName></Paper> <Paper corpusId=\"249674500\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>.\n\n- **Step-by-step reasoning**: The capacity to break down complex problems into manageable steps emerges in sufficiently large models and appears through phase transitions that cannot be predicted by extrapolating from smaller models <Paper corpusId=\"261049443\" paperTitle=\"(Nolfi, 2023)\" isShortName></Paper> <Paper corpusId=\"249674500\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>.\n\n- **Arithmetic skills**: The ability to perform mathematical operations emerges in larger models through distinct phase transitions <Paper corpusId=\"261049443\" paperTitle=\"(Nolfi, 2023)\" isShortName></Paper> <Paper corpusId=\"249674500\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>.\n\n- **Cross-linguistic correspondences**: Language models first acquire language-specific knowledge independently before developing the ability to recognize and leverage similarities across languages. This represents a transition from token-level knowledge to more abstract conceptual understanding <Paper corpusId=\"276902722\" paperTitle=\"(Inaba et al., 2025)\" isShortName></Paper>.\n\n- **Pattern completion**: The development of functional modules during training enables models to recognize and complete complex patterns, which forms the mechanistic basis for other emergent capabilities <Paper corpusId=\"274776135\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"276903421\" paperTitle=\"(Berti et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "The capability for ICL [5] in language models develops gradually during a \"phase change\" observed in the early stages of training. At the same time, models acquire the ability to complete complex patterns through induction heads, suggesting that induction heads are a key mechanistic basis of ICL [40]."
                        ],
                        "paper": {
                            "corpus_id": 274776135,
                            "title": "Understanding Knowledge Hijack Mechanism in In-context Learning through Associative Memory",
                            "authors": [
                                {
                                    "authorId": "2263247139",
                                    "name": "Shuo Wang"
                                },
                                {
                                    "authorId": "2303397982",
                                    "name": "Issei Sato"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.869140625
                    },
                    {
                        "id": "(Berti et al., 2025)",
                        "snippets": [
                            "Some studies frame ICL as a form of Bayesian inference over the latent space of language [94], while others view it as a compositional recombination of linguistic structures [32]. Mechanistically, ICL has been linked to neural architectures, such as induction heads [24]59] and functional modules that emerge naturally during training (Bietti et al., 2023)(Irie et al., 2022)81,84]."
                        ],
                        "paper": {
                            "corpus_id": 276903421,
                            "title": "Emergent Abilities in Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2229196235",
                                    "name": "Leonardo Berti"
                                },
                                {
                                    "authorId": "2325903265",
                                    "name": "Flavio Giorgi"
                                },
                                {
                                    "authorId": "1686448",
                                    "name": "Gjergji Kasneci"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.87060546875
                    },
                    {
                        "id": "(Bietti et al., 2023)",
                        "snippets": [
                            "Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an\"induction head\"mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties."
                        ],
                        "paper": {
                            "corpus_id": 258999187,
                            "title": "Birth of a Transformer: A Memory Viewpoint",
                            "authors": [
                                {
                                    "authorId": "2269602",
                                    "name": "A. Bietti"
                                },
                                {
                                    "authorId": "1387995815",
                                    "name": "Vivien A. Cabannes"
                                },
                                {
                                    "authorId": "3365029",
                                    "name": "Diane Bouchacourt"
                                },
                                {
                                    "authorId": "1681054",
                                    "name": "H. J\u00e9gou"
                                },
                                {
                                    "authorId": "52184096",
                                    "name": "L. Bottou"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 96
                        },
                        "score": 0.8837890625
                    },
                    {
                        "id": "(Irie et al., 2022)",
                        "snippets": [
                            "Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the 1960s, no prior work has effectively studied the operations of NNs in such a form, presumably due to prohibitive time and space complexities and impractical model sizes, all of them growing linearly with the number of training patterns which may get very large. However, this dual formulation offers a possibility of directly visualising how an NN makes use of training patterns at test time, by examining the corresponding attention weights. We conduct experiments on small scale supervised image classification tasks in single-task, multi-task, and continual learning settings, as well as language modelling, and discuss potentials and limits of this view for better understanding and interpreting how NNs exploit training patterns. Our code is public."
                        ],
                        "paper": {
                            "corpus_id": 246823378,
                            "title": "The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention",
                            "authors": [
                                {
                                    "authorId": "2350348",
                                    "name": "Kazuki Irie"
                                },
                                {
                                    "authorId": "2258963332",
                                    "name": "R'obert Csord'as"
                                },
                                {
                                    "authorId": "145341374",
                                    "name": "J. Schmidhuber"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 44
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nolfi, 2023)",
                        "snippets": [
                            "The interest in this topic was raised by the publication of an influential article by (Wei et al., 2022) entitled \"Emergent abilities of large language models\". The authors observe that several abilities, such as in-context learning, instruction following, step-by-step reasoning, and arithmetic skills, are not present in smaller models. They appear in larger models only. Moreover, they show that the acquisition process of these abilities in sufficiently large models is characterized by a sharp transition between a phase in which the system does not show any progress in the ability and a subsequent phase in which the system progresses. According to the authors, these two observations imply that predicting the abilities that will be acquired by scaled-up models will be impossible. In other words, they claim that the abilities that will be developed by scaled-up models cannot be extrapolated from what we know about smaller models. (Wei et al., 2022) later questioned the presence of sharp transitions during the training process and showed that the occurrence of sharp or more continuous transitions in the acquisition process depends on the metric used to evaluate the performance. The occurrence of sharp transitions might thus be induced by the utilization of specific metrics."
                        ],
                        "paper": {
                            "corpus_id": 261049443,
                            "title": "On the Unexpected Abilities of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "3015062",
                                    "name": "S. Nolfi"
                                }
                            ],
                            "year": 2023,
                            "venue": "Adaptive Behavior",
                            "n_citations": 11
                        },
                        "score": 0.88623046875
                    },
                    {
                        "id": "(Wei et al., 2022)",
                        "snippets": [
                            "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models."
                        ],
                        "paper": {
                            "corpus_id": 249674500,
                            "title": "Emergent Abilities of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "150272855",
                                    "name": "Rishi Bommasani"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "2368067",
                                    "name": "Barret Zoph"
                                },
                                {
                                    "authorId": "148016269",
                                    "name": "Sebastian Borgeaud"
                                },
                                {
                                    "authorId": "1755465",
                                    "name": "Dani Yogatama"
                                },
                                {
                                    "authorId": "40377863",
                                    "name": "Maarten Bosma"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                },
                                {
                                    "authorId": "1680617",
                                    "name": "Donald Metzler"
                                },
                                {
                                    "authorId": "2226805",
                                    "name": "Ed H. Chi"
                                },
                                {
                                    "authorId": "2117567142",
                                    "name": "Tatsunori Hashimoto"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "2075292388",
                                    "name": "P. Liang"
                                },
                                {
                                    "authorId": "48448318",
                                    "name": "J. Dean"
                                },
                                {
                                    "authorId": "26958176",
                                    "name": "W. Fedus"
                                }
                            ],
                            "year": 2022,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 2516
                        },
                        "score": 0
                    },
                    {
                        "id": "(Inaba et al., 2025)",
                        "snippets": [
                            "Our findings suggest that LLMs initially acquire language-specific knowledge independently, followed by cross-linguistic correspondences. Moreover, we observe that after mastering token-level knowledge, the model transitions to learning higher-level, abstract concepts, indicating the development of more conceptual understanding."
                        ],
                        "paper": {
                            "corpus_id": 276902722,
                            "title": "How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders",
                            "authors": [
                                {
                                    "authorId": "2342412237",
                                    "name": "Tatsuro Inaba"
                                },
                                {
                                    "authorId": "2261744024",
                                    "name": "Kentaro Inui"
                                },
                                {
                                    "authorId": "2302320083",
                                    "name": "Yusuke Miyao"
                                },
                                {
                                    "authorId": "50856622",
                                    "name": "Yohei Oseki"
                                },
                                {
                                    "authorId": "2266692",
                                    "name": "Benjamin Heinzerling"
                                },
                                {
                                    "authorId": "2310230516",
                                    "name": "Yu Takagi"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.95263671875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Mechanistic Explanations for Phase Transitions",
                "tldr": "Phase transitions in language model training are linked to the emergence of specific neural mechanisms that enable new capabilities. These include the formation of syntactic attention structures, development of induction heads for in-context learning, and the gradual emergence of functional modules that support pattern recognition. (7 sources)",
                "text": "\nThe sudden capability improvements observed during language model training can be mechanistically explained through the development of specific neural structures within the model architecture. Research has identified several key mechanisms underlying these phase transitions, providing insights into how models progress from simpler to more complex linguistic abilities.\n\nOne well-documented mechanism is the emergence of Syntactic Attention Structure (SAS), a property where specific transformer heads begin to focus on particular syntactic relations. Chen et al. observed that models abruptly acquire SAS during a brief window in pretraining, concurrent with a steep drop in masked language modeling loss <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This \"structure onset\" precipitates a subsequent \"capabilities onset\" where models demonstrate improved performance on grammatical tasks, establishing a causal relationship between the emergence of SAS and the development of grammatical capabilities <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\nIn-context learning (ICL), another capability that emerges through phase transitions, has been mechanistically linked to the development of \"induction heads\" in transformer models. These specialized attention heads emerge during early training stages and enable models to detect and leverage patterns within the prompt <Paper corpusId=\"274776135\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Multiple studies support this connection, with some framing ICL as a form of Bayesian inference over latent space and others viewing it as compositional recombination of linguistic structures <Paper corpusId=\"276903421\" paperTitle=\"(Berti et al., 2025)\" isShortName></Paper>.\n\nThe development of functional modules during training represents another mechanistic explanation for emergent capabilities. These modules store training patterns as associative memories within the model's weight matrices, allowing for efficient pattern recognition and completion <Paper corpusId=\"276903421\" paperTitle=\"(Berti et al., 2025)\" isShortName></Paper> <Paper corpusId=\"258999187\" paperTitle=\"(Bietti et al., 2023)\" isShortName></Paper> <Paper corpusId=\"246823378\" paperTitle=\"(Irie et al., 2022)\" isShortName></Paper>. This memory-like functionality provides a foundation for capabilities that emerge later in training, as the model learns to store and retrieve increasingly complex linguistic patterns.\n\nWhile these mechanisms explain many phase transitions, research indicates that not all aspects of language learning proceed through distinct phases. Some studies suggest that linguistic circuits involved in specific subtasks remain stable and are gradually reinforced throughout training rather than developing through abrupt transitions <Paper corpusId=\"273025704\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270370801\" paperTitle=\"(Weber et al., 2024)\" isShortName></Paper>. This mixed evidence highlights the complex nature of capability emergence during language model training, with some abilities developing gradually while others manifest through distinct phase transitions.",
                "citations": [
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process",
                            "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022).\n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes. After the spike, the model starts handling complex linguistic phenomena correctly, as signaled by a break in BLiMP score (which we call the capabilities onset).\n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset."
                        ],
                        "paper": {
                            "corpus_id": 261822542,
                            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                            "authors": [
                                {
                                    "authorId": "13336152",
                                    "name": "Angelica Chen"
                                },
                                {
                                    "authorId": "2240524527",
                                    "name": "Ravid Schwartz-Ziv"
                                },
                                {
                                    "authorId": "1979489",
                                    "name": "Kyunghyun Cho"
                                },
                                {
                                    "authorId": "2240527814",
                                    "name": "Matthew L. Leavitt"
                                },
                                {
                                    "authorId": "2362960",
                                    "name": "Naomi Saphra"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 74
                        },
                        "score": 0.9501953125
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "The capability for ICL [5] in language models develops gradually during a \"phase change\" observed in the early stages of training. At the same time, models acquire the ability to complete complex patterns through induction heads, suggesting that induction heads are a key mechanistic basis of ICL [40]."
                        ],
                        "paper": {
                            "corpus_id": 274776135,
                            "title": "Understanding Knowledge Hijack Mechanism in In-context Learning through Associative Memory",
                            "authors": [
                                {
                                    "authorId": "2263247139",
                                    "name": "Shuo Wang"
                                },
                                {
                                    "authorId": "2303397982",
                                    "name": "Issei Sato"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.869140625
                    },
                    {
                        "id": "(Berti et al., 2025)",
                        "snippets": [
                            "Some studies frame ICL as a form of Bayesian inference over the latent space of language [94], while others view it as a compositional recombination of linguistic structures [32]. Mechanistically, ICL has been linked to neural architectures, such as induction heads [24]59] and functional modules that emerge naturally during training (Bietti et al., 2023)(Irie et al., 2022)81,84]."
                        ],
                        "paper": {
                            "corpus_id": 276903421,
                            "title": "Emergent Abilities in Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2229196235",
                                    "name": "Leonardo Berti"
                                },
                                {
                                    "authorId": "2325903265",
                                    "name": "Flavio Giorgi"
                                },
                                {
                                    "authorId": "1686448",
                                    "name": "Gjergji Kasneci"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.87060546875
                    },
                    {
                        "id": "(Bietti et al., 2023)",
                        "snippets": [
                            "Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an\"induction head\"mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties."
                        ],
                        "paper": {
                            "corpus_id": 258999187,
                            "title": "Birth of a Transformer: A Memory Viewpoint",
                            "authors": [
                                {
                                    "authorId": "2269602",
                                    "name": "A. Bietti"
                                },
                                {
                                    "authorId": "1387995815",
                                    "name": "Vivien A. Cabannes"
                                },
                                {
                                    "authorId": "3365029",
                                    "name": "Diane Bouchacourt"
                                },
                                {
                                    "authorId": "1681054",
                                    "name": "H. J\u00e9gou"
                                },
                                {
                                    "authorId": "52184096",
                                    "name": "L. Bottou"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 96
                        },
                        "score": 0.8837890625
                    },
                    {
                        "id": "(Irie et al., 2022)",
                        "snippets": [
                            "Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the 1960s, no prior work has effectively studied the operations of NNs in such a form, presumably due to prohibitive time and space complexities and impractical model sizes, all of them growing linearly with the number of training patterns which may get very large. However, this dual formulation offers a possibility of directly visualising how an NN makes use of training patterns at test time, by examining the corresponding attention weights. We conduct experiments on small scale supervised image classification tasks in single-task, multi-task, and continual learning settings, as well as language modelling, and discuss potentials and limits of this view for better understanding and interpreting how NNs exploit training patterns. Our code is public."
                        ],
                        "paper": {
                            "corpus_id": 246823378,
                            "title": "The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention",
                            "authors": [
                                {
                                    "authorId": "2350348",
                                    "name": "Kazuki Irie"
                                },
                                {
                                    "authorId": "2258963332",
                                    "name": "R'obert Csord'as"
                                },
                                {
                                    "authorId": "145341374",
                                    "name": "J. Schmidhuber"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 44
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2024)",
                        "snippets": [
                            "Phase transitions during LM training have been found for some, but not all, aspects of language learning. Negative evidence includes that LM circuits involved in linguistic subtasks are stable (Tigges et al., 2024) and gradually reinforced (Weber et al., 2024) over training. Positive evidence for learning phase transitions includes that the ID of BERT's final [CLS] representation tracks sudden syntax acquisition and drops in training loss (Chen et al., 2023), with similar observations on Transformers trained on formal languages (Lubana et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 273025704,
                            "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime",
                            "authors": [
                                {
                                    "authorId": "2323997802",
                                    "name": "Jin Hwa Lee"
                                },
                                {
                                    "authorId": "2187058673",
                                    "name": "Thomas Jiralerspong"
                                },
                                {
                                    "authorId": "2324060912",
                                    "name": "Lei Yu"
                                },
                                {
                                    "authorId": "1865800402",
                                    "name": "Y. Bengio"
                                },
                                {
                                    "authorId": "2323783941",
                                    "name": "Emily Cheng"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Weber et al., 2024)",
                        "snippets": [
                            "The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes. In this paper, we present an alternative approach, concentrating on the quality of LM processing, with a focus on their language abilities. To this end, we construct 'linguistic task spaces' -- representations of an LM's language conceptualisation -- that shed light on the connections LMs draw between language phenomena. Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call 'similarity probing'. To disentangle the learning signals of linguistic phenomena, we further introduce a method called 'fine-tuning via gradient differentials' (FTGD). We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure. Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks. The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs."
                        ],
                        "paper": {
                            "corpus_id": 270370801,
                            "title": "Interpretability of Language Models via Task Spaces",
                            "authors": [
                                {
                                    "authorId": "2261081927",
                                    "name": "Lucas Weber"
                                },
                                {
                                    "authorId": "51247075",
                                    "name": "Jaap Jumelet"
                                },
                                {
                                    "authorId": "2173646563",
                                    "name": "E. Bruni"
                                },
                                {
                                    "authorId": "3449411",
                                    "name": "Dieuwke Hupkes"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 4
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.155256
    }
}