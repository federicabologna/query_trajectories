{
    "query": "What does current research reveal about the effectiveness of knowledge distillation when the teacher and student models are of similar or identical size (size-matched distillation), and how does their performance compare to the traditional setup with a much larger teacher model?",
    "user_id": "lib_user",
    "task_id": "1e656cc9-311a-4e32-b8e4-5fd3dbc90857",
    "timestamp": "2025-06-24T00:28:44.618736",
    "n_retrieval": 256,
    "n_retrieved": 276,
    "n_candidates": 47,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.390858,
    "decomposed_query": {
        "rewritten_query": "Effectiveness of knowledge distillation when teacher and student models are of similar or identical size (size-matched distillation), and performance comparison to traditional setup with a much larger teacher model.",
        "keyword_query": "knowledge distillation size-matched teacher student models performance comparison",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010146,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights",
            "venue": "International Conference on Applications of Natural Language to Data Bases",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.12586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491169373",
                    "name": "Mohamad Ballout"
                },
                {
                    "authorId": "1751765",
                    "name": "U. Krumnack"
                },
                {
                    "authorId": "2238205582",
                    "name": "Gunther Heidemann"
                },
                {
                    "authorId": "1743582",
                    "name": "Kai-Uwe K\u00fchnberger"
                }
            ],
            "abstract": "Enhancing small language models for real-life application deployment is a significant challenge facing the research community. Due to the difficulties and costs of using large language models, researchers are seeking ways to effectively deploy task-specific small models. In this work, we introduce a simple yet effective knowledge distillation method to improve the performance of small language models. Our approach utilizes a teacher model with approximately 3 billion parameters to identify the most influential tokens in its decision-making process. These tokens are extracted from the input based on their attribution scores relative to the output, using methods like saliency maps. These important tokens are then provided as rationales to a student model, aiming to distill the knowledge of the teacher model. This method has proven to be effective, as demonstrated by testing it on four diverse datasets, where it shows improvement over both standard fine-tuning methods and state-of-the-art knowledge distillation models. Furthermore, we explore explanations of the success of the model by analyzing the important tokens extracted from the teacher model. Our findings reveal that in 68\\% of cases, specifically in datasets where labels are part of the answer, such as multiple-choice questions, the extracted tokens are part of the ground truth.",
            "corpus_id": 272753230,
            "sentences": [
                {
                    "corpus_id": "272753230",
                    "title": "Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights",
                    "text": "Our research demonstrates an effective and straightforward approach for generating rationales from a large model to enhance a smaller model. However, a significant performance gap between the teacher model (the large model) and the student model (the small model) still exists. As illustrated in table 6, the flan-t5-3b, which serves as the teacher model, significantly outperforms the smaller t5-small model across all four datasets we tested. \n\nThis limitation underscores the inherent challenge in knowledge distillation and model scaling. While our method effectively transfers knowledge from a large to a small model, the reduced capacity of the smaller model limits its ability to fully replicate the performance of its larger counterpart. This performance discrepancy highlights the trade-offs involved in model downsizing, where gains in efficiency and deployability often come at the cost of reduced accuracy and overall capability. \n\nAddressing this gap remains a key area for future research. Efforts could focus on developing more advanced distillation techniques or optimizing small models to better capture and utilize the knowledge transferred from larger models. Our findings lay the groundwork for such exploration, offering insights into the dynamics of knowledge transfer between models of varying sizes and complexities.",
                    "score": 0.5525754044095452,
                    "section_title": "Limitation",
                    "char_start_offset": 23879,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 140
                        },
                        {
                            "start": 141,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 444
                        },
                        {
                            "start": 447,
                            "end": 542
                        },
                        {
                            "start": 543,
                            "end": 745
                        },
                        {
                            "start": 746,
                            "end": 941
                        },
                        {
                            "start": 944,
                            "end": 1003
                        },
                        {
                            "start": 1004,
                            "end": 1178
                        },
                        {
                            "start": 1179,
                            "end": 1340
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                }
            ],
            "relevance_judgement": 0.9609375,
            "relevance_judgment_input_expanded": "# Title: Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights\n# Venue: International Conference on Applications of Natural Language to Data Bases\n# Authors: Mohamad Ballout, U. Krumnack, Gunther Heidemann, Kai-Uwe K\u00fchnberger\n## Abstract\nEnhancing small language models for real-life application deployment is a significant challenge facing the research community. Due to the difficulties and costs of using large language models, researchers are seeking ways to effectively deploy task-specific small models. In this work, we introduce a simple yet effective knowledge distillation method to improve the performance of small language models. Our approach utilizes a teacher model with approximately 3 billion parameters to identify the most influential tokens in its decision-making process. These tokens are extracted from the input based on their attribution scores relative to the output, using methods like saliency maps. These important tokens are then provided as rationales to a student model, aiming to distill the knowledge of the teacher model. This method has proven to be effective, as demonstrated by testing it on four diverse datasets, where it shows improvement over both standard fine-tuning methods and state-of-the-art knowledge distillation models. Furthermore, we explore explanations of the success of the model by analyzing the important tokens extracted from the teacher model. Our findings reveal that in 68\\% of cases, specifically in datasets where labels are part of the answer, such as multiple-choice questions, the extracted tokens are part of the ground truth.\n## Limitation\nOur research demonstrates an effective and straightforward approach for generating rationales from a large model to enhance a smaller model. However, a significant performance gap between the teacher model (the large model) and the student model (the small model) still exists. As illustrated in table 6, the flan-t5-3b, which serves as the teacher model, significantly outperforms the smaller t5-small model across all four datasets we tested. \n\nThis limitation underscores the inherent challenge in knowledge distillation and model scaling. While our method effectively transfers knowledge from a large to a small model, the reduced capacity of the smaller model limits its ability to fully replicate the performance of its larger counterpart. This performance discrepancy highlights the trade-offs involved in model downsizing, where gains in efficiency and deployability often come at the cost of reduced accuracy and overall capability. \n\nAddressing this gap remains a key area for future research. Efforts could focus on developing more advanced distillation techniques or optimizing small models to better capture and utilize the knowledge transferred from larger models. Our findings lay the groundwork for such exploration, offering insights into the dynamics of knowledge transfer between models of varying sizes and complexities.",
            "reference_string": "[272753230 | Ballout et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Efficient and Controllable Model Compression through Sequential Knowledge Distillation and Pruning",
            "venue": "Big Data and Cognitive Computing",
            "year": 2023,
            "reference_count": 25,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2504-2289/7/3/154/pdf?version=1695126989",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/bdcc7030154?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/bdcc7030154, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "9106542",
                    "name": "Leila Malihi"
                },
                {
                    "authorId": "2254394013",
                    "name": "Gunther Heidemann"
                }
            ],
            "abstract": "Efficient model deployment is a key focus in deep learning. This has led to the exploration of methods such as knowledge distillation and network pruning to compress models and increase their performance. In this study, we investigate the potential synergy between knowledge distillation and network pruning to achieve optimal model efficiency and improved generalization. We introduce an innovative framework for model compression that combines knowledge distillation, pruning, and fine-tuning to achieve enhanced compression while providing control over the degree of compactness. Our research is conducted on popular datasets, CIFAR-10 and CIFAR-100, employing diverse model architectures, including ResNet, DenseNet, and EfficientNet. We could calibrate the amount of compression achieved. This allows us to produce models with different degrees of compression while still being just as accurate, or even better. Notably, we demonstrate its efficacy by producing two compressed variants of ResNet 101: ResNet 50 and ResNet 18. Our results reveal intriguing findings. In most cases, the pruned and distilled student models exhibit comparable or superior accuracy to the distilled student models while utilizing significantly fewer parameters.",
            "corpus_id": 263677297,
            "sentences": [
                {
                    "corpus_id": "263677297",
                    "title": "Efficient and Controllable Model Compression through Sequential Knowledge Distillation and Pruning",
                    "text": "The Knowledge Distillation (KD) paper by Hinton et al. [1], introduces the concept of knowledge distillation. It proposes training a smaller model (the student) to mimic the behavior of a larger, more complex model (the teacher) by transferring its knowledge. The teacher model's soft probabilities (logits) are used as \"soft targets\" during training to guide the student's learning. The approach demonstrates that the student can achieve similar or even superior performance to the teacher despite being much smaller and computationally efficient. Attention Transfer (AT), by Zagoruyko and Komodakis [5], introduces attention transfer for knowledge distillation. Attention maps are used to focus on important regions in the input data. The student model is trained to mimic the attention maps produced by the teacher model. By learning to attend to similar regions, the student can improve their generalization and performance. \n\nVariational Information Distillation (VID), introduced by Ahn et al. [8], incorporates variational distributions in knowledge distillation. The student model learns from the uncertainty present in the teacher's predictions. By considering the variance in the teacher's logits, the student can capture not only the mean representation but also the level of confidence in the teacher's predictions. Contrastive Representation Distillation (CRD), by Tian et al. [2], leverages contrastive learning for knowledge distillation. The teacher and student models are compared using contrastive loss, where positive pairs are from the same class and negative pairs are from different classes. This encourages the student to capture discriminative features while maintaining intra-class similarity and inter-class differences. Similarity-Preserving Knowledge Distillation, by Tung et al. [6], proposes similarity-preserving knowledge distillation, where the student learns to preserve pairwise similarities of the teacher's features during training. By maintaining the relative similarities between the teacher's features, the student model can capture the fine-grained details present in the teacher's representations. \n\nSelf-Residual Representation Learning (SRRL), presented by Zhang et al. [6], proposes SRRL for knowledge distillation, focusing on self-residual representations.",
                    "score": 0.629120962895759,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 5070,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 109
                        },
                        {
                            "start": 110,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 383
                        },
                        {
                            "start": 384,
                            "end": 548
                        },
                        {
                            "start": 549,
                            "end": 663
                        },
                        {
                            "start": 664,
                            "end": 736
                        },
                        {
                            "start": 737,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 928
                        },
                        {
                            "start": 931,
                            "end": 1070
                        },
                        {
                            "start": 1071,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1327
                        },
                        {
                            "start": 1328,
                            "end": 1453
                        },
                        {
                            "start": 1454,
                            "end": 1613
                        },
                        {
                            "start": 1614,
                            "end": 1746
                        },
                        {
                            "start": 1747,
                            "end": 1969
                        },
                        {
                            "start": 1970,
                            "end": 2139
                        },
                        {
                            "start": 2142,
                            "end": 2303
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1000,
                            "end": 1003,
                            "matchedPaperCorpusId": "118649278"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9580078125
                }
            ],
            "relevance_judgement": 0.9580078125,
            "relevance_judgment_input_expanded": "# Title: Efficient and Controllable Model Compression through Sequential Knowledge Distillation and Pruning\n# Venue: Big Data and Cognitive Computing\n# Authors: Leila Malihi, Gunther Heidemann\n## Abstract\nEfficient model deployment is a key focus in deep learning. This has led to the exploration of methods such as knowledge distillation and network pruning to compress models and increase their performance. In this study, we investigate the potential synergy between knowledge distillation and network pruning to achieve optimal model efficiency and improved generalization. We introduce an innovative framework for model compression that combines knowledge distillation, pruning, and fine-tuning to achieve enhanced compression while providing control over the degree of compactness. Our research is conducted on popular datasets, CIFAR-10 and CIFAR-100, employing diverse model architectures, including ResNet, DenseNet, and EfficientNet. We could calibrate the amount of compression achieved. This allows us to produce models with different degrees of compression while still being just as accurate, or even better. Notably, we demonstrate its efficacy by producing two compressed variants of ResNet 101: ResNet 50 and ResNet 18. Our results reveal intriguing findings. In most cases, the pruned and distilled student models exhibit comparable or superior accuracy to the distilled student models while utilizing significantly fewer parameters.\n## Knowledge Distillation\nThe Knowledge Distillation (KD) paper by Hinton et al. [1], introduces the concept of knowledge distillation. It proposes training a smaller model (the student) to mimic the behavior of a larger, more complex model (the teacher) by transferring its knowledge. The teacher model's soft probabilities (logits) are used as \"soft targets\" during training to guide the student's learning. The approach demonstrates that the student can achieve similar or even superior performance to the teacher despite being much smaller and computationally efficient. Attention Transfer (AT), by Zagoruyko and Komodakis [5], introduces attention transfer for knowledge distillation. Attention maps are used to focus on important regions in the input data. The student model is trained to mimic the attention maps produced by the teacher model. By learning to attend to similar regions, the student can improve their generalization and performance. \n\nVariational Information Distillation (VID), introduced by Ahn et al. [8], incorporates variational distributions in knowledge distillation. The student model learns from the uncertainty present in the teacher's predictions. By considering the variance in the teacher's logits, the student can capture not only the mean representation but also the level of confidence in the teacher's predictions. Contrastive Representation Distillation (CRD), by Tian et al. [2], leverages contrastive learning for knowledge distillation. The teacher and student models are compared using contrastive loss, where positive pairs are from the same class and negative pairs are from different classes. This encourages the student to capture discriminative features while maintaining intra-class similarity and inter-class differences. Similarity-Preserving Knowledge Distillation, by Tung et al. [6], proposes similarity-preserving knowledge distillation, where the student learns to preserve pairwise similarities of the teacher's features during training. By maintaining the relative similarities between the teacher's features, the student model can capture the fine-grained details present in the teacher's representations. \n\nSelf-Residual Representation Learning (SRRL), presented by Zhang et al. [6], proposes SRRL for knowledge distillation, focusing on self-residual representations.",
            "reference_string": "[263677297 | Malihi et al. | 2023 | Citations: 5]"
        },
        {
            "title": "A Survey on Collaborative Mechanisms Between Large and Small Language Models",
            "venue": "",
            "year": 2025,
            "reference_count": 93,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.07460, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2362381085",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "2360557831",
                    "name": "JiaHao Zhao"
                },
                {
                    "authorId": "2361480301",
                    "name": "HaoHao Han"
                }
            ],
            "abstract": "Large Language Models (LLMs) deliver powerful AI capabilities but face deployment challenges due to high resource costs and latency, whereas Small Language Models (SLMs) offer efficiency and deployability at the cost of reduced performance. Collaboration between LLMs and SLMs emerges as a crucial paradigm to synergistically balance these trade-offs, enabling advanced AI applications, especially on resource-constrained edge devices. This survey provides a comprehensive overview of LLM-SLM collaboration, detailing various interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion), key enabling technologies, and diverse application scenarios driven by on-device needs like low latency, privacy, personalization, and offline operation. While highlighting the significant potential for creating more efficient, adaptable, and accessible AI, we also discuss persistent challenges including system overhead, inter-model consistency, robust task allocation, evaluation complexity, and security/privacy concerns. Future directions point towards more intelligent adaptive frameworks, deeper model fusion, and expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as a key driver for the next generation of practical and ubiquitous artificial intelligence.",
            "corpus_id": 278502445,
            "sentences": [
                {
                    "corpus_id": "278502445",
                    "title": "A Survey on Collaborative Mechanisms Between Large and Small Language Models",
                    "text": "Knowledge Distillation (KD) is a key technique used to transfer knowledge and capabilities from a large, often proprietary teacher model (LLM) to a smaller, more efficient student model (SLM) (X. Xu et al., 2024;W. Xu et al., 2024). \n\nKD involves multiple aspects, including the type of knowledge (output probabilities, intermediate representations), distillation algorithms (supervised fine-tuning, divergence minimization), and application domains (model compression, skill transfer, domain specialization). Data augmentation also plays an important role in enhancing the effectiveness of KD for LLMs. Depending on whether the teacher model can provide internal information, KD methods can be divided into white-box KD and black-box KD. KD is also used to transfer specific skills, such as reasoning, instruction following, and tool use (X. Xu et al., 2024;Hendriks et al., 2025;Y. Gu et al., 2023). \n\nKnowledge distillation is a critical mechanism for enabling LLM-level intelligence to be deployed in resource-constrained environments by creating smaller, more efficient models. While LLMs offer superior performance, their size and computational requirements limit their applicability in many practical scenarios. KD allows us to transfer the knowledge learned by these large models into smaller models that can be deployed on edge devices or used in applications with strict latency requirements. The success of knowledge distillation depends on several factors, including the choice of teacher and student models, the quality and quantity of distillation data, and the specific distillation techniques employed. Different LLMs have different strengths, and the choice of teacher model will influence the type of knowledge transferred. The distillation data should be representative of the tasks the student model will perform. Selecting the appropriate distillation algorithm and tuning its parameters are also crucial for achieving optimal performance.",
                    "score": 0.5818525075012491,
                    "section_title": "Knowledge Distillation-Driven Collaboration",
                    "char_start_offset": 25278,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 232
                        },
                        {
                            "start": 235,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 603
                        },
                        {
                            "start": 604,
                            "end": 738
                        },
                        {
                            "start": 739,
                            "end": 842
                        },
                        {
                            "start": 843,
                            "end": 901
                        },
                        {
                            "start": 904,
                            "end": 1082
                        },
                        {
                            "start": 1083,
                            "end": 1218
                        },
                        {
                            "start": 1219,
                            "end": 1402
                        },
                        {
                            "start": 1403,
                            "end": 1618
                        },
                        {
                            "start": 1619,
                            "end": 1741
                        },
                        {
                            "start": 1742,
                            "end": 1833
                        },
                        {
                            "start": 1834,
                            "end": 1960
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95556640625
                }
            ],
            "relevance_judgement": 0.95556640625,
            "relevance_judgment_input_expanded": "# Title: A Survey on Collaborative Mechanisms Between Large and Small Language Models\n# Venue: \n# Authors: Yi Chen, JiaHao Zhao, HaoHao Han\n## Abstract\nLarge Language Models (LLMs) deliver powerful AI capabilities but face deployment challenges due to high resource costs and latency, whereas Small Language Models (SLMs) offer efficiency and deployability at the cost of reduced performance. Collaboration between LLMs and SLMs emerges as a crucial paradigm to synergistically balance these trade-offs, enabling advanced AI applications, especially on resource-constrained edge devices. This survey provides a comprehensive overview of LLM-SLM collaboration, detailing various interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion), key enabling technologies, and diverse application scenarios driven by on-device needs like low latency, privacy, personalization, and offline operation. While highlighting the significant potential for creating more efficient, adaptable, and accessible AI, we also discuss persistent challenges including system overhead, inter-model consistency, robust task allocation, evaluation complexity, and security/privacy concerns. Future directions point towards more intelligent adaptive frameworks, deeper model fusion, and expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as a key driver for the next generation of practical and ubiquitous artificial intelligence.\n## Knowledge Distillation-Driven Collaboration\nKnowledge Distillation (KD) is a key technique used to transfer knowledge and capabilities from a large, often proprietary teacher model (LLM) to a smaller, more efficient student model (SLM) (X. Xu et al., 2024;W. Xu et al., 2024). \n\nKD involves multiple aspects, including the type of knowledge (output probabilities, intermediate representations), distillation algorithms (supervised fine-tuning, divergence minimization), and application domains (model compression, skill transfer, domain specialization). Data augmentation also plays an important role in enhancing the effectiveness of KD for LLMs. Depending on whether the teacher model can provide internal information, KD methods can be divided into white-box KD and black-box KD. KD is also used to transfer specific skills, such as reasoning, instruction following, and tool use (X. Xu et al., 2024;Hendriks et al., 2025;Y. Gu et al., 2023). \n\nKnowledge distillation is a critical mechanism for enabling LLM-level intelligence to be deployed in resource-constrained environments by creating smaller, more efficient models. While LLMs offer superior performance, their size and computational requirements limit their applicability in many practical scenarios. KD allows us to transfer the knowledge learned by these large models into smaller models that can be deployed on edge devices or used in applications with strict latency requirements. The success of knowledge distillation depends on several factors, including the choice of teacher and student models, the quality and quantity of distillation data, and the specific distillation techniques employed. Different LLMs have different strengths, and the choice of teacher model will influence the type of knowledge transferred. The distillation data should be representative of the tasks the student model will perform. Selecting the appropriate distillation algorithm and tuning its parameters are also crucial for achieving optimal performance.",
            "reference_string": "[278502445 | Chen et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Comparison of Transfer Learning Performance in Lung and Colon Classification with Knowledge Distillation",
            "venue": "Journal of Advanced Health Informatics Research",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.59247/jahir.v2i2.289?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.59247/jahir.v2i2.289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2355339844",
                    "name": "Annastasya Nabila Elsa Wulandari"
                },
                {
                    "authorId": "2355340858",
                    "name": "Aimar Yudhistira"
                },
                {
                    "authorId": "2355335792",
                    "name": "Purwono"
                },
                {
                    "authorId": "51453929",
                    "name": "Abdel-Nasser Sharkawy"
                }
            ],
            "abstract": "This research aims to apply the knowledge distillation method to medical image classification, specifically in the case of lung and colon image classification using various transfer learning models. Knowledge distillation allows the transfer of knowledge from a larger model (teacher) to a smaller model (student), which enables more efficient model building without sacrificing accuracy. In this research, the DenseNet169 model is used as the teacher model. The student model uses several alternative transfer learning architectures such as DenseNet121, MobileNet, ResNet50, InceptionV3, and Xception. The data used consists of 25,000 histopathology images that have been processed and divided into training, validation, and test data. Data augmentation was performed to enlarge the dataset from 750 to 25,000 images, which helped improve the performance of the model. Model performance evaluation was performed by measuring the accuracy and loss value of each student model compared to the teacher model. The results showed that the student models generated through the knowledge distillation process performed close to or even exceeded the teacher model in some cases, with the Xception model showing the highest accuracy of 96.95%. In conclusion, knowledge distillation is effective in reducing model complexity without compromising performance, which is particularly beneficial for implementation on resource-constrained devices.",
            "corpus_id": 277779273,
            "sentences": [],
            "relevance_judgement": 0.95556640625,
            "relevance_judgment_input_expanded": "# Title: Comparison of Transfer Learning Performance in Lung and Colon Classification with Knowledge Distillation\n# Venue: Journal of Advanced Health Informatics Research\n# Authors: Annastasya Nabila Elsa Wulandari, Aimar Yudhistira, Purwono, Abdel-Nasser Sharkawy\n## Abstract\nThis research aims to apply the knowledge distillation method to medical image classification, specifically in the case of lung and colon image classification using various transfer learning models. Knowledge distillation allows the transfer of knowledge from a larger model (teacher) to a smaller model (student), which enables more efficient model building without sacrificing accuracy. In this research, the DenseNet169 model is used as the teacher model. The student model uses several alternative transfer learning architectures such as DenseNet121, MobileNet, ResNet50, InceptionV3, and Xception. The data used consists of 25,000 histopathology images that have been processed and divided into training, validation, and test data. Data augmentation was performed to enlarge the dataset from 750 to 25,000 images, which helped improve the performance of the model. Model performance evaluation was performed by measuring the accuracy and loss value of each student model compared to the teacher model. The results showed that the student models generated through the knowledge distillation process performed close to or even exceeded the teacher model in some cases, with the Xception model showing the highest accuracy of 96.95%. In conclusion, knowledge distillation is effective in reducing model complexity without compromising performance, which is particularly beneficial for implementation on resource-constrained devices.\n",
            "reference_string": "[277779273 | Wulandari et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 111,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.13825, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333595030",
                    "name": "Junjie Yang"
                },
                {
                    "authorId": "2312846636",
                    "name": "Jun-Jie Song"
                },
                {
                    "authorId": "2356484215",
                    "name": "Xudong Han"
                },
                {
                    "authorId": "2319608188",
                    "name": "Ziqian Bi"
                },
                {
                    "authorId": "2323695882",
                    "name": "Tianyang Wang"
                },
                {
                    "authorId": "2330158457",
                    "name": "Chia Xin Liang"
                },
                {
                    "authorId": "2330144928",
                    "name": "Xinyuan Song"
                },
                {
                    "authorId": "2327005680",
                    "name": "Yichao Zhang"
                },
                {
                    "authorId": "2319611972",
                    "name": "Qian Niu"
                },
                {
                    "authorId": "2319606006",
                    "name": "Benji Peng"
                },
                {
                    "authorId": "2319648854",
                    "name": "Keyu Chen"
                },
                {
                    "authorId": "2322495607",
                    "name": "Ming Liu"
                }
            ],
            "abstract": "Knowledge distillation (KD) is a technique for transferring knowledge from complex teacher models to simpler student models, significantly enhancing model efficiency and accuracy. It has demonstrated substantial advancements in various applications including image classification, object detection, language modeling, text classification, and sentiment analysis. Recent innovations in KD methods, such as attention-based approaches, block-wise logit distillation, and decoupling distillation, have notably improved student model performance. These techniques focus on stimulus complexity, attention mechanisms, and global information capture to optimize knowledge transfer. In addition, KD has proven effective in compressing large language models while preserving accuracy, reducing computational overhead, and improving inference speed. This survey synthesizes the latest literature, highlighting key findings, contributions, and future directions in knowledge distillation to provide insights for researchers and practitioners on its evolving role in artificial intelligence and machine learning.",
            "corpus_id": 277940190,
            "sentences": [
                {
                    "corpus_id": "277940190",
                    "title": "Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models",
                    "text": "Language modeling with knowledge distillation (KD) has emerged as a crucial technique for compressing large language models while maintaining their accuracy, as demonstrated in various studies (Yang et al., 2024b;Wei et al., 2024a;Zhang et al., 2024b). The core idea behind KD is to transfer knowledge from a pretrained teacher model to a smaller student model, thereby reducing computational requirements and improving inference speed without significantly compromising performance. A comprehensive survey of KD for large language models (Yang et al., 2024b) provides an overview of the techniques, evaluation tasks, and applications, highlighting the effectiveness of this approach. \n\nRecent studies have explored different distillation methods, including sentence-level, token-level, and hybrid approaches. For instance, (Wei et al., 2024a) introduced a novel hybrid method that combines token-level and sentence-level distillation, outperforming the individual methods and previous studies. (Zhang et al., 2024b) proposed a multi-level KD method that improves model performance and allows for flexible student attention head number settings. Furthermore, (Park et al., 2021) presented a new KD objective that transfers contextual knowledge via word relations and layer transforming relations without restrictions on architectural changes between teacher and student models. \n\nThe importance of selecting an appropriate distillation method depending on scenario complexity has also been emphasized in the literature. For example, (Wei et al., 2024a) argued that token-level distillation is better suited to simple scenarios, whereas sentence-level distillation excels in complex scenarios. However, other studies have not explicitly made this distinction, highlighting the need for further research on task-agnostic distillation and its applications (Liang et al., 2023). The use of pretrained language models as teachers and the exploration of various student models, including those with smaller sizes or different architectures, are also a common theme in the literature (Yang et al., 2024b;Wang et al., 2022). \n\nTechnical details and methodologies vary across studies, with KD techniques including sentence-level, token-level, hybrid, multi-level, and task-agnostic distillation.",
                    "score": 0.534177817483777,
                    "section_title": "XIX. LANGUAGE MODELING",
                    "char_start_offset": 72228,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 252
                        },
                        {
                            "start": 253,
                            "end": 483
                        },
                        {
                            "start": 484,
                            "end": 684
                        },
                        {
                            "start": 687,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 994
                        },
                        {
                            "start": 995,
                            "end": 1145
                        },
                        {
                            "start": 1146,
                            "end": 1377
                        },
                        {
                            "start": 1380,
                            "end": 1519
                        },
                        {
                            "start": 1520,
                            "end": 1692
                        },
                        {
                            "start": 1693,
                            "end": 1874
                        },
                        {
                            "start": 1875,
                            "end": 2116
                        },
                        {
                            "start": 2119,
                            "end": 2286
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 193,
                            "end": 213,
                            "matchedPaperCorpusId": "270878760"
                        },
                        {
                            "start": 539,
                            "end": 559,
                            "matchedPaperCorpusId": "270878760"
                        },
                        {
                            "start": 2077,
                            "end": 2097,
                            "matchedPaperCorpusId": "270878760"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94970703125
                }
            ],
            "relevance_judgement": 0.94970703125,
            "relevance_judgment_input_expanded": "# Title: Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models\n# Venue: arXiv.org\n# Authors: Junjie Yang, Jun-Jie Song, Xudong Han, Ziqian Bi, Tianyang Wang, Chia Xin Liang, Xinyuan Song, Yichao Zhang, Qian Niu, Benji Peng, Keyu Chen, Ming Liu\n## Abstract\nKnowledge distillation (KD) is a technique for transferring knowledge from complex teacher models to simpler student models, significantly enhancing model efficiency and accuracy. It has demonstrated substantial advancements in various applications including image classification, object detection, language modeling, text classification, and sentiment analysis. Recent innovations in KD methods, such as attention-based approaches, block-wise logit distillation, and decoupling distillation, have notably improved student model performance. These techniques focus on stimulus complexity, attention mechanisms, and global information capture to optimize knowledge transfer. In addition, KD has proven effective in compressing large language models while preserving accuracy, reducing computational overhead, and improving inference speed. This survey synthesizes the latest literature, highlighting key findings, contributions, and future directions in knowledge distillation to provide insights for researchers and practitioners on its evolving role in artificial intelligence and machine learning.\n## XIX. LANGUAGE MODELING\nLanguage modeling with knowledge distillation (KD) has emerged as a crucial technique for compressing large language models while maintaining their accuracy, as demonstrated in various studies (Yang et al., 2024b;Wei et al., 2024a;Zhang et al., 2024b). The core idea behind KD is to transfer knowledge from a pretrained teacher model to a smaller student model, thereby reducing computational requirements and improving inference speed without significantly compromising performance. A comprehensive survey of KD for large language models (Yang et al., 2024b) provides an overview of the techniques, evaluation tasks, and applications, highlighting the effectiveness of this approach. \n\nRecent studies have explored different distillation methods, including sentence-level, token-level, and hybrid approaches. For instance, (Wei et al., 2024a) introduced a novel hybrid method that combines token-level and sentence-level distillation, outperforming the individual methods and previous studies. (Zhang et al., 2024b) proposed a multi-level KD method that improves model performance and allows for flexible student attention head number settings. Furthermore, (Park et al., 2021) presented a new KD objective that transfers contextual knowledge via word relations and layer transforming relations without restrictions on architectural changes between teacher and student models. \n\nThe importance of selecting an appropriate distillation method depending on scenario complexity has also been emphasized in the literature. For example, (Wei et al., 2024a) argued that token-level distillation is better suited to simple scenarios, whereas sentence-level distillation excels in complex scenarios. However, other studies have not explicitly made this distinction, highlighting the need for further research on task-agnostic distillation and its applications (Liang et al., 2023). The use of pretrained language models as teachers and the exploration of various student models, including those with smaller sizes or different architectures, are also a common theme in the literature (Yang et al., 2024b;Wang et al., 2022). \n\nTechnical details and methodologies vary across studies, with KD techniques including sentence-level, token-level, hybrid, multi-level, and task-agnostic distillation.",
            "reference_string": "[277940190 | Yang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Bayesian Optimization Meets Self-Distillation",
            "venue": "IEEE International Conference on Computer Vision",
            "year": 2023,
            "reference_count": 54,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.12666",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.12666, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279782391",
                    "name": "HyunJae Lee"
                },
                {
                    "authorId": "2157781156",
                    "name": "Heon Song"
                },
                {
                    "authorId": "2279825857",
                    "name": "Hyeonsoo Lee"
                },
                {
                    "authorId": "2215364195",
                    "name": "Gi-hyeon Lee"
                },
                {
                    "authorId": "2199808369",
                    "name": "Suyeong Park"
                },
                {
                    "authorId": "2283756",
                    "name": "Donggeun Yoo"
                }
            ],
            "abstract": "Bayesian optimization (BO) has contributed greatly to improving model performance by suggesting promising hyperparameter configurations iteratively based on observations from multiple training trials. However, only partial knowledge (i.e., the measured performances of trained models and their hyperparameter configurations) from previous trials is transferred. On the other hand, Self-Distillation (SD) only transfers partial knowledge learned by the task model itself. To fully leverage the various knowledge gained from all training trials, we propose the BOSS framework, which combines BO and SD. BOSS suggests promising hyperparameter configurations through BO and carefully selects pre-trained models from previous trials for SD, which are otherwise abandoned in the conventional BO process. BOSS achieves significantly better performance than both BO and SD in a wide range of tasks including general image classification, learning with noisy labels, semi-supervised learning, and medical image analysis tasks. Our code is available at https://github.com/sooperset/boss.",
            "corpus_id": 258309753,
            "sentences": [
                {
                    "corpus_id": "258309753",
                    "title": "Bayesian Optimization Meets Self-Distillation",
                    "text": "Our framework is different from these methods in that these utilize prior knowledge from external sources to better estimate the surrogate model, while we utilize the prior knowledge from the given task to enhance the training of the target model. It means that these methods can be applied orthogonally to our framework. \n\nSelf-Distillation. Knowledge distillation (KD) is a model compression method that involves transferring the knowledge of a large teacher model to a small student model while maintaining performance. The original work by Hinton et al. [22] proposed distilling knowledge by matching the softmax distribution of the teacher and student models. Since then, various methods have been introduced to improve the knowledge transfer process. Self-Distillation (SD) is a special form of KD where the teacher and student networks have identical architecture. Born-Again Networks (BAN) [16] demonstrated that when training the student to match the output distribution of the teacher with the identical architecture, it could outperform the teacher. Furthermore, they showed that performing multiple rounds of BAN could further improve the performance where the trained student is set to be a new teacher in the following round. The ef-fectiveness of SD has been theoretically explained by the \"multi-view\" hypothesis introduced by Allen-Zhu and Li, who showed that self-distillation performs an implicit ensemble of various models [2]. Empirical evidence from Pham et al. [36] suggests that SD encourages the student to find flatter minima, leading to better generalization. In this work, we identify that SD can be an effective method for propagating the task knowledge learned in early stages of BO, to late stages of BO. This combination of the SD and BO processes is key to yielding a high-performing model, which we validate experimentally.",
                    "score": 0.5207694591245493,
                    "section_title": "Related Work",
                    "char_start_offset": 5089,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 247
                        },
                        {
                            "start": 248,
                            "end": 321
                        },
                        {
                            "start": 324,
                            "end": 342
                        },
                        {
                            "start": 343,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 664
                        },
                        {
                            "start": 665,
                            "end": 756
                        },
                        {
                            "start": 757,
                            "end": 871
                        },
                        {
                            "start": 872,
                            "end": 1060
                        },
                        {
                            "start": 1061,
                            "end": 1239
                        },
                        {
                            "start": 1240,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1586
                        },
                        {
                            "start": 1587,
                            "end": 1735
                        },
                        {
                            "start": 1736,
                            "end": 1857
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 558,
                            "end": 562,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94921875
                }
            ],
            "relevance_judgement": 0.94921875,
            "relevance_judgment_input_expanded": "# Title: Bayesian Optimization Meets Self-Distillation\n# Venue: IEEE International Conference on Computer Vision\n# Authors: HyunJae Lee, Heon Song, Hyeonsoo Lee, Gi-hyeon Lee, Suyeong Park, Donggeun Yoo\n## Abstract\nBayesian optimization (BO) has contributed greatly to improving model performance by suggesting promising hyperparameter configurations iteratively based on observations from multiple training trials. However, only partial knowledge (i.e., the measured performances of trained models and their hyperparameter configurations) from previous trials is transferred. On the other hand, Self-Distillation (SD) only transfers partial knowledge learned by the task model itself. To fully leverage the various knowledge gained from all training trials, we propose the BOSS framework, which combines BO and SD. BOSS suggests promising hyperparameter configurations through BO and carefully selects pre-trained models from previous trials for SD, which are otherwise abandoned in the conventional BO process. BOSS achieves significantly better performance than both BO and SD in a wide range of tasks including general image classification, learning with noisy labels, semi-supervised learning, and medical image analysis tasks. Our code is available at https://github.com/sooperset/boss.\n## Related Work\nOur framework is different from these methods in that these utilize prior knowledge from external sources to better estimate the surrogate model, while we utilize the prior knowledge from the given task to enhance the training of the target model. It means that these methods can be applied orthogonally to our framework. \n\nSelf-Distillation. Knowledge distillation (KD) is a model compression method that involves transferring the knowledge of a large teacher model to a small student model while maintaining performance. The original work by Hinton et al. [22] proposed distilling knowledge by matching the softmax distribution of the teacher and student models. Since then, various methods have been introduced to improve the knowledge transfer process. Self-Distillation (SD) is a special form of KD where the teacher and student networks have identical architecture. Born-Again Networks (BAN) [16] demonstrated that when training the student to match the output distribution of the teacher with the identical architecture, it could outperform the teacher. Furthermore, they showed that performing multiple rounds of BAN could further improve the performance where the trained student is set to be a new teacher in the following round. The ef-fectiveness of SD has been theoretically explained by the \"multi-view\" hypothesis introduced by Allen-Zhu and Li, who showed that self-distillation performs an implicit ensemble of various models [2]. Empirical evidence from Pham et al. [36] suggests that SD encourages the student to find flatter minima, leading to better generalization. In this work, we identify that SD can be an effective method for propagating the task knowledge learned in early stages of BO, to late stages of BO. This combination of the SD and BO processes is key to yielding a high-performing model, which we validate experimentally.",
            "reference_string": "[258309753 | Lee et al. | 2023 | Citations: 2]"
        },
        {
            "title": "SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 71,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2306.15656",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.15656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2065456071",
                    "name": "Fu-Ming Guo"
                }
            ],
            "abstract": "This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovative optimizer-compiler co-design strategy, demonstrating the potential of inference acceleration (\\textbf{3.37x}, \\textbf{6.30x}, and \\textbf{7.15x} in comparison with Pytorch, TensorFlow, and LLVM generic compile, respectively) in SparseBERT when paired with an appropriately designed compiler. This study represents a significant step forward in the evolution of efficient, scalable, and high-performing large language models, setting a precedent for future exploration and optimization in this domain. The SparseOptimizer code and SparseALBERT model will be publicly available upon paper acceptance.",
            "corpus_id": 259262233,
            "sentences": [
                {
                    "corpus_id": "259262233",
                    "title": "SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design",
                    "text": "Knowledge distillation, a concept first introduced by Hinton et al. [2015], has emerged as an effective model compression technique whereby a smaller student model is trained to mimic the functionality of a larger, more complex teacher model. Subsequent research has expanded upon this original premise, with distillation methodologies now typically falling into two main categories: general distillation [Sanh et al., 2019, Sun et al., 2020, Wang et al., 2020] and task-specific distillation [Jiao et al., 2019]. \n\nGeneral distillation leverages unlabeled data as a vehicle for knowledge transfer. Despite the computational expense, these methods have shown to be essential in retaining performance levels, particularly when pre-training the student network on an unlabeled corpus [Turc et al., 2019, Jiao et al., 2019]. On the other hand, task-specific distillation exploits task-relevant data to instill the knowledge from the teacher model to the student model. \n\nHowever, the real potential of knowledge distillation has begun to shine through in recent research that combines these two distinct approaches. For instance, the work of Jiao et al. [2019] showed that such hybrid methods could push the performance boundaries even further. \n\nThe advancements in distillation techniques have paved the way for increasingly efficient utilization of large language models. One of the notable milestones in this evolution is the study by Hsieh et al. [2023], which demonstrated that superior performance could be achieved even with less training data and smaller model sizes. \n\nIn summary, the continual progress in knowledge distillation research has opened up new avenues for large language model efficiency. It is this body of work that serves as a valuable foundation for our novel SparseOptimizer, offering a new perspective on model sparsification and optimization.",
                    "score": 0.5805861675350821,
                    "section_title": "Distillation",
                    "char_start_offset": 5275,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 242
                        },
                        {
                            "start": 243,
                            "end": 513
                        },
                        {
                            "start": 516,
                            "end": 598
                        },
                        {
                            "start": 599,
                            "end": 821
                        },
                        {
                            "start": 822,
                            "end": 965
                        },
                        {
                            "start": 968,
                            "end": 1112
                        },
                        {
                            "start": 1113,
                            "end": 1241
                        },
                        {
                            "start": 1244,
                            "end": 1371
                        },
                        {
                            "start": 1372,
                            "end": 1573
                        },
                        {
                            "start": 1576,
                            "end": 1708
                        },
                        {
                            "start": 1709,
                            "end": 1869
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94580078125
                }
            ],
            "relevance_judgement": 0.94580078125,
            "relevance_judgment_input_expanded": "# Title: SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design\n# Venue: arXiv.org\n# Authors: Fu-Ming Guo\n## Abstract\nThis paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovative optimizer-compiler co-design strategy, demonstrating the potential of inference acceleration (\\textbf{3.37x}, \\textbf{6.30x}, and \\textbf{7.15x} in comparison with Pytorch, TensorFlow, and LLVM generic compile, respectively) in SparseBERT when paired with an appropriately designed compiler. This study represents a significant step forward in the evolution of efficient, scalable, and high-performing large language models, setting a precedent for future exploration and optimization in this domain. The SparseOptimizer code and SparseALBERT model will be publicly available upon paper acceptance.\n## Distillation\nKnowledge distillation, a concept first introduced by Hinton et al. [2015], has emerged as an effective model compression technique whereby a smaller student model is trained to mimic the functionality of a larger, more complex teacher model. Subsequent research has expanded upon this original premise, with distillation methodologies now typically falling into two main categories: general distillation [Sanh et al., 2019, Sun et al., 2020, Wang et al., 2020] and task-specific distillation [Jiao et al., 2019]. \n\nGeneral distillation leverages unlabeled data as a vehicle for knowledge transfer. Despite the computational expense, these methods have shown to be essential in retaining performance levels, particularly when pre-training the student network on an unlabeled corpus [Turc et al., 2019, Jiao et al., 2019]. On the other hand, task-specific distillation exploits task-relevant data to instill the knowledge from the teacher model to the student model. \n\nHowever, the real potential of knowledge distillation has begun to shine through in recent research that combines these two distinct approaches. For instance, the work of Jiao et al. [2019] showed that such hybrid methods could push the performance boundaries even further. \n\nThe advancements in distillation techniques have paved the way for increasingly efficient utilization of large language models. One of the notable milestones in this evolution is the study by Hsieh et al. [2023], which demonstrated that superior performance could be achieved even with less training data and smaller model sizes. \n\nIn summary, the continual progress in knowledge distillation research has opened up new avenues for large language model efficiency. It is this body of work that serves as a valuable foundation for our novel SparseOptimizer, offering a new perspective on model sparsification and optimization.",
            "reference_string": "[259262233 | Guo | 2023 | Citations: 0]"
        },
        {
            "title": "Understanding the Gains from Repeated Self-Distillation",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.04600, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51176912",
                    "name": "Divyansh Pareek"
                },
                {
                    "authorId": "2330243848",
                    "name": "S. S. Du"
                },
                {
                    "authorId": "2310392746",
                    "name": "Sewoong Oh"
                }
            ],
            "abstract": "Self-Distillation is a special type of knowledge distillation where the student model has the same architecture as the teacher model. Despite using the same architecture and the same training data, self-distillation has been empirically observed to improve performance, especially when applied repeatedly. For such a process, there is a fundamental question of interest: How much gain is possible by applying multiple steps of self-distillation? To investigate this relative gain, we propose studying the simple but canonical task of linear regression. Our analysis shows that the excess risk achieved by multi-step self-distillation can significantly improve upon a single step of self-distillation, reducing the excess risk by a factor as large as $d$, where $d$ is the input dimension. Empirical results on regression tasks from the UCI repository show a reduction in the learnt model's risk (MSE) by up to 47%.",
            "corpus_id": 271039191,
            "sentences": [
                {
                    "corpus_id": "271039191",
                    "title": "Understanding the Gains from Repeated Self-Distillation",
                    "text": "Knowledge distillation [11] was initially proposed as a way to transfer the knowledge learnt by a larger teacher model to a smaller student model, which can then be deployed in limited resource settings.The process is as follows: Train a teacher (T ) model using ground-truth labels, then use its predictions to supervise the training of a student (S) model via a combined per-sample loss, \u03be \u2022 \u2113 \u0177T , y S (\u03b8) + (1 \u2212 \u03be) \u2022 \u2113 y, y S (\u03b8) ,\n\nwhere \u2113 denotes the loss function, y is the ground-truth label, \u0177T denotes the teacher's prediction, and y S (\u03b8) denotes the student's prediction, parameterized by the learnable \u03b8.The extra hyperparameter \u03be is called the imitation parameter [24], generally restricted to \u03be \u2208 [0, 1].It gives additional freedom to the student to balance importance between labels and teacher's predictions.The student trained via this distillation objective (i.e., utilizing the teacher's predictions through \u03be \u0338 = 0) has been widely observed to generalize better than when trained only on the labels (i.e., \u03be = 0).This gain has been attributed to 'dark knowledge' that is (i) impossible to be directly extracted from the training data by the small model, but (ii) easily learnt by the large model and transferred to the small model.\n\nChallenging this interpretation, Li et al. [19] and Furlanello et al. [9] empirically observed performance gains through distillation even when the teacher and student are same-sized models.One can set T and S to have the same architecture, and S trained with the objective in Eq. (1) outperforms T .This is referred to as Born-Again Networks (BANs) or Self-Distillation (SD).Furthermore, repeatedly applying self-distillation on the same training data with a student model having the same architecture provides additional gains on benchmark datasets and architectures [9,35,43].At each step, the student from the previous step acts as the teacher used to train a new student model under the self-distillation loss of Eq. (1).",
                    "score": 0.5694442712749914,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 203
                        },
                        {
                            "start": 203,
                            "end": 435
                        },
                        {
                            "start": 437,
                            "end": 617
                        },
                        {
                            "start": 617,
                            "end": 719
                        },
                        {
                            "start": 719,
                            "end": 825
                        },
                        {
                            "start": 825,
                            "end": 1034
                        },
                        {
                            "start": 1034,
                            "end": 1252
                        },
                        {
                            "start": 1254,
                            "end": 1444
                        },
                        {
                            "start": 1444,
                            "end": 1554
                        },
                        {
                            "start": 1554,
                            "end": 1630
                        },
                        {
                            "start": 1630,
                            "end": 1833
                        },
                        {
                            "start": 1833,
                            "end": 1980
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 27,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1297,
                            "end": 1301,
                            "matchedPaperCorpusId": "14659675"
                        },
                        {
                            "start": 1324,
                            "end": 1327,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 1823,
                            "end": 1826,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 1826,
                            "end": 1829,
                            "matchedPaperCorpusId": "54986302"
                        },
                        {
                            "start": 1829,
                            "end": 1832,
                            "matchedPaperCorpusId": "219558831"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9443359375
                }
            ],
            "relevance_judgement": 0.9443359375,
            "relevance_judgment_input_expanded": "# Title: Understanding the Gains from Repeated Self-Distillation\n# Venue: Neural Information Processing Systems\n# Authors: Divyansh Pareek, S. S. Du, Sewoong Oh\n## Abstract\nSelf-Distillation is a special type of knowledge distillation where the student model has the same architecture as the teacher model. Despite using the same architecture and the same training data, self-distillation has been empirically observed to improve performance, especially when applied repeatedly. For such a process, there is a fundamental question of interest: How much gain is possible by applying multiple steps of self-distillation? To investigate this relative gain, we propose studying the simple but canonical task of linear regression. Our analysis shows that the excess risk achieved by multi-step self-distillation can significantly improve upon a single step of self-distillation, reducing the excess risk by a factor as large as $d$, where $d$ is the input dimension. Empirical results on regression tasks from the UCI repository show a reduction in the learnt model's risk (MSE) by up to 47%.\n## Introduction\nKnowledge distillation [11] was initially proposed as a way to transfer the knowledge learnt by a larger teacher model to a smaller student model, which can then be deployed in limited resource settings.The process is as follows: Train a teacher (T ) model using ground-truth labels, then use its predictions to supervise the training of a student (S) model via a combined per-sample loss, \u03be \u2022 \u2113 \u0177T , y S (\u03b8) + (1 \u2212 \u03be) \u2022 \u2113 y, y S (\u03b8) ,\n\nwhere \u2113 denotes the loss function, y is the ground-truth label, \u0177T denotes the teacher's prediction, and y S (\u03b8) denotes the student's prediction, parameterized by the learnable \u03b8.The extra hyperparameter \u03be is called the imitation parameter [24], generally restricted to \u03be \u2208 [0, 1].It gives additional freedom to the student to balance importance between labels and teacher's predictions.The student trained via this distillation objective (i.e., utilizing the teacher's predictions through \u03be \u0338 = 0) has been widely observed to generalize better than when trained only on the labels (i.e., \u03be = 0).This gain has been attributed to 'dark knowledge' that is (i) impossible to be directly extracted from the training data by the small model, but (ii) easily learnt by the large model and transferred to the small model.\n\nChallenging this interpretation, Li et al. [19] and Furlanello et al. [9] empirically observed performance gains through distillation even when the teacher and student are same-sized models.One can set T and S to have the same architecture, and S trained with the objective in Eq. (1) outperforms T .This is referred to as Born-Again Networks (BANs) or Self-Distillation (SD).Furthermore, repeatedly applying self-distillation on the same training data with a student model having the same architecture provides additional gains on benchmark datasets and architectures [9,35,43].At each step, the student from the previous step acts as the teacher used to train a new student model under the self-distillation loss of Eq. (1).",
            "reference_string": "[271039191 | Pareek et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Knowledge Distillation in Image Classification: The Impact of Datasets",
            "venue": "De Computis",
            "year": 2024,
            "reference_count": 48,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2073-431X/13/8/184/pdf?version=1721890205",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/computers13080184?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/computers13080184, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293888061",
                    "name": "Ange-Gabriel Belinga"
                },
                {
                    "authorId": "2320520399",
                    "name": "St\u00e9phane C\u00e9dric Koum\u00e9tio T\u00e9kouabou"
                },
                {
                    "authorId": "144219703",
                    "name": "M. Haziti"
                },
                {
                    "authorId": "50729883",
                    "name": "M. Hassouni"
                }
            ],
            "abstract": "As the demand for efficient and lightweight models in image classification grows, knowledge distillation has emerged as a promising technique to transfer expertise from complex teacher models to simpler student models. However, the efficacy of knowledge distillation is intricately linked to the choice of datasets used during training. Datasets are pivotal in shaping a model\u2019s learning process, influencing its ability to generalize and discriminate between diverse patterns. While considerable research has independently explored knowledge distillation and image classification, a comprehensive understanding of how different datasets impact knowledge distillation remains a critical gap. This study systematically investigates the impact of diverse datasets on knowledge distillation in image classification. By varying dataset characteristics such as size, domain specificity, and inherent biases, we aim to unravel the nuanced relationship between datasets and the efficacy of knowledge transfer. Our experiments employ a range of datasets to comprehensively explore their impact on the performance gains achieved through knowledge distillation. This study contributes valuable guidance for researchers and practitioners seeking to optimize image classification models through kno-featured applications. By elucidating the intricate interplay between dataset characteristics and knowledge distillation outcomes, our findings empower the community to make informed decisions when selecting datasets, ultimately advancing the field toward more robust and efficient model development.",
            "corpus_id": 271441660,
            "sentences": [
                {
                    "corpus_id": "271441660",
                    "title": "Knowledge Distillation in Image Classification: The Impact of Datasets",
                    "text": "Several works have explored various aspects of knowledge distillation in deep neural networks [26], including teacher and student model architectures, regularization techniques, and optimization methods. \n\nFor example, Li et al. proposed a transferred attention method to improve the performance of convolutional neural networks [27], while Yazdanbakhsh et al. studied the application of knowledge distillation in specific domains such as healthcare [19]. However, despite these significant advances, little attention has been paid to the impact of data on this knowledge transfer process. The authors demonstrated the effectiveness of the distillation on various tasks and highlighted its potential for model compression. The FitNets paper [20] proposed a specific form of knowledge distillation called FitNets, where a student network is guided not only by the output probabilities of a teacher network but also by intermediate representations (or hints). This work aimed to improve the transfer of information in the training process. Ref. [27] introduces attention transfer as a form of knowledge distillation. It focuses on transferring attention maps from a teacher to a student network to improve the student's performance. Attention transfer has proven effective in enhancing the generalization capabilities of the student model. To address the limitations of traditional knowledge distillation, ref. [31] introduces Jacobian matching, a novel method that aims to transfer not only the output probabilities but also the derivatives of the teacher model's predictions. This approach provides a more comprehensive form of knowledge transfer. Ref. [30] explores the benefits of knowledge distillation beyond model compression. The authors show that the knowledge distillation process not only compresses models but also accelerates the optimization process, enabling faster convergence during training. Ref. [32] introduces the concept of a \"teacher assistant\" by proposing an extension to traditional knowledge distillation. The teacher assistant helps bridge the performance gap between the teacher and the student, leading to enhanced knowledge transfer.",
                    "score": 0.5433954586082324,
                    "section_title": "Knowledge Distillation in the Literature",
                    "char_start_offset": 6239,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 203
                        },
                        {
                            "start": 206,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 589
                        },
                        {
                            "start": 590,
                            "end": 722
                        },
                        {
                            "start": 723,
                            "end": 957
                        },
                        {
                            "start": 958,
                            "end": 1037
                        },
                        {
                            "start": 1038,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1230
                        },
                        {
                            "start": 1231,
                            "end": 1337
                        },
                        {
                            "start": 1338,
                            "end": 1575
                        },
                        {
                            "start": 1576,
                            "end": 1647
                        },
                        {
                            "start": 1648,
                            "end": 1731
                        },
                        {
                            "start": 1732,
                            "end": 1907
                        },
                        {
                            "start": 1908,
                            "end": 2030
                        },
                        {
                            "start": 2031,
                            "end": 2162
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 94,
                            "end": 98,
                            "matchedPaperCorpusId": "249209742"
                        },
                        {
                            "start": 450,
                            "end": 454,
                            "matchedPaperCorpusId": "259389158"
                        },
                        {
                            "start": 1409,
                            "end": 1413,
                            "matchedPaperCorpusId": "3603145"
                        },
                        {
                            "start": 1653,
                            "end": 1657,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 1913,
                            "end": 1917,
                            "matchedPaperCorpusId": "212908749"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93115234375
                }
            ],
            "relevance_judgement": 0.93115234375,
            "relevance_judgment_input_expanded": "# Title: Knowledge Distillation in Image Classification: The Impact of Datasets\n# Venue: De Computis\n# Authors: Ange-Gabriel Belinga, St\u00e9phane C\u00e9dric Koum\u00e9tio T\u00e9kouabou, M. Haziti, M. Hassouni\n## Abstract\nAs the demand for efficient and lightweight models in image classification grows, knowledge distillation has emerged as a promising technique to transfer expertise from complex teacher models to simpler student models. However, the efficacy of knowledge distillation is intricately linked to the choice of datasets used during training. Datasets are pivotal in shaping a model\u2019s learning process, influencing its ability to generalize and discriminate between diverse patterns. While considerable research has independently explored knowledge distillation and image classification, a comprehensive understanding of how different datasets impact knowledge distillation remains a critical gap. This study systematically investigates the impact of diverse datasets on knowledge distillation in image classification. By varying dataset characteristics such as size, domain specificity, and inherent biases, we aim to unravel the nuanced relationship between datasets and the efficacy of knowledge transfer. Our experiments employ a range of datasets to comprehensively explore their impact on the performance gains achieved through knowledge distillation. This study contributes valuable guidance for researchers and practitioners seeking to optimize image classification models through kno-featured applications. By elucidating the intricate interplay between dataset characteristics and knowledge distillation outcomes, our findings empower the community to make informed decisions when selecting datasets, ultimately advancing the field toward more robust and efficient model development.\n## Knowledge Distillation in the Literature\nSeveral works have explored various aspects of knowledge distillation in deep neural networks [26], including teacher and student model architectures, regularization techniques, and optimization methods. \n\nFor example, Li et al. proposed a transferred attention method to improve the performance of convolutional neural networks [27], while Yazdanbakhsh et al. studied the application of knowledge distillation in specific domains such as healthcare [19]. However, despite these significant advances, little attention has been paid to the impact of data on this knowledge transfer process. The authors demonstrated the effectiveness of the distillation on various tasks and highlighted its potential for model compression. The FitNets paper [20] proposed a specific form of knowledge distillation called FitNets, where a student network is guided not only by the output probabilities of a teacher network but also by intermediate representations (or hints). This work aimed to improve the transfer of information in the training process. Ref. [27] introduces attention transfer as a form of knowledge distillation. It focuses on transferring attention maps from a teacher to a student network to improve the student's performance. Attention transfer has proven effective in enhancing the generalization capabilities of the student model. To address the limitations of traditional knowledge distillation, ref. [31] introduces Jacobian matching, a novel method that aims to transfer not only the output probabilities but also the derivatives of the teacher model's predictions. This approach provides a more comprehensive form of knowledge transfer. Ref. [30] explores the benefits of knowledge distillation beyond model compression. The authors show that the knowledge distillation process not only compresses models but also accelerates the optimization process, enabling faster convergence during training. Ref. [32] introduces the concept of a \"teacher assistant\" by proposing an extension to traditional knowledge distillation. The teacher assistant helps bridge the performance gap between the teacher and the student, leading to enhanced knowledge transfer.",
            "reference_string": "[271441660 | Belinga et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Guiding the Student\u2019s Learning Curve: Augmenting Knowledge Distillation with Insights from GradCAM",
            "venue": "2023 International Conference on Modeling, Simulation & Intelligent Computing (MoSICom)",
            "year": 2023,
            "reference_count": 20,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/MoSICom59118.2023.10458758?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/MoSICom59118.2023.10458758, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293143380",
                    "name": "Suvaditya Mukherjee"
                },
                {
                    "authorId": "2292319438",
                    "name": "Dev Vinod Chandan"
                },
                {
                    "authorId": "2268069324",
                    "name": "Shreyas Dongre"
                }
            ],
            "abstract": "In the pursuit of model compression techniques, Knowledge Distillation emerges as an effective strategy for developing compact neural network models. This approach primarily revolves around the minimization of a loss function that quantifies the disparity between the Prediction Logits generated by a larger Teacher network and those produced by a smaller Student network. In our research, we propose an enhancement to this method by incorporating the GradCAM (Gradient-weighted Class Activation Mapping) technique from the field of Model Explainability. This augmentation involves leveraging GradCAM as an additional input to the Student network for improved learning. Notably, our findings reveal that this approach facilitates expedited convergence, particularly when the Teacher network exhibits strong performance and a substantial size advantage over the Student network.",
            "corpus_id": 268543692,
            "sentences": [],
            "relevance_judgement": 0.9306640625,
            "relevance_judgment_input_expanded": "# Title: Guiding the Student\u2019s Learning Curve: Augmenting Knowledge Distillation with Insights from GradCAM\n# Venue: 2023 International Conference on Modeling, Simulation & Intelligent Computing (MoSICom)\n# Authors: Suvaditya Mukherjee, Dev Vinod Chandan, Shreyas Dongre\n## Abstract\nIn the pursuit of model compression techniques, Knowledge Distillation emerges as an effective strategy for developing compact neural network models. This approach primarily revolves around the minimization of a loss function that quantifies the disparity between the Prediction Logits generated by a larger Teacher network and those produced by a smaller Student network. In our research, we propose an enhancement to this method by incorporating the GradCAM (Gradient-weighted Class Activation Mapping) technique from the field of Model Explainability. This augmentation involves leveraging GradCAM as an additional input to the Student network for improved learning. Notably, our findings reveal that this approach facilitates expedited convergence, particularly when the Teacher network exhibits strong performance and a substantial size advantage over the Student network.\n",
            "reference_string": "[268543692 | Mukherjee et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Leveraging logit uncertainty for better knowledge distillation",
            "venue": "Scientific Reports",
            "year": 2024,
            "reference_count": 7,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11682123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303981889",
                    "name": "Zhenkai Guo"
                },
                {
                    "authorId": "2337674692",
                    "name": "Dong Wang"
                },
                {
                    "authorId": "2210876822",
                    "name": "Qiang He"
                },
                {
                    "authorId": "2297246012",
                    "name": "Pengzhou Zhang"
                }
            ],
            "abstract": "Knowledge distillation improves student model performance. However, using a larger teacher model does not necessarily result in better distillation gains due to significant architecture and output gaps with smaller student networks. To address this issue, we reconsider teacher outputs and find that categories with strong teacher confidence benefit distillation more, while those with weaker certainty contribute less. Thus, we propose Logits Uncertainty Distillation (LUD) to bridge this gap. We introduce category uncertainty weighting to consider the uncertainty in the teacher model\u2019s predictions. A confidence threshold, based on the teacher\u2019s predictions, helps construct a mask that discounts uncertain classes during distillation. Furthermore, we incorporate two Spearman correlation loss functions to align the logits of the teacher and student models. These loss functions measure the discrepancy between the models\u2019 outputs at the category and sample levels. We also introduce adaptive dynamic temperature factors to optimize the distillation process. By combining these techniques, we enhance knowledge distillation results and facilitate effective knowledge transfer between teacher and student models, even when architectural differences exist. Extensive experiments on multiple datasets demonstrate the effectiveness of our method.",
            "corpus_id": 275117509,
            "sentences": [
                {
                    "corpus_id": "275117509",
                    "title": "Leveraging logit uncertainty for better knowledge distillation",
                    "text": "Knowledge distillation improves student model performance. However, using a larger teacher model does not necessarily result in better distillation gains due to significant architecture and output gaps with smaller student networks. To address this issue, we reconsider teacher outputs and find that categories with strong teacher confidence benefit distillation more, while those with weaker certainty contribute less. Thus, we propose Logits Uncertainty Distillation (LUD) to bridge this gap. We introduce category uncertainty weighting to consider the uncertainty in the teacher model\u2019s predictions. A confidence threshold, based on the teacher\u2019s predictions, helps construct a mask that discounts uncertain classes during distillation. Furthermore, we incorporate two Spearman correlation loss functions to align the logits of the teacher and student models. These loss functions measure the discrepancy between the models\u2019 outputs at the category and sample levels. We also introduce adaptive dynamic temperature factors to optimize the distillation process. By combining these techniques, we enhance knowledge distillation results and facilitate effective knowledge transfer between teacher and student models, even when architectural differences exist. Extensive experiments on multiple datasets demonstrate the effectiveness of our method.",
                    "score": 0.5374426228853173,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93017578125
                }
            ],
            "relevance_judgement": 0.93017578125,
            "relevance_judgment_input_expanded": "# Title: Leveraging logit uncertainty for better knowledge distillation\n# Venue: Scientific Reports\n# Authors: Zhenkai Guo, Dong Wang, Qiang He, Pengzhou Zhang\n## Abstract\nKnowledge distillation improves student model performance. However, using a larger teacher model does not necessarily result in better distillation gains due to significant architecture and output gaps with smaller student networks. To address this issue, we reconsider teacher outputs and find that categories with strong teacher confidence benefit distillation more, while those with weaker certainty contribute less. Thus, we propose Logits Uncertainty Distillation (LUD) to bridge this gap. We introduce category uncertainty weighting to consider the uncertainty in the teacher model\u2019s predictions. A confidence threshold, based on the teacher\u2019s predictions, helps construct a mask that discounts uncertain classes during distillation. Furthermore, we incorporate two Spearman correlation loss functions to align the logits of the teacher and student models. These loss functions measure the discrepancy between the models\u2019 outputs at the category and sample levels. We also introduce adaptive dynamic temperature factors to optimize the distillation process. By combining these techniques, we enhance knowledge distillation results and facilitate effective knowledge transfer between teacher and student models, even when architectural differences exist. Extensive experiments on multiple datasets demonstrate the effectiveness of our method.\n",
            "reference_string": "[275117509 | Guo et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Online Cascade Learning for Efficient Inference over Streams",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.04513, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313699525",
                    "name": "Lunyiu Nie"
                },
                {
                    "authorId": "2283314288",
                    "name": "Zhimin Ding"
                },
                {
                    "authorId": "2283132576",
                    "name": "Erdong Hu"
                },
                {
                    "authorId": "2283132701",
                    "name": "Christopher Jermaine"
                },
                {
                    "authorId": "35865989",
                    "name": "Swarat Chaudhuri"
                }
            ],
            "abstract": "Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to address this challenge. The objective here is to learn a\"cascade\"of models, starting with lower-capacity models (such as logistic regression) and ending with a powerful LLM, along with a deferral policy that determines the model to be used on a given input. We formulate the task of learning cascades online as an imitation-learning problem, where smaller models are updated over time imitating the collected LLM demonstrations, and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90% with strong robustness against input distribution shifts, underscoring its efficacy and adaptability in stream processing.",
            "corpus_id": 267522781,
            "sentences": [
                {
                    "corpus_id": "267522781",
                    "title": "Online Cascade Learning for Efficient Inference over Streams",
                    "text": "Knowledge distillation, originally conceptualized by Hinton et al. (2015), emerged as a technique to transfer knowledge from a large, complex model (teacher) to a smaller, more efficient one (student), intending to retain performance while reducing computational costs. Notable advancements include the works of Sanh et al. (2019), who demonstrated the effectiveness of distilling the capabilities of BERT into smaller models, and Gu et al. (2023) who successfully applied distillation to LLMs, achieving comparable performance with significantly reduced model sizes. However, the significant difference in capabilities between the teacher and student model can lead to challenges, particularly when dealing with complex queries that require advanced reasoning or involve intricate subject matter (Cho & Hariharan, 2019;Zhang et al., 2022), thereby highlighting a performance gap that distillation alone cannot overcome (Rawat et al., 2021).",
                    "score": 0.6143835894323737,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 28566,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 567
                        },
                        {
                            "start": 568,
                            "end": 941
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 797,
                            "end": 820,
                            "matchedPaperCorpusId": "203642130"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92919921875
                }
            ],
            "relevance_judgement": 0.92919921875,
            "relevance_judgment_input_expanded": "# Title: Online Cascade Learning for Efficient Inference over Streams\n# Venue: International Conference on Machine Learning\n# Authors: Lunyiu Nie, Zhimin Ding, Erdong Hu, Christopher Jermaine, Swarat Chaudhuri\n## Abstract\nLarge Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to address this challenge. The objective here is to learn a\"cascade\"of models, starting with lower-capacity models (such as logistic regression) and ending with a powerful LLM, along with a deferral policy that determines the model to be used on a given input. We formulate the task of learning cascades online as an imitation-learning problem, where smaller models are updated over time imitating the collected LLM demonstrations, and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90% with strong robustness against input distribution shifts, underscoring its efficacy and adaptability in stream processing.\n## Knowledge Distillation\nKnowledge distillation, originally conceptualized by Hinton et al. (2015), emerged as a technique to transfer knowledge from a large, complex model (teacher) to a smaller, more efficient one (student), intending to retain performance while reducing computational costs. Notable advancements include the works of Sanh et al. (2019), who demonstrated the effectiveness of distilling the capabilities of BERT into smaller models, and Gu et al. (2023) who successfully applied distillation to LLMs, achieving comparable performance with significantly reduced model sizes. However, the significant difference in capabilities between the teacher and student model can lead to challenges, particularly when dealing with complex queries that require advanced reasoning or involve intricate subject matter (Cho & Hariharan, 2019;Zhang et al., 2022), thereby highlighting a performance gap that distillation alone cannot overcome (Rawat et al., 2021).",
            "reference_string": "[267522781 | Nie et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Class-Incremental Continual Learning Into the eXtended DER-Verse",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2022,
            "reference_count": 94,
            "citation_count": 141,
            "influential_citation_count": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/34/4359286/09891836.pdf",
                "status": "HYBRID",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.00766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51096265",
                    "name": "Matteo Boschini"
                },
                {
                    "authorId": "2123319338",
                    "name": "Lorenzo Bonicelli"
                },
                {
                    "authorId": "1429191945",
                    "name": "Pietro Buzzega"
                },
                {
                    "authorId": "51119730",
                    "name": "Angelo Porrello"
                },
                {
                    "authorId": "2175529",
                    "name": "S. Calderara"
                }
            ],
            "abstract": "The staple of human intelligence is the capability of acquiring knowledge in a continuous fashion. In stark contrast, Deep Networks forget catastrophically and, for this reason, the sub-field of Class-Incremental Continual Learning fosters methods that learn a sequence of tasks incrementally, blending sequentially-gained knowledge into a comprehensive prediction. This work aims at assessing and overcoming the pitfalls of our previous proposal Dark Experience Replay (DER), a simple and effective approach that combines rehearsal and Knowledge Distillation. Inspired by the way our minds constantly rewrite past recollections and set expectations for the future, we endow our model with the abilities to i) revise its replay memory to welcome novel information regarding past data ii) pave the way for learning yet unseen classes. We show that the application of these strategies leads to remarkable improvements; indeed, the resulting method \u2013 termed eXtended-DER (X-DER) \u2013 outperforms the state of the art on both standard benchmarks (such as CIFAR-100 and miniImageNet) and a novel one here introduced. To gain a better understanding, we further provide extensive ablation studies that corroborate and extend the findings of our previous research (e.g., the value of Knowledge Distillation and flatter minima in continual learning setups). We make our results fully reproducible; the codebase is available at https://github.com/aimagelab/mammoth.",
            "corpus_id": 245650327,
            "sentences": [
                {
                    "corpus_id": "245650327",
                    "title": "Class-Incremental Continual Learning Into the eXtended DER-Verse",
                    "text": "Knowledge Distillation (KD) [36] is a training methodology that allows transferring the knowledge of a teacher model into a separate student model. While Hinton et al. originally proposed to distillate large teachers -possibly ensembles -into smaller students, further studies revealed additional interesting properties about this technique. In particular, Furlanello et al. [37] show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student. More recently, other works [38], [39] explore an interesting variation of self-distillation that distills knowledge from the deeper layers of the network to its shallower ones to accelerate convergence and attain higher accuracy. \n\nKnowledge Distillation and Continual Learning. Distillation can be used to hinder catastrophic forgetting by appointing a previous snapshot of the model as teacher and distilling from it while new tasks are learned. Learning Without Forgetting [7] uses teacher responses to new exemplars to constrain the evolution of the student. Several other works combine distillation with rehearsal: iCaRL [11] distills the responses of the model at the previous task boundary, learning latent representations to be used in a nearest meanof-exemplars classifier; EtEIL [40], LUCIR [31] and BiC [41] focus on contrasting the prediction bias that comes from incremental classification; IL2M [30] stores additional statistics to facilitate distillation and compensate bias.",
                    "score": 0.592289755712385,
                    "section_title": "Self-Distillation",
                    "char_start_offset": 8104,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 147
                        },
                        {
                            "start": 148,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 547
                        },
                        {
                            "start": 548,
                            "end": 777
                        },
                        {
                            "start": 780,
                            "end": 826
                        },
                        {
                            "start": 827,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1110
                        },
                        {
                            "start": 1111,
                            "end": 1538
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 28,
                            "end": 32,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 375,
                            "end": 379,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 575,
                            "end": 579,
                            "matchedPaperCorpusId": "159041406"
                        },
                        {
                            "start": 1024,
                            "end": 1027,
                            "matchedPaperCorpusId": "4853851"
                        },
                        {
                            "start": 1174,
                            "end": 1178,
                            "matchedPaperCorpusId": "206596260"
                        },
                        {
                            "start": 1349,
                            "end": 1353,
                            "matchedPaperCorpusId": "195453293"
                        },
                        {
                            "start": 1362,
                            "end": 1366,
                            "matchedPaperCorpusId": "173187918"
                        },
                        {
                            "start": 1457,
                            "end": 1461,
                            "matchedPaperCorpusId": "204923710"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92919921875
                }
            ],
            "relevance_judgement": 0.92919921875,
            "relevance_judgment_input_expanded": "# Title: Class-Incremental Continual Learning Into the eXtended DER-Verse\n# Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence\n# Authors: Matteo Boschini, Lorenzo Bonicelli, Pietro Buzzega, Angelo Porrello, S. Calderara\n## Abstract\nThe staple of human intelligence is the capability of acquiring knowledge in a continuous fashion. In stark contrast, Deep Networks forget catastrophically and, for this reason, the sub-field of Class-Incremental Continual Learning fosters methods that learn a sequence of tasks incrementally, blending sequentially-gained knowledge into a comprehensive prediction. This work aims at assessing and overcoming the pitfalls of our previous proposal Dark Experience Replay (DER), a simple and effective approach that combines rehearsal and Knowledge Distillation. Inspired by the way our minds constantly rewrite past recollections and set expectations for the future, we endow our model with the abilities to i) revise its replay memory to welcome novel information regarding past data ii) pave the way for learning yet unseen classes. We show that the application of these strategies leads to remarkable improvements; indeed, the resulting method \u2013 termed eXtended-DER (X-DER) \u2013 outperforms the state of the art on both standard benchmarks (such as CIFAR-100 and miniImageNet) and a novel one here introduced. To gain a better understanding, we further provide extensive ablation studies that corroborate and extend the findings of our previous research (e.g., the value of Knowledge Distillation and flatter minima in continual learning setups). We make our results fully reproducible; the codebase is available at https://github.com/aimagelab/mammoth.\n## Self-Distillation\nKnowledge Distillation (KD) [36] is a training methodology that allows transferring the knowledge of a teacher model into a separate student model. While Hinton et al. originally proposed to distillate large teachers -possibly ensembles -into smaller students, further studies revealed additional interesting properties about this technique. In particular, Furlanello et al. [37] show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student. More recently, other works [38], [39] explore an interesting variation of self-distillation that distills knowledge from the deeper layers of the network to its shallower ones to accelerate convergence and attain higher accuracy. \n\nKnowledge Distillation and Continual Learning. Distillation can be used to hinder catastrophic forgetting by appointing a previous snapshot of the model as teacher and distilling from it while new tasks are learned. Learning Without Forgetting [7] uses teacher responses to new exemplars to constrain the evolution of the student. Several other works combine distillation with rehearsal: iCaRL [11] distills the responses of the model at the previous task boundary, learning latent representations to be used in a nearest meanof-exemplars classifier; EtEIL [40], LUCIR [31] and BiC [41] focus on contrasting the prediction bias that comes from incremental classification; IL2M [30] stores additional statistics to facilitate distillation and compensate bias.",
            "reference_string": "[245650327 | Boschini et al. | 2022 | Citations: 141]"
        },
        {
            "title": "Bring the Power of Diffusion Model to Defect Detection",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.13845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2317142609",
                    "name": "Xuyi Yu"
                }
            ],
            "abstract": "Due to the high complexity and technical requirements of industrial production processes, surface defects will inevitably appear, which seriously affects the quality of products. Although existing lightweight detection networks are highly efficient, they are susceptible to false or missed detection of non-salient defects due to the lack of semantic information. In contrast, the diffusion model can generate higher-order semantic representations in the denoising process. Therefore, the aim of this paper is to incorporate the higher-order modelling capability of the diffusion model into the detection model, so as to better assist in the classification and localization of difficult targets. First, the denoising diffusion probabilistic model (DDPM) is pre-trained to extract the features of denoising process to construct as a feature repository. In particular, to avoid the potential bottleneck of memory caused by the dataloader loading high-dimensional features, a residual convolutional variational auto-encoder (ResVAE) is designed to further compress the feature repository. The image is fed into both image backbone and feature repository for feature extraction and querying respectively. The queried latent features are reconstructed and filtered to obtain high-dimensional DDPM features. A dynamic cross-fusion method is proposed to fully refine the contextual features of DDPM to optimize the detection model. Finally, we employ knowledge distillation to migrate the higher-order modelling capabilities back into the lightweight baseline model without additional efficiency cost. Experiment results demonstrate that our method achieves competitive results on several industrial datasets.",
            "corpus_id": 271956980,
            "sentences": [
                {
                    "corpus_id": "271956980",
                    "title": "Bring the Power of Diffusion Model to Defect Detection",
                    "text": "Large-scale deep models have achieved remarkable success, but their computational complexity and massive storage requirements make real-time deployment a challenge, especially on resource-constrained devices such as video surveillance and self-driving cars. \n\nKnowledge distillation is a classical approach of model compression and acceleration that effectively learns small student models from large teacher models [18]. In knowledge distillation, small student models are usually supervised with the assistance of large teacher models, allowing the student models to mimic the teacher models for a competitive or even superior performance. Knowledge distillation is similar to the way humans learn, and the key issue in distillation is how to better transfer knowledge from large teacher models to small student models. Inspired by this, recent methods to knowledge distillation have been extended to teacher-student learning [19], mutual learning [20], self-learning [21] and so on. Most of the extensions to knowledge distillation focus on compressing deep neural networks. The generated lightweight student networks can be easily deployed in applications such as visual recognition, speech recognition and natural language processing (NLP). \n\nIn addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled [22]. We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student.",
                    "score": 0.6230847257772065,
                    "section_title": "B. Knowledge Distillation",
                    "char_start_offset": 7189,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 257
                        },
                        {
                            "start": 260,
                            "end": 421
                        },
                        {
                            "start": 422,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 821
                        },
                        {
                            "start": 822,
                            "end": 985
                        },
                        {
                            "start": 986,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1245
                        },
                        {
                            "start": 1248,
                            "end": 1451
                        },
                        {
                            "start": 1452,
                            "end": 1564
                        },
                        {
                            "start": 1565,
                            "end": 1647
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 416,
                            "end": 420,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 928,
                            "end": 932,
                            "matchedPaperCorpusId": "259697005"
                        },
                        {
                            "start": 950,
                            "end": 954,
                            "matchedPaperCorpusId": "249145972"
                        },
                        {
                            "start": 970,
                            "end": 974,
                            "matchedPaperCorpusId": "251196646"
                        },
                        {
                            "start": 1446,
                            "end": 1450,
                            "matchedPaperCorpusId": "212908749"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.927734375
                }
            ],
            "relevance_judgement": 0.927734375,
            "relevance_judgment_input_expanded": "# Title: Bring the Power of Diffusion Model to Defect Detection\n# Venue: arXiv.org\n# Authors: Xuyi Yu\n## Abstract\nDue to the high complexity and technical requirements of industrial production processes, surface defects will inevitably appear, which seriously affects the quality of products. Although existing lightweight detection networks are highly efficient, they are susceptible to false or missed detection of non-salient defects due to the lack of semantic information. In contrast, the diffusion model can generate higher-order semantic representations in the denoising process. Therefore, the aim of this paper is to incorporate the higher-order modelling capability of the diffusion model into the detection model, so as to better assist in the classification and localization of difficult targets. First, the denoising diffusion probabilistic model (DDPM) is pre-trained to extract the features of denoising process to construct as a feature repository. In particular, to avoid the potential bottleneck of memory caused by the dataloader loading high-dimensional features, a residual convolutional variational auto-encoder (ResVAE) is designed to further compress the feature repository. The image is fed into both image backbone and feature repository for feature extraction and querying respectively. The queried latent features are reconstructed and filtered to obtain high-dimensional DDPM features. A dynamic cross-fusion method is proposed to fully refine the contextual features of DDPM to optimize the detection model. Finally, we employ knowledge distillation to migrate the higher-order modelling capabilities back into the lightweight baseline model without additional efficiency cost. Experiment results demonstrate that our method achieves competitive results on several industrial datasets.\n## B. Knowledge Distillation\nLarge-scale deep models have achieved remarkable success, but their computational complexity and massive storage requirements make real-time deployment a challenge, especially on resource-constrained devices such as video surveillance and self-driving cars. \n\nKnowledge distillation is a classical approach of model compression and acceleration that effectively learns small student models from large teacher models [18]. In knowledge distillation, small student models are usually supervised with the assistance of large teacher models, allowing the student models to mimic the teacher models for a competitive or even superior performance. Knowledge distillation is similar to the way humans learn, and the key issue in distillation is how to better transfer knowledge from large teacher models to small student models. Inspired by this, recent methods to knowledge distillation have been extended to teacher-student learning [19], mutual learning [20], self-learning [21] and so on. Most of the extensions to knowledge distillation focus on compressing deep neural networks. The generated lightweight student networks can be easily deployed in applications such as visual recognition, speech recognition and natural language processing (NLP). \n\nIn addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled [22]. We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student.",
            "reference_string": "[271956980 | Yu | 2024 | Citations: 1]"
        },
        {
            "title": "CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 77,
            "citation_count": 29,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2203.06760",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.06760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145802952",
                    "name": "Yuan Gong"
                },
                {
                    "authorId": "40570741",
                    "name": "Sameer Khurana"
                },
                {
                    "authorId": "2110769738",
                    "name": "Andrew Rouditchenko"
                },
                {
                    "authorId": "145898106",
                    "name": "James R. Glass"
                }
            ],
            "abstract": "Audio classification is an active research area with a wide range of applications. Over the past decade, convolutional neural networks (CNNs) have been the de-facto standard building block for end-to-end audio classification models. Recently, neural networks based solely on self-attention mechanisms such as the Audio Spectrogram Transformer (AST) have been shown to outperform CNNs. In this paper, we find an intriguing interaction between the two very different models - CNN and AST models are good teachers for each other. When we use either of them as the teacher and train the other model as the student via knowledge distillation (KD), the performance of the student model noticeably improves, and in many cases, is better than the teacher model. In our experiments with this CNN/Transformer Cross-Model Knowledge Distillation (CMKD) method we achieve new state-of-the-art performance on FSD50K, AudioSet, and ESC-50.",
            "corpus_id": 247446679,
            "sentences": [
                {
                    "corpus_id": "247446679",
                    "title": "CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification",
                    "text": "We first explore the optimal teacher model for each student model. Specifically, we use the KD setting mentioned in Section 2 and test a set of (teacher, student) model pairs. Specifically, we consider the following models: EfficientNet B0, B2, B6 [21] and DenseNet-121 [22] (CNN models); and AST Tiny, Small, Base [11] (Transformer models). The reason for selecting these models are: 1) EfficientNet-B2 and AST-Base are the best CNN and Transformer models, and we want to see if KD can further improve their performance; 2) AST-Base is larger than EfficientNet-B2 in terms of the number of parameters. We include models with various sizes to clearly identify whether the improvement of KD is due to different model sizes or different model architecture; 3) We also include one different CNN architecture (DenseNet) to check if the cross-model KD framework generalizes to different CNN architectures. \n\nFor each (teacher, student) pair, we select the temperature \u03c4 \u2208 {1.0, 2.5} based on the performance on the validation set, all other knowledge distillation settings are identical for all (teacher, student) pairs. The mean average precisions (mAPs) of the student model of various (teacher,student) pairs on the FSD50K evaluation set are shown in Table 3. Key observations are as follows: \n\n1. CNNs and ASTs are good teachers for each other. While knowledge distillation improves the student model performance in almost all settings (denoted by \u2020 in Table 3), we find that models always prefer a different teacher. Specifically, all CNN models (including DenseNet) achieve the best performance when ASTs are the teacher; all AST models achieve the best performance when CNNs are the teacher. Note that even though DenseNet and EfficientNet models also have different architectures, knowledge distillation between these two models leads to limited improvement. This demonstrates that the inherent differences between CNNs and ASTs is crucial for the success of knowledge distillation. 2. For both directions, the student model matches or outperforms its teacher. Usually, in knowledge distillation, the student model gets closer to, but is still weaker than, its teacher model.",
                    "score": 0.5553194282295588,
                    "section_title": "Which model is a good teacher?",
                    "char_start_offset": 14718,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 66
                        },
                        {
                            "start": 67,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 900
                        },
                        {
                            "start": 903,
                            "end": 1115
                        },
                        {
                            "start": 1116,
                            "end": 1290
                        },
                        {
                            "start": 1293,
                            "end": 1343
                        },
                        {
                            "start": 1344,
                            "end": 1516
                        },
                        {
                            "start": 1517,
                            "end": 1693
                        },
                        {
                            "start": 1694,
                            "end": 1861
                        },
                        {
                            "start": 1862,
                            "end": 1985
                        },
                        {
                            "start": 1986,
                            "end": 2063
                        },
                        {
                            "start": 2064,
                            "end": 2178
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 248,
                            "end": 252,
                            "matchedPaperCorpusId": "167217261"
                        },
                        {
                            "start": 270,
                            "end": 274,
                            "matchedPaperCorpusId": "9433631"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.927734375
                }
            ],
            "relevance_judgement": 0.927734375,
            "relevance_judgment_input_expanded": "# Title: CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification\n# Venue: arXiv.org\n# Authors: Yuan Gong, Sameer Khurana, Andrew Rouditchenko, James R. Glass\n## Abstract\nAudio classification is an active research area with a wide range of applications. Over the past decade, convolutional neural networks (CNNs) have been the de-facto standard building block for end-to-end audio classification models. Recently, neural networks based solely on self-attention mechanisms such as the Audio Spectrogram Transformer (AST) have been shown to outperform CNNs. In this paper, we find an intriguing interaction between the two very different models - CNN and AST models are good teachers for each other. When we use either of them as the teacher and train the other model as the student via knowledge distillation (KD), the performance of the student model noticeably improves, and in many cases, is better than the teacher model. In our experiments with this CNN/Transformer Cross-Model Knowledge Distillation (CMKD) method we achieve new state-of-the-art performance on FSD50K, AudioSet, and ESC-50.\n## Which model is a good teacher?\nWe first explore the optimal teacher model for each student model. Specifically, we use the KD setting mentioned in Section 2 and test a set of (teacher, student) model pairs. Specifically, we consider the following models: EfficientNet B0, B2, B6 [21] and DenseNet-121 [22] (CNN models); and AST Tiny, Small, Base [11] (Transformer models). The reason for selecting these models are: 1) EfficientNet-B2 and AST-Base are the best CNN and Transformer models, and we want to see if KD can further improve their performance; 2) AST-Base is larger than EfficientNet-B2 in terms of the number of parameters. We include models with various sizes to clearly identify whether the improvement of KD is due to different model sizes or different model architecture; 3) We also include one different CNN architecture (DenseNet) to check if the cross-model KD framework generalizes to different CNN architectures. \n\nFor each (teacher, student) pair, we select the temperature \u03c4 \u2208 {1.0, 2.5} based on the performance on the validation set, all other knowledge distillation settings are identical for all (teacher, student) pairs. The mean average precisions (mAPs) of the student model of various (teacher,student) pairs on the FSD50K evaluation set are shown in Table 3. Key observations are as follows: \n\n1. CNNs and ASTs are good teachers for each other. While knowledge distillation improves the student model performance in almost all settings (denoted by \u2020 in Table 3), we find that models always prefer a different teacher. Specifically, all CNN models (including DenseNet) achieve the best performance when ASTs are the teacher; all AST models achieve the best performance when CNNs are the teacher. Note that even though DenseNet and EfficientNet models also have different architectures, knowledge distillation between these two models leads to limited improvement. This demonstrates that the inherent differences between CNNs and ASTs is crucial for the success of knowledge distillation. 2. For both directions, the student model matches or outperforms its teacher. Usually, in knowledge distillation, the student model gets closer to, but is still weaker than, its teacher model.",
            "reference_string": "[247446679 | Gong et al. | 2022 | Citations: 29]"
        },
        {
            "title": "Decoupled Classifier Knowledge Distillation",
            "venue": "PLoS ONE",
            "year": 2025,
            "reference_count": 33,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1371/journal.pone.0314267",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11844843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2195337296",
                    "name": "Hairui Wang"
                },
                {
                    "authorId": "2346974967",
                    "name": "Mengjie Dong"
                },
                {
                    "authorId": "2181805784",
                    "name": "Guifu Zhu"
                },
                {
                    "authorId": "50024555",
                    "name": "Ya Li"
                }
            ],
            "abstract": "Mainstream knowledge distillation methods primarily include self-distillation, offline distillation, online distillation, output-based distillation, and feature-based distillation. While each approach has its respective advantages, they are typically employed independently. Simply combining two distillation methods often leads to redundant information. If the information conveyed by both methods is highly similar, this can result in wasted computational resources and increased complexity. To provide a new perspective on distillation research, we aim to explore a compromise solution that aligns complex features without conflicting with output alignment. In this work, we propose to decouple the classifier\u2019s output into two components: non-target classes learned by the student, and target classes obtained by both the teacher and the student. Finally, we introduce Decoupled Classifier Knowledge Distillation (DCKD), where on one hand, we fix the correct knowledge that the student has already acquired, which is crucial for merging the two methods; on the other hand, we encourage the student to further align its output with that of the teacher. Compared to using a single method, DCKD achieves superior results on both the CIFAR-100 and ImageNet datasets for image classification and object detection tasks, without reducing training efficiency. Moreover, it allows relational-based and feature-based distillation to operate more efficiently and flexibly. This work demonstrates the great potential of integrating distillation methods, and we hope it will inspire future research.",
            "corpus_id": 276533262,
            "sentences": [
                {
                    "corpus_id": "276533262",
                    "title": "Decoupled Classifier Knowledge Distillation",
                    "text": "A powerful network is typically built upon a vast model foundation, but this also entails substantial computational costs. With the advancement of technology, such models are progressively transitioning towards being lightweight, with the goal of reducing costs. One potential direction for cost reduction is knowledge distillation (KD). KD refers to the process of aligning or approximating the logic or class predictions of a powerful, large-parameter teacher model on a smaller student model, given the same output [1,2]. The aim is to achieve similar or better performance compared to the large teacher model. Thanks to the robust practical effectiveness of knowledge distillation, it has achieved tremendous success in multiple fields, including object detection [3], semantic segmentation [4], and transformer training [5]. \n\nSince the developments presented in [6], most current research has primarily focused on two directions: distilling knowledge from deep features and methods based on outputs. Feature knowledge distillation has proposed many methods in recent years 7,8, most of which are built on additional supervision of a pre-trained teacher model, especially the intermediate layers [6][7][8][9][10][11][12]. However, even though output-based distillation is less performant compared to feature knowledge distillation, it focuses on a deeper semantic level [13]. Both of these methods have their respective advantages. Feature distillation demonstrates significant performance benefits in various tasks, but due to its complexity, it incurs additional computational costs. On the other hand, output-based distillation requires comparatively fewer computational resources and storage costs, but its performance significantly lags behind that of feature distillation. The differences between the two methods are not just in terms of final performance and computational costs; the core distinction lies in the focus areas of the models they target. Generally, output-based distillation concentrates more on the real labels and the model's outputs, whereas feature distillation emphasizes deep feature extraction. Each method has its own set of advantages that are difficult for the other to match. However, because they focus on different aspects, the knowledge learned by the student model also varies.",
                    "score": 0.5245787588751447,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 122
                        },
                        {
                            "start": 123,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 337
                        },
                        {
                            "start": 338,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 613
                        },
                        {
                            "start": 614,
                            "end": 829
                        },
                        {
                            "start": 832,
                            "end": 1005
                        },
                        {
                            "start": 1006,
                            "end": 1226
                        },
                        {
                            "start": 1227,
                            "end": 1380
                        },
                        {
                            "start": 1381,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1590
                        },
                        {
                            "start": 1591,
                            "end": 1783
                        },
                        {
                            "start": 1784,
                            "end": 1963
                        },
                        {
                            "start": 1964,
                            "end": 2127
                        },
                        {
                            "start": 2128,
                            "end": 2212
                        },
                        {
                            "start": 2213,
                            "end": 2318
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 795,
                            "end": 798,
                            "matchedPaperCorpusId": "73729180"
                        },
                        {
                            "start": 825,
                            "end": 828,
                            "matchedPaperCorpusId": "229363322"
                        },
                        {
                            "start": 1204,
                            "end": 1207,
                            "matchedPaperCorpusId": "118649278"
                        },
                        {
                            "start": 1207,
                            "end": 1210,
                            "matchedPaperCorpusId": "227335337"
                        },
                        {
                            "start": 1213,
                            "end": 1217,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 1217,
                            "end": 1221,
                            "matchedPaperCorpusId": "235613564"
                        },
                        {
                            "start": 1375,
                            "end": 1379,
                            "matchedPaperCorpusId": "247476179"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9267578125
                }
            ],
            "relevance_judgement": 0.9267578125,
            "relevance_judgment_input_expanded": "# Title: Decoupled Classifier Knowledge Distillation\n# Venue: PLoS ONE\n# Authors: Hairui Wang, Mengjie Dong, Guifu Zhu, Ya Li\n## Abstract\nMainstream knowledge distillation methods primarily include self-distillation, offline distillation, online distillation, output-based distillation, and feature-based distillation. While each approach has its respective advantages, they are typically employed independently. Simply combining two distillation methods often leads to redundant information. If the information conveyed by both methods is highly similar, this can result in wasted computational resources and increased complexity. To provide a new perspective on distillation research, we aim to explore a compromise solution that aligns complex features without conflicting with output alignment. In this work, we propose to decouple the classifier\u2019s output into two components: non-target classes learned by the student, and target classes obtained by both the teacher and the student. Finally, we introduce Decoupled Classifier Knowledge Distillation (DCKD), where on one hand, we fix the correct knowledge that the student has already acquired, which is crucial for merging the two methods; on the other hand, we encourage the student to further align its output with that of the teacher. Compared to using a single method, DCKD achieves superior results on both the CIFAR-100 and ImageNet datasets for image classification and object detection tasks, without reducing training efficiency. Moreover, it allows relational-based and feature-based distillation to operate more efficiently and flexibly. This work demonstrates the great potential of integrating distillation methods, and we hope it will inspire future research.\n## Introduction\nA powerful network is typically built upon a vast model foundation, but this also entails substantial computational costs. With the advancement of technology, such models are progressively transitioning towards being lightweight, with the goal of reducing costs. One potential direction for cost reduction is knowledge distillation (KD). KD refers to the process of aligning or approximating the logic or class predictions of a powerful, large-parameter teacher model on a smaller student model, given the same output [1,2]. The aim is to achieve similar or better performance compared to the large teacher model. Thanks to the robust practical effectiveness of knowledge distillation, it has achieved tremendous success in multiple fields, including object detection [3], semantic segmentation [4], and transformer training [5]. \n\nSince the developments presented in [6], most current research has primarily focused on two directions: distilling knowledge from deep features and methods based on outputs. Feature knowledge distillation has proposed many methods in recent years 7,8, most of which are built on additional supervision of a pre-trained teacher model, especially the intermediate layers [6][7][8][9][10][11][12]. However, even though output-based distillation is less performant compared to feature knowledge distillation, it focuses on a deeper semantic level [13]. Both of these methods have their respective advantages. Feature distillation demonstrates significant performance benefits in various tasks, but due to its complexity, it incurs additional computational costs. On the other hand, output-based distillation requires comparatively fewer computational resources and storage costs, but its performance significantly lags behind that of feature distillation. The differences between the two methods are not just in terms of final performance and computational costs; the core distinction lies in the focus areas of the models they target. Generally, output-based distillation concentrates more on the real labels and the model's outputs, whereas feature distillation emphasizes deep feature extraction. Each method has its own set of advantages that are difficult for the other to match. However, because they focus on different aspects, the knowledge learned by the student model also varies.",
            "reference_string": "[276533262 | Wang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "From Efficient Multimodal Models to World Models: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 121,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.00118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276937443",
                    "name": "Xinji Mai"
                },
                {
                    "authorId": "2261831274",
                    "name": "Zeng Tao"
                },
                {
                    "authorId": "2261891655",
                    "name": "Junxiong Lin"
                },
                {
                    "authorId": "2276807843",
                    "name": "Haoran Wang"
                },
                {
                    "authorId": "2276969811",
                    "name": "Yang Chang"
                },
                {
                    "authorId": "2212014366",
                    "name": "Yanlan Kang"
                },
                {
                    "authorId": "2276879376",
                    "name": "Yan Wang"
                },
                {
                    "authorId": "2276819302",
                    "name": "Wenqiang Zhang"
                }
            ],
            "abstract": "Multimodal Large Models (MLMs) are becoming a significant research focus, combining powerful large language models with multimodal learning to perform complex tasks across different data modalities. This review explores the latest developments and challenges in MLMs, emphasizing their potential in achieving artificial general intelligence and as a pathway to world models. We provide an overview of key techniques such as Multimodal Chain of Thought (M-COT), Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning (M-ICL). Additionally, we discuss both the fundamental and specific technologies of multimodal models, highlighting their applications, input/output modalities, and design characteristics. Despite significant advancements, the development of a unified multimodal model remains elusive. We discuss the integration of 3D generation and embodied intelligence to enhance world simulation capabilities and propose incorporating external rule systems for improved reasoning and decision-making. Finally, we outline future research directions to address these challenges and advance the field.",
            "corpus_id": 270870796,
            "sentences": [
                {
                    "corpus_id": "270870796",
                    "title": "From Efficient Multimodal Models to World Models: A Survey",
                    "text": "Knowledge Distillation (KD) is a model compression technique that transfers knowledge from a complex model (called the teacher model) to a smaller model (called the student model).This allows the student model to maintain high computational efficiency while achieving the performance of the teacher model.Knowledge distillation was first proposed by Bucilu\u01ce et al., who trained compressed models with pseudodata classifiers to replicate the original classifier's outputs [14].KD can be divided into homomorphic KD and heteromorphic KD.\n\nHomomorphic KD means the student and teacher models have similar or identical structures.In this approach, the student model learns by mimicking the teacher model's outputs (e.g., logits, feature layer outputs).Common homomorphic KD methods include logit-level distillation, feature-level distillation, and module-level distillation.For instance, TinyViT [46] applies distillation during pre-training, storing logits from a large teacher model on hardware to achieve memory and computational efficiency when transferring knowledge to a smaller student Transformer.DeiT-Tiny [47] adopts patchlevel distillation, training a small student model to match the pre-trained teacher model's patch structure, then optimizing with decomposed manifold matching loss to reduce computational costs.Module-level methods like m2mKD [48] separate the teacher module from a pre-trained unified model, combining student modules with modular models, and using a shared meta-model for composition, enabling student modules to mimic teacher module behavior.Feature-level distillation methods like MiniViT [49] combine weights from consecutive Transformer blocks for cross-layer weight sharing, introducing transformations to enhance learning.\n\nHeteromorphic KD refers to student and teacher models with different structures.In this approach, the student model learns by mimicking the teacher model's outputs or intermediate features, despite different architectures.Heteromorphic KD enhances the student model's adaptability, enabling it to learn useful information from the teacher model.Heteromorphic KD includes soft label distillation, where the student model trains by mimicking the teacher model's soft label outputs.",
                    "score": 0.5827649429981031,
                    "section_title": "C. Knowledge Distillation",
                    "char_start_offset": 25157,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 180,
                            "end": 305
                        },
                        {
                            "start": 305,
                            "end": 476
                        },
                        {
                            "start": 476,
                            "end": 535
                        },
                        {
                            "start": 537,
                            "end": 626
                        },
                        {
                            "start": 626,
                            "end": 748
                        },
                        {
                            "start": 748,
                            "end": 870
                        },
                        {
                            "start": 870,
                            "end": 1101
                        },
                        {
                            "start": 1101,
                            "end": 1322
                        },
                        {
                            "start": 1322,
                            "end": 1573
                        },
                        {
                            "start": 1573,
                            "end": 1758
                        },
                        {
                            "start": 1760,
                            "end": 1840
                        },
                        {
                            "start": 1840,
                            "end": 1982
                        },
                        {
                            "start": 1982,
                            "end": 2105
                        },
                        {
                            "start": 2105,
                            "end": 2239
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 471,
                            "end": 475,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 892,
                            "end": 896,
                            "matchedPaperCorpusId": "250920355"
                        },
                        {
                            "start": 1111,
                            "end": 1115,
                            "matchedPaperCorpusId": "247230104"
                        },
                        {
                            "start": 1621,
                            "end": 1625,
                            "matchedPaperCorpusId": "248177918"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92626953125
                }
            ],
            "relevance_judgement": 0.92626953125,
            "relevance_judgment_input_expanded": "# Title: From Efficient Multimodal Models to World Models: A Survey\n# Venue: arXiv.org\n# Authors: Xinji Mai, Zeng Tao, Junxiong Lin, Haoran Wang, Yang Chang, Yanlan Kang, Yan Wang, Wenqiang Zhang\n## Abstract\nMultimodal Large Models (MLMs) are becoming a significant research focus, combining powerful large language models with multimodal learning to perform complex tasks across different data modalities. This review explores the latest developments and challenges in MLMs, emphasizing their potential in achieving artificial general intelligence and as a pathway to world models. We provide an overview of key techniques such as Multimodal Chain of Thought (M-COT), Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning (M-ICL). Additionally, we discuss both the fundamental and specific technologies of multimodal models, highlighting their applications, input/output modalities, and design characteristics. Despite significant advancements, the development of a unified multimodal model remains elusive. We discuss the integration of 3D generation and embodied intelligence to enhance world simulation capabilities and propose incorporating external rule systems for improved reasoning and decision-making. Finally, we outline future research directions to address these challenges and advance the field.\n## C. Knowledge Distillation\nKnowledge Distillation (KD) is a model compression technique that transfers knowledge from a complex model (called the teacher model) to a smaller model (called the student model).This allows the student model to maintain high computational efficiency while achieving the performance of the teacher model.Knowledge distillation was first proposed by Bucilu\u01ce et al., who trained compressed models with pseudodata classifiers to replicate the original classifier's outputs [14].KD can be divided into homomorphic KD and heteromorphic KD.\n\nHomomorphic KD means the student and teacher models have similar or identical structures.In this approach, the student model learns by mimicking the teacher model's outputs (e.g., logits, feature layer outputs).Common homomorphic KD methods include logit-level distillation, feature-level distillation, and module-level distillation.For instance, TinyViT [46] applies distillation during pre-training, storing logits from a large teacher model on hardware to achieve memory and computational efficiency when transferring knowledge to a smaller student Transformer.DeiT-Tiny [47] adopts patchlevel distillation, training a small student model to match the pre-trained teacher model's patch structure, then optimizing with decomposed manifold matching loss to reduce computational costs.Module-level methods like m2mKD [48] separate the teacher module from a pre-trained unified model, combining student modules with modular models, and using a shared meta-model for composition, enabling student modules to mimic teacher module behavior.Feature-level distillation methods like MiniViT [49] combine weights from consecutive Transformer blocks for cross-layer weight sharing, introducing transformations to enhance learning.\n\nHeteromorphic KD refers to student and teacher models with different structures.In this approach, the student model learns by mimicking the teacher model's outputs or intermediate features, despite different architectures.Heteromorphic KD enhances the student model's adaptability, enabling it to learn useful information from the teacher model.Heteromorphic KD includes soft label distillation, where the student model trains by mimicking the teacher model's soft label outputs.",
            "reference_string": "[270870796 | Mai et al. | 2024 | Citations: 5]"
        },
        {
            "title": "RSMoDM: Multimodal Momentum Distillation Model for Remote Sensing Visual Question Answering",
            "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
            "year": 2024,
            "reference_count": 68,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/jstars.2024.3419035",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSTARS.2024.3419035?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSTARS.2024.3419035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2278383802",
                    "name": "Pengfei Li"
                },
                {
                    "authorId": "2192649954",
                    "name": "Gang Liu"
                },
                {
                    "authorId": "2221231272",
                    "name": "Jinlong He"
                },
                {
                    "authorId": "2299725033",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2053866414",
                    "name": "Shenjun Zhong"
                },
                {
                    "authorId": "2308411177",
                    "name": "Xun Chen"
                }
            ],
            "abstract": "Remote sensing (RS) visual question answering (VQA) is a task that answers questions about a given RS image by utilizing both image and textual information. However, existing methods in RS VQA overlook the fact that the ground truths in RS VQA benchmark datasets, which are algorithmically generated rather than manually annotated, may not always represent the most reasonable answers to the questions. In this article, we propose a multimodal momentum distillation model (RSMoDM) for RS VQA tasks. Specifically, we maintain the momentum distillation model during the training stage that generates stable and reliable pseudolabels for additional supervision, effectively preventing the model from being penalized for producing other reasonable outputs that differ from ground truth. Additionally, to address domain shift in RS, we employ the Vision Transformer (ViT) trained on a large-scale RS dataset for enhanced image feature extraction. Moreover, we introduce the multimodal fusion module with cross-attention for improved cross-modal representation learning. Our extensive experiments across three different RS VQA datasets demonstrate that RSMoDM achieves state-of-the-art performance, particularly excelling in scenarios with limited training data. The strong interpretability of our method is further evidenced by visualized attention maps.",
            "corpus_id": 270755154,
            "sentences": [
                {
                    "corpus_id": "270755154",
                    "title": "RSMoDM: Multimodal Momentum Distillation Model for Remote Sensing Visual Question Answering",
                    "text": "Knowledge distillation aims to extract knowledge from complex, large-scale teacher models and transfer it to smaller student models. This process typically involves minimizing the discrepancy between the output distributions of the teacher and student models to enhance the performance of the student model. According to existing research [54], knowledge can be categorized into different types, including response-based knowledge [55], feature-based knowledge [56], and relation-based knowledge [57]. These types target different aspects of neural network architecture. For instance, response-based knowledge focuses on the output of the final layer, while feature-based knowledge emphasizes the feature representation of intermediate layers to capture higher-order correlations within the network structure [58]. \n\nThe learning schemes for knowledge distillation can be divided into three main categories based on whether the teacher and student models are updated synchronously: offline distillation, online distillation, and self-distillation [54]. Offline distillation is a traditional method where a pretrained teacher model transfers its knowledge to the student model in the form of logits or intermediate features [59]. Online distillation is an end-to-end training framework where both teacher and student models are updated simultaneously, particularly suitable when largecapacity, high-performance teacher models are unavailable [60]. Self-distillation is a special form of online distillation, involving the transfer of knowledge within the same network from deeper to shallower layers or from early to later training stages, thus achieving knowledge transfer and training optimization [61]. \n\nIn this article, we propose a momentum distillation method that can be considered a form of online self-distillation. Inspired by the ALBEF [24], we maintain a momentum version of the model, which generates more stable and reliable pseudolabels for additional supervision. This approach aims to prevent the model from being penalized for producing reasonable outputs that differ from the ground truth. Through momentum distillation, we effectively achieve model generalization across a broader data distribution while reducing reliance on noisy or imprecise labels.",
                    "score": 0.5468729304039444,
                    "section_title": "C. Knowledge Distillation",
                    "char_start_offset": 13127,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 132
                        },
                        {
                            "start": 133,
                            "end": 307
                        },
                        {
                            "start": 308,
                            "end": 501
                        },
                        {
                            "start": 502,
                            "end": 570
                        },
                        {
                            "start": 571,
                            "end": 814
                        },
                        {
                            "start": 817,
                            "end": 1052
                        },
                        {
                            "start": 1053,
                            "end": 1228
                        },
                        {
                            "start": 1229,
                            "end": 1446
                        },
                        {
                            "start": 1447,
                            "end": 1704
                        },
                        {
                            "start": 1707,
                            "end": 1824
                        },
                        {
                            "start": 1825,
                            "end": 1979
                        },
                        {
                            "start": 1980,
                            "end": 2108
                        },
                        {
                            "start": 2109,
                            "end": 2272
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 339,
                            "end": 343,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 431,
                            "end": 435,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 461,
                            "end": 465,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 496,
                            "end": 500,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 809,
                            "end": 813,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 1047,
                            "end": 1051,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 1223,
                            "end": 1227,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1441,
                            "end": 1445,
                            "matchedPaperCorpusId": "208526905"
                        },
                        {
                            "start": 1699,
                            "end": 1703,
                            "matchedPaperCorpusId": "159041406"
                        },
                        {
                            "start": 1847,
                            "end": 1851,
                            "matchedPaperCorpusId": "236034189"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92578125
                }
            ],
            "relevance_judgement": 0.92578125,
            "relevance_judgment_input_expanded": "# Title: RSMoDM: Multimodal Momentum Distillation Model for Remote Sensing Visual Question Answering\n# Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing\n# Authors: Pengfei Li, Gang Liu, Jinlong He, Xiangxu Meng, Shenjun Zhong, Xun Chen\n## Abstract\nRemote sensing (RS) visual question answering (VQA) is a task that answers questions about a given RS image by utilizing both image and textual information. However, existing methods in RS VQA overlook the fact that the ground truths in RS VQA benchmark datasets, which are algorithmically generated rather than manually annotated, may not always represent the most reasonable answers to the questions. In this article, we propose a multimodal momentum distillation model (RSMoDM) for RS VQA tasks. Specifically, we maintain the momentum distillation model during the training stage that generates stable and reliable pseudolabels for additional supervision, effectively preventing the model from being penalized for producing other reasonable outputs that differ from ground truth. Additionally, to address domain shift in RS, we employ the Vision Transformer (ViT) trained on a large-scale RS dataset for enhanced image feature extraction. Moreover, we introduce the multimodal fusion module with cross-attention for improved cross-modal representation learning. Our extensive experiments across three different RS VQA datasets demonstrate that RSMoDM achieves state-of-the-art performance, particularly excelling in scenarios with limited training data. The strong interpretability of our method is further evidenced by visualized attention maps.\n## C. Knowledge Distillation\nKnowledge distillation aims to extract knowledge from complex, large-scale teacher models and transfer it to smaller student models. This process typically involves minimizing the discrepancy between the output distributions of the teacher and student models to enhance the performance of the student model. According to existing research [54], knowledge can be categorized into different types, including response-based knowledge [55], feature-based knowledge [56], and relation-based knowledge [57]. These types target different aspects of neural network architecture. For instance, response-based knowledge focuses on the output of the final layer, while feature-based knowledge emphasizes the feature representation of intermediate layers to capture higher-order correlations within the network structure [58]. \n\nThe learning schemes for knowledge distillation can be divided into three main categories based on whether the teacher and student models are updated synchronously: offline distillation, online distillation, and self-distillation [54]. Offline distillation is a traditional method where a pretrained teacher model transfers its knowledge to the student model in the form of logits or intermediate features [59]. Online distillation is an end-to-end training framework where both teacher and student models are updated simultaneously, particularly suitable when largecapacity, high-performance teacher models are unavailable [60]. Self-distillation is a special form of online distillation, involving the transfer of knowledge within the same network from deeper to shallower layers or from early to later training stages, thus achieving knowledge transfer and training optimization [61]. \n\nIn this article, we propose a momentum distillation method that can be considered a form of online self-distillation. Inspired by the ALBEF [24], we maintain a momentum version of the model, which generates more stable and reliable pseudolabels for additional supervision. This approach aims to prevent the model from being penalized for producing reasonable outputs that differ from the ground truth. Through momentum distillation, we effectively achieve model generalization across a broader data distribution while reducing reliance on noisy or imprecise labels.",
            "reference_string": "[270755154 | Li et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Improve Cross-Architecture Generalization on Dataset Distillation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13007, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284824964",
                    "name": "Binglin Zhou"
                },
                {
                    "authorId": "2284822738",
                    "name": "Linhao Zhong"
                },
                {
                    "authorId": "2284825420",
                    "name": "Wentao Chen"
                }
            ],
            "abstract": "Dataset distillation, a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset. However, existing distillation methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models. In response to this constraint, we propose a novel methodology termed\"model pool\". This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data distillation process. Additionally, we integrate our model pool with the established knowledge distillation approach and apply knowledge distillation to the test process of the distilled dataset. Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies.",
            "corpus_id": 267759723,
            "sentences": [
                {
                    "corpus_id": "267759723",
                    "title": "Improve Cross-Architecture Generalization on Dataset Distillation",
                    "text": "Knowledge distillation is a method to transfer knowledge from a large model to a small model. It's first proposed in this paper [12]. The large model is called the teacher model and the small model is called the student model. The student model is trained to mimic the output of the teacher model. The student model is usually a shallow neural network with fewer parameters and faster inference speed. The teacher model is usually a deep neural network with more parameters and slower inference speed. The student model is trained on the same dataset as the teacher model to minimize the difference between the output of the student model and the output of the teacher model (i.e. maintain the knowledge as much as possible). \n\nAfter knowledge distillation was proposed, a lot of work has been put into exploring further applications in diverse fields, such as speech recognition, image recognition, and natural language processing. It proved that knowledge distillation is a great success in these various fields [11]. \n\nHowever, there are some arguments against knowledge distillation. A paper [22] said that, while knowledge distillation improves the generalization of student models, there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student can perfectly match the teacher.",
                    "score": 0.7084802636794836,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 6172,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 94,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 297
                        },
                        {
                            "start": 298,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 501
                        },
                        {
                            "start": 502,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 725
                        },
                        {
                            "start": 728,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1019
                        },
                        {
                            "start": 1022,
                            "end": 1087
                        },
                        {
                            "start": 1088,
                            "end": 1373
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1014,
                            "end": 1018,
                            "matchedPaperCorpusId": "219559263"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92529296875
                }
            ],
            "relevance_judgement": 0.92529296875,
            "relevance_judgment_input_expanded": "# Title: Improve Cross-Architecture Generalization on Dataset Distillation\n# Venue: arXiv.org\n# Authors: Binglin Zhou, Linhao Zhong, Wentao Chen\n## Abstract\nDataset distillation, a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset. However, existing distillation methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models. In response to this constraint, we propose a novel methodology termed\"model pool\". This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data distillation process. Additionally, we integrate our model pool with the established knowledge distillation approach and apply knowledge distillation to the test process of the distilled dataset. Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies.\n## Knowledge Distillation\nKnowledge distillation is a method to transfer knowledge from a large model to a small model. It's first proposed in this paper [12]. The large model is called the teacher model and the small model is called the student model. The student model is trained to mimic the output of the teacher model. The student model is usually a shallow neural network with fewer parameters and faster inference speed. The teacher model is usually a deep neural network with more parameters and slower inference speed. The student model is trained on the same dataset as the teacher model to minimize the difference between the output of the student model and the output of the teacher model (i.e. maintain the knowledge as much as possible). \n\nAfter knowledge distillation was proposed, a lot of work has been put into exploring further applications in diverse fields, such as speech recognition, image recognition, and natural language processing. It proved that knowledge distillation is a great success in these various fields [11]. \n\nHowever, there are some arguments against knowledge distillation. A paper [22] said that, while knowledge distillation improves the generalization of student models, there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student can perfectly match the teacher.",
            "reference_string": "[267759723 | Zhou et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Distributed Collaborative Learning with Representative Knowledge Sharing",
            "venue": "Mathematics",
            "year": 2025,
            "reference_count": 31,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3390/math13061004",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/math13061004?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/math13061004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2342413409",
                    "name": "Joseph Casey"
                },
                {
                    "authorId": "2351733702",
                    "name": "Qianjiao Chen"
                },
                {
                    "authorId": "2300177011",
                    "name": "Mengchen Fan"
                },
                {
                    "authorId": "2285910606",
                    "name": "Baocheng Geng"
                },
                {
                    "authorId": "2342412176",
                    "name": "Roman Shterenberg"
                },
                {
                    "authorId": "2344089016",
                    "name": "Zhong Chen"
                },
                {
                    "authorId": "2340963079",
                    "name": "Keren Li"
                }
            ],
            "abstract": "Distributed Collaborative Learning (DCL) addresses critical challenges in privacy-aware machine learning by enabling indirect knowledge transfer across nodes with heterogeneous feature distributions. Unlike conventional federated learning approaches, DCL assumes non-IID data and prediction task distributions that span beyond local training data, requiring selective collaboration to achieve generalization. In this work, we propose a novel collaborative transfer learning (CTL) framework that utilizes representative datasets and adaptive distillation weights to facilitate efficient and privacy-preserving collaboration. By leveraging Energy Coefficients to quantify node similarity, CTL dynamically selects optimal collaborators and refines local models through knowledge distillation on shared representative datasets. Simulations demonstrate the efficacy of CTL in improving prediction accuracy across diverse tasks while balancing trade-offs between local and global performance. Furthermore, we explore the impact of data spread and dispersion on collaboration, highlighting the importance of tailored node alignment. This framework provides a scalable foundation for cross-domain generalization in distributed machine learning.",
            "corpus_id": 277217930,
            "sentences": [
                {
                    "corpus_id": "277217930",
                    "title": "Distributed Collaborative Learning with Representative Knowledge Sharing",
                    "text": "Knowledge distillation, a form of transfer learning, facilitates knowledge sharing by compressing complex models into smaller, efficient ones. This technique has proven particularly valuable in large-scale distributed training, as demonstrated by DeepSeek, which integrates knowledge distillation to optimize model convergence and maintain high performance while reducing computational overhead [1]. In a collaborative learning setting, distillation enables nodes to transfer knowledge without sharing raw data or full model parameters, making it a strong alternative to traditional model aggregation in federated learning. \n\nRecent advances have demonstrated the importance of adaptive weighting in knowledge transfer, where confidence-based or similarity-based methods improve the effectiveness of model distillation [14,15]. This aligns with importance sampling techniques in transfer learning and domain adaptation, where distillation weights reflect the reliability of each teacher node [16,17]. Prior works [2,18] emphasize that knowledge transfer effectiveness depends on teacher-student alignment, motivating our approach of using Energy Coefficients for adaptive distillation weighting. \n\nIn federated learning, dynamic model importance weighting has been explored in heterogeneous FL settings [19,20], where similarity-based aggregation improves personalization. \n\nA notable related work is FedED (Federated Ensemble Distillation) [21], which transfers knowledge via ensembles of local models instead of a single teacher-student relationship. While FedED aggregates predictions from multiple clients, our method personalizes distillation weights using Energy Coefficients, ensuring knowledge transfer is selectively tailored to each node's task distribution. This distinction is particularly critical in heterogeneous settings where generalization beyond local training data is essential. \n\nHinton et al. [2] first introduced this concept, demonstrating how teacher-student frameworks could retain high performance. Subsequent works refined this approach: Passalis and Tefas [22] matched probability distributions of teacher and student models in feature space, Huang and Wang [23] introduced loss functions minimizing distributional discrepancies, and Tian et al. [18] incorporated contrastive learning to align teacher-student representations effectively. \n\nIn distributed settings, the use of representative datasets provides a practical alternative to raw data or parameter sharing. Li and Yang [24] constructed representative data points for large datasets to enable efficient training and alignment.",
                    "score": 0.5451661738884008,
                    "section_title": "Knowledge Distillation and Representative Datasets",
                    "char_start_offset": 6845,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 399
                        },
                        {
                            "start": 400,
                            "end": 623
                        },
                        {
                            "start": 626,
                            "end": 827
                        },
                        {
                            "start": 828,
                            "end": 1000
                        },
                        {
                            "start": 1001,
                            "end": 1195
                        },
                        {
                            "start": 1198,
                            "end": 1372
                        },
                        {
                            "start": 1375,
                            "end": 1552
                        },
                        {
                            "start": 1553,
                            "end": 1768
                        },
                        {
                            "start": 1769,
                            "end": 1898
                        },
                        {
                            "start": 1901,
                            "end": 2025
                        },
                        {
                            "start": 2026,
                            "end": 2367
                        },
                        {
                            "start": 2370,
                            "end": 2496
                        },
                        {
                            "start": 2497,
                            "end": 2615
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 823,
                            "end": 826,
                            "matchedPaperCorpusId": "203642130"
                        },
                        {
                            "start": 996,
                            "end": 999,
                            "matchedPaperCorpusId": "740063"
                        },
                        {
                            "start": 1303,
                            "end": 1307,
                            "matchedPaperCorpusId": "235446706"
                        },
                        {
                            "start": 1441,
                            "end": 1445,
                            "matchedPaperCorpusId": "226262346"
                        },
                        {
                            "start": 2085,
                            "end": 2089,
                            "matchedPaperCorpusId": "52012952"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9248046875
                }
            ],
            "relevance_judgement": 0.9248046875,
            "relevance_judgment_input_expanded": "# Title: Distributed Collaborative Learning with Representative Knowledge Sharing\n# Venue: Mathematics\n# Authors: Joseph Casey, Qianjiao Chen, Mengchen Fan, Baocheng Geng, Roman Shterenberg, Zhong Chen, Keren Li\n## Abstract\nDistributed Collaborative Learning (DCL) addresses critical challenges in privacy-aware machine learning by enabling indirect knowledge transfer across nodes with heterogeneous feature distributions. Unlike conventional federated learning approaches, DCL assumes non-IID data and prediction task distributions that span beyond local training data, requiring selective collaboration to achieve generalization. In this work, we propose a novel collaborative transfer learning (CTL) framework that utilizes representative datasets and adaptive distillation weights to facilitate efficient and privacy-preserving collaboration. By leveraging Energy Coefficients to quantify node similarity, CTL dynamically selects optimal collaborators and refines local models through knowledge distillation on shared representative datasets. Simulations demonstrate the efficacy of CTL in improving prediction accuracy across diverse tasks while balancing trade-offs between local and global performance. Furthermore, we explore the impact of data spread and dispersion on collaboration, highlighting the importance of tailored node alignment. This framework provides a scalable foundation for cross-domain generalization in distributed machine learning.\n## Knowledge Distillation and Representative Datasets\nKnowledge distillation, a form of transfer learning, facilitates knowledge sharing by compressing complex models into smaller, efficient ones. This technique has proven particularly valuable in large-scale distributed training, as demonstrated by DeepSeek, which integrates knowledge distillation to optimize model convergence and maintain high performance while reducing computational overhead [1]. In a collaborative learning setting, distillation enables nodes to transfer knowledge without sharing raw data or full model parameters, making it a strong alternative to traditional model aggregation in federated learning. \n\nRecent advances have demonstrated the importance of adaptive weighting in knowledge transfer, where confidence-based or similarity-based methods improve the effectiveness of model distillation [14,15]. This aligns with importance sampling techniques in transfer learning and domain adaptation, where distillation weights reflect the reliability of each teacher node [16,17]. Prior works [2,18] emphasize that knowledge transfer effectiveness depends on teacher-student alignment, motivating our approach of using Energy Coefficients for adaptive distillation weighting. \n\nIn federated learning, dynamic model importance weighting has been explored in heterogeneous FL settings [19,20], where similarity-based aggregation improves personalization. \n\nA notable related work is FedED (Federated Ensemble Distillation) [21], which transfers knowledge via ensembles of local models instead of a single teacher-student relationship. While FedED aggregates predictions from multiple clients, our method personalizes distillation weights using Energy Coefficients, ensuring knowledge transfer is selectively tailored to each node's task distribution. This distinction is particularly critical in heterogeneous settings where generalization beyond local training data is essential. \n\nHinton et al. [2] first introduced this concept, demonstrating how teacher-student frameworks could retain high performance. Subsequent works refined this approach: Passalis and Tefas [22] matched probability distributions of teacher and student models in feature space, Huang and Wang [23] introduced loss functions minimizing distributional discrepancies, and Tian et al. [18] incorporated contrastive learning to align teacher-student representations effectively. \n\nIn distributed settings, the use of representative datasets provides a practical alternative to raw data or parameter sharing. Li and Yang [24] constructed representative data points for large datasets to enable efficient training and alignment.",
            "reference_string": "[277217930 | Casey et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Classroom-Inspired Multi-Mentor Distillation with Adaptive Learning Strategies",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.20237, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2299151992",
                    "name": "Shalini Sarode"
                },
                {
                    "authorId": "2115775339",
                    "name": "M. Khan"
                },
                {
                    "authorId": "2029681143",
                    "name": "Tahira Shehzadi"
                },
                {
                    "authorId": "2258977563",
                    "name": "Didier Stricker"
                },
                {
                    "authorId": "145181206",
                    "name": "Muhammad Zeshan Afzal"
                }
            ],
            "abstract": "We propose ClassroomKD, a novel multi-mentor knowledge distillation framework inspired by classroom environments to enhance knowledge transfer between the student and multiple mentors with different knowledge levels. Unlike traditional methods that rely on fixed mentor-student relationships, our framework dynamically selects and adapts the teaching strategies of diverse mentors based on their effectiveness for each data sample. ClassroomKD comprises two main modules: the Knowledge Filtering (KF) module and the Mentoring module. The KF Module dynamically ranks mentors based on their performance for each input, activating only high-quality mentors to minimize error accumulation and prevent information loss. The Mentoring Module adjusts the distillation strategy by tuning each mentor's influence according to the dynamic performance gap between the student and mentors, effectively modulating the learning pace. Extensive experiments on image classification (CIFAR-100 and ImageNet) and 2D human pose estimation (COCO Keypoints and MPII Human Pose) demonstrate that ClassroomKD outperforms existing knowledge distillation methods for different network architectures. Our results highlight that a dynamic and adaptive approach to mentor selection and guidance leads to more effective knowledge transfer, paving the way for enhanced model performance through distillation.",
            "corpus_id": 272987178,
            "sentences": [
                {
                    "corpus_id": "272987178",
                    "title": "Classroom-Inspired Multi-Mentor Distillation with Adaptive Learning Strategies",
                    "text": "Knowledge distillation (KD) [1] is a widely adopted technique for compressing deep neural networks, where a smaller student model learns from a larger teacher model by minimizing the distance between their output probability distributions, or soft labels. Traditional KD methods primarily focus on logit-based distillation, where the student learns directly from the teacher's output logits. Notable methods include PKT [8], which employs probabilistic knowledge transfer, FT [9], which transfers factorized feature representations, and AB [10], which leverages activation boundaries formed by hidden neurons. \n\nFeature-based distillation methods transfer knowledge by aligning intermediate representations between the teacher and student. FitNets [3] introduced this approach using intermediate feature maps for training. Later methods like AT [2], VID [11], and CRD [12] enhance knowledge transfer by matching attention maps, utilizing variational information distillation, and employing contrastive learning, respectively. \n\nRelation-based methods focus on preserving the structural relationships within the teacher's feature maps. RKD [13] maintains data point structures through relational knowledge distillation, while SP [14] and SRRL [15] optimize for similarity-preserving objectives. DIST [16] addresses large capacity gaps by applying a correlation-based loss to maintain both inter-class and intra-class relationships, enhancing distillation efficiency. \n\nRecent approaches have explored more specialized distillation techniques. WSLD [17] introduces weighted soft labels to balance bias-variance trade-offs, while One-to-All Spatial Matching KD [18] focuses on spatial matching techniques. OFA [7] optimizes feature-based KD by projecting features onto the logit space, significantly improving performance for heterogeneous models. To enhance distillation effectiveness, several methods have incorporated adaptive strategies. CTKD [19] dynamically learns the temperature during training to gradually increase learning difficulty, and DTKD [20] employs real-time temperature scaling to improve knowledge transfer efficiency.",
                    "score": 0.5602742233124574,
                    "section_title": "Knowledge Distillation Approaches",
                    "char_start_offset": 4393,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 255
                        },
                        {
                            "start": 256,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 609
                        },
                        {
                            "start": 612,
                            "end": 739
                        },
                        {
                            "start": 740,
                            "end": 822
                        },
                        {
                            "start": 823,
                            "end": 1025
                        },
                        {
                            "start": 1028,
                            "end": 1134
                        },
                        {
                            "start": 1135,
                            "end": 1293
                        },
                        {
                            "start": 1294,
                            "end": 1465
                        },
                        {
                            "start": 1468,
                            "end": 1541
                        },
                        {
                            "start": 1542,
                            "end": 1702
                        },
                        {
                            "start": 1703,
                            "end": 1844
                        },
                        {
                            "start": 1845,
                            "end": 1938
                        },
                        {
                            "start": 1939,
                            "end": 2136
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 28,
                            "end": 31,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 420,
                            "end": 423,
                            "matchedPaperCorpusId": "52012952"
                        },
                        {
                            "start": 476,
                            "end": 479,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 540,
                            "end": 544,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 748,
                            "end": 751,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 854,
                            "end": 858,
                            "matchedPaperCorpusId": "118649278"
                        },
                        {
                            "start": 868,
                            "end": 872,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 1139,
                            "end": 1143,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 1228,
                            "end": 1232,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 1242,
                            "end": 1246,
                            "matchedPaperCorpusId": "235613564"
                        },
                        {
                            "start": 1299,
                            "end": 1303,
                            "matchedPaperCorpusId": "248986690"
                        },
                        {
                            "start": 1547,
                            "end": 1551,
                            "matchedPaperCorpusId": "231740588"
                        },
                        {
                            "start": 1658,
                            "end": 1662,
                            "matchedPaperCorpusId": "248986384"
                        },
                        {
                            "start": 1707,
                            "end": 1710,
                            "matchedPaperCorpusId": "265871679"
                        },
                        {
                            "start": 1944,
                            "end": 1948,
                            "matchedPaperCorpusId": "254069919"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                }
            ],
            "relevance_judgement": 0.92431640625,
            "relevance_judgment_input_expanded": "# Title: Classroom-Inspired Multi-Mentor Distillation with Adaptive Learning Strategies\n# Venue: arXiv.org\n# Authors: Shalini Sarode, M. Khan, Tahira Shehzadi, Didier Stricker, Muhammad Zeshan Afzal\n## Abstract\nWe propose ClassroomKD, a novel multi-mentor knowledge distillation framework inspired by classroom environments to enhance knowledge transfer between the student and multiple mentors with different knowledge levels. Unlike traditional methods that rely on fixed mentor-student relationships, our framework dynamically selects and adapts the teaching strategies of diverse mentors based on their effectiveness for each data sample. ClassroomKD comprises two main modules: the Knowledge Filtering (KF) module and the Mentoring module. The KF Module dynamically ranks mentors based on their performance for each input, activating only high-quality mentors to minimize error accumulation and prevent information loss. The Mentoring Module adjusts the distillation strategy by tuning each mentor's influence according to the dynamic performance gap between the student and mentors, effectively modulating the learning pace. Extensive experiments on image classification (CIFAR-100 and ImageNet) and 2D human pose estimation (COCO Keypoints and MPII Human Pose) demonstrate that ClassroomKD outperforms existing knowledge distillation methods for different network architectures. Our results highlight that a dynamic and adaptive approach to mentor selection and guidance leads to more effective knowledge transfer, paving the way for enhanced model performance through distillation.\n## Knowledge Distillation Approaches\nKnowledge distillation (KD) [1] is a widely adopted technique for compressing deep neural networks, where a smaller student model learns from a larger teacher model by minimizing the distance between their output probability distributions, or soft labels. Traditional KD methods primarily focus on logit-based distillation, where the student learns directly from the teacher's output logits. Notable methods include PKT [8], which employs probabilistic knowledge transfer, FT [9], which transfers factorized feature representations, and AB [10], which leverages activation boundaries formed by hidden neurons. \n\nFeature-based distillation methods transfer knowledge by aligning intermediate representations between the teacher and student. FitNets [3] introduced this approach using intermediate feature maps for training. Later methods like AT [2], VID [11], and CRD [12] enhance knowledge transfer by matching attention maps, utilizing variational information distillation, and employing contrastive learning, respectively. \n\nRelation-based methods focus on preserving the structural relationships within the teacher's feature maps. RKD [13] maintains data point structures through relational knowledge distillation, while SP [14] and SRRL [15] optimize for similarity-preserving objectives. DIST [16] addresses large capacity gaps by applying a correlation-based loss to maintain both inter-class and intra-class relationships, enhancing distillation efficiency. \n\nRecent approaches have explored more specialized distillation techniques. WSLD [17] introduces weighted soft labels to balance bias-variance trade-offs, while One-to-All Spatial Matching KD [18] focuses on spatial matching techniques. OFA [7] optimizes feature-based KD by projecting features onto the logit space, significantly improving performance for heterogeneous models. To enhance distillation effectiveness, several methods have incorporated adaptive strategies. CTKD [19] dynamically learns the temperature during training to gradually increase learning difficulty, and DTKD [20] employs real-time temperature scaling to improve knowledge transfer efficiency.",
            "reference_string": "[272987178 | Sarode et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Review of Recent Distillation Studies",
            "venue": "MATEC Web of Conferences",
            "year": 2023,
            "reference_count": 25,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.matec-conferences.org/articles/matecconf/pdf/2023/09/matecconf_amme2023_01034.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1051/matecconf/202338201034?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1051/matecconf/202338201034, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2147417425",
                    "name": "Min Gao"
                }
            ],
            "abstract": "Knowledge distillation has gained a lot of interest in recent years because it allows for compressing a large deep neural network (teacher DNN) into a smaller DNN (student DNN), while maintaining its accuracy. Recent improvements have been made to knowledge distillation. One such improvement is the teaching assistant distillation method. This method involves introducing an intermediate \"teaching assistant\" model between the teacher and student. The teaching assistant is first trained to mimic the teacher, and then the student is trained to mimic the teaching assistant. This multi-step process can improve student performance. Another improvement to knowledge distillation is curriculum distillation. This method involves gradually training the student by exposing it to increasingly difficult concepts over time, similar to curriculum learning in humans. This process can help the student learn in a more stable and consistent manner. Finally, there is the mask distillation method. Here, the student is trained to specifically mimic the attention mechanisms learned by the teacher, not just the overall output of the teacher DNN. These improvements help to enhance the knowledge distillation process and enable the creation of more efficient DNNs.",
            "corpus_id": 259378480,
            "sentences": [
                {
                    "corpus_id": "259378480",
                    "title": "Review of Recent Distillation Studies",
                    "text": "Knowledge distillation [7] is a method of transferring the knowledge from a complex model, called the teacher, to a smaller and simpler model, called the student. In recent years, several variants of knowledge distillation have been proposed, including teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation. This literature review summarizes the recent developments in these variants of knowledge distillation and discusses their strengths and limitations. Knowledge distillation is a method of compressing a complex deep neural network (DNN) into a smaller and faster DNN while preserving its accuracy. The process of knowledge distillation involves training a smaller DNN, called the student, to imitate the predictions of a larger and more complex DNN, called the teacher. The student network is trained to produce similar results as the teacher network, but with fewer parameters and lower computational cost. Knowledge distillation has been widely used for model compression and acceleration, and has shown great promise in various applications [5,8,13,18], such as computer vision and natural language processing. In recent years, several variants of knowledge distillation [3,4,21] have been proposed and explored to improve the performance of knowledge distillation [6,9]. One of these variants is teaching assistant distillation, which introduces an intermediate model, called the teaching assistant, between the teacher and the student. \n\nThe teaching assistant is trained to mimic the behavior of the teacher, and the student is trained to imitate the outputs of the teaching assistant. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better capture the knowledge learned by the teacher. Another variant of knowledge distillation [10,11] is curriculum distillation, which designs the learning process to follow a curriculum, similar to human education. The curriculum is designed to present easy examples first and gradually increase the difficulty of the examples as the student improves. This approach has been shown to provide better performance than traditional knowledge distillation, especially for tasks that require a lot of prior knowledge. Mask distillation is a variant of knowledge distillation [12,20] that focuses on transferring the attention mechanism learned by the teacher to the student.",
                    "score": 0.5539827561954486,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 358
                        },
                        {
                            "start": 359,
                            "end": 507
                        },
                        {
                            "start": 508,
                            "end": 654
                        },
                        {
                            "start": 655,
                            "end": 826
                        },
                        {
                            "start": 827,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1170
                        },
                        {
                            "start": 1171,
                            "end": 1331
                        },
                        {
                            "start": 1332,
                            "end": 1497
                        },
                        {
                            "start": 1500,
                            "end": 1648
                        },
                        {
                            "start": 1649,
                            "end": 1811
                        },
                        {
                            "start": 1812,
                            "end": 1976
                        },
                        {
                            "start": 1977,
                            "end": 2113
                        },
                        {
                            "start": 2114,
                            "end": 2273
                        },
                        {
                            "start": 2274,
                            "end": 2430
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1104,
                            "end": 1106,
                            "matchedPaperCorpusId": "235677396"
                        },
                        {
                            "start": 1328,
                            "end": 1330,
                            "matchedPaperCorpusId": "253270253"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                }
            ],
            "relevance_judgement": 0.92431640625,
            "relevance_judgment_input_expanded": "# Title: Review of Recent Distillation Studies\n# Venue: MATEC Web of Conferences\n# Authors: Min Gao\n## Abstract\nKnowledge distillation has gained a lot of interest in recent years because it allows for compressing a large deep neural network (teacher DNN) into a smaller DNN (student DNN), while maintaining its accuracy. Recent improvements have been made to knowledge distillation. One such improvement is the teaching assistant distillation method. This method involves introducing an intermediate \"teaching assistant\" model between the teacher and student. The teaching assistant is first trained to mimic the teacher, and then the student is trained to mimic the teaching assistant. This multi-step process can improve student performance. Another improvement to knowledge distillation is curriculum distillation. This method involves gradually training the student by exposing it to increasingly difficult concepts over time, similar to curriculum learning in humans. This process can help the student learn in a more stable and consistent manner. Finally, there is the mask distillation method. Here, the student is trained to specifically mimic the attention mechanisms learned by the teacher, not just the overall output of the teacher DNN. These improvements help to enhance the knowledge distillation process and enable the creation of more efficient DNNs.\n## Introduction\nKnowledge distillation [7] is a method of transferring the knowledge from a complex model, called the teacher, to a smaller and simpler model, called the student. In recent years, several variants of knowledge distillation have been proposed, including teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation. This literature review summarizes the recent developments in these variants of knowledge distillation and discusses their strengths and limitations. Knowledge distillation is a method of compressing a complex deep neural network (DNN) into a smaller and faster DNN while preserving its accuracy. The process of knowledge distillation involves training a smaller DNN, called the student, to imitate the predictions of a larger and more complex DNN, called the teacher. The student network is trained to produce similar results as the teacher network, but with fewer parameters and lower computational cost. Knowledge distillation has been widely used for model compression and acceleration, and has shown great promise in various applications [5,8,13,18], such as computer vision and natural language processing. In recent years, several variants of knowledge distillation [3,4,21] have been proposed and explored to improve the performance of knowledge distillation [6,9]. One of these variants is teaching assistant distillation, which introduces an intermediate model, called the teaching assistant, between the teacher and the student. \n\nThe teaching assistant is trained to mimic the behavior of the teacher, and the student is trained to imitate the outputs of the teaching assistant. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better capture the knowledge learned by the teacher. Another variant of knowledge distillation [10,11] is curriculum distillation, which designs the learning process to follow a curriculum, similar to human education. The curriculum is designed to present easy examples first and gradually increase the difficulty of the examples as the student improves. This approach has been shown to provide better performance than traditional knowledge distillation, especially for tasks that require a lot of prior knowledge. Mask distillation is a variant of knowledge distillation [12,20] that focuses on transferring the attention mechanism learned by the teacher to the student.",
            "reference_string": "[259378480 | Gao | 2023 | Citations: 0]"
        },
        {
            "title": "A Survey on Recent Teacher-student Learning Studies",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 23,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.04615",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.04615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2147417425",
                    "name": "Min Gao"
                }
            ],
            "abstract": "Knowledge distillation is a method of transferring the knowledge from a complex deep neural network (DNN) to a smaller and faster DNN, while preserving its accuracy. Recent variants of knowledge distillation include teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation, which aim to improve the performance of knowledge distillation by introducing additional components or by changing the learning process. Teaching assistant distillation involves an intermediate model called the teaching assistant, while curriculum distillation follows a curriculum similar to human education. Mask distillation focuses on transferring the attention mechanism learned by the teacher, and decoupling distillation decouples the distillation loss from the task loss. Overall, these variants of knowledge distillation have shown promising results in improving the performance of knowledge distillation.",
            "corpus_id": 258048760,
            "sentences": [
                {
                    "corpus_id": "258048760",
                    "title": "A Survey on Recent Teacher-student Learning Studies",
                    "text": "Knowledge distillation [7] is a method of transferring the knowledge from a complex model, called the teacher, to a smaller and simpler model, called the student. In recent years, several variants of knowledge distillation have been proposed, including teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation. This literature review summarizes the recent developments in these variants of knowledge distillation and discusses their strengths and limitations. \n\nKnowledge distillation is a method of compressing a complex deep neural network (DNN) into a smaller and faster DNN while preserving its accuracy. The process of knowledge distillation involves training a smaller DNN, called the student, to imitate the predictions of a larger and more complex DNN, called the teacher. The student network is trained to produce similar results as the teacher network, but with fewer parameters and lower computational cost. Knowledge distillation has been widely used for model compression and acceleration, and has shown great promise in various applications [5,8,13,18], such as computer vision and natural language processing. \n\nIn recent years, several variants of knowledge distillation [3,4,21] have been proposed and explored to improve the performance of knowledge distillation [6,9]. One of these variants is teaching assistant distillation, which introduces an intermediate model, called the teaching assistant, between the teacher and the student. The teaching assistant is trained to mimic the behavior of the teacher, and the student is trained to imitate the outputs of the teaching assistant. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better capture the knowledge learned by the teacher. \n\nAnother variant of knowledge distillation [10,11] is curriculum distillation, which designs the learning process to follow a curriculum, similar to human education. The curriculum is designed to present easy examples first and gradually increase the difficulty of the examples as the student improves. This approach has been shown to provide better performance than traditional knowledge distillation, especially for tasks that require a lot of prior knowledge. \n\nMask distillation is a variant of knowledge distillation [12,20] that focuses on transferring the attention mechanism learned by the teacher to the student.",
                    "score": 0.5537844811146724,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 358
                        },
                        {
                            "start": 359,
                            "end": 507
                        },
                        {
                            "start": 510,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 828
                        },
                        {
                            "start": 829,
                            "end": 966
                        },
                        {
                            "start": 967,
                            "end": 1172
                        },
                        {
                            "start": 1175,
                            "end": 1335
                        },
                        {
                            "start": 1336,
                            "end": 1501
                        },
                        {
                            "start": 1502,
                            "end": 1650
                        },
                        {
                            "start": 1651,
                            "end": 1813
                        },
                        {
                            "start": 1816,
                            "end": 1980
                        },
                        {
                            "start": 1981,
                            "end": 2117
                        },
                        {
                            "start": 2118,
                            "end": 2277
                        },
                        {
                            "start": 2280,
                            "end": 2436
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1106,
                            "end": 1108,
                            "matchedPaperCorpusId": "235677396"
                        },
                        {
                            "start": 1111,
                            "end": 1114,
                            "matchedPaperCorpusId": "245218794"
                        },
                        {
                            "start": 1235,
                            "end": 1238,
                            "matchedPaperCorpusId": "257771511"
                        },
                        {
                            "start": 1240,
                            "end": 1243,
                            "matchedPaperCorpusId": "258841675"
                        },
                        {
                            "start": 1332,
                            "end": 1334,
                            "matchedPaperCorpusId": "253270253"
                        },
                        {
                            "start": 1862,
                            "end": 1865,
                            "matchedPaperCorpusId": "252625376"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                },
                {
                    "corpus_id": "258048760",
                    "title": "A Survey on Recent Teacher-student Learning Studies",
                    "text": "Mask distillation is a variant of knowledge distillation [12,20] that focuses on transferring the attention mechanism learned by the teacher to the student. In mask distillation, the teacher is trained to produce a mask that indicates the importance of each input feature for a particular prediction. The student is then trained to imitate the predictions of the teacher while using the mask to weigh the importance of the input features. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better capture the attention mechanism learned by the teacher. \n\nFinally, decoupling distillation is a variant of knowledge distillation [1] that decouples the distillation loss from the task loss. In decoupling distillation, the student is trained to imitate the outputs of the teacher on a validation set, while being trained on the task loss on the training set. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better balance the trade-off between preserving the knowledge of the teacher and adapting to the task. \n\nIn conclusion, knowledge distillation is a widely used method for compressing and accelerating deep neural networks 2 TEACHER ASSISTANT When knowledge distillation is performed, increasing the model size of the teacher network instead makes the knowledge distillation worse. The results are as follows: the student network is a 2-layer CNN, while the teacher network is a 4-10-layer CNN, respectively, and it can be seen from the results that continuing to increase the depth of the teacher network does not necessarily improve the performance of knowledge distillation. \n\nTo further prove that the gap does affect knowledge distillation, the authors conducted a small comparison experiment by fixing the teacher network as a 10-layer CNN and changing the number of layers of the student network, and the results are as follows. \n\nBased on the above observations, it is not difficult to think that a medium-sized network (teacher assistant [16], TA) should be added between the large network and the small network, and the teacher network should be used to distill the TA first, and then the student network should be distilled by the TA. When TA was added, the distillation effect of the student network was significantly better than the direct distillation of the teacher network.",
                    "score": 0.5273725185097047,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 2295,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 156
                        },
                        {
                            "start": 157,
                            "end": 300
                        },
                        {
                            "start": 301,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 611
                        },
                        {
                            "start": 614,
                            "end": 746
                        },
                        {
                            "start": 747,
                            "end": 914
                        },
                        {
                            "start": 915,
                            "end": 1127
                        },
                        {
                            "start": 1130,
                            "end": 1404
                        },
                        {
                            "start": 1405,
                            "end": 1700
                        },
                        {
                            "start": 1703,
                            "end": 1958
                        },
                        {
                            "start": 1961,
                            "end": 2268
                        },
                        {
                            "start": 2269,
                            "end": 2412
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 57,
                            "end": 61,
                            "matchedPaperCorpusId": "252626689"
                        },
                        {
                            "start": 686,
                            "end": 689,
                            "matchedPaperCorpusId": "256105379"
                        },
                        {
                            "start": 2070,
                            "end": 2074,
                            "matchedPaperCorpusId": "212908749"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91943359375
                }
            ],
            "relevance_judgement": 0.92431640625,
            "relevance_judgment_input_expanded": "# Title: A Survey on Recent Teacher-student Learning Studies\n# Venue: arXiv.org\n# Authors: Min Gao\n## Abstract\nKnowledge distillation is a method of transferring the knowledge from a complex deep neural network (DNN) to a smaller and faster DNN, while preserving its accuracy. Recent variants of knowledge distillation include teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation, which aim to improve the performance of knowledge distillation by introducing additional components or by changing the learning process. Teaching assistant distillation involves an intermediate model called the teaching assistant, while curriculum distillation follows a curriculum similar to human education. Mask distillation focuses on transferring the attention mechanism learned by the teacher, and decoupling distillation decouples the distillation loss from the task loss. Overall, these variants of knowledge distillation have shown promising results in improving the performance of knowledge distillation.\n## INTRODUCTION\nKnowledge distillation [7] is a method of transferring the knowledge from a complex model, called the teacher, to a smaller and simpler model, called the student. In recent years, several variants of knowledge distillation have been proposed, including teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation. This literature review summarizes the recent developments in these variants of knowledge distillation and discusses their strengths and limitations. \n\nKnowledge distillation is a method of compressing a complex deep neural network (DNN) into a smaller and faster DNN while preserving its accuracy. The process of knowledge distillation involves training a smaller DNN, called the student, to imitate the predictions of a larger and more complex DNN, called the teacher. The student network is trained to produce similar results as the teacher network, but with fewer parameters and lower computational cost. Knowledge distillation has been widely used for model compression and acceleration, and has shown great promise in various applications [5,8,13,18], such as computer vision and natural language processing. \n\nIn recent years, several variants of knowledge distillation [3,4,21] have been proposed and explored to improve the performance of knowledge distillation [6,9]. One of these variants is teaching assistant distillation, which introduces an intermediate model, called the teaching assistant, between the teacher and the student. The teaching assistant is trained to mimic the behavior of the teacher, and the student is trained to imitate the outputs of the teaching assistant. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better capture the knowledge learned by the teacher. \n\nAnother variant of knowledge distillation [10,11] is curriculum distillation, which designs the learning process to follow a curriculum, similar to human education. The curriculum is designed to present easy examples first and gradually increase the difficulty of the examples as the student improves. This approach has been shown to provide better performance than traditional knowledge distillation, especially for tasks that require a lot of prior knowledge. \n\nMask distillation is a variant of knowledge distillation [12,20] that focuses on transferring the attention mechanism learned by the teacher to the student.\n...\nMask distillation is a variant of knowledge distillation [12,20] that focuses on transferring the attention mechanism learned by the teacher to the student. In mask distillation, the teacher is trained to produce a mask that indicates the importance of each input feature for a particular prediction. The student is then trained to imitate the predictions of the teacher while using the mask to weigh the importance of the input features. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better capture the attention mechanism learned by the teacher. \n\nFinally, decoupling distillation is a variant of knowledge distillation [1] that decouples the distillation loss from the task loss. In decoupling distillation, the student is trained to imitate the outputs of the teacher on a validation set, while being trained on the task loss on the training set. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better balance the trade-off between preserving the knowledge of the teacher and adapting to the task. \n\nIn conclusion, knowledge distillation is a widely used method for compressing and accelerating deep neural networks 2 TEACHER ASSISTANT When knowledge distillation is performed, increasing the model size of the teacher network instead makes the knowledge distillation worse. The results are as follows: the student network is a 2-layer CNN, while the teacher network is a 4-10-layer CNN, respectively, and it can be seen from the results that continuing to increase the depth of the teacher network does not necessarily improve the performance of knowledge distillation. \n\nTo further prove that the gap does affect knowledge distillation, the authors conducted a small comparison experiment by fixing the teacher network as a 10-layer CNN and changing the number of layers of the student network, and the results are as follows. \n\nBased on the above observations, it is not difficult to think that a medium-sized network (teacher assistant [16], TA) should be added between the large network and the small network, and the teacher network should be used to distill the TA first, and then the student network should be distilled by the TA. When TA was added, the distillation effect of the student network was significantly better than the direct distillation of the teacher network.",
            "reference_string": "[258048760 | Gao | 2023 | Citations: 3]"
        },
        {
            "title": "What Knowledge Gets Distilled in Knowledge Distillation?",
            "venue": "Neural Information Processing Systems",
            "year": 2022,
            "reference_count": 43,
            "citation_count": 19,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.16004",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.16004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47284770",
                    "name": "Utkarsh Ojha"
                },
                {
                    "authorId": "1527091339",
                    "name": "Yuheng Li"
                },
                {
                    "authorId": "144756076",
                    "name": "Yong Jae Lee"
                }
            ],
            "abstract": "Knowledge distillation aims to transfer useful information from a teacher network to a student network, with the primary goal of improving the student's performance for the task at hand. Over the years, there has a been a deluge of novel techniques and use cases of knowledge distillation. Yet, despite the various improvements, there seems to be a glaring gap in the community's fundamental understanding of the process. Specifically, what is the knowledge that gets distilled in knowledge distillation? In other words, in what ways does the student become similar to the teacher? Does it start to localize objects in the same way? Does it get fooled by the same adversarial samples? Does its data invariance properties become similar? Our work presents a comprehensive study to try to answer these questions and more. Our results, using image classification as a case study and three state-of-the-art knowledge distillation techniques, show that knowledge distillation methods can indeed indirectly distill other kinds of properties beyond improving task performance. And while we believe that understanding the distillation process is important in itself, we also demonstrate that our results can pave the path for important practical applications as well.",
            "corpus_id": 249209742,
            "sentences": [
                {
                    "corpus_id": "249209742",
                    "title": "What Knowledge Gets Distilled in Knowledge Distillation?",
                    "text": "The need for such answers has become particularly relevant because there have been studies which present some surprising findings about the distillation process. Cho & Hariharan (2019) showed that performing knowledge distillation with a bigger teacher does not necessarily improve the student's performance over that with a smaller teacher, and thus raised questions about the effectiveness of the distillation procedure in such cases. Stanton et al. (2021) showed that the agreement between Preprint the teacher and distilled student's predictions on test images is not that different to the agreement of those between the teacher and an independently trained student, raising further doubts about how knowledge distillation works, if it works at all. \n\nOur goal is hence to shed some light on the 'dark knowledge' (Hinton et al., 2014a) that gets distilled in knowledge distillation. In this work, we present a comprehensive study tackling the above questions. We use the task of image classification as a case study, and analyze three popular state-of-the-art knowledge distillation methods (Hinton et al., 2014b;Romero et al., 2015;Tian et al., 2020). Many of our findings are quite surprising. For example, by simply mimicking the teacher's output on ImageNet images using the method of Hinton et al. (2014b), the student can inherit many implicit properties of the teacher. It can gain the adversarial vulnerability that the teacher has. If the teacher is invariant to color, the student also improves its invariance to color, even without explicitly being trained to be that way. While the specific transferred properties and their extent differ across different distillation methods and teacher-student architectures, our study reveals that there is much more happening in the background of the distillation process than what meets the eye: distillation makes the student extract similar intermediate representations as the teacher, something we study quantitatively. We show that this has practical implications, a good one and a bad one. The good is that a student can inherit many useful properties of a teacher even without being exposed to it explicitly, which saves computation. The bad is that an otherwise fair student can inherit biases from an unfair teacher.",
                    "score": 0.5570921142335242,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1874,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 436
                        },
                        {
                            "start": 437,
                            "end": 753
                        },
                        {
                            "start": 756,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1156
                        },
                        {
                            "start": 1157,
                            "end": 1199
                        },
                        {
                            "start": 1200,
                            "end": 1380
                        },
                        {
                            "start": 1381,
                            "end": 1444
                        },
                        {
                            "start": 1445,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1976
                        },
                        {
                            "start": 1977,
                            "end": 2048
                        },
                        {
                            "start": 2049,
                            "end": 2193
                        },
                        {
                            "start": 2194,
                            "end": 2278
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 162,
                            "end": 184,
                            "matchedPaperCorpusId": "203642130"
                        },
                        {
                            "start": 1095,
                            "end": 1117,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1137,
                            "end": 1155,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 1293,
                            "end": 1314,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.923828125
                }
            ],
            "relevance_judgement": 0.923828125,
            "relevance_judgment_input_expanded": "# Title: What Knowledge Gets Distilled in Knowledge Distillation?\n# Venue: Neural Information Processing Systems\n# Authors: Utkarsh Ojha, Yuheng Li, Yong Jae Lee\n## Abstract\nKnowledge distillation aims to transfer useful information from a teacher network to a student network, with the primary goal of improving the student's performance for the task at hand. Over the years, there has a been a deluge of novel techniques and use cases of knowledge distillation. Yet, despite the various improvements, there seems to be a glaring gap in the community's fundamental understanding of the process. Specifically, what is the knowledge that gets distilled in knowledge distillation? In other words, in what ways does the student become similar to the teacher? Does it start to localize objects in the same way? Does it get fooled by the same adversarial samples? Does its data invariance properties become similar? Our work presents a comprehensive study to try to answer these questions and more. Our results, using image classification as a case study and three state-of-the-art knowledge distillation techniques, show that knowledge distillation methods can indeed indirectly distill other kinds of properties beyond improving task performance. And while we believe that understanding the distillation process is important in itself, we also demonstrate that our results can pave the path for important practical applications as well.\n## INTRODUCTION\nThe need for such answers has become particularly relevant because there have been studies which present some surprising findings about the distillation process. Cho & Hariharan (2019) showed that performing knowledge distillation with a bigger teacher does not necessarily improve the student's performance over that with a smaller teacher, and thus raised questions about the effectiveness of the distillation procedure in such cases. Stanton et al. (2021) showed that the agreement between Preprint the teacher and distilled student's predictions on test images is not that different to the agreement of those between the teacher and an independently trained student, raising further doubts about how knowledge distillation works, if it works at all. \n\nOur goal is hence to shed some light on the 'dark knowledge' (Hinton et al., 2014a) that gets distilled in knowledge distillation. In this work, we present a comprehensive study tackling the above questions. We use the task of image classification as a case study, and analyze three popular state-of-the-art knowledge distillation methods (Hinton et al., 2014b;Romero et al., 2015;Tian et al., 2020). Many of our findings are quite surprising. For example, by simply mimicking the teacher's output on ImageNet images using the method of Hinton et al. (2014b), the student can inherit many implicit properties of the teacher. It can gain the adversarial vulnerability that the teacher has. If the teacher is invariant to color, the student also improves its invariance to color, even without explicitly being trained to be that way. While the specific transferred properties and their extent differ across different distillation methods and teacher-student architectures, our study reveals that there is much more happening in the background of the distillation process than what meets the eye: distillation makes the student extract similar intermediate representations as the teacher, something we study quantitatively. We show that this has practical implications, a good one and a bad one. The good is that a student can inherit many useful properties of a teacher even without being exposed to it explicitly, which saves computation. The bad is that an otherwise fair student can inherit biases from an unfair teacher.",
            "reference_string": "[249209742 | Ojha et al. | 2022 | Citations: 19]"
        },
        {
            "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
            "venue": "ACM Transactions on Intelligent Systems and Technology",
            "year": 2024,
            "reference_count": 156,
            "citation_count": 33,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3699518",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01885, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309482604",
                    "name": "Chuanpeng Yang"
                },
                {
                    "authorId": "2290230390",
                    "name": "Yao Zhu"
                },
                {
                    "authorId": "2309429610",
                    "name": "Wang Lu"
                },
                {
                    "authorId": "2293893759",
                    "name": "Yidong Wang"
                },
                {
                    "authorId": "2276257486",
                    "name": "Qian Chen"
                },
                {
                    "authorId": "26155233",
                    "name": "Chenlong Gao"
                },
                {
                    "authorId": "2276089941",
                    "name": "Bingjie Yan"
                },
                {
                    "authorId": "2276178270",
                    "name": "Yiqiang Chen"
                }
            ],
            "abstract": "Large Language Models (LLMs) have showcased exceptional capabilities in various domains, attracting significant interest from both academia and industry. Despite their impressive performance, the substantial size and computational demands of LLMs pose considerable challenges for practical deployment, particularly in environments with limited resources. The endeavor to compress language models while maintaining their accuracy has become a focal point of research. Among the various methods, knowledge distillation has emerged as an effective technique to enhance inference speed without greatly compromising performance. This paper presents a thorough survey from three aspects: method, evaluation, and application, exploring knowledge distillation techniques tailored specifically for LLMs. Specifically, we divide the methods into white-box KD and black-box KD to better illustrate their differences. Furthermore, we also explored the evaluation tasks and distillation effects between different distillation methods, and proposed directions for future research. Through in-depth understanding of the latest advancements and practical applications, this survey provides valuable resources for researchers, paving the way for sustained progress in this field.",
            "corpus_id": 270878760,
            "sentences": [
                {
                    "corpus_id": "270878760",
                    "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
                    "text": "The feature-based knowledge distillation methods [46,122] extract knowledge from the embedding space, transformer layers, and prediction layers, allowing the student model to learn various aspects of the teacher model comprehensively.For instance, Sun et al. [122] proposed a patient knowledge distillation (PKD) method aimed at compressing a large-scale teacher model into an equally effective lightweight student model.They proposed two distinct distillation strategies: 1) PKD-Last: The student model learns from the last  layers of the teacher model, based on the assumption that the top layers contain the most informative knowledge.2) PKD-Skip: The student learns from every -layer of the teacher, suggesting that the lower layers also contain essential information that should be gradually transferred during distillation.Experiments conducted on seven datasets across four tasks-sentiment classification, paraphrase similarity matching, natural language inference, and machine reading comprehension-showed that the PKD method outperformed standard knowledge distillation methods.It achieved superior performance and better generalization, significantly enhancing training efficiency and reducing storage requirements while maintaining accuracy comparable to the original large-scale model.MetaDistill [168] offers a simple and efficient alternative to traditional KD methods by keeping the teacher model fixed during training.Within the meta-learning framework, teacher networks enhance knowledge transfer to student networks by distilling feedback on student performance.Additionally, a pilot update mechanism is introduced to improve the alignment between internal learners and meta-learners, focusing on enhancing internal learners' performance.Extensive experiments have validated the effectiveness and versatility of this method across text and image classification tasks.Furthermore, experiments on the GLUE benchmark have shown that MetaDistill significantly outperforms traditional knowledge distillation, achieving state-of-the-art performance compression.AD-KD [155] addresses two key limitations of existing knowledge distillation methods.First, student models often merely mimic the teacher's behavior without developing their own reasoning capabilities.Second, these methods typically focus on transferring knowledge specific to complex models while neglecting data-specific knowledge.",
                    "score": 0.5972719870036843,
                    "section_title": "Hint-based KD.",
                    "char_start_offset": 16483,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 234
                        },
                        {
                            "start": 234,
                            "end": 421
                        },
                        {
                            "start": 421,
                            "end": 638
                        },
                        {
                            "start": 638,
                            "end": 829
                        },
                        {
                            "start": 829,
                            "end": 1087
                        },
                        {
                            "start": 1087,
                            "end": 1297
                        },
                        {
                            "start": 1297,
                            "end": 1434
                        },
                        {
                            "start": 1434,
                            "end": 1580
                        },
                        {
                            "start": 1580,
                            "end": 1756
                        },
                        {
                            "start": 1756,
                            "end": 1885
                        },
                        {
                            "start": 1885,
                            "end": 2073
                        },
                        {
                            "start": 2073,
                            "end": 2158
                        },
                        {
                            "start": 2158,
                            "end": 2274
                        },
                        {
                            "start": 2274,
                            "end": 2406
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 49,
                            "end": 53,
                            "matchedPaperCorpusId": "215415863"
                        },
                        {
                            "start": 53,
                            "end": 57,
                            "matchedPaperCorpusId": "201670719"
                        },
                        {
                            "start": 259,
                            "end": 264,
                            "matchedPaperCorpusId": "201670719"
                        },
                        {
                            "start": 1309,
                            "end": 1314,
                            "matchedPaperCorpusId": "237250417"
                        },
                        {
                            "start": 2079,
                            "end": 2084,
                            "matchedPaperCorpusId": "258740796"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9228515625
                },
                {
                    "corpus_id": "270878760",
                    "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
                    "text": "The distillation of Bidirectional Long Short-Term Memory Networks (BiL-STM) [126] marks the earliest attempt to apply knowledge distillation to BERT [60].The distillation objective is to minimize the mean squared error loss between the logits of the student network and those of the teacher.This approach has been tested on three tasks: sentence classification and sentence matching.Experimental results show that the shallow BiLSTM-based model achieves performance comparable to the ELMo language model [104], but with approximately 100 times fewer parameters and a 15-fold increase in inference speed.Similarly, DistillBERT [111] initializes a shallower student model using the teacher's parameters and minimizes the difference in soft target probabilities between the teacher and student, a technique known as word-level knowledge distillation.It introduced a triple loss that combines language modeling, distillation, and cosine distance loss to leverage the inductive bias learned by the pre-trained model.DistilBERT achieved performance equivalent to or exceeding the ELMo baseline in nine tasks.Compared to BERT, DistilBERT maintains 97% of the performance while reducing the number of parameters by 40%.MixKD [75] extends the concept of encouraging students to mimic teachers' logits by using linear interpolation of example pairs.It improves the effectiveness of knowledge distillation by using data augmentation to create additional samples from the available task-specific data.This approach mirrors students learning more effectively from teachers by asking further questions to explore their answers and concepts in depth, providing more data for student models to extract insights from large-scale language models.Evaluation results across six datasets show that MixKD significantly outperforms traditional knowledge distillation and previous methods in compressing large language models.ReAugKD [163] includes both an inference stage and a training stage.In the inference stage, it aggregates soft labels generated by teachers that closely resemble student embeddings.\n\nDuring the training phase, a novel relationship KD loss is used to minimize the differences between teacher-student embeddings and their distributions.",
                    "score": 0.5266861923405708,
                    "section_title": "Logits-based KD.",
                    "char_start_offset": 8921,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 154
                        },
                        {
                            "start": 154,
                            "end": 291
                        },
                        {
                            "start": 291,
                            "end": 383
                        },
                        {
                            "start": 383,
                            "end": 603
                        },
                        {
                            "start": 603,
                            "end": 847
                        },
                        {
                            "start": 847,
                            "end": 1011
                        },
                        {
                            "start": 1011,
                            "end": 1102
                        },
                        {
                            "start": 1102,
                            "end": 1211
                        },
                        {
                            "start": 1211,
                            "end": 1339
                        },
                        {
                            "start": 1339,
                            "end": 1489
                        },
                        {
                            "start": 1489,
                            "end": 1728
                        },
                        {
                            "start": 1728,
                            "end": 1902
                        },
                        {
                            "start": 1902,
                            "end": 1970
                        },
                        {
                            "start": 1970,
                            "end": 2083
                        },
                        {
                            "start": 2085,
                            "end": 2236
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 149,
                            "end": 153,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 504,
                            "end": 509,
                            "matchedPaperCorpusId": "3626819"
                        },
                        {
                            "start": 1217,
                            "end": 1221,
                            "matchedPaperCorpusId": "226226888"
                        },
                        {
                            "start": 1910,
                            "end": 1915,
                            "matchedPaperCorpusId": "259370551"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90625
                }
            ],
            "relevance_judgement": 0.9228515625,
            "relevance_judgment_input_expanded": "# Title: Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application\n# Venue: ACM Transactions on Intelligent Systems and Technology\n# Authors: Chuanpeng Yang, Yao Zhu, Wang Lu, Yidong Wang, Qian Chen, Chenlong Gao, Bingjie Yan, Yiqiang Chen\n## Abstract\nLarge Language Models (LLMs) have showcased exceptional capabilities in various domains, attracting significant interest from both academia and industry. Despite their impressive performance, the substantial size and computational demands of LLMs pose considerable challenges for practical deployment, particularly in environments with limited resources. The endeavor to compress language models while maintaining their accuracy has become a focal point of research. Among the various methods, knowledge distillation has emerged as an effective technique to enhance inference speed without greatly compromising performance. This paper presents a thorough survey from three aspects: method, evaluation, and application, exploring knowledge distillation techniques tailored specifically for LLMs. Specifically, we divide the methods into white-box KD and black-box KD to better illustrate their differences. Furthermore, we also explored the evaluation tasks and distillation effects between different distillation methods, and proposed directions for future research. Through in-depth understanding of the latest advancements and practical applications, this survey provides valuable resources for researchers, paving the way for sustained progress in this field.\n## Logits-based KD.\nThe distillation of Bidirectional Long Short-Term Memory Networks (BiL-STM) [126] marks the earliest attempt to apply knowledge distillation to BERT [60].The distillation objective is to minimize the mean squared error loss between the logits of the student network and those of the teacher.This approach has been tested on three tasks: sentence classification and sentence matching.Experimental results show that the shallow BiLSTM-based model achieves performance comparable to the ELMo language model [104], but with approximately 100 times fewer parameters and a 15-fold increase in inference speed.Similarly, DistillBERT [111] initializes a shallower student model using the teacher's parameters and minimizes the difference in soft target probabilities between the teacher and student, a technique known as word-level knowledge distillation.It introduced a triple loss that combines language modeling, distillation, and cosine distance loss to leverage the inductive bias learned by the pre-trained model.DistilBERT achieved performance equivalent to or exceeding the ELMo baseline in nine tasks.Compared to BERT, DistilBERT maintains 97% of the performance while reducing the number of parameters by 40%.MixKD [75] extends the concept of encouraging students to mimic teachers' logits by using linear interpolation of example pairs.It improves the effectiveness of knowledge distillation by using data augmentation to create additional samples from the available task-specific data.This approach mirrors students learning more effectively from teachers by asking further questions to explore their answers and concepts in depth, providing more data for student models to extract insights from large-scale language models.Evaluation results across six datasets show that MixKD significantly outperforms traditional knowledge distillation and previous methods in compressing large language models.ReAugKD [163] includes both an inference stage and a training stage.In the inference stage, it aggregates soft labels generated by teachers that closely resemble student embeddings.\n\nDuring the training phase, a novel relationship KD loss is used to minimize the differences between teacher-student embeddings and their distributions.\n\n## Hint-based KD.\nThe feature-based knowledge distillation methods [46,122] extract knowledge from the embedding space, transformer layers, and prediction layers, allowing the student model to learn various aspects of the teacher model comprehensively.For instance, Sun et al. [122] proposed a patient knowledge distillation (PKD) method aimed at compressing a large-scale teacher model into an equally effective lightweight student model.They proposed two distinct distillation strategies: 1) PKD-Last: The student model learns from the last  layers of the teacher model, based on the assumption that the top layers contain the most informative knowledge.2) PKD-Skip: The student learns from every -layer of the teacher, suggesting that the lower layers also contain essential information that should be gradually transferred during distillation.Experiments conducted on seven datasets across four tasks-sentiment classification, paraphrase similarity matching, natural language inference, and machine reading comprehension-showed that the PKD method outperformed standard knowledge distillation methods.It achieved superior performance and better generalization, significantly enhancing training efficiency and reducing storage requirements while maintaining accuracy comparable to the original large-scale model.MetaDistill [168] offers a simple and efficient alternative to traditional KD methods by keeping the teacher model fixed during training.Within the meta-learning framework, teacher networks enhance knowledge transfer to student networks by distilling feedback on student performance.Additionally, a pilot update mechanism is introduced to improve the alignment between internal learners and meta-learners, focusing on enhancing internal learners' performance.Extensive experiments have validated the effectiveness and versatility of this method across text and image classification tasks.Furthermore, experiments on the GLUE benchmark have shown that MetaDistill significantly outperforms traditional knowledge distillation, achieving state-of-the-art performance compression.AD-KD [155] addresses two key limitations of existing knowledge distillation methods.First, student models often merely mimic the teacher's behavior without developing their own reasoning capabilities.Second, these methods typically focus on transferring knowledge specific to complex models while neglecting data-specific knowledge.",
            "reference_string": "[270878760 | Yang et al. | 2024 | Citations: 33]"
        },
        {
            "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models",
            "venue": "IEEE Transactions on Artificial Intelligence",
            "year": 2024,
            "reference_count": 199,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.10210, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237801390",
                    "name": "Kamal Acharya"
                },
                {
                    "authorId": "2237801384",
                    "name": "Alvaro Velasquez"
                },
                {
                    "authorId": "2261907178",
                    "name": "H. Song"
                }
            ],
            "abstract": "This survey article delves into the emerging and critical area of symbolic knowledge distillation in large language models (LLMs). As LLMs such as generative pretrained transformer-3 (GPT-3) and bidirectional encoder representations from transformers (BERT) continue to expand in scale and complexity, the challenge of effectively harnessing their extensive knowledge becomes paramount. This survey concentrates on the process of distilling the intricate, often implicit knowledge contained within these models into a more symbolic, explicit form. This transformation is crucial for enhancing the interpretability, efficiency, and applicability of LLMs. We categorize the existing research based on methodologies and applications, focusing on how symbolic knowledge distillation can be used to improve the transparency and functionality of smaller, more efficient artificial intelligence (AI) models. The survey discusses the core challenges, including maintaining the depth of knowledge in a comprehensible format, and explores the various approaches and techniques that have been developed in this field. We identify gaps in current research and potential opportunities for future advancements. This survey aims to provide a comprehensive overview of symbolic knowledge distillation in LLMs, spotlighting its significance in the progression toward more accessible and efficient AI systems.",
            "corpus_id": 271227251,
            "sentences": [
                {
                    "corpus_id": "271227251",
                    "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models",
                    "text": "Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student) with the goal of retaining much of the teacher model's performance [117]. This process is crucial in scenarios where computational resources are limited or where deployment requires lightweight models. There are various types of traditional knowledge distillation techniques: response-based, feature-based and relation-based and one modern symbolic knowledge distillation, each with its unique approach and area of application: \n\n1) Response-based Knowledge Distillation: Responsebased knowledge distillation involves transferring knowledge from the teacher model's final output layer to the student model, aiming to mimic the teacher's final predictions. This approach is straightforward and has proven effective across various tasks, employing a loss function based on the divergence between the teacher's and student's logits. It's widely applied in model compression and has been adapted for different types of model predictions, including object detection and human pose estimation, where the teacher's output may include additional information like bounding box offsets [118] or heatmaps for landmarks [119]. A key application of responsebased knowledge distillation is in image classification [44], where \"soft targets\" -the probabilities assigned to each class by the teacher model -play a crucial role. These probabilities are adjusted using a temperature factor to control the softness of the targets, allowing the transfer of knowledge from the teacher to the student. The distillation process typically employs the Kullback-Leibler divergence loss to optimize the similarity between the teacher's and student's probability distributions. \n\nThis method is praised for its simplicity and effectiveness, particularly in leveraging knowledge for training. However, its reliance on the final layer's output means it may not fully utilize intermediate-level supervision from the teacher, an aspect crucial for representation learning in deep neural networks. \n\n2) Feature-based Knowledge Distillation: Feature-based knowledge distillation taps into the strength of deep neural networks to learn hierarchical feature representations, a process central to representation learning [120].",
                    "score": 0.5806509192219911,
                    "section_title": "A. Knowledge Distillation",
                    "char_start_offset": 14819,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 348
                        },
                        {
                            "start": 349,
                            "end": 574
                        },
                        {
                            "start": 577,
                            "end": 802
                        },
                        {
                            "start": 803,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1626
                        },
                        {
                            "start": 1627,
                            "end": 1796
                        },
                        {
                            "start": 1799,
                            "end": 1910
                        },
                        {
                            "start": 1911,
                            "end": 2111
                        },
                        {
                            "start": 2114,
                            "end": 2337
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 214,
                            "end": 219,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 1223,
                            "end": 1228,
                            "matchedPaperCorpusId": "29308926"
                        },
                        {
                            "start": 1255,
                            "end": 1260,
                            "matchedPaperCorpusId": "53292120"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.921875
                },
                {
                    "corpus_id": "271227251",
                    "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models",
                    "text": "2) Feature-based Knowledge Distillation: Feature-based knowledge distillation taps into the strength of deep neural networks to learn hierarchical feature representations, a process central to representation learning [120]. Unlike responsebased knowledge distillation, which focuses on the outputs of the last layer, feature-based distillation utilizes the outputs from intermediate layers, or feature maps, to guide the student model. This approach is particularly beneficial for training models that are both narrower and deeper, as it provides a richer set of training signals. \n\nThe concept was first introduced with Fitnets [49], aiming to improve student model training by matching feature activations between the teacher and student directly. Following this, several methodologies have been developed to facilitate this matching process, either directly or indirectly [121]. Notable contributions include the derivation of \"attention maps\" to express the use of neuron selectivity transfer [122], matching probability distributions in feature space [123], and introducing \"factors\" for more interpretable intermediate representations [54]. Techniques like route constrained hint learning [124] and the use of activation boundaries [125] have been proposed to minimize the performance gap between teacher and student models, alongside innovative strategies like cross-layer knowledge distillation [121] which adaptively matches teacher and student layers. \n\nDespite the effectiveness of feature-based knowledge transfer in enriching the student model's learning, challenges remain in selecting appropriate layers for hints and guidance due to the size discrepancies between teacher and student models. This necessitates further exploration into how best to match the feature representations between teacher and student models effectively. \n\n3) Relation-based Knowledge Distillation: Relation-based knowledge distillation goes beyond the scope of responsebased and feature-based methods by examining the relationships between different layers or data samples within the teacher model. This approach delves into the dynamics between feature maps, layers, and even the relationships between different teachers or data samples, offering a more nuanced form of knowledge transfer. \n\nFlow of solution process (FSP) [53] utilizes the Gram matrix between two layers to encapsulate the relationships between pairs of feature maps through inner product calculations. Knowledge distillation via singular value decomposition [126] distill essential information from these relationships.",
                    "score": 0.5424488063617605,
                    "section_title": "A. Knowledge Distillation",
                    "char_start_offset": 16933,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 223
                        },
                        {
                            "start": 224,
                            "end": 435
                        },
                        {
                            "start": 436,
                            "end": 580
                        },
                        {
                            "start": 583,
                            "end": 749
                        },
                        {
                            "start": 750,
                            "end": 881
                        },
                        {
                            "start": 882,
                            "end": 1146
                        },
                        {
                            "start": 1147,
                            "end": 1461
                        },
                        {
                            "start": 1464,
                            "end": 1707
                        },
                        {
                            "start": 1708,
                            "end": 1844
                        },
                        {
                            "start": 1847,
                            "end": 2089
                        },
                        {
                            "start": 2090,
                            "end": 2281
                        },
                        {
                            "start": 2284,
                            "end": 2462
                        },
                        {
                            "start": 2463,
                            "end": 2580
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 217,
                            "end": 222,
                            "matchedPaperCorpusId": "393948"
                        },
                        {
                            "start": 875,
                            "end": 880,
                            "matchedPaperCorpusId": "227335337"
                        },
                        {
                            "start": 1056,
                            "end": 1061,
                            "matchedPaperCorpusId": "52012952"
                        },
                        {
                            "start": 1141,
                            "end": 1145,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 1195,
                            "end": 1200,
                            "matchedPaperCorpusId": "125985701"
                        },
                        {
                            "start": 1238,
                            "end": 1243,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 1403,
                            "end": 1408,
                            "matchedPaperCorpusId": "227335337"
                        },
                        {
                            "start": 2315,
                            "end": 2319,
                            "matchedPaperCorpusId": "206596723"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9111328125
                }
            ],
            "relevance_judgement": 0.921875,
            "relevance_judgment_input_expanded": "# Title: A Survey on Symbolic Knowledge Distillation of Large Language Models\n# Venue: IEEE Transactions on Artificial Intelligence\n# Authors: Kamal Acharya, Alvaro Velasquez, H. Song\n## Abstract\nThis survey article delves into the emerging and critical area of symbolic knowledge distillation in large language models (LLMs). As LLMs such as generative pretrained transformer-3 (GPT-3) and bidirectional encoder representations from transformers (BERT) continue to expand in scale and complexity, the challenge of effectively harnessing their extensive knowledge becomes paramount. This survey concentrates on the process of distilling the intricate, often implicit knowledge contained within these models into a more symbolic, explicit form. This transformation is crucial for enhancing the interpretability, efficiency, and applicability of LLMs. We categorize the existing research based on methodologies and applications, focusing on how symbolic knowledge distillation can be used to improve the transparency and functionality of smaller, more efficient artificial intelligence (AI) models. The survey discusses the core challenges, including maintaining the depth of knowledge in a comprehensible format, and explores the various approaches and techniques that have been developed in this field. We identify gaps in current research and potential opportunities for future advancements. This survey aims to provide a comprehensive overview of symbolic knowledge distillation in LLMs, spotlighting its significance in the progression toward more accessible and efficient AI systems.\n## A. Knowledge Distillation\nKnowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student) with the goal of retaining much of the teacher model's performance [117]. This process is crucial in scenarios where computational resources are limited or where deployment requires lightweight models. There are various types of traditional knowledge distillation techniques: response-based, feature-based and relation-based and one modern symbolic knowledge distillation, each with its unique approach and area of application: \n\n1) Response-based Knowledge Distillation: Responsebased knowledge distillation involves transferring knowledge from the teacher model's final output layer to the student model, aiming to mimic the teacher's final predictions. This approach is straightforward and has proven effective across various tasks, employing a loss function based on the divergence between the teacher's and student's logits. It's widely applied in model compression and has been adapted for different types of model predictions, including object detection and human pose estimation, where the teacher's output may include additional information like bounding box offsets [118] or heatmaps for landmarks [119]. A key application of responsebased knowledge distillation is in image classification [44], where \"soft targets\" -the probabilities assigned to each class by the teacher model -play a crucial role. These probabilities are adjusted using a temperature factor to control the softness of the targets, allowing the transfer of knowledge from the teacher to the student. The distillation process typically employs the Kullback-Leibler divergence loss to optimize the similarity between the teacher's and student's probability distributions. \n\nThis method is praised for its simplicity and effectiveness, particularly in leveraging knowledge for training. However, its reliance on the final layer's output means it may not fully utilize intermediate-level supervision from the teacher, an aspect crucial for representation learning in deep neural networks. \n\n2) Feature-based Knowledge Distillation: Feature-based knowledge distillation taps into the strength of deep neural networks to learn hierarchical feature representations, a process central to representation learning [120].\n...\n2) Feature-based Knowledge Distillation: Feature-based knowledge distillation taps into the strength of deep neural networks to learn hierarchical feature representations, a process central to representation learning [120]. Unlike responsebased knowledge distillation, which focuses on the outputs of the last layer, feature-based distillation utilizes the outputs from intermediate layers, or feature maps, to guide the student model. This approach is particularly beneficial for training models that are both narrower and deeper, as it provides a richer set of training signals. \n\nThe concept was first introduced with Fitnets [49], aiming to improve student model training by matching feature activations between the teacher and student directly. Following this, several methodologies have been developed to facilitate this matching process, either directly or indirectly [121]. Notable contributions include the derivation of \"attention maps\" to express the use of neuron selectivity transfer [122], matching probability distributions in feature space [123], and introducing \"factors\" for more interpretable intermediate representations [54]. Techniques like route constrained hint learning [124] and the use of activation boundaries [125] have been proposed to minimize the performance gap between teacher and student models, alongside innovative strategies like cross-layer knowledge distillation [121] which adaptively matches teacher and student layers. \n\nDespite the effectiveness of feature-based knowledge transfer in enriching the student model's learning, challenges remain in selecting appropriate layers for hints and guidance due to the size discrepancies between teacher and student models. This necessitates further exploration into how best to match the feature representations between teacher and student models effectively. \n\n3) Relation-based Knowledge Distillation: Relation-based knowledge distillation goes beyond the scope of responsebased and feature-based methods by examining the relationships between different layers or data samples within the teacher model. This approach delves into the dynamics between feature maps, layers, and even the relationships between different teachers or data samples, offering a more nuanced form of knowledge transfer. \n\nFlow of solution process (FSP) [53] utilizes the Gram matrix between two layers to encapsulate the relationships between pairs of feature maps through inner product calculations. Knowledge distillation via singular value decomposition [126] distill essential information from these relationships.",
            "reference_string": "[271227251 | Acharya et al. | 2024 | Citations: 7]"
        },
        {
            "title": "What do larger image classifiers memorise?",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2023,
            "reference_count": 87,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.05337",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256999920",
                    "name": "Michal Lukasik"
                },
                {
                    "authorId": "34602162",
                    "name": "Vaishnavh Nagarajan"
                },
                {
                    "authorId": "2241094",
                    "name": "A. Rawat"
                },
                {
                    "authorId": "2844480",
                    "name": "A. Menon"
                },
                {
                    "authorId": "2254172047",
                    "name": "Sanjiv Kumar"
                }
            ],
            "abstract": "The success of modern neural networks has prompted study of the connection between memorisation and generalisation: overparameterised models generalise well, despite being able to perfectly fit (memorise) completely random labels. To carefully study this issue, Feldman proposed a metric to quantify the degree of memorisation of individual training examples, and empirically computed the corresponding memorisation profile of a ResNet on image classification bench-marks. While an exciting first glimpse into what real-world models memorise, this leaves open a fundamental question: do larger neural models memorise more? We present a comprehensive empirical analysis of this question on image classification benchmarks. We find that training examples exhibit an unexpectedly diverse set of memorisation trajectories across model sizes: most samples experience decreased memorisation under larger models, while the rest exhibit cap-shaped or increasing memorisation. We show that various proxies for the Feldman memorization score fail to capture these fundamental trends. Lastly, we find that knowledge distillation, an effective and popular model compression technique, tends to inhibit memorisation, while also improving generalisation. Specifically, memorisation is mostly inhibited on examples with increasing memorisation trajectories, thus pointing at how distillation improves generalisation.",
            "corpus_id": 263830209,
            "sentences": [
                {
                    "corpus_id": "263830209",
                    "title": "What do larger image classifiers memorise?",
                    "text": "Despite the impressive performance of large neural models on a range of challenging tasks in vision and NLP, practical deployment of such models is often infeasible due to their high inference cost. Recently, knowledge distillation [18,41] has emerged as a promising approach to compress these large models into more tractable models. Here, one feeds a large (\"teacher\") model's predicted distribution over labels as the prediction targets for a small (\"student\") model. Compared to the standard training on raw labels (as considered in earlier sections), distillation can provide significant performance gains; these are informally attributed to distillation performing \"knowledge transfer\". Knowledge distillation has been successfully applied across many applications, including: computer vision [10], language modeling [72], information retrieval [51], machine translation [87], and ads recommendation [1,52]. \n\nCompared to standard training, distillation presents us with an interesting setup where models of different sizes interact during the training procedure. This prompts us to ask: how does coupling between models of different sizes during distillation affect, if at all, the memorisation behavior of the resulting student model? Interestingly, while distillation has been shown to yield significant performance gains on average, training accuracy has been shown to be systematically harmed [24]. Previous work also showed how distillation can lead to worsened accuracy on a subset of hard examples [54]. In a related study, model compression has been shown to harm accuracy on tail classes [42]. These observations indeed hint at the potential impact of distillation procedure on student's memorisation behavior, which we systematically explore in this section. \n\nTowards this, we consider knowledge distillation as conducted using logit matching, and where the teacher is trained on the same sub-sample as the student for estimating the memorisation scores per Equation 2. We provide the hyperparameter details in Appendix D. \n\nDistillation inhibits memorisation on average. We begin by investigating what happens to average memorisation under distillation.",
                    "score": 0.5513621752965363,
                    "section_title": "Distillation lowers memorisation",
                    "char_start_offset": 25608,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 198
                        },
                        {
                            "start": 199,
                            "end": 334
                        },
                        {
                            "start": 335,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 913
                        },
                        {
                            "start": 916,
                            "end": 1069
                        },
                        {
                            "start": 1070,
                            "end": 1242
                        },
                        {
                            "start": 1243,
                            "end": 1409
                        },
                        {
                            "start": 1410,
                            "end": 1517
                        },
                        {
                            "start": 1518,
                            "end": 1609
                        },
                        {
                            "start": 1610,
                            "end": 1775
                        },
                        {
                            "start": 1778,
                            "end": 2040
                        },
                        {
                            "start": 2043,
                            "end": 2089
                        },
                        {
                            "start": 2090,
                            "end": 2172
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 232,
                            "end": 236,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 851,
                            "end": 855,
                            "matchedPaperCorpusId": "235720578"
                        },
                        {
                            "start": 877,
                            "end": 881,
                            "matchedPaperCorpusId": "207847275"
                        },
                        {
                            "start": 1404,
                            "end": 1408,
                            "matchedPaperCorpusId": "203642130"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.921875
                }
            ],
            "relevance_judgement": 0.921875,
            "relevance_judgment_input_expanded": "# Title: What do larger image classifiers memorise?\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Michal Lukasik, Vaishnavh Nagarajan, A. Rawat, A. Menon, Sanjiv Kumar\n## Abstract\nThe success of modern neural networks has prompted study of the connection between memorisation and generalisation: overparameterised models generalise well, despite being able to perfectly fit (memorise) completely random labels. To carefully study this issue, Feldman proposed a metric to quantify the degree of memorisation of individual training examples, and empirically computed the corresponding memorisation profile of a ResNet on image classification bench-marks. While an exciting first glimpse into what real-world models memorise, this leaves open a fundamental question: do larger neural models memorise more? We present a comprehensive empirical analysis of this question on image classification benchmarks. We find that training examples exhibit an unexpectedly diverse set of memorisation trajectories across model sizes: most samples experience decreased memorisation under larger models, while the rest exhibit cap-shaped or increasing memorisation. We show that various proxies for the Feldman memorization score fail to capture these fundamental trends. Lastly, we find that knowledge distillation, an effective and popular model compression technique, tends to inhibit memorisation, while also improving generalisation. Specifically, memorisation is mostly inhibited on examples with increasing memorisation trajectories, thus pointing at how distillation improves generalisation.\n## Distillation lowers memorisation\nDespite the impressive performance of large neural models on a range of challenging tasks in vision and NLP, practical deployment of such models is often infeasible due to their high inference cost. Recently, knowledge distillation [18,41] has emerged as a promising approach to compress these large models into more tractable models. Here, one feeds a large (\"teacher\") model's predicted distribution over labels as the prediction targets for a small (\"student\") model. Compared to the standard training on raw labels (as considered in earlier sections), distillation can provide significant performance gains; these are informally attributed to distillation performing \"knowledge transfer\". Knowledge distillation has been successfully applied across many applications, including: computer vision [10], language modeling [72], information retrieval [51], machine translation [87], and ads recommendation [1,52]. \n\nCompared to standard training, distillation presents us with an interesting setup where models of different sizes interact during the training procedure. This prompts us to ask: how does coupling between models of different sizes during distillation affect, if at all, the memorisation behavior of the resulting student model? Interestingly, while distillation has been shown to yield significant performance gains on average, training accuracy has been shown to be systematically harmed [24]. Previous work also showed how distillation can lead to worsened accuracy on a subset of hard examples [54]. In a related study, model compression has been shown to harm accuracy on tail classes [42]. These observations indeed hint at the potential impact of distillation procedure on student's memorisation behavior, which we systematically explore in this section. \n\nTowards this, we consider knowledge distillation as conducted using logit matching, and where the teacher is trained on the same sub-sample as the student for estimating the memorisation scores per Equation 2. We provide the hyperparameter details in Appendix D. \n\nDistillation inhibits memorisation on average. We begin by investigating what happens to average memorisation under distillation.",
            "reference_string": "[263830209 | Lukasik et al. | 2023 | Citations: 5]"
        },
        {
            "title": "On the Impact of Knowledge Distillation for Model Interpretability",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 58,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.15734",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.15734, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2049039698",
                    "name": "Hyeongrok Han"
                },
                {
                    "authorId": "48388878",
                    "name": "Siwon Kim"
                },
                {
                    "authorId": "3386011",
                    "name": "Hyun-Soo Choi"
                },
                {
                    "authorId": "2999019",
                    "name": "Sungroh Yoon"
                }
            ],
            "abstract": "Several recent studies have elucidated why knowledge distillation (KD) improves model performance. However, few have researched the other advantages of KD in addition to its improving model performance. In this study, we have attempted to show that KD enhances the interpretability as well as the accuracy of models. We measured the number of concept detectors identified in network dissection for a quantitative comparison of model interpretability. We attributed the improvement in interpretability to the class-similarity information transferred from the teacher to student models. First, we confirmed the transfer of class-similarity information from the teacher to student model via logit distillation. Then, we analyzed how class-similarity information affects model interpretability in terms of its presence or absence and degree of similarity information. We conducted various quantitative and qualitative experiments and examined the results on different datasets, different KD methods, and according to different measures of interpretability. Our research showed that KD models by large models could be used more reliably in various fields.",
            "corpus_id": 258887701,
            "sentences": [
                {
                    "corpus_id": "258887701",
                    "title": "On the Impact of Knowledge Distillation for Model Interpretability",
                    "text": "Several recent studies have elucidated why knowledge distillation (KD) improves model performance. However, few have researched the other advantages of KD in addition to its improving model performance. In this study, we have attempted to show that KD enhances the interpretability as well as the accuracy of models. We measured the number of concept detectors identified in network dissection for a quantitative comparison of model interpretability. We attributed the improvement in interpretability to the class-similarity information transferred from the teacher to student models. First, we confirmed the transfer of class-similarity information from the teacher to student model via logit distillation. Then, we analyzed how class-similarity information affects model interpretability in terms of its presence or absence and degree of similarity information. We conducted various quantitative and qualitative experiments and examined the results on different datasets, different KD methods, and according to different measures of interpretability. Our research showed that KD models by large models could be used more reliably in various fields.",
                    "score": 0.5215732502788866,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.921875
                }
            ],
            "relevance_judgement": 0.921875,
            "relevance_judgment_input_expanded": "# Title: On the Impact of Knowledge Distillation for Model Interpretability\n# Venue: International Conference on Machine Learning\n# Authors: Hyeongrok Han, Siwon Kim, Hyun-Soo Choi, Sungroh Yoon\n## Abstract\nSeveral recent studies have elucidated why knowledge distillation (KD) improves model performance. However, few have researched the other advantages of KD in addition to its improving model performance. In this study, we have attempted to show that KD enhances the interpretability as well as the accuracy of models. We measured the number of concept detectors identified in network dissection for a quantitative comparison of model interpretability. We attributed the improvement in interpretability to the class-similarity information transferred from the teacher to student models. First, we confirmed the transfer of class-similarity information from the teacher to student model via logit distillation. Then, we analyzed how class-similarity information affects model interpretability in terms of its presence or absence and degree of similarity information. We conducted various quantitative and qualitative experiments and examined the results on different datasets, different KD methods, and according to different measures of interpretability. Our research showed that KD models by large models could be used more reliably in various fields.\n",
            "reference_string": "[258887701 | Han et al. | 2023 | Citations: 5]"
        },
        {
            "title": "IMF: Integrating Matched Features Using Attentive Logit in Knowledge Distillation",
            "venue": "International Joint Conference on Artificial Intelligence",
            "year": 2023,
            "reference_count": 27,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2023/0108.pdf",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2023/108?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2023/108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2164327036",
                    "name": "Jeongho Kim"
                },
                {
                    "authorId": "2143329283",
                    "name": "Hanbeen Lee"
                },
                {
                    "authorId": "35154708",
                    "name": "Simon S. Woo"
                }
            ],
            "abstract": "Knowledge distillation (KD) is an effective method for transferring the knowledge of a teacher model to a student model, that aims to improve the latter's performance efficiently. Although generic knowledge distillation methods such as softmax representation distillation and intermediate feature matching have demonstrated improvements with various tasks, only marginal improvements are shown in student networks due to their limited model capacity. In this work, to address the student model's limitation, we propose a novel flexible KD framework, Integrating Matched Features using Attentive Logit in Knowledge Distillation (IMF). Our approach introduces an intermediate feature distiller (IFD) to improve the overall performance of the student model by directly distilling the teacher's knowledge into branches of student models. The generated output of IFD, which is trained by the teacher model, is effectively combined by attentive logit. We use only a few blocks of the student and the trained IFD during inference, requiring an equal or less number of parameters. Through extensive experiments, we demonstrate that IMF consistently outperforms other state-of-the-art methods with a large margin over the various datasets in different tasks without extra computation.",
            "corpus_id": 260846373,
            "sentences": [],
            "relevance_judgement": 0.92138671875,
            "relevance_judgment_input_expanded": "# Title: IMF: Integrating Matched Features Using Attentive Logit in Knowledge Distillation\n# Venue: International Joint Conference on Artificial Intelligence\n# Authors: Jeongho Kim, Hanbeen Lee, Simon S. Woo\n## Abstract\nKnowledge distillation (KD) is an effective method for transferring the knowledge of a teacher model to a student model, that aims to improve the latter's performance efficiently. Although generic knowledge distillation methods such as softmax representation distillation and intermediate feature matching have demonstrated improvements with various tasks, only marginal improvements are shown in student networks due to their limited model capacity. In this work, to address the student model's limitation, we propose a novel flexible KD framework, Integrating Matched Features using Attentive Logit in Knowledge Distillation (IMF). Our approach introduces an intermediate feature distiller (IFD) to improve the overall performance of the student model by directly distilling the teacher's knowledge into branches of student models. The generated output of IFD, which is trained by the teacher model, is effectively combined by attentive logit. We use only a few blocks of the student and the trained IFD during inference, requiring an equal or less number of parameters. Through extensive experiments, we demonstrate that IMF consistently outperforms other state-of-the-art methods with a large margin over the various datasets in different tasks without extra computation.\n",
            "reference_string": "[260846373 | Kim et al. | 2023 | Citations: 3]"
        },
        {
            "title": "Creating a Good Teacher for Knowledge Distillation in Acoustic Scene Classification",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 25,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.11363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2103245223",
                    "name": "Tobias Morocutti"
                },
                {
                    "authorId": "2261390466",
                    "name": "Florian Schmid"
                },
                {
                    "authorId": "28921847",
                    "name": "Khaled Koutini"
                },
                {
                    "authorId": "2261390402",
                    "name": "Gerhard Widmer"
                }
            ],
            "abstract": "Knowledge Distillation (KD) is a widespread technique for compressing the knowledge of large models into more compact and efficient models. KD has proved to be highly effective in building well-performing low-complexity Acoustic Scene Classification (ASC) systems and was used in all the top-ranked submissions to this task of the annual DCASE challenge in the past three years. There is extensive research available on establishing the KD process, designing efficient student models, and forming well-performing teacher ensembles. However, less research has been conducted on investigating which teacher model attributes are beneficial for low-complexity students. In this work, we try to close this gap by studying the effects on the student's performance when using different teacher network architectures, varying the teacher model size, training them with different device generalization methods, and applying different ensembling strategies. The results show that teacher model sizes, device generalization methods, the ensembling strategy and the ensemble size are key factors for a well-performing student network.",
            "corpus_id": 277043070,
            "sentences": [],
            "relevance_judgement": 0.92138671875,
            "relevance_judgment_input_expanded": "# Title: Creating a Good Teacher for Knowledge Distillation in Acoustic Scene Classification\n# Venue: arXiv.org\n# Authors: Tobias Morocutti, Florian Schmid, Khaled Koutini, Gerhard Widmer\n## Abstract\nKnowledge Distillation (KD) is a widespread technique for compressing the knowledge of large models into more compact and efficient models. KD has proved to be highly effective in building well-performing low-complexity Acoustic Scene Classification (ASC) systems and was used in all the top-ranked submissions to this task of the annual DCASE challenge in the past three years. There is extensive research available on establishing the KD process, designing efficient student models, and forming well-performing teacher ensembles. However, less research has been conducted on investigating which teacher model attributes are beneficial for low-complexity students. In this work, we try to close this gap by studying the effects on the student's performance when using different teacher network architectures, varying the teacher model size, training them with different device generalization methods, and applying different ensembling strategies. The results show that teacher model sizes, device generalization methods, the ensembling strategy and the ensemble size are key factors for a well-performing student network.\n",
            "reference_string": "[277043070 | Morocutti et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Performance Analysis and Comparison of Knowledge Distillation Among Diverse Teacher and Student Models",
            "venue": "2023 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)",
            "year": 2023,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCE-Asia59966.2023.10326343?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCE-Asia59966.2023.10326343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268630173",
                    "name": "Jaeung Lee"
                },
                {
                    "authorId": "2123471573",
                    "name": "T. Vuong"
                },
                {
                    "authorId": "2268635281",
                    "name": "Jiamu Wang"
                },
                {
                    "authorId": "2268633939",
                    "name": "Jiang Kan"
                },
                {
                    "authorId": "2189380650",
                    "name": "Keunho Byeon"
                },
                {
                    "authorId": "2256462988",
                    "name": "Ju Cheon Lee"
                },
                {
                    "authorId": "2268556136",
                    "name": "Sumin Jung"
                },
                {
                    "authorId": "2146735404",
                    "name": "D. C. Bui"
                },
                {
                    "authorId": "2268724842",
                    "name": "A. Nguyen"
                },
                {
                    "authorId": "2268628029",
                    "name": "Hyun Yang"
                },
                {
                    "authorId": "2260351554",
                    "name": "Jin Tae Kwak"
                }
            ],
            "abstract": "With the advancements in deep learning, the development of large-scale models has become essential for many machine learning tasks in various fields such as computer vision and natural language. Such large-scale models demand extensive computational power and memory, posing challenges for practical deployment on mobile and edge devices. To address these issues, knowledge distillation, a technique for reproducing the performance of large-scale models with a smaller model, has been proposed. This paper investigates the effectiveness of knowledge distillation across various combinations of teach-student model pairs for image classification to deepen the understanding of knowledge distillation and the relationship between large- and small-scale models.",
            "corpus_id": 265486383,
            "sentences": [],
            "relevance_judgement": 0.9208984375,
            "relevance_judgment_input_expanded": "# Title: Performance Analysis and Comparison of Knowledge Distillation Among Diverse Teacher and Student Models\n# Venue: 2023 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)\n# Authors: Jaeung Lee, T. Vuong, Jiamu Wang, Jiang Kan, Keunho Byeon, Ju Cheon Lee, Sumin Jung, D. C. Bui, A. Nguyen, Hyun Yang, Jin Tae Kwak\n## Abstract\nWith the advancements in deep learning, the development of large-scale models has become essential for many machine learning tasks in various fields such as computer vision and natural language. Such large-scale models demand extensive computational power and memory, posing challenges for practical deployment on mobile and edge devices. To address these issues, knowledge distillation, a technique for reproducing the performance of large-scale models with a smaller model, has been proposed. This paper investigates the effectiveness of knowledge distillation across various combinations of teach-student model pairs for image classification to deepen the understanding of knowledge distillation and the relationship between large- and small-scale models.\n",
            "reference_string": "[265486383 | Lee et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07168, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2330192141",
                    "name": "Ra'ul de la Fuente"
                },
                {
                    "authorId": "2047941908",
                    "name": "Luciano Radrig\u00e1n"
                },
                {
                    "authorId": "2330183718",
                    "name": "Anibal S Morales"
                }
            ],
            "abstract": "Mining machinery operating in variable environments faces high wear and unpredictable stress, challenging Predictive Maintenance (PdM). This paper introduces the Edge Sensor Network for Predictive Maintenance (ESN-PdM), a hierarchical inference framework across edge devices, gateways, and cloud services for real-time condition monitoring. The system dynamically adjusts inference locations--on-device, on-gateway, or on-cloud--based on trade-offs among accuracy, latency, and battery life, leveraging Tiny Machine Learning (TinyML) techniques for model optimization on resource-constrained devices. Performance evaluations showed that on-sensor and on-gateway inference modes achieved over 90\\% classification accuracy, while cloud-based inference reached 99\\%. On-sensor inference reduced power consumption by approximately 44\\%, enabling up to 104 hours of operation. Latency was lowest for on-device inference (3.33 ms), increasing when offloading to the gateway (146.67 ms) or cloud (641.71 ms). The ESN-PdM framework provides a scalable, adaptive solution for reliable anomaly detection and PdM, crucial for maintaining machinery uptime in remote environments. By balancing accuracy, latency, and energy consumption, this approach advances PdM frameworks for industrial applications.",
            "corpus_id": 273963558,
            "sentences": [
                {
                    "corpus_id": "273963558",
                    "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network",
                    "text": "Knowledge distillation transfers knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student) [33]. The process takes into account two main factors: the type of knowledge and the distillation scheme. Knowledge types include response-based, where the student mimics the teacher's final predictions; feature-based, where the student learns from the teacher's intermediate layers; and relation-based, where the student learns from the relationships between layers or samples [21]. Distillation schemes include offline distillation, where the teacher is trained first and then guides the student; online distillation, where both models train simultaneously; and self-distillation, where the teacher and student share the same architecture [21], [34]. This method creates lightweight models that can maintain or even surpass the performance of larger models. Figure 1.c illustrates the knowledge distillation process, showing the transfer of knowledge from a teacher model to a student model.",
                    "score": 0.6850232415041992,
                    "section_title": "3) Knowledge Distillation",
                    "char_start_offset": 11001,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 132
                        },
                        {
                            "start": 133,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 510
                        },
                        {
                            "start": 511,
                            "end": 779
                        },
                        {
                            "start": 780,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1020
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 127,
                            "end": 131,
                            "matchedPaperCorpusId": "264109973"
                        },
                        {
                            "start": 505,
                            "end": 509,
                            "matchedPaperCorpusId": "248507674"
                        },
                        {
                            "start": 768,
                            "end": 772,
                            "matchedPaperCorpusId": "248507674"
                        },
                        {
                            "start": 774,
                            "end": 778,
                            "matchedPaperCorpusId": "219559263"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92041015625
                }
            ],
            "relevance_judgement": 0.92041015625,
            "relevance_judgment_input_expanded": "# Title: Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network\n# Venue: arXiv.org\n# Authors: Ra'ul de la Fuente, Luciano Radrig\u00e1n, Anibal S Morales\n## Abstract\nMining machinery operating in variable environments faces high wear and unpredictable stress, challenging Predictive Maintenance (PdM). This paper introduces the Edge Sensor Network for Predictive Maintenance (ESN-PdM), a hierarchical inference framework across edge devices, gateways, and cloud services for real-time condition monitoring. The system dynamically adjusts inference locations--on-device, on-gateway, or on-cloud--based on trade-offs among accuracy, latency, and battery life, leveraging Tiny Machine Learning (TinyML) techniques for model optimization on resource-constrained devices. Performance evaluations showed that on-sensor and on-gateway inference modes achieved over 90\\% classification accuracy, while cloud-based inference reached 99\\%. On-sensor inference reduced power consumption by approximately 44\\%, enabling up to 104 hours of operation. Latency was lowest for on-device inference (3.33 ms), increasing when offloading to the gateway (146.67 ms) or cloud (641.71 ms). The ESN-PdM framework provides a scalable, adaptive solution for reliable anomaly detection and PdM, crucial for maintaining machinery uptime in remote environments. By balancing accuracy, latency, and energy consumption, this approach advances PdM frameworks for industrial applications.\n## 3) Knowledge Distillation\nKnowledge distillation transfers knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student) [33]. The process takes into account two main factors: the type of knowledge and the distillation scheme. Knowledge types include response-based, where the student mimics the teacher's final predictions; feature-based, where the student learns from the teacher's intermediate layers; and relation-based, where the student learns from the relationships between layers or samples [21]. Distillation schemes include offline distillation, where the teacher is trained first and then guides the student; online distillation, where both models train simultaneously; and self-distillation, where the teacher and student share the same architecture [21], [34]. This method creates lightweight models that can maintain or even surpass the performance of larger models. Figure 1.c illustrates the knowledge distillation process, showing the transfer of knowledge from a teacher model to a student model.",
            "reference_string": "[273963558 | Fuente et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Vertical Layering of Quantized Neural Networks for Heterogeneous Inference",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2022,
            "reference_count": 65,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2212.05326",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.05326, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2187214993",
                    "name": "Hai Wu"
                },
                {
                    "authorId": "2053865518",
                    "name": "Ruifei He"
                },
                {
                    "authorId": "3218666",
                    "name": "Hao Hao Tan"
                },
                {
                    "authorId": "50844674",
                    "name": "Xiaojuan Qi"
                },
                {
                    "authorId": "2149775992",
                    "name": "Kaibin Huang"
                }
            ],
            "abstract": "Although considerable progress has been obtained in neural network quantization for efficient inference, existing methods are not scalable to heterogeneous devices as one dedicated model needs to be trained, transmitted, and stored for one specific hardware setting, incurring considerable costs in model training and maintenance. In this paper, we study a new vertical-layered representation of neural network weights for encapsulating all quantized models into a single one. It represents weights as a group of bits (i.e., vertical layers) organized from the most significant bit (also called the basic layer) to less significant bits (i.e., enhance layers). Hence, a neural network with an arbitrary quantization precision can be obtained by adding corresponding enhance layers to the basic layer. However, we empirically find that models obtained with existing quantization methods suffer severe performance degradation if they are adapted to vertical-layered weight representation. To this end, we propose a simple once quantization-aware training (QAT) scheme for obtaining high-performance vertical-layered models. Our design incorporates a cascade downsampling mechanism with the multi-objective optimization employed to train the shared source model weights such that they can be updated simultaneously, considering the performance of all networks. After the model is trained, to construct a vertical-layered network, the lowest bit-width quantized weights become the basic layer, and every bit dropped along the downsampling process act as an enhance layer. Our design is extensively evaluated on CIFAR-100 and ImageNet datasets. Experiments show that the proposed vertical-layered representation and developed once QAT scheme are effective in embodying multiple quantized networks into a single one and allow one-time training, and it delivers comparable performance as that of quantized models tailored to any specific bit-width.",
            "corpus_id": 254564348,
            "sentences": [
                {
                    "corpus_id": "254564348",
                    "title": "Vertical Layering of Quantized Neural Networks for Heterogeneous Inference",
                    "text": "Knowledge distillation transfers the knowledge learned by large models (i.e., teacher) to a compact one (i.e., student) via a weighted combination of original loss using ground-truth labels and distillation loss calculated with the soft output of a teacher model [47]. There is a broad consensus that knowledge distillation can help improve the performance of low-bit-width quantized networks by transferring the knowledge of a full precision network to quantized ones [48], [49], [50]. QKD [49] proposes a quantization-aware knowledge distillation scheme by co-training a low-bit-width student and a larger teacher model. SPEQ [50] introduces a novel self-training paradigm where the high-precision teacher and the low-bit-width student from the same full precision source model are trained collaboratively. In our training process, where different bit-width networks are enclosed, we design a self-knowledge transfer that utilizes a relatively higher precision network's output as soft labels for the cooperative training of the lower precision ones.",
                    "score": 0.5550604263043402,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 14280,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 486
                        },
                        {
                            "start": 487,
                            "end": 622
                        },
                        {
                            "start": 623,
                            "end": 808
                        },
                        {
                            "start": 809,
                            "end": 1052
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 263,
                            "end": 267,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 481,
                            "end": 485,
                            "matchedPaperCorpusId": "222066738"
                        },
                        {
                            "start": 628,
                            "end": 632,
                            "matchedPaperCorpusId": "222066738"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92041015625
                }
            ],
            "relevance_judgement": 0.92041015625,
            "relevance_judgment_input_expanded": "# Title: Vertical Layering of Quantized Neural Networks for Heterogeneous Inference\n# Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence\n# Authors: Hai Wu, Ruifei He, Hao Hao Tan, Xiaojuan Qi, Kaibin Huang\n## Abstract\nAlthough considerable progress has been obtained in neural network quantization for efficient inference, existing methods are not scalable to heterogeneous devices as one dedicated model needs to be trained, transmitted, and stored for one specific hardware setting, incurring considerable costs in model training and maintenance. In this paper, we study a new vertical-layered representation of neural network weights for encapsulating all quantized models into a single one. It represents weights as a group of bits (i.e., vertical layers) organized from the most significant bit (also called the basic layer) to less significant bits (i.e., enhance layers). Hence, a neural network with an arbitrary quantization precision can be obtained by adding corresponding enhance layers to the basic layer. However, we empirically find that models obtained with existing quantization methods suffer severe performance degradation if they are adapted to vertical-layered weight representation. To this end, we propose a simple once quantization-aware training (QAT) scheme for obtaining high-performance vertical-layered models. Our design incorporates a cascade downsampling mechanism with the multi-objective optimization employed to train the shared source model weights such that they can be updated simultaneously, considering the performance of all networks. After the model is trained, to construct a vertical-layered network, the lowest bit-width quantized weights become the basic layer, and every bit dropped along the downsampling process act as an enhance layer. Our design is extensively evaluated on CIFAR-100 and ImageNet datasets. Experiments show that the proposed vertical-layered representation and developed once QAT scheme are effective in embodying multiple quantized networks into a single one and allow one-time training, and it delivers comparable performance as that of quantized models tailored to any specific bit-width.\n## Knowledge Distillation\nKnowledge distillation transfers the knowledge learned by large models (i.e., teacher) to a compact one (i.e., student) via a weighted combination of original loss using ground-truth labels and distillation loss calculated with the soft output of a teacher model [47]. There is a broad consensus that knowledge distillation can help improve the performance of low-bit-width quantized networks by transferring the knowledge of a full precision network to quantized ones [48], [49], [50]. QKD [49] proposes a quantization-aware knowledge distillation scheme by co-training a low-bit-width student and a larger teacher model. SPEQ [50] introduces a novel self-training paradigm where the high-precision teacher and the low-bit-width student from the same full precision source model are trained collaboratively. In our training process, where different bit-width networks are enclosed, we design a self-knowledge transfer that utilizes a relatively higher precision network's output as soft labels for the cooperative training of the lower precision ones.",
            "reference_string": "[254564348 | Wu et al. | 2022 | Citations: 2]"
        },
        {
            "title": "Human-Timescale Adaptation in an Open-Ended Task Space",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 133,
            "citation_count": 119,
            "influential_citation_count": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2301.07608",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.07608, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2202022003",
                    "name": "Adaptive Agent Team"
                },
                {
                    "authorId": "2067935448",
                    "name": "Jakob Bauer"
                },
                {
                    "authorId": "1734809439",
                    "name": "Kate Baumli"
                },
                {
                    "authorId": "70336016",
                    "name": "Satinder Baveja"
                },
                {
                    "authorId": "145124447",
                    "name": "Feryal M. P. Behbahani"
                },
                {
                    "authorId": "7567594",
                    "name": "Avishkar Bhoopchand"
                },
                {
                    "authorId": "1440707913",
                    "name": "N. Bradley-Schmieg"
                },
                {
                    "authorId": "47235561",
                    "name": "Michael Chang"
                },
                {
                    "authorId": "2201776471",
                    "name": "Natalie Clay"
                },
                {
                    "authorId": "69041729",
                    "name": "Adrian Collister"
                },
                {
                    "authorId": "2871171",
                    "name": "Vibhavari Dasagi"
                },
                {
                    "authorId": "2202047544",
                    "name": "Lucy Gonzalez"
                },
                {
                    "authorId": "144717963",
                    "name": "Karol Gregor"
                },
                {
                    "authorId": "37591038",
                    "name": "Edward Hughes"
                },
                {
                    "authorId": "80976942",
                    "name": "Sheleem Kashem"
                },
                {
                    "authorId": "2202022196",
                    "name": "Maria Loks-Thompson"
                },
                {
                    "authorId": "2085293302",
                    "name": "Hannah Openshaw"
                },
                {
                    "authorId": "1410302742",
                    "name": "Jack Parker-Holder"
                },
                {
                    "authorId": "1987123565",
                    "name": "Shreyaan Pathak"
                },
                {
                    "authorId": "1999879303",
                    "name": "Nicolas Perez Nieves"
                },
                {
                    "authorId": "3287144",
                    "name": "Nemanja Rakicevic"
                },
                {
                    "authorId": "2620211",
                    "name": "Tim Rockt\u00e4schel"
                },
                {
                    "authorId": "3403061",
                    "name": "Yannick Schroecker"
                },
                {
                    "authorId": "3407592",
                    "name": "Jakub Sygnowski"
                },
                {
                    "authorId": "2274623",
                    "name": "K. Tuyls"
                },
                {
                    "authorId": "143981350",
                    "name": "Sarah York"
                },
                {
                    "authorId": "2156928090",
                    "name": "Alexander Zacherl"
                },
                {
                    "authorId": "2152836492",
                    "name": "Lei M. Zhang"
                }
            ],
            "abstract": "Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.",
            "corpus_id": 255998274,
            "sentences": [
                {
                    "corpus_id": "255998274",
                    "title": "Human-Timescale Adaptation in an Open-Ended Task Space",
                    "text": "All of the scaling comparison experiments shown in the previous section use an identical distillation teacher for the first frames of training, as detailed in Appendix D.6. Now, we look at the role distillation plays in scaling. In short, we find that kickstarting training with a distillation period is crucial when scaling up model size. As shown in Figure 15, training a 265M parameter Transformer model without distillation results in poor performance compared to a much smaller 23M parameter Transformer trained in the same way. However, when training with distillation from a 23M parameter teacher for the first 4 billion training frames, the 265M model clearly outperforms the 23M variant. See experiment details in Appendix D.11.\n\nAdditionally, we find that even when the model size is the same for both student and teacher, we observe large gains from distillation, for a constant total frame budget ( Figure 16). We speculate that this is due to bad representations learned early on by the student agent (Cetin et al., 2022;Nikishin et al., 2022), which can be avoided by using distillation. This is also consistent with findings in offline RL, where additional data is often required to effectively scale the model (Reid et al., 2022). The effect is largest for the first round of distillation, with diminishing returns in subsequent rounds of distillation ( Figure E.5).",
                    "score": 0.5230192666336384,
                    "section_title": "Distillation improves performance and enables scaling agents",
                    "char_start_offset": 40779,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92041015625
                }
            ],
            "relevance_judgement": 0.92041015625,
            "relevance_judgment_input_expanded": "# Title: Human-Timescale Adaptation in an Open-Ended Task Space\n# Venue: International Conference on Machine Learning\n# Authors: Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal M. P. Behbahani, Avishkar Bhoopchand, N. Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson, Hannah Openshaw, Jack Parker-Holder, Shreyaan Pathak, Nicolas Perez Nieves, Nemanja Rakicevic, Tim Rockt\u00e4schel, Yannick Schroecker, Jakub Sygnowski, K. Tuyls, Sarah York, Alexander Zacherl, Lei M. Zhang\n## Abstract\nFoundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.\n## Distillation improves performance and enables scaling agents\nAll of the scaling comparison experiments shown in the previous section use an identical distillation teacher for the first frames of training, as detailed in Appendix D.6. Now, we look at the role distillation plays in scaling. In short, we find that kickstarting training with a distillation period is crucial when scaling up model size. As shown in Figure 15, training a 265M parameter Transformer model without distillation results in poor performance compared to a much smaller 23M parameter Transformer trained in the same way. However, when training with distillation from a 23M parameter teacher for the first 4 billion training frames, the 265M model clearly outperforms the 23M variant. See experiment details in Appendix D.11.\n\nAdditionally, we find that even when the model size is the same for both student and teacher, we observe large gains from distillation, for a constant total frame budget ( Figure 16). We speculate that this is due to bad representations learned early on by the student agent (Cetin et al., 2022;Nikishin et al., 2022), which can be avoided by using distillation. This is also consistent with findings in offline RL, where additional data is often required to effectively scale the model (Reid et al., 2022). The effect is largest for the first round of distillation, with diminishing returns in subsequent rounds of distillation ( Figure E.5).",
            "reference_string": "[255998274 | Team et al. | 2023 | Citations: 119]"
        },
        {
            "title": "Multi-Level Feature Distillation of Joint Teachers Trained on Distinct Image Datasets",
            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
            "year": 2024,
            "reference_count": 51,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.22184, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2328085900",
                    "name": "Adrian Iordache"
                },
                {
                    "authorId": "2365442",
                    "name": "B. Alexe"
                },
                {
                    "authorId": "2249763264",
                    "name": "R. Ionescu"
                }
            ],
            "abstract": "We propose a novel teacher-student framework to distill knowledge from multiple teachers trained on distinct datasets. Each teacher is first trained from scratch on its own dataset. Then, the teachers are combined into a joint architecture, which fuses the features of all teachers at multiple representation levels. The joint teacher architecture is fine-tuned on samples from all datasets, thus gathering useful generic information from all data samples. Finally, we employ a multi-level feature distillation procedure to transfer the knowledge to a student model for each of the considered datasets. We conduct image classification experiments on seven benchmarks, and action recognition experiments on three benchmarks. To illustrate the power of our feature distillation procedure, the student architectures are chosen to be identical to those of the individual teachers. To demonstrate the flexibility of our approach, we combine teachers with distinct architectures. We show that our novel Multi-Level Feature Distillation (MLFD) can significantly surpass equivalent architectures that are either trained on individual datasets, or jointly trained on all datasets at once. Furthermore, we confirm that each step of the proposed training procedure is well motivated by a comprehensive ablation study. We publicly release our code at https://github.com/AdrianIordache/MLFD.",
            "corpus_id": 273662110,
            "sentences": [
                {
                    "corpus_id": "273662110",
                    "title": "Multi-Level Feature Distillation of Joint Teachers Trained on Distinct Image Datasets",
                    "text": "Knowledge distillation. The goal of Knowledge Distillation (KD) is to transfer knowledge from a large teacher model to a small student model, such that the obtained student model mimics the behavior of the larger model [4,5,15,17,31,34]. The work of [15] defines the knowledge in the form of the output of the large model (logits). \n\nThe basic form of knowledge distillation consists in training the small student model to reproduce the logits of the large teacher model. This can be done either in the unsupervised [1,22] or supervised [26] scenario. In the supervised scenario, with ground-truth labels available at training time, Hinton et al. [15] show that significant improvements can be obtained by minimizing an objective function that takes into account the cross-entropy between the logits of the two models, but also another term that enforces the student model to predict ground-truth labels. Other works [3,39,51] consider knowledge at the feature level, employing distillation by matching feature distributions as well as logit outputs between the teacher and student models. Romero et al. penalize structural differences in relations. Li [33] reuses channel-wise and layer-wise meaningful features within the student to provide teacher-like knowledge without an additional model, in a teacher-free feature distillation framework. Liu et al. [29] propose a two-stage knowledge distillation method, which relies on a simple Feature Transform module consisting of two linear layers. Our proposed multilevel feature distillation method considers knowledge both at the logits level and feature level, but, unlike other approaches, it uses multiple teachers, each of them trained on a distinct dataset. Usually, knowledge distillation is applied for model compression [15], where the teacher model has a much larger capacity and memory footprint w.r.t. the student model. Our method allows student architectures to be identical to those of the individual teachers. Moreover, our method is very flexible, since it can combine teachers with distinct architectures.",
                    "score": 0.5918338486782901,
                    "section_title": "Related Work",
                    "char_start_offset": 3719,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 23
                        },
                        {
                            "start": 24,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 331
                        },
                        {
                            "start": 334,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 551
                        },
                        {
                            "start": 552,
                            "end": 904
                        },
                        {
                            "start": 905,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1149
                        },
                        {
                            "start": 1150,
                            "end": 1344
                        },
                        {
                            "start": 1345,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1711
                        },
                        {
                            "start": 1712,
                            "end": 1880
                        },
                        {
                            "start": 1881,
                            "end": 1973
                        },
                        {
                            "start": 1974,
                            "end": 2071
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 219,
                            "end": 222,
                            "matchedPaperCorpusId": "212633769"
                        },
                        {
                            "start": 222,
                            "end": 224,
                            "matchedPaperCorpusId": "203642130"
                        },
                        {
                            "start": 233,
                            "end": 236,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 516,
                            "end": 519,
                            "matchedPaperCorpusId": "251800257"
                        },
                        {
                            "start": 519,
                            "end": 522,
                            "matchedPaperCorpusId": "267022515"
                        },
                        {
                            "start": 537,
                            "end": 541,
                            "matchedPaperCorpusId": "257771519"
                        },
                        {
                            "start": 917,
                            "end": 920,
                            "matchedPaperCorpusId": "233296935"
                        },
                        {
                            "start": 920,
                            "end": 923,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 923,
                            "end": 925,
                            "matchedPaperCorpusId": "221559239"
                        },
                        {
                            "start": 1153,
                            "end": 1157,
                            "matchedPaperCorpusId": "253270253"
                        },
                        {
                            "start": 1356,
                            "end": 1360,
                            "matchedPaperCorpusId": "258841675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.919921875
                }
            ],
            "relevance_judgement": 0.919921875,
            "relevance_judgment_input_expanded": "# Title: Multi-Level Feature Distillation of Joint Teachers Trained on Distinct Image Datasets\n# Venue: IEEE Workshop/Winter Conference on Applications of Computer Vision\n# Authors: Adrian Iordache, B. Alexe, R. Ionescu\n## Abstract\nWe propose a novel teacher-student framework to distill knowledge from multiple teachers trained on distinct datasets. Each teacher is first trained from scratch on its own dataset. Then, the teachers are combined into a joint architecture, which fuses the features of all teachers at multiple representation levels. The joint teacher architecture is fine-tuned on samples from all datasets, thus gathering useful generic information from all data samples. Finally, we employ a multi-level feature distillation procedure to transfer the knowledge to a student model for each of the considered datasets. We conduct image classification experiments on seven benchmarks, and action recognition experiments on three benchmarks. To illustrate the power of our feature distillation procedure, the student architectures are chosen to be identical to those of the individual teachers. To demonstrate the flexibility of our approach, we combine teachers with distinct architectures. We show that our novel Multi-Level Feature Distillation (MLFD) can significantly surpass equivalent architectures that are either trained on individual datasets, or jointly trained on all datasets at once. Furthermore, we confirm that each step of the proposed training procedure is well motivated by a comprehensive ablation study. We publicly release our code at https://github.com/AdrianIordache/MLFD.\n## Related Work\nKnowledge distillation. The goal of Knowledge Distillation (KD) is to transfer knowledge from a large teacher model to a small student model, such that the obtained student model mimics the behavior of the larger model [4,5,15,17,31,34]. The work of [15] defines the knowledge in the form of the output of the large model (logits). \n\nThe basic form of knowledge distillation consists in training the small student model to reproduce the logits of the large teacher model. This can be done either in the unsupervised [1,22] or supervised [26] scenario. In the supervised scenario, with ground-truth labels available at training time, Hinton et al. [15] show that significant improvements can be obtained by minimizing an objective function that takes into account the cross-entropy between the logits of the two models, but also another term that enforces the student model to predict ground-truth labels. Other works [3,39,51] consider knowledge at the feature level, employing distillation by matching feature distributions as well as logit outputs between the teacher and student models. Romero et al. penalize structural differences in relations. Li [33] reuses channel-wise and layer-wise meaningful features within the student to provide teacher-like knowledge without an additional model, in a teacher-free feature distillation framework. Liu et al. [29] propose a two-stage knowledge distillation method, which relies on a simple Feature Transform module consisting of two linear layers. Our proposed multilevel feature distillation method considers knowledge both at the logits level and feature level, but, unlike other approaches, it uses multiple teachers, each of them trained on a distinct dataset. Usually, knowledge distillation is applied for model compression [15], where the teacher model has a much larger capacity and memory footprint w.r.t. the student model. Our method allows student architectures to be identical to those of the individual teachers. Moreover, our method is very flexible, since it can combine teachers with distinct architectures.",
            "reference_string": "[273662110 | Iordache et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Task Integration Distillation for Object Detectors",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01699, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152173244",
                    "name": "Hai Su"
                },
                {
                    "authorId": "2294574600",
                    "name": "Zhenwen Jian"
                },
                {
                    "authorId": "2112454661",
                    "name": "Songsen Yu"
                }
            ],
            "abstract": "Knowledge distillation is a widely adopted technique for model lightening. However, the performance of most knowledge distillation methods in the domain of object detection is not satisfactory. Typically, knowledge distillation approaches consider only the classification task among the two sub-tasks of an object detector, largely overlooking the regression task. This oversight leads to a partial understanding of the object detector's comprehensive task, resulting in skewed estimations and potentially adverse effects. Therefore, we propose a knowledge distillation method that addresses both the classification and regression tasks, incorporating a task significance strategy. By evaluating the importance of features based on the output of the detector's two sub-tasks, our approach ensures a balanced consideration of both classification and regression tasks in object detection. Drawing inspiration from real-world teaching processes and the definition of learning condition, we introduce a method that focuses on both key and weak areas. By assessing the value of features for knowledge distillation based on their importance differences, we accurately capture the current model's learning situation. This method effectively prevents the issue of biased predictions about the model's learning reality caused by an incomplete utilization of the detector's outputs.",
            "corpus_id": 268857025,
            "sentences": [
                {
                    "corpus_id": "268857025",
                    "title": "Task Integration Distillation for Object Detectors",
                    "text": "Knowledge distillation is an effective model compression technique that facilitates the transfer of knowledge from a large model to a smaller one, enabling the smaller model to achieve, or even approximate, the performance of the larger model.Initially introduced by Hinton et al. [12], knowledge distillation has been predominantly applied in the image classification domain.The technique primarily transfers \"dark knowledge\" to the student model through the soft labels of the teacher model.To smoothly extract this \"dark knowledge\", a hyperparameter known as temperature is introduced.\n\nKnowledge distillation was initially proposed in the domain of image classification and has since been widely applied across various fields.Currently, knowledge distillation can be categorized into three types based on the source of knowledge: response-based knowledge distillation [3,5,22,35,42], relation-based knowledge distillation [23,24], and feature-based knowledge distillation [10,11,13,14].Response-based knowledge distillation, the earliest proposed method, involves extracting the output of the teacher network's last layer and directly mimicking the teacher's final prediction.Relation-based knowledge distillation extracts relationships between different layers or data samples within the teacher network, transferring such relational knowledge to the student to achieve distillation.Feature-based knowledge distillation involves extracting and aligning feature maps from both the teacher and student models to the same size, then having the student model's feature maps fit those of the teacher model, facilitating the transfer of knowledge.Due to its ease of handling knowledge sources and almost uniform performance across most domains, feature-based knowledge distillation has garnered widespread attention from scholars for its strong versatility.\n\nKnowledge distillation methods are not only widely applied in the field of computer vision, such as image retrieval [34] and face recognition [17], but also in natural language processing [20,30], speech recognition [15], recommendation systems [28], information security [44], multimodal data [43], and finance and securities [8].Knowledge distillation, with its capability to transfer knowledge between different models, has garnered significant attention across multiple domains due to its versatility.",
                    "score": 0.5328204401026782,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 7226,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 243
                        },
                        {
                            "start": 243,
                            "end": 376
                        },
                        {
                            "start": 376,
                            "end": 493
                        },
                        {
                            "start": 493,
                            "end": 588
                        },
                        {
                            "start": 590,
                            "end": 730
                        },
                        {
                            "start": 730,
                            "end": 990
                        },
                        {
                            "start": 990,
                            "end": 1180
                        },
                        {
                            "start": 1180,
                            "end": 1388
                        },
                        {
                            "start": 1388,
                            "end": 1646
                        },
                        {
                            "start": 1646,
                            "end": 1856
                        },
                        {
                            "start": 1858,
                            "end": 2189
                        },
                        {
                            "start": 2189,
                            "end": 2363
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 872,
                            "end": 875,
                            "matchedPaperCorpusId": "203642130"
                        },
                        {
                            "start": 875,
                            "end": 877,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 877,
                            "end": 880,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 880,
                            "end": 883,
                            "matchedPaperCorpusId": "54436113"
                        },
                        {
                            "start": 883,
                            "end": 886,
                            "matchedPaperCorpusId": "26071966"
                        },
                        {
                            "start": 926,
                            "end": 930,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 930,
                            "end": 933,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 976,
                            "end": 980,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 980,
                            "end": 983,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 986,
                            "end": 989,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 1974,
                            "end": 1978,
                            "matchedPaperCorpusId": "250581330"
                        },
                        {
                            "start": 2000,
                            "end": 2004,
                            "matchedPaperCorpusId": "260068567"
                        },
                        {
                            "start": 2074,
                            "end": 2078,
                            "matchedPaperCorpusId": "235417286"
                        },
                        {
                            "start": 2103,
                            "end": 2107,
                            "matchedPaperCorpusId": "2552056"
                        },
                        {
                            "start": 2130,
                            "end": 2134,
                            "matchedPaperCorpusId": "245693198"
                        },
                        {
                            "start": 2152,
                            "end": 2156,
                            "matchedPaperCorpusId": "237490408"
                        },
                        {
                            "start": 2185,
                            "end": 2188,
                            "matchedPaperCorpusId": "203605465"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91748046875
                }
            ],
            "relevance_judgement": 0.91748046875,
            "relevance_judgment_input_expanded": "# Title: Task Integration Distillation for Object Detectors\n# Venue: arXiv.org\n# Authors: Hai Su, Zhenwen Jian, Songsen Yu\n## Abstract\nKnowledge distillation is a widely adopted technique for model lightening. However, the performance of most knowledge distillation methods in the domain of object detection is not satisfactory. Typically, knowledge distillation approaches consider only the classification task among the two sub-tasks of an object detector, largely overlooking the regression task. This oversight leads to a partial understanding of the object detector's comprehensive task, resulting in skewed estimations and potentially adverse effects. Therefore, we propose a knowledge distillation method that addresses both the classification and regression tasks, incorporating a task significance strategy. By evaluating the importance of features based on the output of the detector's two sub-tasks, our approach ensures a balanced consideration of both classification and regression tasks in object detection. Drawing inspiration from real-world teaching processes and the definition of learning condition, we introduce a method that focuses on both key and weak areas. By assessing the value of features for knowledge distillation based on their importance differences, we accurately capture the current model's learning situation. This method effectively prevents the issue of biased predictions about the model's learning reality caused by an incomplete utilization of the detector's outputs.\n## Knowledge distillation\nKnowledge distillation is an effective model compression technique that facilitates the transfer of knowledge from a large model to a smaller one, enabling the smaller model to achieve, or even approximate, the performance of the larger model.Initially introduced by Hinton et al. [12], knowledge distillation has been predominantly applied in the image classification domain.The technique primarily transfers \"dark knowledge\" to the student model through the soft labels of the teacher model.To smoothly extract this \"dark knowledge\", a hyperparameter known as temperature is introduced.\n\nKnowledge distillation was initially proposed in the domain of image classification and has since been widely applied across various fields.Currently, knowledge distillation can be categorized into three types based on the source of knowledge: response-based knowledge distillation [3,5,22,35,42], relation-based knowledge distillation [23,24], and feature-based knowledge distillation [10,11,13,14].Response-based knowledge distillation, the earliest proposed method, involves extracting the output of the teacher network's last layer and directly mimicking the teacher's final prediction.Relation-based knowledge distillation extracts relationships between different layers or data samples within the teacher network, transferring such relational knowledge to the student to achieve distillation.Feature-based knowledge distillation involves extracting and aligning feature maps from both the teacher and student models to the same size, then having the student model's feature maps fit those of the teacher model, facilitating the transfer of knowledge.Due to its ease of handling knowledge sources and almost uniform performance across most domains, feature-based knowledge distillation has garnered widespread attention from scholars for its strong versatility.\n\nKnowledge distillation methods are not only widely applied in the field of computer vision, such as image retrieval [34] and face recognition [17], but also in natural language processing [20,30], speech recognition [15], recommendation systems [28], information security [44], multimodal data [43], and finance and securities [8].Knowledge distillation, with its capability to transfer knowledge between different models, has garnered significant attention across multiple domains due to its versatility.",
            "reference_string": "[268857025 | Su et al. | 2024 | Citations: 1]"
        },
        {
            "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning",
            "venue": "IEEE Access",
            "year": 2023,
            "reference_count": 176,
            "citation_count": 43,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10433185.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.11932, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135267479",
                    "name": "Luigi Capogrosso"
                },
                {
                    "authorId": "1396330675",
                    "name": "Federico Cunico"
                },
                {
                    "authorId": "1780197",
                    "name": "D. Cheng"
                },
                {
                    "authorId": "2243336023",
                    "name": "Franco Fummi"
                },
                {
                    "authorId": "2238815087",
                    "name": "Marco Cristani"
                }
            ],
            "abstract": "The emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly, we will examine the three different workflows for implementing a TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly, we propose a taxonomy that covers the learning panorama under the TinyML lens, examining in detail the different families of model optimization and design, as well as the state-of-the-art learning techniques. Thirdly, this survey will present the distinct features of hardware devices and software tools that represent the current state-of-the-art for TinyML intelligent edge applications. Finally, we discuss the challenges and future directions.",
            "corpus_id": 262084420,
            "sentences": [
                {
                    "corpus_id": "262084420",
                    "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning",
                    "text": "This technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) [84]. This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies. \n\nThe extraction of knowledge from teachers and its utilization for training student networks can be classified into three categories: response-based, feature-based, and relationbased. Specifically, response-based knowledge distillation involves mimicking the final predictions of the teacher model by capturing the neural response in the last output layer [87]. Feature-based knowledge expands upon this approach by using both the outputs of the last layer and intermediate layers to train thinner networks [81]. Finally, relation-based knowledge takes a step further by exploring the relationships between different layers or data samples in addition to the outputs of specific layers in the teacher model [86]. \n\nThe distillation schemes are also crucial for the student learning process. Depending on the training strategy, the following three different categories are presented: offline distillation, online distillation, self-distillation. Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits [80]. On the other hand, online distillation is an end-to-end approach where both the teacher and student models are updated simultaneously, making it suitable when the teacher model is not significantly larger or higher performing [85]. Finally, self-distillation is a special case of online distillation where the teacher and student networks have the same architecture [79]. \n\nIn general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy [88]. For this reason, it is widely adopted in several fields where existing models are well-performing but unable to be deployed ''as they are'' in resource-constrained hardware.",
                    "score": 0.6206902286892579,
                    "section_title": "3) Knowledge Distillation",
                    "char_start_offset": 28218,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 252
                        },
                        {
                            "start": 253,
                            "end": 397
                        },
                        {
                            "start": 398,
                            "end": 500
                        },
                        {
                            "start": 503,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 1014
                        },
                        {
                            "start": 1015,
                            "end": 1214
                        },
                        {
                            "start": 1217,
                            "end": 1292
                        },
                        {
                            "start": 1293,
                            "end": 1446
                        },
                        {
                            "start": 1447,
                            "end": 1685
                        },
                        {
                            "start": 1686,
                            "end": 1917
                        },
                        {
                            "start": 1918,
                            "end": 2057
                        },
                        {
                            "start": 2060,
                            "end": 2188
                        },
                        {
                            "start": 2189,
                            "end": 2362
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 111,
                            "end": 115,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 858,
                            "end": 862,
                            "matchedPaperCorpusId": "232104927"
                        },
                        {
                            "start": 1009,
                            "end": 1013,
                            "matchedPaperCorpusId": "235613518"
                        },
                        {
                            "start": 1209,
                            "end": 1213,
                            "matchedPaperCorpusId": "239486869"
                        },
                        {
                            "start": 1680,
                            "end": 1684,
                            "matchedPaperCorpusId": "198179767"
                        },
                        {
                            "start": 1912,
                            "end": 1916,
                            "matchedPaperCorpusId": "224914013"
                        },
                        {
                            "start": 2052,
                            "end": 2056,
                            "matchedPaperCorpusId": "214727822"
                        },
                        {
                            "start": 2183,
                            "end": 2187,
                            "matchedPaperCorpusId": "255266316"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9169921875
                }
            ],
            "relevance_judgement": 0.9169921875,
            "relevance_judgment_input_expanded": "# Title: A Machine Learning-Oriented Survey on Tiny Machine Learning\n# Venue: IEEE Access\n# Authors: Luigi Capogrosso, Federico Cunico, D. Cheng, Franco Fummi, Marco Cristani\n## Abstract\nThe emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly, we will examine the three different workflows for implementing a TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly, we propose a taxonomy that covers the learning panorama under the TinyML lens, examining in detail the different families of model optimization and design, as well as the state-of-the-art learning techniques. Thirdly, this survey will present the distinct features of hardware devices and software tools that represent the current state-of-the-art for TinyML intelligent edge applications. Finally, we discuss the challenges and future directions.\n## 3) Knowledge Distillation\nThis technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) [84]. This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies. \n\nThe extraction of knowledge from teachers and its utilization for training student networks can be classified into three categories: response-based, feature-based, and relationbased. Specifically, response-based knowledge distillation involves mimicking the final predictions of the teacher model by capturing the neural response in the last output layer [87]. Feature-based knowledge expands upon this approach by using both the outputs of the last layer and intermediate layers to train thinner networks [81]. Finally, relation-based knowledge takes a step further by exploring the relationships between different layers or data samples in addition to the outputs of specific layers in the teacher model [86]. \n\nThe distillation schemes are also crucial for the student learning process. Depending on the training strategy, the following three different categories are presented: offline distillation, online distillation, self-distillation. Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits [80]. On the other hand, online distillation is an end-to-end approach where both the teacher and student models are updated simultaneously, making it suitable when the teacher model is not significantly larger or higher performing [85]. Finally, self-distillation is a special case of online distillation where the teacher and student networks have the same architecture [79]. \n\nIn general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy [88]. For this reason, it is widely adopted in several fields where existing models are well-performing but unable to be deployed ''as they are'' in resource-constrained hardware.",
            "reference_string": "[262084420 | Capogrosso et al. | 2023 | Citations: 43]"
        },
        {
            "title": "Revisiting Knowledge Distillation for Autoregressive Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 19,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2114810150",
                    "name": "Qihuang Zhong"
                },
                {
                    "authorId": "46573238",
                    "name": "Liang Ding"
                },
                {
                    "authorId": "2248152216",
                    "name": "Li Shen"
                },
                {
                    "authorId": "46701032",
                    "name": "Juhua Liu"
                },
                {
                    "authorId": "2212029373",
                    "name": "Bo Du"
                },
                {
                    "authorId": "2255502438",
                    "name": "D. Tao"
                }
            ],
            "abstract": "Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD. The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes. More encouragingly, ATKD can improve the student model generalization effectively.",
            "corpus_id": 267750520,
            "sentences": [],
            "relevance_judgement": 0.91650390625,
            "relevance_judgment_input_expanded": "# Title: Revisiting Knowledge Distillation for Autoregressive Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Qihuang Zhong, Liang Ding, Li Shen, Juhua Liu, Bo Du, D. Tao\n## Abstract\nKnowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD. The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes. More encouragingly, ATKD can improve the student model generalization effectively.\n",
            "reference_string": "[267750520 | Zhong et al. | 2024 | Citations: 19]"
        },
        {
            "title": "Mamba base PKD for efficient knowledge compression",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 22,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348480395",
                    "name": "Jos\u00e9 Medina"
                },
                {
                    "authorId": "3221342",
                    "name": "Amnir Hadachi"
                },
                {
                    "authorId": "2256850325",
                    "name": "Paul Honeine"
                },
                {
                    "authorId": "2314690317",
                    "name": "Abdelaziz Bensrhair"
                }
            ],
            "abstract": "Deep neural networks (DNNs) have remarkably succeeded in various image processing tasks. However, their large size and computational complexity present significant challenges for deploying them in resource-constrained environments. This paper presents an innovative approach for integrating Mamba Architecture within a Progressive Knowledge Distillation (PKD) process to address the challenge of reducing model complexity while maintaining accuracy in image classification tasks. The proposed framework distills a large teacher model into progressively smaller student models, designed using Mamba blocks. Each student model is trained using Selective-State-Space Models (S-SSM) within the Mamba blocks, focusing on important input aspects while reducing computational complexity. The work's preliminary experiments use MNIST and CIFAR-10 as datasets to demonstrate the effectiveness of this approach. For MNIST, the teacher model achieves 98% accuracy. A set of seven student models as a group retained 63% of the teacher's FLOPs, approximating the teacher's performance with 98% accuracy. The weak student used only 1% of the teacher's FLOPs and maintained 72% accuracy. Similarly, for CIFAR-10, the students achieved 1% less accuracy compared to the teacher, with the small student retaining 5% of the teacher's FLOPs to achieve 50% accuracy. These results confirm the flexibility and scalability of Mamba Architecture, which can be integrated into PKD, succeeding in the process of finding students as weak learners. The framework provides a solution for deploying complex neural networks in real-time applications with a reduction in computational cost.",
            "corpus_id": 276776153,
            "sentences": [
                {
                    "corpus_id": "276776153",
                    "title": "Mamba base PKD for efficient knowledge compression",
                    "text": "Knowledge distillation has been extensively studied as a model compression technique that allows a smaller student model to learn from a larger teacher model [5,9] (see Figure 1). Traditional KD includes transferring the knowledge to a student model from a teacher model's output [2], intermediate layers [16], or only the relationships between different layers and data samples [15,21]. These techniques lead to computational savings while preserving high accuracy. However, one limitation of traditional KD is the performance drop when there is a large capacity gap between the teacher and student models because the student cannot effectively represent key features of the teacher. This limitation has been addressed by various methods [15,16,21,22] that modify distillation algorithms or employ multi-stage learning. \n\nTo overcome these challenges, hierarchical or layer-wise distillation approaches were proposed, where knowledge is progressively transferred across multiple layers of the teacher to the student. FitNets, introduced in [16], transfer intermediate representations from the teacher model to the student, improving the student's ability to learn fine-grained features progressively. Similarly, selfdistillation [22] trains a model by distilling knowledge into itself at different stages, a concept related to progressive distillation where multiple students incrementally improve performance [4]. \n\n. . .",
                    "score": 0.6098977361374206,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 5819,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 466
                        },
                        {
                            "start": 467,
                            "end": 684
                        },
                        {
                            "start": 685,
                            "end": 820
                        },
                        {
                            "start": 823,
                            "end": 1017
                        },
                        {
                            "start": 1018,
                            "end": 1201
                        },
                        {
                            "start": 1202,
                            "end": 1415
                        },
                        {
                            "start": 1418,
                            "end": 1423
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 158,
                            "end": 161,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 379,
                            "end": 383,
                            "matchedPaperCorpusId": "229679667"
                        },
                        {
                            "start": 383,
                            "end": 386,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 739,
                            "end": 743,
                            "matchedPaperCorpusId": "229679667"
                        },
                        {
                            "start": 746,
                            "end": 749,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 749,
                            "end": 752,
                            "matchedPaperCorpusId": "159041406"
                        },
                        {
                            "start": 1230,
                            "end": 1234,
                            "matchedPaperCorpusId": "159041406"
                        },
                        {
                            "start": 1411,
                            "end": 1414,
                            "matchedPaperCorpusId": "265067529"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.916015625
                }
            ],
            "relevance_judgement": 0.916015625,
            "relevance_judgment_input_expanded": "# Title: Mamba base PKD for efficient knowledge compression\n# Venue: arXiv.org\n# Authors: Jos\u00e9 Medina, Amnir Hadachi, Paul Honeine, Abdelaziz Bensrhair\n## Abstract\nDeep neural networks (DNNs) have remarkably succeeded in various image processing tasks. However, their large size and computational complexity present significant challenges for deploying them in resource-constrained environments. This paper presents an innovative approach for integrating Mamba Architecture within a Progressive Knowledge Distillation (PKD) process to address the challenge of reducing model complexity while maintaining accuracy in image classification tasks. The proposed framework distills a large teacher model into progressively smaller student models, designed using Mamba blocks. Each student model is trained using Selective-State-Space Models (S-SSM) within the Mamba blocks, focusing on important input aspects while reducing computational complexity. The work's preliminary experiments use MNIST and CIFAR-10 as datasets to demonstrate the effectiveness of this approach. For MNIST, the teacher model achieves 98% accuracy. A set of seven student models as a group retained 63% of the teacher's FLOPs, approximating the teacher's performance with 98% accuracy. The weak student used only 1% of the teacher's FLOPs and maintained 72% accuracy. Similarly, for CIFAR-10, the students achieved 1% less accuracy compared to the teacher, with the small student retaining 5% of the teacher's FLOPs to achieve 50% accuracy. These results confirm the flexibility and scalability of Mamba Architecture, which can be integrated into PKD, succeeding in the process of finding students as weak learners. The framework provides a solution for deploying complex neural networks in real-time applications with a reduction in computational cost.\n## Knowledge Distillation\nKnowledge distillation has been extensively studied as a model compression technique that allows a smaller student model to learn from a larger teacher model [5,9] (see Figure 1). Traditional KD includes transferring the knowledge to a student model from a teacher model's output [2], intermediate layers [16], or only the relationships between different layers and data samples [15,21]. These techniques lead to computational savings while preserving high accuracy. However, one limitation of traditional KD is the performance drop when there is a large capacity gap between the teacher and student models because the student cannot effectively represent key features of the teacher. This limitation has been addressed by various methods [15,16,21,22] that modify distillation algorithms or employ multi-stage learning. \n\nTo overcome these challenges, hierarchical or layer-wise distillation approaches were proposed, where knowledge is progressively transferred across multiple layers of the teacher to the student. FitNets, introduced in [16], transfer intermediate representations from the teacher model to the student, improving the student's ability to learn fine-grained features progressively. Similarly, selfdistillation [22] trains a model by distilling knowledge into itself at different stages, a concept related to progressive distillation where multiple students incrementally improve performance [4]. \n\n. . .",
            "reference_string": "[276776153 | Medina et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Structured Knowledge Distillation for Accurate and Efficient Object Detection",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2023,
            "reference_count": 137,
            "citation_count": 27,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/34/4359286/10198386.pdf",
                "status": "HYBRID",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TPAMI.2023.3300470?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TPAMI.2023.3300470, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50081570",
                    "name": "Linfeng Zhang"
                },
                {
                    "authorId": "2075321204",
                    "name": "Kaisheng Ma"
                }
            ],
            "abstract": "Knowledge distillation, which aims to transfer the knowledge learned by a cumbersome teacher model to a lightweight student model, has become one of the most popular and effective techniques in computer vision. However, many previous knowledge distillation methods are designed for image classification and fail in more challenging tasks such as object detection. In this paper, we first suggest that the failure of knowledge distillation on object detection is mainly caused by two reasons: (1) the imbalance between pixels of foreground and background and (2) lack of knowledge distillation on the relation among different pixels. Then, we propose a structured knowledge distillation scheme, including attention-guided distillation and non-local distillation to address the two issues, respectively. Attention-guided distillation is proposed to find the crucial pixels of foreground objects with an attention mechanism and then make the students take more effort to learn their features. Non-local distillation is proposed to enable students to learn not only the feature of an individual pixel but also the relation between different pixels captured by non-local modules. Experimental results have demonstrated the effectiveness of our method on thirteen kinds of object detection models with twelve comparison methods for both object detection and instance segmentation. For instance, Faster RCNN with our distillation achieves 43.9 mAP on MS COCO2017, which is 4.1 higher than the baseline. Additionally, we show that our method is also beneficial to the robustness and domain generalization ability of detectors. Codes and model weights have been released on GitHub1.",
            "corpus_id": 260378128,
            "sentences": [
                {
                    "corpus_id": "260378128",
                    "title": "Structured Knowledge Distillation for Accurate and Efficient Object Detection",
                    "text": "Traditional knowldistillation follows a two-stage training pipeline, where large teacher model is initially trained and then distilled to a lightweight student model. Recently, have proposed a one-stage knowledge distillation approach called online knowledge distillation or deep mutual learning. These methods involve training two or multiple student models and distilling their knowledge to each other. In this subsection, we compare the effectiveness of our method in both the two-stage and one-stage knowledge distillation frameworks. Experiments are conducted on MS COCO2017 using Faster RCNN students with ResNet50 backbones. For onestage knowledge distillation experiments, we adopt the deep mutual learning approach and train two student detectors to mimic each other using our methods. For two-stage knowledge distillation experiments, we explore the following two schemes. \n\nr Strong Teacher Scheme: distilling knowledge from a pre- trained Cascade Mask RCNN teacher with ResNetX101-DCN backbone, which achieves 47.3 AP. \n\nr Weak Teacher Scheme: distilling knowledge from a pre- trained teacher which has the identical architecture with the student (i.e., Faster RCNN with ResNet50 backbone) and achieves 38.4 AP. Experimental results are shown in Table X. It is observed that one-stage knowledge distillation leads to 2.3 AP improvements over the baseline, which is 0.2 higher than two-stage knowledge distillation with a weak teacher, but still 0.8 AP lower than the two-stage knowledge distillation with a strong teacher. This observation indicates that one-stage knowledge distillation can achieve comparable and even better performance than two-stage knowledge distillation when no powerful teacher is available. Moreover, two-stage knowledge distillation can make more use of the knowledge from a powerful teacher and achieve better performance.",
                    "score": 0.581930633182782,
                    "section_title": "5) Two-Stage KD versus One-Stage KD:",
                    "char_start_offset": 46007,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 166
                        },
                        {
                            "start": 167,
                            "end": 296
                        },
                        {
                            "start": 297,
                            "end": 404
                        },
                        {
                            "start": 405,
                            "end": 538
                        },
                        {
                            "start": 539,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 794
                        },
                        {
                            "start": 795,
                            "end": 882
                        },
                        {
                            "start": 885,
                            "end": 1030
                        },
                        {
                            "start": 1033,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1266
                        },
                        {
                            "start": 1267,
                            "end": 1534
                        },
                        {
                            "start": 1535,
                            "end": 1727
                        },
                        {
                            "start": 1728,
                            "end": 1861
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91552734375
                }
            ],
            "relevance_judgement": 0.91552734375,
            "relevance_judgment_input_expanded": "# Title: Structured Knowledge Distillation for Accurate and Efficient Object Detection\n# Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence\n# Authors: Linfeng Zhang, Kaisheng Ma\n## Abstract\nKnowledge distillation, which aims to transfer the knowledge learned by a cumbersome teacher model to a lightweight student model, has become one of the most popular and effective techniques in computer vision. However, many previous knowledge distillation methods are designed for image classification and fail in more challenging tasks such as object detection. In this paper, we first suggest that the failure of knowledge distillation on object detection is mainly caused by two reasons: (1) the imbalance between pixels of foreground and background and (2) lack of knowledge distillation on the relation among different pixels. Then, we propose a structured knowledge distillation scheme, including attention-guided distillation and non-local distillation to address the two issues, respectively. Attention-guided distillation is proposed to find the crucial pixels of foreground objects with an attention mechanism and then make the students take more effort to learn their features. Non-local distillation is proposed to enable students to learn not only the feature of an individual pixel but also the relation between different pixels captured by non-local modules. Experimental results have demonstrated the effectiveness of our method on thirteen kinds of object detection models with twelve comparison methods for both object detection and instance segmentation. For instance, Faster RCNN with our distillation achieves 43.9 mAP on MS COCO2017, which is 4.1 higher than the baseline. Additionally, we show that our method is also beneficial to the robustness and domain generalization ability of detectors. Codes and model weights have been released on GitHub1.\n## 5) Two-Stage KD versus One-Stage KD:\nTraditional knowldistillation follows a two-stage training pipeline, where large teacher model is initially trained and then distilled to a lightweight student model. Recently, have proposed a one-stage knowledge distillation approach called online knowledge distillation or deep mutual learning. These methods involve training two or multiple student models and distilling their knowledge to each other. In this subsection, we compare the effectiveness of our method in both the two-stage and one-stage knowledge distillation frameworks. Experiments are conducted on MS COCO2017 using Faster RCNN students with ResNet50 backbones. For onestage knowledge distillation experiments, we adopt the deep mutual learning approach and train two student detectors to mimic each other using our methods. For two-stage knowledge distillation experiments, we explore the following two schemes. \n\nr Strong Teacher Scheme: distilling knowledge from a pre- trained Cascade Mask RCNN teacher with ResNetX101-DCN backbone, which achieves 47.3 AP. \n\nr Weak Teacher Scheme: distilling knowledge from a pre- trained teacher which has the identical architecture with the student (i.e., Faster RCNN with ResNet50 backbone) and achieves 38.4 AP. Experimental results are shown in Table X. It is observed that one-stage knowledge distillation leads to 2.3 AP improvements over the baseline, which is 0.2 higher than two-stage knowledge distillation with a weak teacher, but still 0.8 AP lower than the two-stage knowledge distillation with a strong teacher. This observation indicates that one-stage knowledge distillation can achieve comparable and even better performance than two-stage knowledge distillation when no powerful teacher is available. Moreover, two-stage knowledge distillation can make more use of the knowledge from a powerful teacher and achieve better performance.",
            "reference_string": "[260378128 | Zhang et al. | 2023 | Citations: 27]"
        },
        {
            "title": "From Toxic to Trustworthy: Using Self-Distillation and Semi-supervised Methods to Refine Neural Networks",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 26,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/29629/31068",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29629?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293439514",
                    "name": "Xianda Zhang"
                },
                {
                    "authorId": "2293440141",
                    "name": "Baolin Zheng"
                },
                {
                    "authorId": "2293555173",
                    "name": "Jianbao Hu"
                },
                {
                    "authorId": "2293661355",
                    "name": "Chengyang Li"
                },
                {
                    "authorId": "2293560877",
                    "name": "Xiaoying Bai"
                }
            ],
            "abstract": "Despite the tremendous success of deep neural networks (DNNs) across various fields, their susceptibility to potential backdoor attacks seriously threatens their application security, particularly in safety-critical or security-sensitive ones. Given this growing threat, there is a pressing need for research into purging backdoors from DNNs. However, prior efforts on erasing backdoor triggers not only failed to withstand increasingly powerful attacks but also resulted in reduced model performance. In this paper, we propose From Toxic to Trustworthy (FTT), an innovative approach to eliminate backdoor triggers while simultaneously enhancing model accuracy. Following the stringent and practical assumption of limited availability of clean data, we introduce a self-attention distillation (SAD) method to remove the backdoor by aligning the shallow and deep parts of the network. Furthermore, we first devise a semi-supervised learning (SSL) method that leverages ubiquitous and available poisoned data to further purify backdoors and improve accuracy. Extensive experiments on various attacks and models have shown that our FTT can reduce the attack success rate from 97% to 1% and improve the accuracy of 4% on average, demonstrating its effectiveness in mitigating backdoor attacks and improving model performance. Compared to state-of-the-art (SOTA) methods, our FTT can reduce the attack success rate by 2 times and improve the accuracy by 5%, shedding light on backdoor cleansing.",
            "corpus_id": 268707594,
            "sentences": [
                {
                    "corpus_id": "268707594",
                    "title": "From Toxic to Trustworthy: Using Self-Distillation and Semi-supervised Methods to Refine Neural Networks",
                    "text": "Knowledge distillation, as shown in Fig. 3 is a technique that aims to transfer the knowledge from a large, overparameterized teacher model to a smaller, more compact student model. This process is often used as a compression approach and can lead to improved performance, as well as enable high compression and rapid acceleration. By transferring the knowledge from the teacher model to the student model, the student model can achieve comparable performance with the teacher model while being more efficient in terms of memory and computation. The basic idea behind knowledge distillation is to train the student model to mimic the outputs of the teacher model, such as the probability vectors, while minimizing the difference between the student's predictions and the teacher's predictions. This allows the student model to learn from the teacher's expertise, improving its performance and generalization ability on the target task. However, traditional knowledge distillation methods suffer from low efficiency in knowledge transfer and challenges in designing and training appropriate teacher models. To address these issues, self-distillation (Zhang et al. 2019) has been proposed as a novel one-step framework, focusing directly on training the student model. This approach not only reduces training time significantly but also achieves higher accuracy, making it a promising alternative to traditional knowledge distillation methods.",
                    "score": 0.6236252862767719,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 6190,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 182,
                            "end": 331
                        },
                        {
                            "start": 332,
                            "end": 545
                        },
                        {
                            "start": 546,
                            "end": 793
                        },
                        {
                            "start": 794,
                            "end": 935
                        },
                        {
                            "start": 936,
                            "end": 1105
                        },
                        {
                            "start": 1106,
                            "end": 1266
                        },
                        {
                            "start": 1267,
                            "end": 1441
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1149,
                            "end": 1167,
                            "matchedPaperCorpusId": "159041406"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9150390625
                }
            ],
            "relevance_judgement": 0.9150390625,
            "relevance_judgment_input_expanded": "# Title: From Toxic to Trustworthy: Using Self-Distillation and Semi-supervised Methods to Refine Neural Networks\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Xianda Zhang, Baolin Zheng, Jianbao Hu, Chengyang Li, Xiaoying Bai\n## Abstract\nDespite the tremendous success of deep neural networks (DNNs) across various fields, their susceptibility to potential backdoor attacks seriously threatens their application security, particularly in safety-critical or security-sensitive ones. Given this growing threat, there is a pressing need for research into purging backdoors from DNNs. However, prior efforts on erasing backdoor triggers not only failed to withstand increasingly powerful attacks but also resulted in reduced model performance. In this paper, we propose From Toxic to Trustworthy (FTT), an innovative approach to eliminate backdoor triggers while simultaneously enhancing model accuracy. Following the stringent and practical assumption of limited availability of clean data, we introduce a self-attention distillation (SAD) method to remove the backdoor by aligning the shallow and deep parts of the network. Furthermore, we first devise a semi-supervised learning (SSL) method that leverages ubiquitous and available poisoned data to further purify backdoors and improve accuracy. Extensive experiments on various attacks and models have shown that our FTT can reduce the attack success rate from 97% to 1% and improve the accuracy of 4% on average, demonstrating its effectiveness in mitigating backdoor attacks and improving model performance. Compared to state-of-the-art (SOTA) methods, our FTT can reduce the attack success rate by 2 times and improve the accuracy by 5%, shedding light on backdoor cleansing.\n## Knowledge Distillation\nKnowledge distillation, as shown in Fig. 3 is a technique that aims to transfer the knowledge from a large, overparameterized teacher model to a smaller, more compact student model. This process is often used as a compression approach and can lead to improved performance, as well as enable high compression and rapid acceleration. By transferring the knowledge from the teacher model to the student model, the student model can achieve comparable performance with the teacher model while being more efficient in terms of memory and computation. The basic idea behind knowledge distillation is to train the student model to mimic the outputs of the teacher model, such as the probability vectors, while minimizing the difference between the student's predictions and the teacher's predictions. This allows the student model to learn from the teacher's expertise, improving its performance and generalization ability on the target task. However, traditional knowledge distillation methods suffer from low efficiency in knowledge transfer and challenges in designing and training appropriate teacher models. To address these issues, self-distillation (Zhang et al. 2019) has been proposed as a novel one-step framework, focusing directly on training the student model. This approach not only reduces training time significantly but also achieves higher accuracy, making it a promising alternative to traditional knowledge distillation methods.",
            "reference_string": "[268707594 | Zhang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Towards Optimal Trade-Offs in Knowledge Distillation for CNNs and Vision Transformers at the Edge",
            "venue": "European Signal Processing Conference",
            "year": 2024,
            "reference_count": 12,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2573061",
                    "name": "John Violos"
                },
                {
                    "authorId": "2276779989",
                    "name": "Symeon Papadopoulos"
                },
                {
                    "authorId": "1715604",
                    "name": "Y. Kompatsiaris"
                }
            ],
            "abstract": "This paper discusses four facets of the Knowledge Distillation (KD) process for Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) architectures, particularly when executed on edge devices with constrained processing capabilities. First, we conduct a comparative analysis of the KD process between CNNs and ViT architectures, aiming to elucidate the feasibility and efficacy of employing different architectural configurations for the teacher and student, while assessing their performance and efficiency. Second, we explore the impact of varying the size of the student model on accuracy and inference speed, while maintaining a constant KD duration. Third, we examine the effects of employing higher resolution images on the accuracy, memory footprint and computational workload. Last, we examine the performance improvements obtained by fine-tuning the student model after KD to specific downstream tasks. Through empirical evaluations and analyses, this research provides AI practitioners with insights into optimal strategies for maximizing the effectiveness of the KD process on edge devices.",
            "corpus_id": 271270180,
            "sentences": [
                {
                    "corpus_id": "271270180",
                    "title": "Towards Optimal Trade-Offs in Knowledge Distillation for CNNs and Vision Transformers at the Edge",
                    "text": "The gap of model capacity between the teacher and the student is a critical consideration in the KD process.An optimal balance must be struck to ensure effective distillation outcomes.When the student model's capacity is too low relative to the teacher, the student struggles to effectively incorporate the logits information provided by the teacher.This limitation impedes the student's ability to capture the distilled knowledge, thus hindering its performance.\n\nConversely, when the student model's capacity is excessively large, the expected improvements in distillation efficacy may not materialize.Larger student models tend to exhibit slower learning rates and are more susceptible to overfitting, resulting in diminished generalization performance.Furthermore, the computational and memory resources required for training and deploying larger student models pose significant challenges, particularly in resource-constrained environments such as edge devices.The increased computational demands, higher memory footprint, and slower inference speeds associated with larger models render them less practical for deployment in real-world edge scenarios.Therefore, determining the optimal student size also requires experimental investigation.",
                    "score": 0.5358932322514787,
                    "section_title": "B. Student Model Capacity",
                    "char_start_offset": 9530,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 108
                        },
                        {
                            "start": 108,
                            "end": 184
                        },
                        {
                            "start": 184,
                            "end": 350
                        },
                        {
                            "start": 350,
                            "end": 463
                        },
                        {
                            "start": 465,
                            "end": 604
                        },
                        {
                            "start": 604,
                            "end": 756
                        },
                        {
                            "start": 756,
                            "end": 966
                        },
                        {
                            "start": 966,
                            "end": 1157
                        },
                        {
                            "start": 1157,
                            "end": 1246
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91455078125
                }
            ],
            "relevance_judgement": 0.91455078125,
            "relevance_judgment_input_expanded": "# Title: Towards Optimal Trade-Offs in Knowledge Distillation for CNNs and Vision Transformers at the Edge\n# Venue: European Signal Processing Conference\n# Authors: John Violos, Symeon Papadopoulos, Y. Kompatsiaris\n## Abstract\nThis paper discusses four facets of the Knowledge Distillation (KD) process for Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) architectures, particularly when executed on edge devices with constrained processing capabilities. First, we conduct a comparative analysis of the KD process between CNNs and ViT architectures, aiming to elucidate the feasibility and efficacy of employing different architectural configurations for the teacher and student, while assessing their performance and efficiency. Second, we explore the impact of varying the size of the student model on accuracy and inference speed, while maintaining a constant KD duration. Third, we examine the effects of employing higher resolution images on the accuracy, memory footprint and computational workload. Last, we examine the performance improvements obtained by fine-tuning the student model after KD to specific downstream tasks. Through empirical evaluations and analyses, this research provides AI practitioners with insights into optimal strategies for maximizing the effectiveness of the KD process on edge devices.\n## B. Student Model Capacity\nThe gap of model capacity between the teacher and the student is a critical consideration in the KD process.An optimal balance must be struck to ensure effective distillation outcomes.When the student model's capacity is too low relative to the teacher, the student struggles to effectively incorporate the logits information provided by the teacher.This limitation impedes the student's ability to capture the distilled knowledge, thus hindering its performance.\n\nConversely, when the student model's capacity is excessively large, the expected improvements in distillation efficacy may not materialize.Larger student models tend to exhibit slower learning rates and are more susceptible to overfitting, resulting in diminished generalization performance.Furthermore, the computational and memory resources required for training and deploying larger student models pose significant challenges, particularly in resource-constrained environments such as edge devices.The increased computational demands, higher memory footprint, and slower inference speeds associated with larger models render them less practical for deployment in real-world edge scenarios.Therefore, determining the optimal student size also requires experimental investigation.",
            "reference_string": "[271270180 | Violos et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Comparative Knowledge Distillation",
            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.02253, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2000786644",
                    "name": "Alex Wilf"
                },
                {
                    "authorId": "2265490543",
                    "name": "Alex Tianyi Xu"
                },
                {
                    "authorId": "28130078",
                    "name": "Paul Pu Liang"
                },
                {
                    "authorId": "71931352",
                    "name": "A. Obolenskiy"
                },
                {
                    "authorId": "2259931814",
                    "name": "Daniel Fried"
                },
                {
                    "authorId": "49933077",
                    "name": "Louis-philippe Morency"
                }
            ],
            "abstract": "In the era of large-scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally-heavy teacher models to lightweight, efficient student models while preserving performance. Yet KD settings often assume readily available access to teacher models capable of performing many in-ferences-a notion increasingly at odds with the realities of costly large-scale models. Addressing this gap, we study an important question: how KD algorithms fare as the number of teacher inferences decreases, a setting we term Reduced-Teacher-Inference Knowledge Distillation (RTI-KD). We observe that the performance of prevalent KD techniques and state-of-the-art data augmentation strategies suffers considerably as the number of teacher inferences is reduced. One class of approaches, termed \u201crelational\u201d knowledge distillation underperforms the rest, yet we hypothesize that they hold promise for reduced dependency on teacher models because they can augment the effective dataset size without additional teacher calls. We find that a simple change - performing high-dimensional comparisons instead of low-dimensional relations, which we term Comparative Knowledge Distillation - vaults performance well over existing KD approaches. We perform empirical evaluation across varied experimental settings and rigorous analysis to understand the learning outcomes of our method. All code is made publicly available.",
            "corpus_id": 265033252,
            "sentences": [
                {
                    "corpus_id": "265033252",
                    "title": "Comparative Knowledge Distillation",
                    "text": "In the era of large-scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally-heavy teacher models to lightweight, efficient student models while preserving performance. Yet KD settings often assume readily available access to teacher models capable of performing many in-ferences-a notion increasingly at odds with the realities of costly large-scale models. Addressing this gap, we study an important question: how KD algorithms fare as the number of teacher inferences decreases, a setting we term Reduced-Teacher-Inference Knowledge Distillation (RTI-KD). We observe that the performance of prevalent KD techniques and state-of-the-art data augmentation strategies suffers considerably as the number of teacher inferences is reduced. One class of approaches, termed \u201crelational\u201d knowledge distillation underperforms the rest, yet we hypothesize that they hold promise for reduced dependency on teacher models because they can augment the effective dataset size without additional teacher calls. We find that a simple change - performing high-dimensional comparisons instead of low-dimensional relations, which we term Comparative Knowledge Distillation - vaults performance well over existing KD approaches. We perform empirical evaluation across varied experimental settings and rigorous analysis to understand the learning outcomes of our method. All code is made publicly available.",
                    "score": 0.5819804243536113,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9140625
                }
            ],
            "relevance_judgement": 0.9140625,
            "relevance_judgment_input_expanded": "# Title: Comparative Knowledge Distillation\n# Venue: IEEE Workshop/Winter Conference on Applications of Computer Vision\n# Authors: Alex Wilf, Alex Tianyi Xu, Paul Pu Liang, A. Obolenskiy, Daniel Fried, Louis-philippe Morency\n## Abstract\nIn the era of large-scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally-heavy teacher models to lightweight, efficient student models while preserving performance. Yet KD settings often assume readily available access to teacher models capable of performing many in-ferences-a notion increasingly at odds with the realities of costly large-scale models. Addressing this gap, we study an important question: how KD algorithms fare as the number of teacher inferences decreases, a setting we term Reduced-Teacher-Inference Knowledge Distillation (RTI-KD). We observe that the performance of prevalent KD techniques and state-of-the-art data augmentation strategies suffers considerably as the number of teacher inferences is reduced. One class of approaches, termed \u201crelational\u201d knowledge distillation underperforms the rest, yet we hypothesize that they hold promise for reduced dependency on teacher models because they can augment the effective dataset size without additional teacher calls. We find that a simple change - performing high-dimensional comparisons instead of low-dimensional relations, which we term Comparative Knowledge Distillation - vaults performance well over existing KD approaches. We perform empirical evaluation across varied experimental settings and rigorous analysis to understand the learning outcomes of our method. All code is made publicly available.\n",
            "reference_string": "[265033252 | Wilf et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
            "venue": "2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 20,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2405.11704",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.11704, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2298916636",
                    "name": "Taiyuan Mei"
                },
                {
                    "authorId": "2298918720",
                    "name": "Yun Zi"
                },
                {
                    "authorId": "2222987403",
                    "name": "X. Cheng"
                },
                {
                    "authorId": "2297725659",
                    "name": "Zijun Gao"
                },
                {
                    "authorId": "2297735971",
                    "name": "Qi Wang"
                },
                {
                    "authorId": "2302372513",
                    "name": "Haowei Yang"
                }
            ],
            "abstract": "The internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies. Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint. By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice. In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation. By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy. In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research. In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models.",
            "corpus_id": 269921267,
            "sentences": [
                {
                    "corpus_id": "269921267",
                    "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
                    "text": "This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.\n\nThe core idea of knowledge distillation is that while large teacher models may have high predictive accuracy due to the large number of parameters, these advantages are not always directly applicable to resource-constrained environments.Knowledge distillation therefore aims to extract the \"dark knowledge\" of the teacher model -not just the final predictions, but also its decision-making processes and uncertainty estimates for the input data -and then infuse this knowledge into a more concise student model:  The knowledge distillation technique adopts a typical teacher-student training framework, as shown in Figure 2. In this framework, the \"teacher\" model represents a highly complex deep neural network.In contrast, the Student model is designed to be more streamlined and aims to achieve a similar level of performance with fewer parameters.The process begins with the pre-training of the Teacher model, a step that allows it to accumulate broad and deep knowledge on the specified data set.Instead of directly replicating the weights of the \"teacher\" model, the subsequent knowledge transfer phase uses its output -particularly soft probability distributions (rather than hard classification labels).This process combines the rich experience of the \"teacher\" model with direct guidance from real-world data to form a dual oversight mechanism designed to efficiently extract and transfer deep learning capabilities, while reducing resource burdens through model compression techniques and enhancing the feasibility and flexibility of the model in practical applications.\n\nKnowledge distillation is generally categorized into two main types: output-based and feature-based distillation.In this approach, the student model acquires similar predictive abilities by assimilating the output probability distribution from the teacher model, essentially grasping and transforming high-level abstract concepts.This process is achieved by designing specialized distillation loss functions that ensure that the student model can capture and mimic the high-level transformation logic of the teacher model to the input information, thus maintaining consistent or close performance to the teacher model on the prediction task.\n\nFeature-based distillation emphasizes the assimilation of intermediate layer features from the teacher model.",
                    "score": 0.5467840743454149,
                    "section_title": "B. Knowledge distillation",
                    "char_start_offset": 14355,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 182,
                            "end": 419
                        },
                        {
                            "start": 419,
                            "end": 894
                        },
                        {
                            "start": 894,
                            "end": 1033
                        },
                        {
                            "start": 1033,
                            "end": 1183
                        },
                        {
                            "start": 1183,
                            "end": 1393
                        },
                        {
                            "start": 1393,
                            "end": 1762
                        },
                        {
                            "start": 1764,
                            "end": 1877
                        },
                        {
                            "start": 1877,
                            "end": 2405
                        },
                        {
                            "start": 2407,
                            "end": 2516
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91162109375
                }
            ],
            "relevance_judgement": 0.91162109375,
            "relevance_judgment_input_expanded": "# Title: Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks\n# Venue: 2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)\n# Authors: Taiyuan Mei, Yun Zi, X. Cheng, Zijun Gao, Qi Wang, Haowei Yang\n## Abstract\nThe internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies. Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint. By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice. In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation. By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy. In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research. In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models.\n## B. Knowledge distillation\nThis process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.\n\nThe core idea of knowledge distillation is that while large teacher models may have high predictive accuracy due to the large number of parameters, these advantages are not always directly applicable to resource-constrained environments.Knowledge distillation therefore aims to extract the \"dark knowledge\" of the teacher model -not just the final predictions, but also its decision-making processes and uncertainty estimates for the input data -and then infuse this knowledge into a more concise student model:  The knowledge distillation technique adopts a typical teacher-student training framework, as shown in Figure 2. In this framework, the \"teacher\" model represents a highly complex deep neural network.In contrast, the Student model is designed to be more streamlined and aims to achieve a similar level of performance with fewer parameters.The process begins with the pre-training of the Teacher model, a step that allows it to accumulate broad and deep knowledge on the specified data set.Instead of directly replicating the weights of the \"teacher\" model, the subsequent knowledge transfer phase uses its output -particularly soft probability distributions (rather than hard classification labels).This process combines the rich experience of the \"teacher\" model with direct guidance from real-world data to form a dual oversight mechanism designed to efficiently extract and transfer deep learning capabilities, while reducing resource burdens through model compression techniques and enhancing the feasibility and flexibility of the model in practical applications.\n\nKnowledge distillation is generally categorized into two main types: output-based and feature-based distillation.In this approach, the student model acquires similar predictive abilities by assimilating the output probability distribution from the teacher model, essentially grasping and transforming high-level abstract concepts.This process is achieved by designing specialized distillation loss functions that ensure that the student model can capture and mimic the high-level transformation logic of the teacher model to the input information, thus maintaining consistent or close performance to the teacher model on the prediction task.\n\nFeature-based distillation emphasizes the assimilation of intermediate layer features from the teacher model.",
            "reference_string": "[269921267 | Mei et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Can a student Large Language Model perform as well as it's teacher?",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 29,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.02421",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.02421, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2141639835",
                    "name": "Sia Gholami"
                },
                {
                    "authorId": "2239197098",
                    "name": "Marwan Omar"
                }
            ],
            "abstract": "The burgeoning complexity of contemporary deep learning models, while achieving unparalleled accuracy, has inadvertently introduced deployment challenges in resource-constrained environments. Knowledge distillation, a technique aiming to transfer knowledge from a high-capacity\"teacher\"model to a streamlined\"student\"model, emerges as a promising solution to this dilemma. This paper provides a comprehensive overview of the knowledge distillation paradigm, emphasizing its foundational principles such as the utility of soft labels and the significance of temperature scaling. Through meticulous examination, we elucidate the critical determinants of successful distillation, including the architecture of the student model, the caliber of the teacher, and the delicate balance of hyperparameters. While acknowledging its profound advantages, we also delve into the complexities and challenges inherent in the process. Our exploration underscores knowledge distillation's potential as a pivotal technique in optimizing the trade-off between model performance and deployment efficiency.",
            "corpus_id": 263620565,
            "sentences": [
                {
                    "corpus_id": "263620565",
                    "title": "Can a student Large Language Model perform as well as it's teacher?",
                    "text": "Capacity: There's also a limit to how much a smaller student model can learn from a large teacher model. If the student model's capacity is too small, it may not effectively learn the teacher's knowledge. \n\n7. Overfitting Risk: If the teacher model has overfit to the training data, it's possible that the student model might learn these overfit predictions, leading to poor generalization to new data. \n\nDespite these limitations, knowledge distillation can still be a very useful technique, especially when dealing with constraints on computational resources or when deploying models in real-world applications where efficiency is key. \n\n7 Future Work \n\nThere are several directions that future work on knowledge distillation could take to further improve this technique and its application in various fields: \n\n\u2022 Improved Understanding of Distillation Dynamics: Further research is needed to understand the dynamics of knowledge transfer during distillation. For example, understanding which aspects of the teacher's knowledge are most effectively transferred and why could help optimize the process. \n\n\u2022 Automated Hyperparameter Tuning: Given the sensitivity of the distillation process to hyperparameters like the temperature and the weighting between the original loss and distillation loss, developing methods for automatic or more efficient hyperparameter tuning could be beneficial. \n\n\u2022 Advanced Distillation Techniques: Exploring advanced distillation techniques beyond the standard approach could lead to better results. This could involve novel loss functions, training methods, or types of teacher-student relationships. \n\n\u2022 Multi-Teacher Distillation: The idea of distilling knowledge from multiple teacher models into a single student model is an interesting area for exploration. This could potentially combine the strengths of various models into a single efficient student model. \n\n\u2022 Domain-Specific Adaptations: Adapting and optimizing knowledge distillation techniques for specific domains or tasks could also be a valuable avenue for future work. Different tasks might benefit from different distillation strategies. \n\n\u2022 Privacy and Security in Distillation: As distillation involves transferring knowledge from a teacher model, there could be concerns about privacy and security, especially when the teacher model has been trained on sensitive data. Future work could look at how to ensure that distillation does not leak sensitive information.",
                    "score": 0.5955351901957875,
                    "section_title": "Student Model",
                    "char_start_offset": 19367,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 104
                        },
                        {
                            "start": 105,
                            "end": 204
                        },
                        {
                            "start": 207,
                            "end": 402
                        },
                        {
                            "start": 405,
                            "end": 637
                        },
                        {
                            "start": 640,
                            "end": 653
                        },
                        {
                            "start": 656,
                            "end": 811
                        },
                        {
                            "start": 814,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1103
                        },
                        {
                            "start": 1106,
                            "end": 1391
                        },
                        {
                            "start": 1394,
                            "end": 1531
                        },
                        {
                            "start": 1532,
                            "end": 1633
                        },
                        {
                            "start": 1636,
                            "end": 1795
                        },
                        {
                            "start": 1796,
                            "end": 1897
                        },
                        {
                            "start": 1900,
                            "end": 2067
                        },
                        {
                            "start": 2068,
                            "end": 2137
                        },
                        {
                            "start": 2140,
                            "end": 2371
                        },
                        {
                            "start": 2372,
                            "end": 2466
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9111328125
                }
            ],
            "relevance_judgement": 0.9111328125,
            "relevance_judgment_input_expanded": "# Title: Can a student Large Language Model perform as well as it's teacher?\n# Venue: arXiv.org\n# Authors: Sia Gholami, Marwan Omar\n## Abstract\nThe burgeoning complexity of contemporary deep learning models, while achieving unparalleled accuracy, has inadvertently introduced deployment challenges in resource-constrained environments. Knowledge distillation, a technique aiming to transfer knowledge from a high-capacity\"teacher\"model to a streamlined\"student\"model, emerges as a promising solution to this dilemma. This paper provides a comprehensive overview of the knowledge distillation paradigm, emphasizing its foundational principles such as the utility of soft labels and the significance of temperature scaling. Through meticulous examination, we elucidate the critical determinants of successful distillation, including the architecture of the student model, the caliber of the teacher, and the delicate balance of hyperparameters. While acknowledging its profound advantages, we also delve into the complexities and challenges inherent in the process. Our exploration underscores knowledge distillation's potential as a pivotal technique in optimizing the trade-off between model performance and deployment efficiency.\n## Student Model\nCapacity: There's also a limit to how much a smaller student model can learn from a large teacher model. If the student model's capacity is too small, it may not effectively learn the teacher's knowledge. \n\n7. Overfitting Risk: If the teacher model has overfit to the training data, it's possible that the student model might learn these overfit predictions, leading to poor generalization to new data. \n\nDespite these limitations, knowledge distillation can still be a very useful technique, especially when dealing with constraints on computational resources or when deploying models in real-world applications where efficiency is key. \n\n7 Future Work \n\nThere are several directions that future work on knowledge distillation could take to further improve this technique and its application in various fields: \n\n\u2022 Improved Understanding of Distillation Dynamics: Further research is needed to understand the dynamics of knowledge transfer during distillation. For example, understanding which aspects of the teacher's knowledge are most effectively transferred and why could help optimize the process. \n\n\u2022 Automated Hyperparameter Tuning: Given the sensitivity of the distillation process to hyperparameters like the temperature and the weighting between the original loss and distillation loss, developing methods for automatic or more efficient hyperparameter tuning could be beneficial. \n\n\u2022 Advanced Distillation Techniques: Exploring advanced distillation techniques beyond the standard approach could lead to better results. This could involve novel loss functions, training methods, or types of teacher-student relationships. \n\n\u2022 Multi-Teacher Distillation: The idea of distilling knowledge from multiple teacher models into a single student model is an interesting area for exploration. This could potentially combine the strengths of various models into a single efficient student model. \n\n\u2022 Domain-Specific Adaptations: Adapting and optimizing knowledge distillation techniques for specific domains or tasks could also be a valuable avenue for future work. Different tasks might benefit from different distillation strategies. \n\n\u2022 Privacy and Security in Distillation: As distillation involves transferring knowledge from a teacher model, there could be concerns about privacy and security, especially when the teacher model has been trained on sensitive data. Future work could look at how to ensure that distillation does not leak sensitive information.",
            "reference_string": "[263620565 | Gholami et al. | 2023 | Citations: 12]"
        },
        {
            "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 150,
            "citation_count": 16,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.15347, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108418112",
                    "name": "Seungcheol Park"
                },
                {
                    "authorId": "2281832669",
                    "name": "Jaehyeon Choi"
                },
                {
                    "authorId": "2281792832",
                    "name": "Sojin Lee"
                },
                {
                    "authorId": "2281746333",
                    "name": "U. Kang"
                }
            ],
            "abstract": "How can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of compression algorithms, and the desired properties of low-cost compression algorithms which have a significant impact due to the emergence of large language models. Finally, we introduce promising future research topics based on our survey results.",
            "corpus_id": 267312283,
            "sentences": [
                {
                    "corpus_id": "267312283",
                    "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
                    "text": "Knowledge distillation (KD) is a technique that transfers useful knowledge from a large teacher model T to a small student model S to improve the accuracy of S. This process encourages the student model to generate outputs that are similar to that of the teacher model to obtain the teacher model's generalized knowledge. KD matches the outputs of classifiers, embedding layers, and sublayers in T and S to distill diverse types of knowledge within different distillation sources. We explain diverse types of distillation sources used for KD in Section 5.1.1. \n\nWhen we apply KD for language models, we need to consider how to match sublayers in T and S since they have different numbers of sublayers. We categorize the sublayer matching strategies into three groups: 1:1, many:1, and many:many strategies. We explain more details about each of the sublayer matching strategies in Section 5.1.2. \n\nWe summarize the performance of diverse KD algorithms for encoder-only Transformers in Table 6. We denote the distillation sources and the sublayer-matching strategies of each KD algorithm in columns 2 and 3, respectively. We Table 6. Comparison of the performance of knowledge distillation (KD) algorithms for encoder-only Transformers on MNLI, QQP, and SQuAD 1.1 (SQD) benchmarks.",
                    "score": 0.539894395702395,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 53658,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 480
                        },
                        {
                            "start": 481,
                            "end": 559
                        },
                        {
                            "start": 562,
                            "end": 701
                        },
                        {
                            "start": 702,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 895
                        },
                        {
                            "start": 898,
                            "end": 993
                        },
                        {
                            "start": 994,
                            "end": 1120
                        },
                        {
                            "start": 1121,
                            "end": 1132
                        },
                        {
                            "start": 1133,
                            "end": 1280
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90966796875
                }
            ],
            "relevance_judgement": 0.90966796875,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey of Compression Algorithms for Language Models\n# Venue: arXiv.org\n# Authors: Seungcheol Park, Jaehyeon Choi, Sojin Lee, U. Kang\n## Abstract\nHow can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of compression algorithms, and the desired properties of low-cost compression algorithms which have a significant impact due to the emergence of large language models. Finally, we introduce promising future research topics based on our survey results.\n## Knowledge Distillation\nKnowledge distillation (KD) is a technique that transfers useful knowledge from a large teacher model T to a small student model S to improve the accuracy of S. This process encourages the student model to generate outputs that are similar to that of the teacher model to obtain the teacher model's generalized knowledge. KD matches the outputs of classifiers, embedding layers, and sublayers in T and S to distill diverse types of knowledge within different distillation sources. We explain diverse types of distillation sources used for KD in Section 5.1.1. \n\nWhen we apply KD for language models, we need to consider how to match sublayers in T and S since they have different numbers of sublayers. We categorize the sublayer matching strategies into three groups: 1:1, many:1, and many:many strategies. We explain more details about each of the sublayer matching strategies in Section 5.1.2. \n\nWe summarize the performance of diverse KD algorithms for encoder-only Transformers in Table 6. We denote the distillation sources and the sublayer-matching strategies of each KD algorithm in columns 2 and 3, respectively. We Table 6. Comparison of the performance of knowledge distillation (KD) algorithms for encoder-only Transformers on MNLI, QQP, and SQuAD 1.1 (SQD) benchmarks.",
            "reference_string": "[267312283 | Park et al. | 2024 | Citations: 16]"
        },
        {
            "title": "DSP-KD: Dual-Stage Progressive Knowledge Distillation for Skin Disease Classification",
            "venue": "Bioengineering",
            "year": 2024,
            "reference_count": 44,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2306-5354/11/1/70/pdf?version=1704886529",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10813127, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218023537",
                    "name": "Xinyi Zeng"
                },
                {
                    "authorId": "2099822427",
                    "name": "Zhanlin Ji"
                },
                {
                    "authorId": "2108899818",
                    "name": "Haiyang Zhang"
                },
                {
                    "authorId": "2279208319",
                    "name": "Rui Chen"
                },
                {
                    "authorId": "2266118205",
                    "name": "Qinping Liao"
                },
                {
                    "authorId": "2258293202",
                    "name": "Jingkun Wang"
                },
                {
                    "authorId": "2105243765",
                    "name": "Tao Lyu"
                },
                {
                    "authorId": "2116548447",
                    "name": "Li Zhao"
                }
            ],
            "abstract": "The increasing global demand for skin disease diagnostics emphasizes the urgent need for advancements in AI-assisted diagnostic technologies for dermatoscopic images. In current practical medical systems, the primary challenge is balancing lightweight models with accurate image analysis to address constraints like limited storage and computational costs. While knowledge distillation methods hold immense potential in healthcare applications, related research on multi-class skin disease tasks is scarce. To bridge this gap, our study introduces an enhanced multi-source knowledge fusion distillation framework, termed DSP-KD, which improves knowledge transfer in a dual-stage progressive distillation approach to maximize mutual information between teacher and student representations. The experimental results highlight the superior performance of our distilled ShuffleNetV2 on both the ISIC2019 dataset and our private skin disorders dataset. Compared to other state-of-the-art distillation methods using diverse knowledge sources, the DSP-KD demonstrates remarkable effectiveness with a smaller computational burden.",
            "corpus_id": 266939439,
            "sentences": [
                {
                    "corpus_id": "266939439",
                    "title": "DSP-KD: Dual-Stage Progressive Knowledge Distillation for Skin Disease Classification",
                    "text": "Broadly speaking, knowledge distillation refers to extracting knowledge from a larger deep neural network and transferring it into a smaller neural network model by aligning two different types of information sources [30]: (a) those utilizing the teacher's output probabilities (logit-based). In 2015, Hinton et al. formally popularized vanilla knowledge distillation, with the core idea of employing the teacher's prediction logits as soft labels to guide the student [9]. Zhou et al. addressed the bias-variance tradeoff by proposing weighted soft labels and validated its effectiveness through experiments with standard evaluation benchmarks [31]. Chi et al. introduced Normalized Knowledge Distillation (NormKD), aiming to customize the temperature for each sample based on the characteristic of the sample's logit distribution. It demonstrated significantly better classification performance than vanilla knowledge distillation with almost no additional computational or storage costs [11]. (b) Those utilizing the teacher's intermediate representations (featurebased). Romero et al. first introduced the idea that activations, neurons, or features from intermediate layers can serve as knowledge to guide the student model [32]. Inspired by this, Chen et al. enhanced the effectiveness of knowledge distillation by learning feature maps from the teacher network through two main techniques-shared classifier and generative adversarial network [12]. Kim et al. utilized a paraphraser to perform teacher factor extraction and transferred the learned factor to the student through convolutional operations [33]. Yang et al. proposed Masked Generative Distillation, a versatile feature-based distillation method applicable to diverse tasks. It involved masking random pixels of the student's feature, compelling it to generate the teacher's full feature [34]. \n\nIn the human world, we integrate information from various sources to gain cognition. Similarly, in the task of knowledge distillation, a student model can also acquire multi-level knowledge. Based on this concept, most recent research has focused on multi-teacher distillation, using several distinct deep learning models as teachers to impart knowledge to the student.",
                    "score": 0.5306584576826459,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 12932,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 292
                        },
                        {
                            "start": 293,
                            "end": 473
                        },
                        {
                            "start": 474,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 832
                        },
                        {
                            "start": 833,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1074
                        },
                        {
                            "start": 1075,
                            "end": 1234
                        },
                        {
                            "start": 1235,
                            "end": 1454
                        },
                        {
                            "start": 1455,
                            "end": 1614
                        },
                        {
                            "start": 1615,
                            "end": 1742
                        },
                        {
                            "start": 1743,
                            "end": 1861
                        },
                        {
                            "start": 1864,
                            "end": 1948
                        },
                        {
                            "start": 1949,
                            "end": 2054
                        },
                        {
                            "start": 2055,
                            "end": 2233
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 469,
                            "end": 472,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 645,
                            "end": 649,
                            "matchedPaperCorpusId": "231740588"
                        },
                        {
                            "start": 1449,
                            "end": 1453,
                            "matchedPaperCorpusId": "54441502"
                        },
                        {
                            "start": 1609,
                            "end": 1613,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 1856,
                            "end": 1860,
                            "matchedPaperCorpusId": "248506080"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90966796875
                }
            ],
            "relevance_judgement": 0.90966796875,
            "relevance_judgment_input_expanded": "# Title: DSP-KD: Dual-Stage Progressive Knowledge Distillation for Skin Disease Classification\n# Venue: Bioengineering\n# Authors: Xinyi Zeng, Zhanlin Ji, Haiyang Zhang, Rui Chen, Qinping Liao, Jingkun Wang, Tao Lyu, Li Zhao\n## Abstract\nThe increasing global demand for skin disease diagnostics emphasizes the urgent need for advancements in AI-assisted diagnostic technologies for dermatoscopic images. In current practical medical systems, the primary challenge is balancing lightweight models with accurate image analysis to address constraints like limited storage and computational costs. While knowledge distillation methods hold immense potential in healthcare applications, related research on multi-class skin disease tasks is scarce. To bridge this gap, our study introduces an enhanced multi-source knowledge fusion distillation framework, termed DSP-KD, which improves knowledge transfer in a dual-stage progressive distillation approach to maximize mutual information between teacher and student representations. The experimental results highlight the superior performance of our distilled ShuffleNetV2 on both the ISIC2019 dataset and our private skin disorders dataset. Compared to other state-of-the-art distillation methods using diverse knowledge sources, the DSP-KD demonstrates remarkable effectiveness with a smaller computational burden.\n## Knowledge Distillation\nBroadly speaking, knowledge distillation refers to extracting knowledge from a larger deep neural network and transferring it into a smaller neural network model by aligning two different types of information sources [30]: (a) those utilizing the teacher's output probabilities (logit-based). In 2015, Hinton et al. formally popularized vanilla knowledge distillation, with the core idea of employing the teacher's prediction logits as soft labels to guide the student [9]. Zhou et al. addressed the bias-variance tradeoff by proposing weighted soft labels and validated its effectiveness through experiments with standard evaluation benchmarks [31]. Chi et al. introduced Normalized Knowledge Distillation (NormKD), aiming to customize the temperature for each sample based on the characteristic of the sample's logit distribution. It demonstrated significantly better classification performance than vanilla knowledge distillation with almost no additional computational or storage costs [11]. (b) Those utilizing the teacher's intermediate representations (featurebased). Romero et al. first introduced the idea that activations, neurons, or features from intermediate layers can serve as knowledge to guide the student model [32]. Inspired by this, Chen et al. enhanced the effectiveness of knowledge distillation by learning feature maps from the teacher network through two main techniques-shared classifier and generative adversarial network [12]. Kim et al. utilized a paraphraser to perform teacher factor extraction and transferred the learned factor to the student through convolutional operations [33]. Yang et al. proposed Masked Generative Distillation, a versatile feature-based distillation method applicable to diverse tasks. It involved masking random pixels of the student's feature, compelling it to generate the teacher's full feature [34]. \n\nIn the human world, we integrate information from various sources to gain cognition. Similarly, in the task of knowledge distillation, a student model can also acquire multi-level knowledge. Based on this concept, most recent research has focused on multi-teacher distillation, using several distinct deep learning models as teachers to impart knowledge to the student.",
            "reference_string": "[266939439 | Zeng et al. | 2024 | Citations: 7]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "253254911",
            "title": "Reduce, Reuse, Recycle: Improving Training Efficiency with Distillation",
            "text": "Knowledge distillation (Hinton et al., 2015) is a wellestablished practice for model compression and improving the quality of models (Gou et al., 2021). Distillation is often an ingredient in training recipes that push the limits of model quality (Xie et al., 2020;Touvron et al., 2021;Beyer et al., 2021). Teacher models are traditionally larger than student models, though too large of a size discrepancy between teacher and student models can reduce the efficacy of distillation (Mirzadeh et al., 2020). As such, numerous approaches have been proposed to more effectively utilize models of similar or identical architecture as teachers (Yim et al., 2017;Yang et al., 2018;Furlanello et al., 2018;Zhang et al., 2019;Wang et al., 2022).",
            "score": 0.7581028313568992,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 737
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 151,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 247,
                    "end": 265,
                    "matchedPaperCorpusId": "207853355"
                },
                {
                    "start": 482,
                    "end": 505,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 639,
                    "end": 657,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 675,
                    "end": 699,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 699,
                    "end": 718,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.884765625
        },
        {
            "corpus_id": "267759723",
            "title": "Improve Cross-Architecture Generalization on Dataset Distillation",
            "text": "Knowledge distillation is a method to transfer knowledge from a large model to a small model. It's first proposed in this paper [12]. The large model is called the teacher model and the small model is called the student model. The student model is trained to mimic the output of the teacher model. The student model is usually a shallow neural network with fewer parameters and faster inference speed. The teacher model is usually a deep neural network with more parameters and slower inference speed. The student model is trained on the same dataset as the teacher model to minimize the difference between the output of the student model and the output of the teacher model (i.e. maintain the knowledge as much as possible). \n\nAfter knowledge distillation was proposed, a lot of work has been put into exploring further applications in diverse fields, such as speech recognition, image recognition, and natural language processing. It proved that knowledge distillation is a great success in these various fields [11]. \n\nHowever, there are some arguments against knowledge distillation. A paper [22] said that, while knowledge distillation improves the generalization of student models, there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student can perfectly match the teacher.",
            "score": 0.7084802636794836,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1373
                }
            ],
            "ref_mentions": [
                {
                    "start": 1014,
                    "end": 1018,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92529296875
        },
        {
            "corpus_id": "249192332",
            "title": "A Closer Look at Self-supervised Lightweight Vision Transformers",
            "text": "We further conduct additional experiments with various models as teachers and compared their performance on various downstream tasks (see Tab. A9). The configurations of the student model (ViT-Tiny) and teacher models are presented in Tab. A7. The results indicate that an appropriately sized teacher model provides the most improvement gains in distillation, which is a common finding in the area of knowledge distillation (Cho & Hariharan, 2019;Jin et al., 2019;Mirzadeh et al., 2020). To further investigate the impact of teacher size, we conducted CKA-based layer representation analyses of these teachers, as shown in Fig. A7. It can be seen that a teacher that is too small (MAE-Small) also suffers from degraded representation on higher layers and can not provide sufficient knowledge, while a teacher that is too large (MAE-Large) would result in a mismatch of capacity with the tiny student model, considering it has over 50 times more parameters than ViT-Tiny with different depths and attention head numbers, which leads to a little distinct learned pattern compared to the reference tiny model, and may not be suitable for the student.",
            "score": 0.7078492652024935,
            "section_title": "C.4. Distilling with Larger Teachers",
            "char_start_offset": 34492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 1147
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 447,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 447,
                    "end": 464,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 464,
                    "end": 486,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7548828125
        },
        {
            "corpus_id": "268060109",
            "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems",
            "text": "Knowledge distillation involves training a smaller, more compact model (the student) using the knowledge gained by a larger, more complex model (the teacher). The primary aim is to transfer the essence of the teacher model's knowledge to the student, ensuring that the student achieves comparable performance with reduced computational overhead.",
            "score": 0.6871905605248003,
            "section_title": "Concept of distillation",
            "char_start_offset": 11680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 345
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8232421875
        },
        {
            "corpus_id": "273963558",
            "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network",
            "text": "Knowledge distillation transfers knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student) [33]. The process takes into account two main factors: the type of knowledge and the distillation scheme. Knowledge types include response-based, where the student mimics the teacher's final predictions; feature-based, where the student learns from the teacher's intermediate layers; and relation-based, where the student learns from the relationships between layers or samples [21]. Distillation schemes include offline distillation, where the teacher is trained first and then guides the student; online distillation, where both models train simultaneously; and self-distillation, where the teacher and student share the same architecture [21], [34]. This method creates lightweight models that can maintain or even surpass the performance of larger models. Figure 1.c illustrates the knowledge distillation process, showing the transfer of knowledge from a teacher model to a student model.",
            "score": 0.6850232415041992,
            "section_title": "3) Knowledge Distillation",
            "char_start_offset": 11001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1020
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 131,
                    "matchedPaperCorpusId": "264109973"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "248507674"
                },
                {
                    "start": 768,
                    "end": 772,
                    "matchedPaperCorpusId": "248507674"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "264941356",
            "title": "Incorporating Natural Language Processing into Virtual Assistants: An Intelligent Assessment Strategy for Enhancing Language Comprehension",
            "text": "Given the imperative of modest model sizes for lowlatency applications, a direct distillation from extensively www.ijacsa.thesai.org large teacher models to considerably smaller student models might hinder the effective transfer of the teacher's expertise. As a remedy, a two-stage teacher assistance setup was devised for the distillation process, as illustrated in Fig. 1. This strategy aimed to strike a balance between knowledge transfer and model size reduction. In the initial stage, the immense teacher model was compressed into an intermediate-sized model. Subsequently, the final student model was trained using this intermediate model as a guide. This approach ensured that the transfer of knowledge from the teacher to the student was well-optimized, despite the significant size reduction. Drawing inspiration from the teacher's pretraining methodology, a distillation process was initiated from a randomly initialized student model. Convergence in training signaled a seamless transition to the deployment of the Stage 2 teacher model, thus continuing the distillation process. Importantly, the data employed for distillation in both stages remained consistent with the data utilized for teacher pretraining in their respective stages. Within the intermediate student/teacher pairing, a balanced blend of categorical crossentropy (MLM loss) and soft cross-entropy was applied, with equal weighting. Remarkably, experimentation indicated no substantial benefits from incorporating the attention and hidden layer outputs of the teacher model. Transitioning to the final student model, a dual-stage process was undertaken. First, the intermediate model underwent further pretraining, exclusively using Stage 2 data and without teacher involvement. Subsequently, a distillation procedure was executed to seamlessly transfer knowledge to the compact final student model. During this distillation phase, techniques mirrored those employed in the initial distillation, with the addition of hidden-layer output matching. In essence, the approach mirrors the core principles of the process outlined in the source paper. The process ensures effective knowledge transfer while mitigating the challenges arising from substantial model size reductions.",
            "score": 0.6787402782061798,
            "section_title": "D. Distillation",
            "char_start_offset": 34882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2252
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7734375
        },
        {
            "corpus_id": "252873105",
            "title": "You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models",
            "text": "The purpose of this experiment is to visualize how the model's performance scales with the size of the pre-training dataset. Also, we check the behavior of the teacher-student knowledge distillation with the change of data size used to train a teacher and a student in a monolingual setting. \n\nWe train a monolingual model on German Wikipedia data with five sizes (in millions of characters): 10, 20, 30, 50, and 100. Subsequently, we designate 10, 50, and 100 million character models as teachers and distill their knowledge into students on the same size or smaller corpus. 7 s presented in figure 4, the teacher performance Shared script Diverse script HL -2.9 -2.5 HL Balanced -9.2 -12 Ours -6.1 -5.2  can be nearly matched by a student trained on a considerably smaller corpus. For the teacher trained on the largest split, the student performance rises steadily with the increase of distillation detest from 10 to 30 million characters and drops after that point. The performance of the student trained on 100 million characters is noticeably low. It is a sign of over-fitting, as in our setting, distillation set is always a subset of the teacher's training set. Also, in the case of teachers trained on smaller corpora, distillation on the dataset of the same size (as the teacher training set) leads to a drop in performance. Therefore, we claim that the distillation is beneficial when the teacher's training set is larger than the student's one.",
            "score": 0.6606611807969829,
            "section_title": "C Teacher-Student Method in the Monolingual World",
            "char_start_offset": 18211,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1456
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6591796875
        },
        {
            "corpus_id": "271516472",
            "title": "Leveraging Foundation Models via Knowledge Distillation in Multi-Object Tracking: Distilling DINOv2 Features to FairMOT",
            "text": "The effectiveness of the knowledge distillation process can also be influenced by the size of the teacher model. To investigate this, different sizes of the DINOv2 model were tested to determine their impact on the student model's performance. The results for the different model sizes-Small, Base, and Large-are summarized in Table 5. \n\nBased on these results, it can be inferred that the larger the teacher model, the better the overall performance in terms of MOTA. However, the Base model provided a good balance across all metrics, making it a suitable choice for knowledge distillation without the additional computational costs associated with the Large model.",
            "score": 0.6593625168716086,
            "section_title": "DINOv2 size",
            "char_start_offset": 34940,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 335
                },
                {
                    "start": 338,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 667
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "254815049",
            "title": "Sound Event Detection System Based on VGGSKCCT Model Architecture with Knowledge Distillation",
            "text": "With knowledge distillation, the prediction of a pre-trained (often sophisticated) teacher model is used to guide a student (often simple) model.\n\nNormally, there is only one student and one teacher when using knowledge distillation.However, as we apply both knowledge distillation and mean teacher method in our system, our system has one student model and two teacher models.To distinguish the teacher models, we call the teacher model in knowledge distillation the \"pre-trained teacher model.\"The teacher model in mean teacher method is stilled called \"mean teacher model.\"\n\nTo distill knowledge from the pre-trained teacher model, we add another term to the consistency loss\n\nwhere the superscript KD denotes the prediction of the pre-trained teacher model.L KD is similar to the loss between the mean teacher and the student model.It is based on mean square error, but it compares the student model with the pre-trained teacher model, instead of the mean teacher model.With the addition of knowledge distillation loss, the updated consistency loss is The binary output is then passed through a median filter to further smooth the result and avoid spurious predictions.Our system uses the same postprocessing setting as the baseline system.Specifically, all thresholds of event classes are 0.5 and the median filter size is 7 frames (i.e. about 0.45 s).",
            "score": 0.653410760073964,
            "section_title": "Loss Function of VGGSKCCT System with Knowledge Distillation",
            "char_start_offset": 14196,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 147,
                    "end": 233
                },
                {
                    "start": 233,
                    "end": 377
                },
                {
                    "start": 377,
                    "end": 496
                },
                {
                    "start": 496,
                    "end": 576
                },
                {
                    "start": 578,
                    "end": 678
                },
                {
                    "start": 680,
                    "end": 761
                },
                {
                    "start": 761,
                    "end": 836
                },
                {
                    "start": 836,
                    "end": 974
                },
                {
                    "start": 974,
                    "end": 1173
                },
                {
                    "start": 1173,
                    "end": 1244
                },
                {
                    "start": 1244,
                    "end": 1357
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427490234375
        },
        {
            "corpus_id": "253018808",
            "title": "Similarity of Neural Architectures Using Adversarial Attack Transferability",
            "text": "Knowledge distillation (KD) [47] is a training method for transferring rich knowledge of a well-trained teacher network. Intuitively, KD performance affects a lot by choice of the teacher network; however, the relationship between similarity and KD performance has not yet been explored enough, especially for ViT. This subsection investigates how the similarity between teacher and student networks contributes to the distillation performance. There are several studies showing two contradictory conclusions; Jin et al . [54] and Mirzadeh et al . [73] showed that a similar teacher leads to better KD performance; Touvron et al . [100] reports that distillation from a substantially different teacher is beneficial for ViT. \n\nWe train 25 ViT-Ti models with different teacher networks from 69 models that we used by the hard distillation strategy [47]. Experimental details are described in Appendix. Fig. 11a illustrates the relationship between the teacherstudent similarity and the distillation performance. Fig. 11a tends to show a not significant negative correlation between teacher-student similarity and distillation performance (\u22120.32 Pearson correlation coefficient with 0.12 p-value). However, if we only focus on when the teacher and student networks are based on the same architecture (i.e., Transformer), we can observe a strong positive correlation (Fig. 11b) -0.70 Pearson correlation coefficient with 0.078 p-value. In this case, our observation is aligned with [54,73]: a teacher similar to the student improves distillation performance. However, when the teacher and student networks are based on different architectures (e.g., CNN), then we can observe a stronger negative correlation (Fig. 11c) with \u22120.51 Pearson correlation coefficient and 0.030 p-value. In this case, a more dissimilar teacher leads  to better distillation performance. We also test other factors that can affect distillation performance in Appendix; We observe that distillation performance is not correlated to teacher accuracy in our experiments. \n\nWhy do we observe contradictory results for Transformer teachers (Fig. 11b) and other teachers (Fig. 11c)?",
            "score": 0.6469680882867721,
            "section_title": "Model Diversity and Knowledge Distillation",
            "char_start_offset": 27153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2040
                },
                {
                    "start": 2043,
                    "end": 2149
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 522,
                    "end": 526,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 548,
                    "end": 552,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 631,
                    "end": 636,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1479,
                    "end": 1483,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 1483,
                    "end": 1486,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8564453125
        },
        {
            "corpus_id": "248848394",
            "title": "Bearing Faulty Prediction Method Based on Federated Transfer Learning and Knowledge Distillation",
            "text": "After completing the knowledge distillation process, the three established teacher models and the student model are performed on the testing set of the target datasets of case study II. As shown in Figure 20 and Table 10 in case study II, the student model outperforms the three teacher models in terms of both the average prediction accuracy and the average KL-divergence loss with a smaller parameter size. The simulation result of the comparison study of case study II is illustrated as shown in Figure 21 and Table 11. The performance of the student model becomes better with the increase in teacher models, indicating the effectiveness of introducing the multi sources federated transfer learning into the knowledge distillation. The simulation result of the comparison study of case study II is illustrated as shown in Figure 21 and Table 11. The performance of the student model becomes better with the increase in teacher models, indicating the effectiveness of introducing the multi sources federated transfer learning into the knowledge distillation.",
            "score": 0.6449593762323103,
            "section_title": "Model Evaluation",
            "char_start_offset": 51658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1060
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.873046875
        },
        {
            "corpus_id": "248848394",
            "title": "Bearing Faulty Prediction Method Based on Federated Transfer Learning and Knowledge Distillation",
            "text": "After the proposed approach finishes the knowledge distillation process, the three established teacher models and the student model are validated on the testing set of the target datasets and the testing accuracy is illustrated as shown in Figure 14. All three teacher models and the student model can reach an accuracy above 95%. The experiment is repeated ten times and the average testing accuracy, the parameter size and the average KL divergence loss of three teacher models and the student model is illustrated as shown in Table 5. It should be noted that although the student model does not have the best behavior among all the models in terms of average accuracy, the student model has a comparatively smaller parameter size and a lower average KL divergence loss. behavior among all the models in terms of average accuracy, the student model comparatively smaller parameter size and a lower average KL divergence loss. In order to further evaluate the effectiveness of the proposed approach, m knowledge distillation frameworks with different arrangement of teachers and sour tasets are introduced for comparison. The specific detail is illustrated in Table 6. As in Table 6, case I is the knowledge distillation framework proposed in the paper the student model is guided by three teachers; cases II-IV represent the framework the students are guided by only two teachers; cases V-VII represent the framework the students are guided by only one teacher, which is the same as the single teacher distillation referred in section II. The single teacher-based distillation regulation us comparison here is illustrated as shown in Equation ( 12), where the D (Q ||y notes the KL divergence loss of the teacher model in the current epoch  In order to further evaluate the effectiveness of the proposed approach, multiple knowledge distillation frameworks with different arrangement of teachers and source datasets are introduced for comparison. The specific detail is illustrated in Table 6.",
            "score": 0.6422929936262801,
            "section_title": "Model Evaluation",
            "char_start_offset": 39307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 1995
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67138671875
        },
        {
            "corpus_id": "263677297",
            "title": "Efficient and Controllable Model Compression through Sequential Knowledge Distillation and Pruning",
            "text": "The Knowledge Distillation (KD) paper by Hinton et al. [1], introduces the concept of knowledge distillation. It proposes training a smaller model (the student) to mimic the behavior of a larger, more complex model (the teacher) by transferring its knowledge. The teacher model's soft probabilities (logits) are used as \"soft targets\" during training to guide the student's learning. The approach demonstrates that the student can achieve similar or even superior performance to the teacher despite being much smaller and computationally efficient. Attention Transfer (AT), by Zagoruyko and Komodakis [5], introduces attention transfer for knowledge distillation. Attention maps are used to focus on important regions in the input data. The student model is trained to mimic the attention maps produced by the teacher model. By learning to attend to similar regions, the student can improve their generalization and performance. \n\nVariational Information Distillation (VID), introduced by Ahn et al. [8], incorporates variational distributions in knowledge distillation. The student model learns from the uncertainty present in the teacher's predictions. By considering the variance in the teacher's logits, the student can capture not only the mean representation but also the level of confidence in the teacher's predictions. Contrastive Representation Distillation (CRD), by Tian et al. [2], leverages contrastive learning for knowledge distillation. The teacher and student models are compared using contrastive loss, where positive pairs are from the same class and negative pairs are from different classes. This encourages the student to capture discriminative features while maintaining intra-class similarity and inter-class differences. Similarity-Preserving Knowledge Distillation, by Tung et al. [6], proposes similarity-preserving knowledge distillation, where the student learns to preserve pairwise similarities of the teacher's features during training. By maintaining the relative similarities between the teacher's features, the student model can capture the fine-grained details present in the teacher's representations. \n\nSelf-Residual Representation Learning (SRRL), presented by Zhang et al. [6], proposes SRRL for knowledge distillation, focusing on self-residual representations.",
            "score": 0.629120962895759,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2139
                },
                {
                    "start": 2142,
                    "end": 2303
                }
            ],
            "ref_mentions": [
                {
                    "start": 1000,
                    "end": 1003,
                    "matchedPaperCorpusId": "118649278"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9580078125
        },
        {
            "corpus_id": "264492472",
            "title": "Intra- and Inter-Modal Curriculum for Multimodal Learning",
            "text": "Knowledge distillation refers to the transfer of knowledge from teacher models to student ones. Since it is proposed for the goal of model compression [10], teacher models are usually large-scale, ensembled and pretrained, while students are relatively small and fast, which is named offline distillation [23,32,57,68,81]. There are also methods of online distillation [3,82], where teacher and student models are trained simultaneously, and self-distillation [15,45], where teacher and student models are the same. Apart from the training strategy, the form of knowledge is another important component [24], which can be categorized into response-based [32], feature-based [56] and relation-based [75]. In this paper, we adopt offline distillation and feature-based knowledge.",
            "score": 0.6262858722427935,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9458,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 777
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 155,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 315,
                    "end": 318,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 372,
                    "end": 375,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "233296717"
                },
                {
                    "start": 464,
                    "end": 467,
                    "matchedPaperCorpusId": "236034189"
                },
                {
                    "start": 603,
                    "end": 607,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 698,
                    "end": 702,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88134765625
        },
        {
            "corpus_id": "277961724",
            "title": "Transferring enhanced material knowledge via image quality enhancement and feature distillation for pavement condition identification",
            "text": "Knowledge distillation (KD) transfers the implicit but valuable knowledge from a teacher network to a student network with the goal of greatly improving the performance of the student network. For instance, Hinton et al. 65 utilized pre-trained teacher-generated logits as an additional goal for students. Motivated by KD, various logit-based approaches have been proposed for performance improvement. For example, Zhang et al. 66 proposed a deep mutual learning (DML) model, which replaces a pre-trained teacher with a set of students so that the distillation mechanism needs to train a large network of students in advance. Mirzadeh et al. 67 proposed a teacherassistant knowledge distillation (TAKD) model, in which better teachers distill poorer students due to the large performance gap between them. Therefore, a similarity-based distillation method was proposed that is different from the traditional logits-based method, which attempts to explore the potential relationship between samples in the feature space. Tung et al. 68 proposed a similarity-preserving knowledge distillation (SPKD) method to allow pairs of input samples with similar activations in the teacher network to produce the same activations in the student network, thus guiding the learning procedure of the student network. Unlike logit-based method, feature distillation aims to make the mid-layer features of student and teacher as similar as possible. To make it easier to transfer knowledge from the teacher network, Kim et al. 69 introduced so-called \"factors\" as a more understandable form of intermediate representations. To match the semantics gap between teacher and student, Chen et al. 70 proposed a kind of cross-layer KD method, which adaptively assigns the appropriate teacher layer to each student layer through attention allocation. Furthermore, contrastive representation distillation (CRD) 71 and Softmax regression representation learning (SRRL) 72 show that the last-layer feature representations are more suitable for KD. One potential reason is that the last-layer feature representation is relatively closer to the final classifier. This study also draws on this idea to use the final layer for feature distillation.",
            "score": 0.6247940740736997,
            "section_title": "Feature distillation",
            "char_start_offset": 17369,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 428,
                    "end": 430,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 642,
                    "end": 644,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1032,
                    "end": 1034,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1509,
                    "end": 1511,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 1674,
                    "end": 1676,
                    "matchedPaperCorpusId": "227335337"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "268707594",
            "title": "From Toxic to Trustworthy: Using Self-Distillation and Semi-supervised Methods to Refine Neural Networks",
            "text": "Knowledge distillation, as shown in Fig. 3 is a technique that aims to transfer the knowledge from a large, overparameterized teacher model to a smaller, more compact student model. This process is often used as a compression approach and can lead to improved performance, as well as enable high compression and rapid acceleration. By transferring the knowledge from the teacher model to the student model, the student model can achieve comparable performance with the teacher model while being more efficient in terms of memory and computation. The basic idea behind knowledge distillation is to train the student model to mimic the outputs of the teacher model, such as the probability vectors, while minimizing the difference between the student's predictions and the teacher's predictions. This allows the student model to learn from the teacher's expertise, improving its performance and generalization ability on the target task. However, traditional knowledge distillation methods suffer from low efficiency in knowledge transfer and challenges in designing and training appropriate teacher models. To address these issues, self-distillation (Zhang et al. 2019) has been proposed as a novel one-step framework, focusing directly on training the student model. This approach not only reduces training time significantly but also achieves higher accuracy, making it a promising alternative to traditional knowledge distillation methods.",
            "score": 0.6236252862767719,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1441
                }
            ],
            "ref_mentions": [
                {
                    "start": 1149,
                    "end": 1167,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9150390625
        },
        {
            "corpus_id": "271956980",
            "title": "Bring the Power of Diffusion Model to Defect Detection",
            "text": "Large-scale deep models have achieved remarkable success, but their computational complexity and massive storage requirements make real-time deployment a challenge, especially on resource-constrained devices such as video surveillance and self-driving cars. \n\nKnowledge distillation is a classical approach of model compression and acceleration that effectively learns small student models from large teacher models [18]. In knowledge distillation, small student models are usually supervised with the assistance of large teacher models, allowing the student models to mimic the teacher models for a competitive or even superior performance. Knowledge distillation is similar to the way humans learn, and the key issue in distillation is how to better transfer knowledge from large teacher models to small student models. Inspired by this, recent methods to knowledge distillation have been extended to teacher-student learning [19], mutual learning [20], self-learning [21] and so on. Most of the extensions to knowledge distillation focus on compressing deep neural networks. The generated lightweight student networks can be easily deployed in applications such as visual recognition, speech recognition and natural language processing (NLP). \n\nIn addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled [22]. We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student.",
            "score": 0.6230847257772065,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 7189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 260,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1647
                }
            ],
            "ref_mentions": [
                {
                    "start": 416,
                    "end": 420,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "259697005"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "249145972"
                },
                {
                    "start": 970,
                    "end": 974,
                    "matchedPaperCorpusId": "251196646"
                },
                {
                    "start": 1446,
                    "end": 1450,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "258887887",
            "title": "Triplet Knowledge Distillation",
            "text": "Intuitively, the performance of the student should increase when the teacher has larger capacity and higher performance. \n\nAn overview of Triplet Knowledge Distillation. In the gth generation, a pre-trained anchor Ag supervises a pair of randomly initialized student Sg and teacher Tg; the student and the teacher also learn mutually from each other. After the gth generation, the student Sg will become the new anchor Ag+1 for the (g + 1)th generation. Supervision from task label is omitted in the figure. \n\nHowever, Cho et al. (Cho & Hariharan, 2019) identify that very large teacher actually makes the student deteriorate. This phenomenon has also been witnessed by following works (Mirzadeh et al., 2020;Zhu & Wang, 2021), and has been attributed to the capacity mismatch between teacher and student. To overcome this problem, ESKD (Cho & Hariharan, 2019) proposes an early-stopping strategy, and SCKD (Zhu & Wang, 2021) automatically adjusts the distillation process through considering the gradient similarity between the teacher's and the student's distillation loss. TAKD (Mirzadeh et al., 2020) divides the distillation process into multiple stages, and introduces intermediatesized models, called teacher assistant, to bridge the capacity gap between the original teacher and student. While TAKD (Mirzadeh et al., 2020) treats mimicking difficulty as an inherent property of teacher model capacity, i.e., larger teachers are inherently harder to mimic, we believe that a given large network with fixed capacity should be able to fit both hard and easy functions, and we could make a large teacher still easy to mimic by deliberately making the function it expresses easy. Detailed comparisons between TAKD and our TriKD are provided in C.1 in Appendix.",
            "score": 0.6229960151347498,
            "section_title": "'Larger Teacher, Worse Student'",
            "char_start_offset": 6892,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 123,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1763
                }
            ],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 553,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 686,
                    "end": 709,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 709,
                    "end": 726,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 837,
                    "end": 860,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 907,
                    "end": 925,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1081,
                    "end": 1104,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1307,
                    "end": 1330,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8427734375
        },
        {
            "corpus_id": "266891284",
            "title": "Research on Semantic Segmentation of Fish-Eye Images for Autonomous Driving",
            "text": "Knowledge distillation is proposed by Hinton et al. [16], and is used to compress model size, as well as speed up training. In knowledge distillation, the hidden or output layer of the teacher model is used as a target for the student network to learn so that it can achieve similar performance as the larger model. Knowledge distillation can be divided into two main approaches, one for the output distillation of the model and one for the intermediate layers. \n\nThe main purpose of distillation of the output of the model is to make the model directly mimic the final output of the teacher network, the distillation loss can be defined as: (2) where is the KL scatter. \n\nare the outputs of the teacher and the student, respectively. And the KL loss can be expressed as: \n\n(3) \n\nThe distillation of the middle layer of the network was first proposed in Fitnets [17], suggesting an idea of matching the activation layers of the teacher-student network features. The feature-based knowledge distillation can be formulated as: (4) where and are the intermediate layers of the teacher and student networks, and refer to the transformation function in case of a mismatch in the shape of the feature maps of the teacher-student network, and is the l2 -norm distance, or MSE loss: \n\n(5)",
            "score": 0.6213991895160629,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1280
                }
            ],
            "ref_mentions": [
                {
                    "start": 642,
                    "end": 645,
                    "matchedPaperCorpusId": "247362954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7734375
        },
        {
            "corpus_id": "262084420",
            "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning",
            "text": "This technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) [84]. This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies. \n\nThe extraction of knowledge from teachers and its utilization for training student networks can be classified into three categories: response-based, feature-based, and relationbased. Specifically, response-based knowledge distillation involves mimicking the final predictions of the teacher model by capturing the neural response in the last output layer [87]. Feature-based knowledge expands upon this approach by using both the outputs of the last layer and intermediate layers to train thinner networks [81]. Finally, relation-based knowledge takes a step further by exploring the relationships between different layers or data samples in addition to the outputs of specific layers in the teacher model [86]. \n\nThe distillation schemes are also crucial for the student learning process. Depending on the training strategy, the following three different categories are presented: offline distillation, online distillation, self-distillation. Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits [80]. On the other hand, online distillation is an end-to-end approach where both the teacher and student models are updated simultaneously, making it suitable when the teacher model is not significantly larger or higher performing [85]. Finally, self-distillation is a special case of online distillation where the teacher and student networks have the same architecture [79]. \n\nIn general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy [88]. For this reason, it is widely adopted in several fields where existing models are well-performing but unable to be deployed ''as they are'' in resource-constrained hardware.",
            "score": 0.6206902286892579,
            "section_title": "3) Knowledge Distillation",
            "char_start_offset": 28218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2057
                },
                {
                    "start": 2060,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 115,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 858,
                    "end": 862,
                    "matchedPaperCorpusId": "232104927"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "235613518"
                },
                {
                    "start": 1209,
                    "end": 1213,
                    "matchedPaperCorpusId": "239486869"
                },
                {
                    "start": 1680,
                    "end": 1684,
                    "matchedPaperCorpusId": "198179767"
                },
                {
                    "start": 1912,
                    "end": 1916,
                    "matchedPaperCorpusId": "224914013"
                },
                {
                    "start": 2052,
                    "end": 2056,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 2183,
                    "end": 2187,
                    "matchedPaperCorpusId": "255266316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9169921875
        },
        {
            "corpus_id": "258887887",
            "title": "Triplet Knowledge Distillation",
            "text": "Large models tend to generalize better. However, existing studies (Mirzadeh et al., 2020;Zhu & Wang, 2021;Cho & Hariharan, 2019) have shown that in knowledge distillation, the performance of the student would indeed deteriorate when the capacity of the teacher increases. To boost the performance of the student when the capacity gap between the teacher and the student is large, TAKD (Mirzadeh et al., 2020) proposed to bridge the gap by introducing intermediate-sized models named teacher assistant. Both TAKD and our TriKD attempt to reduce the difficulty for the student to mimic the teacher. However, TAKD treats learning difficulty as an inherent property of teacher model capacity, i.e. larger teachers are inherently harder, and smaller teachers are easier. In contrast, we believe that a given network architecture with fixed capacity should be able to fit both hard and easy functions, and we could make a large teacher still easy to mimic by deliberately making the function it expresses easy; the reason why large teacher usually fails in existing distillation frameworks is that the teacher would spontaneously learn to express sophisticated functions when trained without constraint. This is easy to understand when considering the teacher model's function identity: with larger capacity, the larger teacher should be able to easily fit the same function as a smaller teacher does, and thus in distillation a student supervised by a larger teacher should at least perform no worse than supervised by a smaller one. Here we also provide an experiment to compare our TriKD with TAKD. The experiment is conducted on CIFAR100. For fair comparision, following TAKD, we use resnet8 as the student and resnet110 as the teacher, and we use stochastic gradient descent with Nesterov momentum of 0.9 and learning rate of 0.1 for 150 epochs. we decrease learning rate to 0.01 on epoch 80 and 0.001 on epoch 120. Weight decay is set to 0.0001. The result is shown in Table 8. Its shows that our TriKD consistently outperforms TAKD with different teacher assistant size.",
            "score": 0.6196035520466769,
            "section_title": "C. More experiments C.1. Comparing with TAKD",
            "char_start_offset": 33956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2071
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 89,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 89,
                    "end": 106,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 106,
                    "end": 128,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 385,
                    "end": 408,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8544921875
        },
        {
            "corpus_id": "264490946",
            "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
            "text": "Early works in knowledge distillation focus on compressing large teacher models into smaller student models. Bucila et al. (2006) achieve this by matching the soft targets of the teacher. Hinton et al. (2015) propose temperature scaling for lower probabilities. Recent works extend this with structural context: attention-transfer (Zagoruyko and Komodakis, 2017) encourages similar feature response Figure 1: (a) We show the share of complementary knowledge (\u03c1 pos , perc. of pos. label flips of teacher w.r.t. student) against perform. differences \u2206 acc for 466 teacher/student pairs, and find significant complementary context even for much weaker teachers. (b) Looking at the entropy of the compl. knowledge distribution over classes, we find context to be more specialized for weaker teachers. patterns, Romero et al. (2015) and Zagoruyko and Komodakis (2017) propose (weighted) regressionguided student feature activations. However, these approaches are limited to matching student and teacher architectures. Tian et al. (2020) propose contrastive distillation, aligning teacher and student feature spaces with flexibility in feature dimensionalities, while highlighting performance overlap among most distillation objectives. These insights transfer to multiple teacher distillation (Luo et al., 2019;de Carvalho et al., 2022;Shen et al., 2019a;b), e.g. via teacher outputs reweighting (Wu et al., 2021;Liu et al., 2020;Yu et al., 2022;Yuan et al., 2020a). Unlike standard distillation, we look at knowledge transfer between arbitrary, already trained models -a much more difficult task, particularly when no restrictions (in contrast to e.g. (Yuan et al., 2020b)) are imposed on relative performances or architectures, and initial knowledge should be retained. On a conceptual level, this also connects to recent works on weak-to-strong model generalization for superalignment, for which our work can provide an orthogonal perspective and useful practical insights (Burns et al., 2023).",
            "score": 0.6190722306107486,
            "section_title": "RELATED WORK",
            "char_start_offset": 6699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 129,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 331,
                    "end": 362,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 808,
                    "end": 828,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 833,
                    "end": 863,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1014,
                    "end": 1032,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1289,
                    "end": 1307,
                    "matchedPaperCorpusId": "195584436"
                },
                {
                    "start": 1332,
                    "end": 1351,
                    "matchedPaperCorpusId": "53231387"
                },
                {
                    "start": 1409,
                    "end": 1426,
                    "matchedPaperCorpusId": "224818016"
                },
                {
                    "start": 1442,
                    "end": 1461,
                    "matchedPaperCorpusId": "228376532"
                },
                {
                    "start": 1649,
                    "end": 1669,
                    "matchedPaperCorpusId": "219962714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "267522781",
            "title": "Online Cascade Learning for Efficient Inference over Streams",
            "text": "Knowledge distillation, originally conceptualized by Hinton et al. (2015), emerged as a technique to transfer knowledge from a large, complex model (teacher) to a smaller, more efficient one (student), intending to retain performance while reducing computational costs. Notable advancements include the works of Sanh et al. (2019), who demonstrated the effectiveness of distilling the capabilities of BERT into smaller models, and Gu et al. (2023) who successfully applied distillation to LLMs, achieving comparable performance with significantly reduced model sizes. However, the significant difference in capabilities between the teacher and student model can lead to challenges, particularly when dealing with complex queries that require advanced reasoning or involve intricate subject matter (Cho & Hariharan, 2019;Zhang et al., 2022), thereby highlighting a performance gap that distillation alone cannot overcome (Rawat et al., 2021).",
            "score": 0.6143835894323737,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 28566,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 941
                }
            ],
            "ref_mentions": [
                {
                    "start": 797,
                    "end": 820,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "257513218",
            "title": "Model Compression for Deep Neural Networks: A Survey",
            "text": "As shown in Figure 5, knowledge distillation is a teacher-student architecture [106][107][108].The teacher network is a complex pre-trained network, and the student network is a simple small network.The teacher network provides the student network with prior knowledge so that the student network achieves similar performance to that of the teacher network.Deploying deep models in mobile devices is challenging due to the limited processing power and memory of these devices.To address these issues, Bucilu\u0203 et al. [109] first proposed model compression to transfer information from a large model to train a small model without significant accuracy degradation.Henceforth, the training of small models by large models was called knowledge distillation [108,110,111].Chen et al. [112] posited that feature embedding from deep neural networks could convey complementary information and, thus, proposed a novel knowledge-distilling strategy to improve its performance.The main idea of knowledge distillation was that the student model imitated the teacher model to achieve competitive, or even superior, performance.The key focus was how to transfer knowledge from a large teacher model to a small student model.\n\nIn the process of knowledge distillation, knowledge types, distillation strategies, and teacher-student architectures have played key roles in the student learning process.The activations, neurons, and features of the middle layer were available as knowledge to guide the learning of the student model [113][114][115][116][117]. The relationship between different activations, neurons, and features contained the rich knowledge learned by the teacher model [118][119][120][121][122]. As shown in Figure 6, three methods of knowledge distillation were introduced.These three distillation methods are described in detail in the following sections.",
            "score": 0.6118065854157273,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 26253,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 767
                },
                {
                    "start": 767,
                    "end": 966
                },
                {
                    "start": 966,
                    "end": 1114
                },
                {
                    "start": 1114,
                    "end": 1210
                },
                {
                    "start": 1212,
                    "end": 1384
                },
                {
                    "start": 1384,
                    "end": 1774
                },
                {
                    "start": 1774,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 89,
                    "matchedPaperCorpusId": "236159551"
                },
                {
                    "start": 516,
                    "end": 521,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 758,
                    "end": 762,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 762,
                    "end": 766,
                    "matchedPaperCorpusId": "16550689"
                },
                {
                    "start": 779,
                    "end": 784,
                    "matchedPaperCorpusId": "52927917"
                },
                {
                    "start": 1524,
                    "end": 1529,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1529,
                    "end": 1534,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1534,
                    "end": 1539,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1669,
                    "end": 1674,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1674,
                    "end": 1679,
                    "matchedPaperCorpusId": "195847947"
                },
                {
                    "start": 1679,
                    "end": 1684,
                    "matchedPaperCorpusId": "198185886"
                },
                {
                    "start": 1684,
                    "end": 1689,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1689,
                    "end": 1694,
                    "matchedPaperCorpusId": "102351826"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.865234375
        },
        {
            "corpus_id": "276776153",
            "title": "Mamba base PKD for efficient knowledge compression",
            "text": "Knowledge distillation has been extensively studied as a model compression technique that allows a smaller student model to learn from a larger teacher model [5,9] (see Figure 1). Traditional KD includes transferring the knowledge to a student model from a teacher model's output [2], intermediate layers [16], or only the relationships between different layers and data samples [15,21]. These techniques lead to computational savings while preserving high accuracy. However, one limitation of traditional KD is the performance drop when there is a large capacity gap between the teacher and student models because the student cannot effectively represent key features of the teacher. This limitation has been addressed by various methods [15,16,21,22] that modify distillation algorithms or employ multi-stage learning. \n\nTo overcome these challenges, hierarchical or layer-wise distillation approaches were proposed, where knowledge is progressively transferred across multiple layers of the teacher to the student. FitNets, introduced in [16], transfer intermediate representations from the teacher model to the student, improving the student's ability to learn fine-grained features progressively. Similarly, selfdistillation [22] trains a model by distilling knowledge into itself at different stages, a concept related to progressive distillation where multiple students incrementally improve performance [4]. \n\n. . .",
            "score": 0.6098977361374206,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5819,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1415
                },
                {
                    "start": 1418,
                    "end": 1423
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 161,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "229679667"
                },
                {
                    "start": 383,
                    "end": 386,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "229679667"
                },
                {
                    "start": 746,
                    "end": 749,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 749,
                    "end": 752,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1230,
                    "end": 1234,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1411,
                    "end": 1414,
                    "matchedPaperCorpusId": "265067529"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "246294774",
            "title": "Dynamic Rectification Knowledge Distillation",
            "text": "The primary challenge with conventional knowledge distillation is training a cumbersome model to create resource-and time-intensive teacher models. To circumvent this constraint, research in the area of Teacher Free Knowledge Distillation has seen some interest [24]. The concept of this framework was motivated by two further novel experiments conducted by the authors. The first experiment established that a poorly trained teacher model may enhance the performance of a student model, which they termed Defective Knowledge Distillation (De-KD). The second experiment established that a student model can distill knowledge back to a teacher model in order to enhance its performance, which they termed Reversed Knowledge Distillation (Re-KD). By examining the results of these two experiments, the researchers demonstrated that a student model may improve its performance without the assistance of a teacher model completely, which they termed Teacher Free Knowledge Distillation (Tf-KD) [24]. \n\nThe first Tf-KD approach is referred to as \"Self-Training Knowledge Distillation\" or Tf-KD self . As the authors explain in their study, a student may teach a teacher, and a poorly trained teacher can benefit a student as well. The authors devised this strategy in response to the absence of a large teacher model. They trained a model with the configuration of student S over a certain number of epochs using cross-entropy loss to build a pre-trained self-teacher model that was then utilized as the weak teacher S t in order to distill knowledge to the student S using soft target transfer. In their experiment, the authors employed Hinton's [7] knowledge distillation setup, including the loss function, described in section (III-A), but they substituted the cumbersome teacher T with the weak teacher S t . This means the output scores O t = S t (x) instead of coming from T . Meaning now O \u03c4 t = \u03c3(S t (x), \u03c4 ) , that is the KL divergence term learns from the weak teacher.",
            "score": 0.6083047661280462,
            "section_title": "B. Teacher Free knowledge Distillation",
            "char_start_offset": 11002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 995
                },
                {
                    "start": 998,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1976
                }
            ],
            "ref_mentions": [
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 990,
                    "end": 994,
                    "matchedPaperCorpusId": "219962714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.806640625
        },
        {
            "corpus_id": "257912519",
            "title": "MiniRBT: A Two-stage Distilled Small Chinese Pre-trained Model",
            "text": "The traditional knowledge distillation method transfers knowledge directly from the teacher to the student. However, when there is a significant difference in the structures of the teacher and student models, this approach may result in a performance gap. To address this issue, we proposed using a twostage distillation approach during the pre-training stage, which builds on the concept of Teacher Assistant Knowledge Distillation (Mirzadeh et al., 2020). \n\nAs depicted in Figure 1, this method involves distilling knowledge from the teacher (RoBERTa) to the teacher assistant (RBT6), and then from the teacher assistant to the student (MiniRBT). The intermediate step of the teacher assistant helps to reduce the size gap between the teacher and the student model, subsequently improving the student models' performance in downstream tasks. \n\nTo apply knowledge distillation (KD) with hidden layer distillation and prediction layer distillation, we employed TextBrewer (Yang et al., 2020), a PyTorch-based model distillation toolkit designed for natural language processing. We distill the knowledge from the output of the hidden layer. The objective is \n\nwhere the matrices H i s \u2208 R l\u00d7d and H i t \u2208 R l\u00d7d represent the hidden representation of the i -th student's hidden layer and the i-th teacher's hidden layer respectively. The W h \u2208 R d \u00d7d is a linear transformation that matches the hidden state of the student network and the hidden state of the teacher network. Apart from mimicking the hidden layer behavior of the teacher, we also trained the student model by employing the cross-entropy loss with the teacher's soft target probability \n\nwhere z S and z T are the logits vectors predicted by the student and teacher respectively, and p = softmax(z/t) is the scaled probability with temperature t and logits z. Finally, we combine the hidden layer distillation with the prediction layer distillation: \n\n(3)",
            "score": 0.6078091706863136,
            "section_title": "Two-stage Distillation",
            "char_start_offset": 4988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1649
                },
                {
                    "start": 1652,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1913
                },
                {
                    "start": 1916,
                    "end": 1919
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 456,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 972,
                    "end": 991,
                    "matchedPaperCorpusId": "211572557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78955078125
        },
        {
            "corpus_id": "247447127",
            "title": "Wavelet Knowledge Distillation: Towards Efficient Image-to-Image Translation",
            "text": "Knowledge distillation is first proposed in a teacherstudent (TSKD) paradigm, where the teacher model is first trained and then distilled to a student model. Recently, abundant knowledge distillation paradigms are proposed to achieve better performance, such as deep mutual learning  (DML) [57], self-distillation (SD) [55] and teacher-assistant knowledge distillation (TAKD) [34]. Many of these methods lead to higher effectiveness than the traditional (TSKD) paradigm. Unfortunately, these KD paradigms are usually only evaluated on classification tasks and their performance in more challenging tasks has not been well-studied. In this subsection, we have given a comparison of the following KD paradigms on Image-to-Image translation with GANs. \n\n\u2022 TSKD is the most common KD diagram which trains a large teacher first and then distills it to a small student. \n\n\u2022 TAKD is proposed to bridge the gap between students and teachers with a teacher-assistant. It first distills knowledge from teachers to teacher-assistants and then distills knowledge from teacher assistants to the students [34]. \n\n\u2022 SD is a special case in TSKD when the student and the teacher have the identical architecture. Experimental and theoretical results have proven its success [35]. \n\n\u2022 DML (a.k.a. online knowledge distillation, collaborative learning) trains several students (a.k.a. peers) to learn from each other [57]. \n\nObservation Experimental results of different knowledge distillation paradigms have been shown in Table 5. It is observed that: (a) All the knowledge distillation schemes lead to performance gain compared with the baseline. Besides, the most common TSKD achieves better performance than the other KD schemes. (b) The performance improvements in DML and SD are much lower than that in TSKD and TAKD, which indicates a pre-trained and highquality teacher is very crucial on image-to-image translation. (c) There is no significant performance difference between TSKD and TAKD, which means that the teacher assistants can not facilitate the training of a tiny student in knowledge distillation on image-to-image translation.",
            "score": 0.607041445043474,
            "section_title": "Knowledge Distillation Paradigm",
            "char_start_offset": 18345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 748
                },
                {
                    "start": 751,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1403
                },
                {
                    "start": 1406,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 294,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1398,
                    "end": 1402,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "264426761",
            "title": "PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation",
            "text": "Knowledge distillation (Bucila et al., 2006;Hinton et al., 2015;West et al., 2022) refers to training a smaller student model mimicking the behavior of a much larger teacher model. In particular, the objective function aims to match the output distribution of the student model to that of the teacher model. By doing so, knowledge of the teacher model is effectively distilled into a much smaller student model, allowing a similar level of performance as the teacher at a lower computational cost. Shridhar et al. (2022) distill a GPT3 (6B) model into a GPT-2 model for a Chain-of-thought (CoT) reasoning task. Liang et al. (2021) propose MixKD to encourage the student model to mimic the teacher's behavior on not only the available training examples but also on interpolated examples. Finally, Sun et al. (2020) distill knowledge through intermediate layers of the teacher via a contrastive objective. In comparison, our approach allows knowledge distillation of a massive teacher model like GPT3.5 (175B parameters) into significantly smaller student models like DistilBERT and BERT (67M and 110M parameters, respectively).",
            "score": 0.6067847366072757,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1126
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 44,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 44,
                    "end": 64,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 64,
                    "end": 82,
                    "matchedPaperCorpusId": "238857304"
                },
                {
                    "start": 611,
                    "end": 630,
                    "matchedPaperCorpusId": "226226888"
                },
                {
                    "start": 796,
                    "end": 813,
                    "matchedPaperCorpusId": "221995575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8544921875
        },
        {
            "corpus_id": "257482435",
            "title": "Robust Knowledge Distillation from RNN-T Models with Noisy Training Labels Using Full-Sum Loss",
            "text": "As our aim is to improve knowledge distillation when using bad or high-WER teacher models, we report results on the AMI dataset [16] since it is noisy and considered a difficult task. Table 1 shows the results of applying hard distillation using the strongest teacher L and the weakest teacher S3. We can observe that even with teacher L, hard distillation is much worse than using soft distillation. In addition, WER increases when using the weakest teacher S3. The reason behind this is that the pseudo labels generated by such teacher models are very noisy, especially on AMI which requires utilizing other distillation methods. Figure 1 shows a comparison between soft distillation and FS distillation when using varying quality teachers. We use L1 loss for FS distillation. First, we can observe that the quality of the teacher model has a significant effect on improving the WER of the student model. FS distillation outperforms soft distillation for all teachers. The student model outperforms the teacher models M5, L5, and S3 when using FS distillation whereas with soft distillation it only outperforms S3. This shows the robustness of FS distillation method.",
            "score": 0.6065605148834845,
            "section_title": "Distillation Results",
            "char_start_offset": 8913,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1169
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 132,
                    "matchedPaperCorpusId": "6118869"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66357421875
        },
        {
            "corpus_id": "253734490",
            "title": "Scalable Collaborative Learning via Representation Sharing",
            "text": "The concept of knowledge distillation (KD) originated in Bucila et al. [8] as a way of compressing models, and was later generalized by Hinton et al. [19] (see Gou et al. [14] for an overview of the field). The standard use case for KD is that of a Teacher-Student (or offline) configuration, in which the teacher model (usually a large and well-trained model) transfers its knowledge to the student model (usually a smaller model) by sharing its last layer activations on a given transfer dataset (see Fig. 1a). The knowledge is then distilled into the student model using a divergence loss between the teacher and student models outputs (response-based KD) or intermediate layers (feature-based KD) on the transfer dataset. Traditional KD schemes use a transfer set that is similar (or identical) to the teacher training dataset, but some recent work has focused on data-free (or zero-shot) KD. This can be achieved either by looking at some of the teacher model statistics to generate synthetic transfer data [35,39,6], or by training a GAN in parallel [37,10,2]. It has also been shown that positive results can be obtained using mismatched or random unlabeled data for the distillation [27,40]. A key concept for our framework is the idea of online KD (or co-distillation [3], see Fig. 1b), where each model is treated as both a teacher and a student, meaning that the KD is performed synchronously with the training of each model (rather than after the training of the teacher model) [15,57,45]. Finally, ensemble KD (see Fig. 1c) refers to the setup where the knowledge is distilled from an ensemble of teacher (offline) or teacher-student (online) models. \n\nCollaborative Learning via Knowledge Distillation A growing body of literature has recently investigated ways of using online knowledge distillation (KD) [8,19] for collaborative learning in order to alleviate the need of sharing model updates (FL) or individual smashed data (SL).",
            "score": 0.60528715279937,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6216,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1947
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 74,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 171,
                    "end": 175,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1016,
                    "end": 1019,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "162183830"
                },
                {
                    "start": 1060,
                    "end": 1063,
                    "matchedPaperCorpusId": "91183944"
                },
                {
                    "start": 1063,
                    "end": 1065,
                    "matchedPaperCorpusId": "209500810"
                },
                {
                    "start": 1195,
                    "end": 1198,
                    "matchedPaperCorpusId": "227013462"
                },
                {
                    "start": 1490,
                    "end": 1494,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1494,
                    "end": 1497,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "253499102",
            "title": "PILE: Pairwise Iterative Logits Ensemble for Multi-Teacher Labeled Distillation",
            "text": "We conduct several comparison experiments to verify our proposed method. The models in the offline comparison experiments include: \n\n\u2022 Base: We use an ERNIE-based ranking model as our base model, which is finetuned with a pairwise loss using human-labeled querydocument pairs without any guidelines from teachers; \n\n\u2022 single-KD: In this setting we add knowledge distillation loss when training the base model using the teacher that performs best on the test set;  \u2022 AE-KD: Instead of using the single teacher, this variant uses an ensemble of 3 teachers with averaged weight to perform knowledge distillation; \n\n\u2022 PILE-KD: When performing knowledge distillation, PILE-KD uses human-annotated labels with the help of the PILE algorithm to conduct a dynamic weighting process for the ensemble of 3 teachers. \n\nThe results of each model are shown in Table 2 with the improvement compared to the base model. We also report the performance of the teachers used in knowledge distillation. As we expected, all the distilled models consistently outperform finetuned base model thanks to teacher models' guidance and regularization. And besides, using an ensemble of teachers gains further promotion than the single teacher distillation. After ensembling multiple teachers by averaging distillation logits, the PNR reaches 3.16, exceeding the base by 1.61%. This shows that the remission from biased distillation by the cooperation of multiple teachers improves students in semantic matching. Moreover, by applying the PILE algorithm, we can see that the student can beat the base model by a large margin w.r.t PNR, where the value is improved to 3.18 by 2.25% improvement. It shows the effectiveness of dynamic reduction of biased teachers' weight in the ensemble process.",
            "score": 0.6026585527470341,
            "section_title": "Offline Experimental Results",
            "char_start_offset": 16999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 130
                },
                {
                    "start": 133,
                    "end": 313
                },
                {
                    "start": 316,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1764
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72802734375
        },
        {
            "corpus_id": "270094801",
            "title": "Aligning in a Compact Space: Contrastive Knowledge Distillation between Heterogeneous Architectures",
            "text": "Knowledge distillation aims to transfer knowledge from a large-size teacher model T to a small-size student model S.This process often uses logits and features as carriers for knowledge transfer.\n\nLogits-based Distillation.Logits carry category distribution information that directly impacts classification probabilities.Logit-based approach has proven to be an effective strategy to circumvent the discrepancies in the spatial distribution of features among heterogeneous models, as recent research has demonstrated.Specifically, the output category distribution of the student model can be made to approximate that of the teacher model, thereby improving the performance of the student model.\n\nwhere p s and p t are the predicted logits of student model and teacher model, respectively.L KLD is the Kullback-Leibler divergence loss function.y is the one-hot groundtruth labels.\u03bb is the hyperparameter adjusts the weight of soft label p t .\n\nFeature-based Distillation.By minimizing the Euclidean distance between features, feature distillation offers a finer-grained supervision for the student model through the introduction of sample-level comparisons.Formally, the objective of feature distillation approaches can be written as:\n\nwhere F s and F t are the features extracted from the i-th layer of student model and teacher model.\u03d5(\u2022) is the projector that transform student feature to match the dimension of teacher feature.\u03bb KLD and \u03bb MSE balance the logit loss and feature loss.\n\nExisting feature distillation methods simply align feature dimensions through convolution or MLP modules, which is effective when both the teacher and student models belong to the same metaarchitecture.However, models with different meta-architectures exhibit significant differences in terms of model inputs, inductive biases, and spatial distributions, among others.Consequently, enforcing complete feature-level alignment between the student and teacher models may not be the optimal approach.",
            "score": 0.602536093389223,
            "section_title": "Recap of knowledge distillation",
            "char_start_offset": 8706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 116,
                    "end": 195
                },
                {
                    "start": 197,
                    "end": 223
                },
                {
                    "start": 223,
                    "end": 321
                },
                {
                    "start": 321,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 694
                },
                {
                    "start": 696,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 879
                },
                {
                    "start": 879,
                    "end": 941
                },
                {
                    "start": 943,
                    "end": 970
                },
                {
                    "start": 970,
                    "end": 1156
                },
                {
                    "start": 1156,
                    "end": 1233
                },
                {
                    "start": 1235,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1486
                },
                {
                    "start": 1488,
                    "end": 1690
                },
                {
                    "start": 1690,
                    "end": 1856
                },
                {
                    "start": 1856,
                    "end": 1984
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86767578125
        },
        {
            "corpus_id": "273654967",
            "title": "A Survey of Small Language Models",
            "text": "In its classical form, knowledge distillation (Hinton et al., 2015) involves training an efficient model, known as the \"student,\" to replicate the behavior of a larger, more complex model, referred to as the \"teacher.\" In this section, we particularly focus on distillation strategies from one or multiple white-box teacher language model to a target student language model. Babyllama (Timiryasov and Tastet, 2023b) is among the first to develop a compact 58M parameter language model using a Llama model as the teacher. A key finding of this work is that distillation from a robust teacher can outperform traditional pre-training on the same dataset. In a similar vein, (Gu et al., 2024) introduce modifications in the distillation loss, which enables the student models to generate better quality responses with improved calibration and lower exposure bias. Sequence-level distillation loss can also be improved by using a generalized version of f-divergences as shown in (Wen et al., 2023). Liang et al. (2023) extend layer-wise distillation strategies for language models by using task-aware filters which distill only the task specific knowledge from the teacher. Recent works (Wan et al., 2024a,b) show that multiple language models can be fused as a teacher towards distilling knowledge into small language models by strategically merging their output probability distributions. \n\nOne of the issues in knowledge distillation for language models is that the distillation strategies are primarily effective when (1) the teacher and the student language model share the same tokenizer and (2) the teacher's pre-training data is available. Boizard et al. (2024) addresses this issue by introducing an universal logit distillation loss inspired from the optimal transport literature. Often distillation is also combined with pruning techniques towards creating smaller language models. For example, (Sreenivas et al., 2024;Muralidharan et al., 2024) show that an iterative step of pruning a large language model followed by retraining with distillation losses, can enable strong smaller models.",
            "score": 0.6000475944259008,
            "section_title": "Knowledge Distillation Techniques",
            "char_start_offset": 22622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1385
                },
                {
                    "start": 1388,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2096
                }
            ],
            "ref_mentions": [
                {
                    "start": 974,
                    "end": 992,
                    "matchedPaperCorpusId": "256662734"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8974609375
        },
        {
            "corpus_id": "266977086",
            "title": "Graph Relation Distillation for Efficient Biomedical Instance Segmentation",
            "text": "We conduct an ablation study on reduced-size models to evaluate the effectiveness of distillation. The models are created by reducing the number of channels in each layer of the networks. Specifically, we generate student networks with width reductions of approximately 1  20 , 1 15 , 1 10 , and 1 5 compared to the original width. The experiments utilize ResUet network pairs on the CVPPP dataset. The results in Tab. VI demonstrate that our knowledge distillation method improves the performance of all student networks, even when they have very few parameters. However, it is important to note that the effectiveness of knowledge distillation depends on the initial performance gap between the teacher and student networks. When this gap is minimal, achieving significant improvements becomes challenging.",
            "score": 0.5974499288021838,
            "section_title": "4) Student networks with different widths:",
            "char_start_offset": 38367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 808
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86572265625
        },
        {
            "corpus_id": "270878760",
            "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
            "text": "The feature-based knowledge distillation methods [46,122] extract knowledge from the embedding space, transformer layers, and prediction layers, allowing the student model to learn various aspects of the teacher model comprehensively.For instance, Sun et al. [122] proposed a patient knowledge distillation (PKD) method aimed at compressing a large-scale teacher model into an equally effective lightweight student model.They proposed two distinct distillation strategies: 1) PKD-Last: The student model learns from the last  layers of the teacher model, based on the assumption that the top layers contain the most informative knowledge.2) PKD-Skip: The student learns from every -layer of the teacher, suggesting that the lower layers also contain essential information that should be gradually transferred during distillation.Experiments conducted on seven datasets across four tasks-sentiment classification, paraphrase similarity matching, natural language inference, and machine reading comprehension-showed that the PKD method outperformed standard knowledge distillation methods.It achieved superior performance and better generalization, significantly enhancing training efficiency and reducing storage requirements while maintaining accuracy comparable to the original large-scale model.MetaDistill [168] offers a simple and efficient alternative to traditional KD methods by keeping the teacher model fixed during training.Within the meta-learning framework, teacher networks enhance knowledge transfer to student networks by distilling feedback on student performance.Additionally, a pilot update mechanism is introduced to improve the alignment between internal learners and meta-learners, focusing on enhancing internal learners' performance.Extensive experiments have validated the effectiveness and versatility of this method across text and image classification tasks.Furthermore, experiments on the GLUE benchmark have shown that MetaDistill significantly outperforms traditional knowledge distillation, achieving state-of-the-art performance compression.AD-KD [155] addresses two key limitations of existing knowledge distillation methods.First, student models often merely mimic the teacher's behavior without developing their own reasoning capabilities.Second, these methods typically focus on transferring knowledge specific to complex models while neglecting data-specific knowledge.",
            "score": 0.5972719870036843,
            "section_title": "Hint-based KD.",
            "char_start_offset": 16483,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 421
                },
                {
                    "start": 421,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 829
                },
                {
                    "start": 829,
                    "end": 1087
                },
                {
                    "start": 1087,
                    "end": 1297
                },
                {
                    "start": 1297,
                    "end": 1434
                },
                {
                    "start": 1434,
                    "end": 1580
                },
                {
                    "start": 1580,
                    "end": 1756
                },
                {
                    "start": 1756,
                    "end": 1885
                },
                {
                    "start": 1885,
                    "end": 2073
                },
                {
                    "start": 2073,
                    "end": 2158
                },
                {
                    "start": 2158,
                    "end": 2274
                },
                {
                    "start": 2274,
                    "end": 2406
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 53,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 53,
                    "end": 57,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 259,
                    "end": 264,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 1309,
                    "end": 1314,
                    "matchedPaperCorpusId": "237250417"
                },
                {
                    "start": 2079,
                    "end": 2084,
                    "matchedPaperCorpusId": "258740796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "248683566",
            "title": "Teacher-student collaborative knowledge distillation for image classification",
            "text": "Self-distillation is a new approach that was developed from knowledge distillation. Unlike traditional knowledge distillation architectures, the teacher-student architecture of self-distillation uses the same model [20][21][22], or a network framework without teachers [23][24][25]. [20] used neural networks to study knowledge distillation from a new perspective; instead of compressing the model, the student network is optimized based on a teacher network with an equivalent parameter settings. [24] proposed a general training framework for self-distillation by constructing a multi-exit network for teacher-free distillation within the student network itself. [26] argued that self-distillation, as a regularization method, mitigates the overconfident predictions of the network and reduces intraclass gaps. All of the above studies found that self-distillation can effectively improve the performance of a student network. \n\nAlthough self-distillation gets rid of the need for a strong network of teacher, it loses the guidance of teacher network. In contrast to [24], we use a shared weighting strategy for the fully connected layer of the applied multi-branch network to reduce the number of model parameters. Moreover, each branch receives guidance from an extra teacher network. In particular, we found that the introduction of new teacher knowledge in the selfdistillation network further enhances the effectiveness of self-distillation.",
            "score": 0.5970396951361984,
            "section_title": "Self-distillation",
            "char_start_offset": 7443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1448
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 219,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 219,
                    "end": 223,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 223,
                    "end": 227,
                    "matchedPaperCorpusId": "211096976"
                },
                {
                    "start": 269,
                    "end": 273,
                    "matchedPaperCorpusId": "207994757"
                },
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 277,
                    "end": 281,
                    "matchedPaperCorpusId": "232232777"
                },
                {
                    "start": 283,
                    "end": 287,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 498,
                    "end": 502,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 665,
                    "end": 669,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "264405606",
            "title": "DistillCSE: Distilled Contrastive Learning for Sentence Embeddings",
            "text": "Knowledge distillation (Hinton et al., 2015) involves training a compact model, often referred to as a student model, to mimic the behavior and knowledge of a larger, more complex model known as the teacher model. It has been successfully applied to various tasks, such as language modeling (Zhuang et al., 2021), text classification (Heinzerling and Strube, 2018;Chia et al., 2019), named entity recognition (Zhou et al., 2021), machine translation (Tan et al., 2019), language generation (Melas-Kyriazi et al., 2019). \n\nTeacher model knowledge guide students in multiple ways during the distillation. Its predictions or soft targets, attention weights, or hidden representations, could all be used to guide the training of the student model. Consequently, the student is provided with stronger training signals from the teacher and achieves even superior performance. For example, Zhuang et al. (2021) directly mimics the output logits on vocabulary while Jiao et al. (2020) utilizes both hidden representations and attention matrix. \n\nDifferent from those studies, we employ knowledge distillation as an element of our self-training framework. Therefore, we focus on the most fundamental and general form of distillation which only minimize the cross entropy of prediction logits distribution between teacher and students. We use the homogeneous structure model for both the student and teacher model for distillation. Our research mainly focuses on the output logit distribution from the teacher instead of a special distillation framework. Therefore, our method is generic for more advanced distillation technologies.",
            "score": 0.5966245810213371,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 25385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1622
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 44,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 291,
                    "end": 312,
                    "matchedPaperCorpusId": "236164690"
                },
                {
                    "start": 334,
                    "end": 364,
                    "matchedPaperCorpusId": "21697629"
                },
                {
                    "start": 490,
                    "end": 518,
                    "matchedPaperCorpusId": "208050764"
                },
                {
                    "start": 883,
                    "end": 903,
                    "matchedPaperCorpusId": "236164690"
                },
                {
                    "start": 958,
                    "end": 976,
                    "matchedPaperCorpusId": "202719327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "263620565",
            "title": "Can a student Large Language Model perform as well as it's teacher?",
            "text": "Capacity: There's also a limit to how much a smaller student model can learn from a large teacher model. If the student model's capacity is too small, it may not effectively learn the teacher's knowledge. \n\n7. Overfitting Risk: If the teacher model has overfit to the training data, it's possible that the student model might learn these overfit predictions, leading to poor generalization to new data. \n\nDespite these limitations, knowledge distillation can still be a very useful technique, especially when dealing with constraints on computational resources or when deploying models in real-world applications where efficiency is key. \n\n7 Future Work \n\nThere are several directions that future work on knowledge distillation could take to further improve this technique and its application in various fields: \n\n\u2022 Improved Understanding of Distillation Dynamics: Further research is needed to understand the dynamics of knowledge transfer during distillation. For example, understanding which aspects of the teacher's knowledge are most effectively transferred and why could help optimize the process. \n\n\u2022 Automated Hyperparameter Tuning: Given the sensitivity of the distillation process to hyperparameters like the temperature and the weighting between the original loss and distillation loss, developing methods for automatic or more efficient hyperparameter tuning could be beneficial. \n\n\u2022 Advanced Distillation Techniques: Exploring advanced distillation techniques beyond the standard approach could lead to better results. This could involve novel loss functions, training methods, or types of teacher-student relationships. \n\n\u2022 Multi-Teacher Distillation: The idea of distilling knowledge from multiple teacher models into a single student model is an interesting area for exploration. This could potentially combine the strengths of various models into a single efficient student model. \n\n\u2022 Domain-Specific Adaptations: Adapting and optimizing knowledge distillation techniques for specific domains or tasks could also be a valuable avenue for future work. Different tasks might benefit from different distillation strategies. \n\n\u2022 Privacy and Security in Distillation: As distillation involves transferring knowledge from a teacher model, there could be concerns about privacy and security, especially when the teacher model has been trained on sensitive data. Future work could look at how to ensure that distillation does not leak sensitive information.",
            "score": 0.5955351901957875,
            "section_title": "Student Model",
            "char_start_offset": 19367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 204
                },
                {
                    "start": 207,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1103
                },
                {
                    "start": 1106,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1633
                },
                {
                    "start": 1636,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1897
                },
                {
                    "start": 1900,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2137
                },
                {
                    "start": 2140,
                    "end": 2371
                },
                {
                    "start": 2372,
                    "end": 2466
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9111328125
        },
        {
            "corpus_id": "255942245",
            "title": "Dataset Distillation: A Comprehensive Review",
            "text": "Knowledge distillation (KD) [37], [38], [39], [40] aims to transfer knowledge from a large teacher network to a smaller student network, such that the student network can preserve the performance of the teacher with reduced computational overhead. The seminal work by Hinton et al. [37] leads the student to mimic the outputs of the teacher, which can represent knowledge acquired by the teacher network. Afterward, improvements of KD have focused on four aspects: representations of knowledge, teacherstudent architectures, distillation algorithms, and distillation schemes. First, knowledge can be represented by model response/output [37], [41], features [38], [42], [43], and relation [44], [45], [46]. Second, teacher-student architectures refer to the network architectures of teacher and student models, which determines the quality of knowledge acquisition and distillation from teacher to student [40]. Third, distillation algorithms determine the ways of knowledge transfer. A simple and typical way is to match the knowledge captured by the teacher and student models directly [37], [38]. Beyond that, many different algorithms are proposed to handle more complex settings, such as adversarial distillation [47], attention-based distillation [39], and data-free distillation [48], [49]. Finally, distillation schemes control training configurations of teacher and student, and there are offline- [37], [38], online- [50], and self-distillation [51]. As for application, KD is widely used in ensemble learning [52] and model compression [38], [53], [54]. \n\nThe concept of DD is inspired by KD [18]. Specifically, DD aims at a lightweight dataset, while KD aims at a lightweight model. In this view, DD and KD are only conceptually related but technically orthogonal. It is worth noting that, similar to DD, recent data-free KD methods [48], [49], [55] are also concerned with the generation of synthetic training samples since original training datasets are unavailable. Their differences are two-fold.",
            "score": 0.5941475245495302,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1564
                },
                {
                    "start": 1567,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 50,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 643,
                    "end": 647,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 664,
                    "end": 668,
                    "matchedPaperCorpusId": "221559239"
                },
                {
                    "start": 670,
                    "end": 674,
                    "matchedPaperCorpusId": "227232038"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 695,
                    "end": 699,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "198160865"
                },
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1218,
                    "end": 1222,
                    "matchedPaperCorpusId": "53976534"
                },
                {
                    "start": 1292,
                    "end": 1296,
                    "matchedPaperCorpusId": "261514306"
                },
                {
                    "start": 1427,
                    "end": 1431,
                    "matchedPaperCorpusId": "24982157"
                },
                {
                    "start": 1455,
                    "end": 1459,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "7350432"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1851,
                    "end": 1855,
                    "matchedPaperCorpusId": "261514306"
                },
                {
                    "start": 1857,
                    "end": 1861,
                    "matchedPaperCorpusId": "159041346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89697265625
        },
        {
            "corpus_id": "245650327",
            "title": "Class-Incremental Continual Learning Into the eXtended DER-Verse",
            "text": "Knowledge Distillation (KD) [36] is a training methodology that allows transferring the knowledge of a teacher model into a separate student model. While Hinton et al. originally proposed to distillate large teachers -possibly ensembles -into smaller students, further studies revealed additional interesting properties about this technique. In particular, Furlanello et al. [37] show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student. More recently, other works [38], [39] explore an interesting variation of self-distillation that distills knowledge from the deeper layers of the network to its shallower ones to accelerate convergence and attain higher accuracy. \n\nKnowledge Distillation and Continual Learning. Distillation can be used to hinder catastrophic forgetting by appointing a previous snapshot of the model as teacher and distilling from it while new tasks are learned. Learning Without Forgetting [7] uses teacher responses to new exemplars to constrain the evolution of the student. Several other works combine distillation with rehearsal: iCaRL [11] distills the responses of the model at the previous task boundary, learning latent representations to be used in a nearest meanof-exemplars classifier; EtEIL [40], LUCIR [31] and BiC [41] focus on contrasting the prediction bias that comes from incremental classification; IL2M [30] stores additional statistics to facilitate distillation and compensate bias.",
            "score": 0.592289755712385,
            "section_title": "Self-Distillation",
            "char_start_offset": 8104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1538
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 375,
                    "end": 379,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1024,
                    "end": 1027,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 1174,
                    "end": 1178,
                    "matchedPaperCorpusId": "206596260"
                },
                {
                    "start": 1349,
                    "end": 1353,
                    "matchedPaperCorpusId": "195453293"
                },
                {
                    "start": 1362,
                    "end": 1366,
                    "matchedPaperCorpusId": "173187918"
                },
                {
                    "start": 1457,
                    "end": 1461,
                    "matchedPaperCorpusId": "204923710"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "273662110",
            "title": "Multi-Level Feature Distillation of Joint Teachers Trained on Distinct Image Datasets",
            "text": "Knowledge distillation. The goal of Knowledge Distillation (KD) is to transfer knowledge from a large teacher model to a small student model, such that the obtained student model mimics the behavior of the larger model [4,5,15,17,31,34]. The work of [15] defines the knowledge in the form of the output of the large model (logits). \n\nThe basic form of knowledge distillation consists in training the small student model to reproduce the logits of the large teacher model. This can be done either in the unsupervised [1,22] or supervised [26] scenario. In the supervised scenario, with ground-truth labels available at training time, Hinton et al. [15] show that significant improvements can be obtained by minimizing an objective function that takes into account the cross-entropy between the logits of the two models, but also another term that enforces the student model to predict ground-truth labels. Other works [3,39,51] consider knowledge at the feature level, employing distillation by matching feature distributions as well as logit outputs between the teacher and student models. Romero et al. penalize structural differences in relations. Li [33] reuses channel-wise and layer-wise meaningful features within the student to provide teacher-like knowledge without an additional model, in a teacher-free feature distillation framework. Liu et al. [29] propose a two-stage knowledge distillation method, which relies on a simple Feature Transform module consisting of two linear layers. Our proposed multilevel feature distillation method considers knowledge both at the logits level and feature level, but, unlike other approaches, it uses multiple teachers, each of them trained on a distinct dataset. Usually, knowledge distillation is applied for model compression [15], where the teacher model has a much larger capacity and memory footprint w.r.t. the student model. Our method allows student architectures to be identical to those of the individual teachers. Moreover, our method is very flexible, since it can combine teachers with distinct architectures.",
            "score": 0.5918338486782901,
            "section_title": "Related Work",
            "char_start_offset": 3719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 331
                },
                {
                    "start": 334,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2071
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 222,
                    "matchedPaperCorpusId": "212633769"
                },
                {
                    "start": 222,
                    "end": 224,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 233,
                    "end": 236,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "251800257"
                },
                {
                    "start": 519,
                    "end": 522,
                    "matchedPaperCorpusId": "267022515"
                },
                {
                    "start": 537,
                    "end": 541,
                    "matchedPaperCorpusId": "257771519"
                },
                {
                    "start": 917,
                    "end": 920,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 920,
                    "end": 923,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 923,
                    "end": 925,
                    "matchedPaperCorpusId": "221559239"
                },
                {
                    "start": 1153,
                    "end": 1157,
                    "matchedPaperCorpusId": "253270253"
                },
                {
                    "start": 1356,
                    "end": 1360,
                    "matchedPaperCorpusId": "258841675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.919921875
        },
        {
            "corpus_id": "260815602",
            "title": "Learning Lightweight Object Detectors via Multi-Teacher Progressive Distillation",
            "text": "In this section, we provide more details about distillation with heterogeneous teachers (Section 11). We investigate the heterogeneous cases where the backbones or input resolutions are different between the teachers and student. \n\nOverall performance: Again, Tables 11 and 12 show that MTPD is consistently effective with respect to all the teachers and their combinations, e.g., the box AP improves from 33.3% to 37.0%, and the mask AP improves from 30.5% to 33.7%. \n\nTwo key findings in heterogeneous distillation: Compared with the homogeneous case, we find that the capacity gap between models is a more important factor, and to bridge this gap a proper teacher order plays a more critical role. Details are explained as follows. \n\nThe student-teacher capacity gap is more pronounced in heterogeneous distillation. Among the four teachers, Teacher I shares exactly the same neck and head structure with the student, and has a similar but larger backbone; Teacher V has the same head with the student as well, but has different backbone and neck; Teacher III has similar backbone and neck, but has a different head; and Teacher IV is the most powerful one with completely different architecture. Table 12 (rows 3-6) summarizes the distillation results with single teachers. First, directly distilling from the strongest teacher (Teacher IV) does not yield the largest improvement. Second, a relatively less powerful but more similar teacher (Teacher I) leads to the best distillation performance, improving the APs by 2%, although Teachers V, III, and IV are all stronger than Teacher I. One possible reason is that Teacher I has the same neck and head as Student III as well as a similar but deeper backbone, so the capacity gap between Student III and Teacher I is the smallest. Finally, we find that Teacher III is a strong but not particularly helpful teacher, achieving the worst distillation results. One possible reason is that Teacher III has a very different head from Student III, while not as accurate as Teacher IV, making it unable to provide enough guidance to Student III. These observations suggest that a smaller capacity gap between the student and the teacher may facilities knowledge transfer. \n\nThe sequential order of the teachers plays a more critical role in the heterogeneous setting.",
            "score": 0.5917209532116481,
            "section_title": "C. Ablation on Distillation with Heterogeneous Teachers",
            "char_start_offset": 36909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2217
                },
                {
                    "start": 2220,
                    "end": 2313
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "277272885",
            "title": "Education distillation:getting student models to learn in shcools",
            "text": "In this section, to prove the effectiveness of the proposed education distillation (ED) method, we conducted experiments on the CIFAR100 dataset [15] using nine different teacher -student architectures. We compared our method with single -teacher knowledge distillation methods and multi -teacher knowledge distillation methods. In the multi -teacher knowledge distillation methods, three teacher models were used in all cases. These three teacher models exhibited an average top -1 accuracy value. Hyperparameters: All neural networks were optimized using Stochastic Gradient Descent (SGD) with a momentum of 0.9 and a weight decay of 0.0001. The batch size was set to 32, and the initial learning rate was set to 0.05. During a total of 240 training epochs, the learning rate was multiplied by 0.1 at epochs 150, 180, and 210. The temperature T for all methods was set to 4, and \u03b1 was set to 0.3. In epochs 50, 80, and 210, we added building blocks to the student network and updated the teaching reference layers. Meanwhile, the sub -datasets were divided in a 1:1:1 ratio for training. \n\nComparison of Isomorphic Teacher-Student Models: In the network of isomorphic teacher-student models, Table 1 and Table 2 show the comparison of top-1 accuracies of single-teacher and multi-teacher knowledge distillation methods on the CIFAR100 dataset. We found that the education distillation (ED) method outperforms all its competitors across various architectures. \n\nFor example, in single-teacher knowledge distillation, when using WRN40_2 as the teacher model and WRN16_2 as the student model, the ED method achieves an average accuracy improvement of 1.854%. In multi-teacher knowledge distillation, when using ResNet56 as the teacher model and ResNet20 as the student model, the ED method increases the average accuracy by 2.2%. \n\nOverall, the ED method generally performs better in teacher-student configurations with the same architecture. Carefully selecting the distillation strategy and the combination of teacher-student architectures can maximize the learning effect of the student model.",
            "score": 0.5903188221916796,
            "section_title": "Experiments",
            "char_start_offset": 11210,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1089
                },
                {
                    "start": 1092,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1828
                },
                {
                    "start": 1831,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2095
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "273186993",
            "title": "Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher",
            "text": "Knowledge distillation. Knowledge distillation (KD) (Hinton et al., 2015) transfers knowledge from a teacher to a smaller student model. Methods improve this by focusing on logits or intermediate features (Sun et al., 2024;Jin et al., 2023;Zhao et al., 2022a;Li et al., 2023;Passalis et al., 2021;Tian et al., 2019;Zagoruyko & Komodakis, 2017a;Heo et al., 2019a;Chen et al., 2021b;Heo et al., 2019b;Kim et al., 2018). Standard methods prioritize fully converged teachers with high performance, yet the performance gap can hinder knowledge transfer (Wang et al., 2022;Gao et al., 2020;Cho & Hariharan, 2019;Yuan et al., 2019). Strategies to address this include using intermediatestage teachers (Cho & Hariharan, 2019;Zhao et al., 2022b), pre-training student-friendly teacher model (Yang et al., 2019a;Park et al., 2021;Dong et al., 2024), introducing intermediate-sized assistant teachers (Mirzadeh et al., 2020;Son et al., 2021) or introducing auxiliary networks (Gao et al., 2021). These methods often rely on specially designed and pre-trained intermediate models. Feature-based methods like DTSKD (Li et al., 2024) and DiffKD (Huang et al., 2023) focus on bridging semantic gaps or denoising features. SCKD (Zhu & Wang, 2021) optimizes transfer using gradient similarity. Recent works refine soft labels (Yuan et al., 2024;Rao et al., 2023) or student's output entropy (Zhu et al., 2024a) to enhance knowledge transfer. In contrast, our GPD constructs a trainable dynamic teacher based on the student model, maintaining an appropriate accuracy gap throughout distillation for effective knowledge transfer. Reparameterization.",
            "score": 0.5902082481167814,
            "section_title": "RELATED WORK",
            "char_start_offset": 6141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 223,
                    "matchedPaperCorpusId": "268247468"
                },
                {
                    "start": 223,
                    "end": 240,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 240,
                    "end": 259,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 259,
                    "end": 275,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 275,
                    "end": 297,
                    "matchedPaperCorpusId": "219169868"
                },
                {
                    "start": 315,
                    "end": 344,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 344,
                    "end": 362,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 362,
                    "end": 381,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 381,
                    "end": 399,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 399,
                    "end": 416,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 548,
                    "end": 567,
                    "matchedPaperCorpusId": "252846591"
                },
                {
                    "start": 584,
                    "end": 606,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 694,
                    "end": 717,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 717,
                    "end": 736,
                    "matchedPaperCorpusId": "198179767"
                },
                {
                    "start": 782,
                    "end": 802,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 802,
                    "end": 820,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 820,
                    "end": 838,
                    "matchedPaperCorpusId": "249642077"
                },
                {
                    "start": 890,
                    "end": 913,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 913,
                    "end": 930,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 965,
                    "end": 983,
                    "matchedPaperCorpusId": "229400079"
                },
                {
                    "start": 1102,
                    "end": 1119,
                    "matchedPaperCorpusId": "268439162"
                },
                {
                    "start": 1131,
                    "end": 1151,
                    "matchedPaperCorpusId": "258888057"
                },
                {
                    "start": 1212,
                    "end": 1230,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1328,
                    "end": 1345,
                    "matchedPaperCorpusId": "249209818"
                },
                {
                    "start": 1374,
                    "end": 1393,
                    "matchedPaperCorpusId": "258564799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "265191521",
            "title": "A New Multinetwork Mean Distillation Loss Function for Open-World Domain Incremental Object Detection",
            "text": "Knowledge distillation [4] methods have been extended to mutual distillation learning [12][13][14], assisted distillation learning [15][16][17], spatial location distillation learning [18,19], and dataset distillation learning [20][21][22]. In addition, knowledge distillation can be used in incremental learning because of its ability to transfer knowledge from one model to another [8][9][10][11][12][13]. Knowledge distillation in incremental learning typically transfers old information from the teacher's network to the student's network to alleviate the forgetting of old knowledge. As a traditional incremental learning method for knowledge distillation, LwF [23] mitigates forgetting by freezing the old model as the teacher network, using a temperature factor to soften the softmax output of the logit, then adding the factor to the current task loss as a regularization term, and constraining the model to mitigate forgetting through parameter updates. However, LwF is vulnerable to a signifcant learning bias when there is an imbalance between the old and new classes. To address this problem, Zhao et al. [24] combined weight aligning (WA) with knowledge distillation by utilizing WA to balance the weights of the old and new class information in the fnal fully connected layer while using knowledge distillation to maintain the model's discrimination of the old classes. In contrast, Dong et al. [25] used a dual-teacher distillation framework to mitigate the class imbalance problem using sampled unlabeled data to extract knowledge from the base class teacher and new class teacher models and transfer the knowledge to the student model. Similarly, Abdelsalam et al. [26]",
            "score": 0.5891421747688413,
            "section_title": "Incremental Learning Approach for the Knowledge Distillation Strategy.",
            "char_start_offset": 5640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1686
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "252701352"
                },
                {
                    "start": 86,
                    "end": 90,
                    "matchedPaperCorpusId": "244073884"
                },
                {
                    "start": 90,
                    "end": 94,
                    "matchedPaperCorpusId": "213768264"
                },
                {
                    "start": 94,
                    "end": 98,
                    "matchedPaperCorpusId": "249947464"
                },
                {
                    "start": 131,
                    "end": 135,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 139,
                    "end": 143,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 184,
                    "end": 188,
                    "matchedPaperCorpusId": "230101161"
                },
                {
                    "start": 188,
                    "end": 191,
                    "matchedPaperCorpusId": "221266067"
                },
                {
                    "start": 227,
                    "end": 231,
                    "matchedPaperCorpusId": "236469183"
                },
                {
                    "start": 231,
                    "end": 235,
                    "matchedPaperCorpusId": "247597241"
                },
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "251052130"
                },
                {
                    "start": 384,
                    "end": 387,
                    "matchedPaperCorpusId": "10328909"
                },
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "1358160"
                },
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "212633539"
                },
                {
                    "start": 394,
                    "end": 398,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 398,
                    "end": 402,
                    "matchedPaperCorpusId": "244073884"
                },
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "213768264"
                },
                {
                    "start": 666,
                    "end": 670,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 1117,
                    "end": 1121,
                    "matchedPaperCorpusId": "208139404"
                },
                {
                    "start": 1682,
                    "end": 1686,
                    "matchedPaperCorpusId": "229363426"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83154296875
        },
        {
            "corpus_id": "274655747",
            "title": "Optimising TinyML with quantization and distillation of transformer and mamba models for indoor localisation on edge devices",
            "text": "Knowledge distillation (KD) is a model compression technique in which a smaller, less complex model (referred to as the \"student\") is trained to replicate the behaviour of a larger, more complex model (referred to as the \"teacher\") 10 . The process involves training the student model to mimic the teacher model's predictions rather than relying on the original training data labels. This approach allows the student model to achieve similar performance levels to the teacher model while significantly reducing computational resources required for inference. \n\nTraditional model compression techniques, such as pruning and quantization, primarily focus on reducing model size by removing redundant parameters or lowering numerical precision. While effective, these methods may lead to a substantial loss in model accuracy, particularly for complex tasks. In contrast, knowledge distillation leverages the rich information embedded in the teacher model's output to train the student model, providing more nuanced guidance and helping the student model to generalize better. KD also offers significant flexibility in both the size and architecture of the teacher and student models, making it a superior model compression technique. \n\nThe teacher model can be any large, high-accuracy model with no constraints on its architecture, allowing the use of state-of-the-art models. The student model, in contrast, can be smaller and simpler, tailored to the specific constraints of the deployment environment, such as limited memory and computational power. This flexibility enables KD to significantly reduce the student model's size and computational complexity without substantial performance degradation. The architectures of the teacher and student models can differ, with the teacher being a complex model like a state-of-the-art transformer, and the student being a simpler neural network, such as a small transformer with a minimal attention core. This adaptability makes KD highly suitable for various use cases and deployment constraints. \n\nWe perform knowledge distillation using a distillation loss to guide the student model to mimic the behaviour of the teacher model, as demonstrated in Eq. ( 4): \n\nHere, \u03b1 is the proportion between student loss (LS) and distillation loss (LD). LS is calculated from the student predictions matching the raw labels, while LD measures the match between teacher and student predictions.",
            "score": 0.5886167800555966,
            "section_title": "Knowledge distillation",
            "char_start_offset": 26575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 558
                },
                {
                    "start": 561,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2041
                },
                {
                    "start": 2044,
                    "end": 2204
                },
                {
                    "start": 2207,
                    "end": 2286
                },
                {
                    "start": 2287,
                    "end": 2426
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8515625
        },
        {
            "corpus_id": "249712279",
            "title": "Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems",
            "text": "Low-latency applications require models of relatively small sizes. However, distilling from large pretrained models into much smaller models directly can hinder the student from fully taking advantage of the teacher's knowledge [7]. Therefore, we distill the pretrained teacher models in two phases with a teacher assistant setup [26,44]. The distillation workflow is depicted in Figure 1. First, we distill an intermediate sized model from the large teacher model. We then use this distilled model as a teacher for the final student. \n\nWhen distilling the intermediate model, we followed a similar approach to the pretraining of the teacher. A randomly initialized student model was distilled from the Stage 1 teacher model. Once training converged, we switched the teacher model to the Stage 2 teacher and resumed the distillation process. For both of these stages, the distillation data is the same that was used for teacher pretraining for its respective stage. As for our distillation techniques, we explored different components described in [17]. Our final run for the intermediate student/teacher used the sum of categorical cross-entropy (MLM loss) and soft cross-entropy weighted equally, because we did not observe any gain from utilizing the attention and hidden layer outputs of the teacher. \n\nFor our final student, we first pretrained the intermediate model further without teacher involvement on Stage 2 data only. Next, we distilled it into the final, small student. The distillation techniques in this phase were similar to the first distillation phase, with an additional usage of hidden-layer output matching as in [17].",
            "score": 0.5879220345099647,
            "section_title": "Distillation",
            "char_start_offset": 11231,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1640
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 231,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1635,
                    "end": 1639,
                    "matchedPaperCorpusId": "202719327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "274131310",
            "title": "Exploring Feature-based Knowledge Distillation for Recommender System: A Frequency Perspective",
            "text": "Let   and   denote the dimensions of student and teacher features, respectively. In this work, we use \n\nall the features of the student model and the teacher model, respectively. Note that, when performing knowledge for users, we have \n\nIn feature-based knowledge distillation methods [10,13], a projector is used to align the dimensionalities of the student model and the teacher model. Then, the student model is trained to minimize the MSE loss to make the student model mimic the teacher's features. Formally, the feature-based knowledge distillation loss is given by: L   = \u2225  (S) \u2212 T\u2225 2  , where   (\u2022) denotes the projector that aligns the dimensionalities of S and T, and is usually a linear transformation.",
            "score": 0.5876822255695672,
            "section_title": "Feature-based Knowledge Distillation",
            "char_start_offset": 12956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 714
                }
            ],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "224282941"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "245324791"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71923828125
        },
        {
            "corpus_id": "246200092",
            "title": "Explainable CNN With Fuzzy Tree Regularization for Respiratory Sound Analysis",
            "text": "The development of deep learning has brought about complex models with huge overhead. The application of these complex models in production requires a lot of inference time. Knowledge distillation based on teacher-student framework as an effective model compression method tries to achieve a trade-off between model accuracy and inference efficiency [28]. To improve the applicability of the knowledge distillation framework and the accuracy of the model, some researchers have proposed to improve the framework of distillation from a single teacher to multiple teachers. For example, multiple pre-trained teacher models are directly assigned fixed weights to integrate their predictions [29]. Some researchers have used different teacher models to learn different types of inputs and then used the weighted average to teach student models [30]. In addition, to accelerate the training of the student model in the word embeddings task, multiple teacher models were used to train a student model by combining their logit values such that the student no longer needs the teachers during decoding [31]. However, these approaches only treat teacher models equally, without taking into account the differences between them. In order to solve the conflicts and competition among all teachers, Shangchen et al. formulated ensemble knowledge distillation as a multi-objective optimization problem and assigned dynamic weights to each teacher model [32]. You et al. proposed to use a voting mechanism to unify multiple relative dissimilarity information, which can be transferred into the student network [33]. These existing methods use multiple teacher models to learn category-fixed training samples. In this paper, we use multiple teachers models to learn local data distributed across multiple hospitals with flexible categories.",
            "score": 0.587296922141941,
            "section_title": "B. Knowledge Distillation with Multiple Teachers",
            "char_start_offset": 9962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1825
                }
            ],
            "ref_mentions": [
                {
                    "start": 688,
                    "end": 692,
                    "matchedPaperCorpusId": "18195425"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "145841463"
                },
                {
                    "start": 1094,
                    "end": 1098,
                    "matchedPaperCorpusId": "173990395"
                },
                {
                    "start": 1440,
                    "end": 1444,
                    "matchedPaperCorpusId": "227276362"
                },
                {
                    "start": 1596,
                    "end": 1600,
                    "matchedPaperCorpusId": "26021416"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.791015625
        },
        {
            "corpus_id": "271843401",
            "title": "ProFuser: Progressive Fusion of Large Language Models",
            "text": "Knowledge distillation (KD, Hinton et al. (2015)) aims to compress one or more large teacher models into a smaller student model without a significant performance drop. In the NLP domain, for text classification, many works let the student model mimic the teacher's output distribution (Turc et al., 2019;Zhang et al., 2023), hidden states (Sun et al., 2019;Jiao et al., 2020), or attention scores (Wang et al., 2021). For text generation, the student model could learn from the teacher's logits distribution on ground truth (Agarwal et al., 2024;Gu et al., 2024) or generations (Peng et al., 2023). Multiteacher knowledge distillation (MTKD) boosts the effectiveness of distillation by averaging the distributions (You et al., 2017) or blending the sequences (Wang et al., 2024) from multiple teachers. Compared to KD, model fusion serves distinct purposes by integrating strengths from multiple source models into a unified model, leading to a comprehensively stronger model.",
            "score": 0.5868908017346032,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 977
                }
            ],
            "ref_mentions": [
                {
                    "start": 715,
                    "end": 733,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8974609375
        },
        {
            "corpus_id": "263620565",
            "title": "Can a student Large Language Model perform as well as it's teacher?",
            "text": "While knowledge distillation is a powerful tool, it's not without its limitations and challenges. Here are a few to consider: \n\n1. Performance Gap: The performance of the distilled student model, although better than a similarly-sized model trained from scratch, typically doesn't reach the performance level of the larger teacher model. There is usually a trade-off between model size and accuracy. \n\n2. Dependence on a Good Teacher Model: The effectiveness of knowledge distillation heavily depends on the quality of the teacher model. The student model can only be as good as the teacher model allows. If the teacher model isn't well-trained or doesn't perform well, the student model is unlikely to perform well either. \n\n3. Hyperparameter Sensitivity: The process of knowledge distillation requires careful tuning of several hyperparameters, such as the temperature parameter and the weighting between the original loss and the distillation loss. Finding the right settings can be tricky and might require a lot of experimentation. \n\n4. Computational Overhead: Although the student model is smaller and more efficient, the distillation process itself requires the teacher model to generate predictions for the data, which could be computationally expensive, especially for large models and datasets. \n\n5. Opaque Process: The process of knowledge distillation is somewhat opaque and difficult to interpret. It's not always clear why a certain student model learns effectively from a teacher model, or why certain hyperparameters work better than others.",
            "score": 0.5863933488580946,
            "section_title": "Limitations",
            "char_start_offset": 17792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 125
                },
                {
                    "start": 128,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1557
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "269604725",
            "title": "A Generalization Theory of Cross-Modality Distillation with Contrastive Learning",
            "text": "Distillation with more structures.Though the modality difference can be seen as the main distillation source in cross-modality distillation, it is still natural to consider distilling a large model into a small model in this case.Thus, in this part, we mainly study how model sizes or structures affect the performance of our method and illustrate that our method can also be used even when distilling the large teacher to a small student model.In detail, we test our algorithm with ResNet50 (1x/2x/4x) as teacher nets and ResNet18, ResNet50 as the student nets.Here, the ResNet50 (1x/2x/4x) means the original ResNet50 with different widths which is the same setting in Chen et al. (2020b).From Table 3, we can find when the teacher model size becomes larger the distilled student models will have a better performance which just fits with the classical results in knowledge distillation.It shows our algorithm's ability to distill information captured not only by the modality difference but also by the model structure inductive bias.\n\nDistillation with less numbers of samples.We further test our algorithm with less paired data when cross-modality distilling.We take m as the number of paired data used in distillation and M as the number of whole paired data.On both Sketchy and TUBerlin, as shown in Figure 1, our method works well even when the paired data reduces to m/M = 20% of the whole samples.Notably, when we decrease the entire training data to only 5% of the original training setting, our method experiences a marginal drop of approximately 3%.In contrast, the performances of CMKD and PreSSL methods decline as the number of training samples decreases, and they exhibit a rapid drop when only 5% of the data remains.These results highlight the resilience of our algorithm when working with a limited number of paired data for cross-modality distillation.\n\nDownstream tasks under few-shot setting.To better show that our method can learn meaningful semantic information by cross-modality distillation, we use a few-shot setting test that is used in many self-supervised learning works (He et al., 2020).",
            "score": 0.586265393151474,
            "section_title": "MORE",
            "char_start_offset": 25332,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 34,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 562
                },
                {
                    "start": 562,
                    "end": 691
                },
                {
                    "start": 691,
                    "end": 889
                },
                {
                    "start": 889,
                    "end": 1037
                },
                {
                    "start": 1039,
                    "end": 1081
                },
                {
                    "start": 1081,
                    "end": 1164
                },
                {
                    "start": 1164,
                    "end": 1265
                },
                {
                    "start": 1265,
                    "end": 1407
                },
                {
                    "start": 1407,
                    "end": 1562
                },
                {
                    "start": 1562,
                    "end": 1735
                },
                {
                    "start": 1735,
                    "end": 1873
                },
                {
                    "start": 1875,
                    "end": 1915
                },
                {
                    "start": 1915,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 671,
                    "end": 690,
                    "matchedPaperCorpusId": "211096730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "252846591",
            "title": "Efficient Knowledge Distillation from Model Checkpoints",
            "text": "Knowledge distillation (KD) [1,2] has been proved to be an effective technique to promote the performance of a low-capacity model by transferring \"dark knowledge\" from a large teacher model. Empirically, there usually exists a strong correlation between the performance of the teacher model and the student model. For this reason, it is a standard practice to use a well trained network or an ensemble of multiple well trained networks as the teacher [3,4,5], and some researches are attempted to improve distillation performance via boosting the ensemble performance [6,7]. The underlying assumption is that high performing teachers lead to better student models. However, this viewpoint has been challenged by some recent works [8,9,10,11,12], in which it has been observed that a large model capacity gap between the teacher and student may have a negative effect for knowledge transfer. To address this issue, researchers have proposed to employ an intermediate-size network [8] or an assistant network [9] to improve the distillation performance in such scenarios. In [10], a \"tolerant\" teacher model is designed by using a softened loss function. In [11], Park et al. proposed to learn student-friendly teacher by plugging in student branches during the training procedure. Nevertheless, there is no clear theoretical explanation for the gap between teacher and student, and the search for a substitute teacher is not straightforward. [13] can serve as a better teacher than the strong Full Ensemble. \n\nIn this paper, we make an intriguing observation that further supports the viewpoint that high performing models may not necessarily be good teachers, but from a novel perspective. Specifically, we find that an unconverged intermediate model from the middle of the training procedure, often serves as a better teacher than the final converged model, although the former has much lower accuracy (as illustrated in Figure 1(a)) . Moreover, a weak snapshot ensemble of intermediate teacher models along the same optimization path (denoted as Snapshot Ensemble, which is a variant of [13] 3 ) can outperform the standard ensemble of an equal number of independently trained teacher models (denoted as Full Ensemble).",
            "score": 0.5861900083919914,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 31,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 451,
                    "end": 454,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 454,
                    "end": 456,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 456,
                    "end": 458,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 568,
                    "end": 571,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 571,
                    "end": 573,
                    "matchedPaperCorpusId": "222124879"
                },
                {
                    "start": 730,
                    "end": 733,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 735,
                    "end": 738,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 738,
                    "end": 741,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 741,
                    "end": 744,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 979,
                    "end": 982,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 1156,
                    "end": 1160,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 1441,
                    "end": 1445,
                    "matchedPaperCorpusId": "6820006"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89599609375
        },
        {
            "corpus_id": "268820185",
            "title": "A Comprehensive Review of Knowledge Distillation in Computer Vision",
            "text": "A powerful machine learning approach called knowledge distillation enables the transfer of knowledge from a large and complex model to a more compact and computationally efficient model.The idea of knowledge distillation was first introduced by Bucila et al. [12] in 2006, and since then it has gained significant attention from researchers and practitioners in the field of machine learning.\n\nTraining a smaller model, known as a student model, to do the same tasks as the bigger model, known as a teacher model, is the fundamental aim of knowledge distillation.To achieve cutting-edge performance, the teacher model is often trained on a specific task and dataset.On the other hand, the student model is smaller and more computationally effective, making it suitable for deployment on devices with constrained resources.Transferring knowledge from the teacher model to the student model allows us to improve the performance of the student model without appreciably raising its complexity.The student model is trained using the same task and dataset as the teacher model throughout the knowledge distillation process (figure 2).The student model is trained according to the teacher model's predictions, which provide the student model additional knowledge and help it perform better.Knowledge transfer techniques include soft target training, attention transfer, and feature mimicry [52].\n\nFig 2 : The generic teacher-student framework for knowledge distillation [50] In the distillation loss, the lightweight student model duplicates the output produced by the teacher model by using the loss function.The weighted total contains the cross-entropy loss between the student's output and the real labels, as well as the cross-entropy loss between the student's output and the teacher's output (both after temperature scaling).\n\nwhere   is the student model output,  \u210e is the teacher model output, t is the true labels,  is a hyperparameter that regulates the weight given to each term, and T is the temperature parameter used for temperature scaling.The use of temperature scaling to soften soft logits produced by the softmax function is another important feature of knowledge distillation.This is accomplished by dividing the logits by a temperature parameter T, followed by the softmax function.The temperature parameter governs the probability distribution's \"softness,\" with higher temperatures resulting in softer distributions.",
            "score": 0.5854744714027075,
            "section_title": "INTRODUCTION TO KNOWLEDGE DISTILLATION",
            "char_start_offset": 12659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 392
                },
                {
                    "start": 394,
                    "end": 563
                },
                {
                    "start": 563,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1389
                },
                {
                    "start": 1391,
                    "end": 1604
                },
                {
                    "start": 1604,
                    "end": 1826
                },
                {
                    "start": 1828,
                    "end": 2050
                },
                {
                    "start": 2050,
                    "end": 2191
                },
                {
                    "start": 2191,
                    "end": 2298
                },
                {
                    "start": 2298,
                    "end": 2434
                }
            ],
            "ref_mentions": [
                {
                    "start": 1384,
                    "end": 1388,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86572265625
        },
        {
            "corpus_id": "270123704",
            "title": "Estimating Human Poses Across Datasets: A Unified Skeleton and Multi-Teacher Distillation Approach",
            "text": "Knowledge distillation [13] is a technique originally developed to compress the knowledge of a large, complex model (teacher) into a smaller, more efficient model (student) with minimal performance loss.Logit-based knowledge distillation [3,13,17,24,34,56,57,57] utilizes the soft output probabilities from the teacher model to guide the training of the student model.By employing a loss function that combines temperature-scaled cross-entropy with Kullback-Leibler divergence [19], the student model is encouraged not only to mimic the teacher's output distribution but also to achieve correct label predictions on its own.This method effectively transfers the intricate decision boundaries learned by the teacher to the student, enhancing its performance beyond what could be achieved through direct training alone.Feature-based distillation [1,4,5,11,12,25,27,30,42,48,54] and relation-based distillaion [14,21,40,41,49,52] have also been proposed to improve the knowledge transfer between teacher and student.Recent advancements, such as the introduction of a teacher assistant model [35], have further refined this process by staging the knowledge transfer through an intermediate-sized model, which bridges the gap between the high-capacity teacher and the smaller student.Knowledge distillation has also been used to enhance the performance of human pose estimation methods [26,47,53,58], including some works [47,58] that use multiple teacher networks.",
            "score": 0.5834603980983495,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5951,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 1013
                },
                {
                    "start": 1013,
                    "end": 1279
                },
                {
                    "start": 1279,
                    "end": 1460
                }
            ],
            "ref_mentions": [
                {
                    "start": 238,
                    "end": 241,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 244,
                    "end": 247,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "222124879"
                },
                {
                    "start": 250,
                    "end": 253,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 256,
                    "end": 259,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 259,
                    "end": 262,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 844,
                    "end": 847,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 847,
                    "end": 849,
                    "matchedPaperCorpusId": "247762862"
                },
                {
                    "start": 849,
                    "end": 851,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 851,
                    "end": 854,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 854,
                    "end": 857,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 857,
                    "end": 860,
                    "matchedPaperCorpusId": "236912875"
                },
                {
                    "start": 860,
                    "end": 863,
                    "matchedPaperCorpusId": "248986384"
                },
                {
                    "start": 863,
                    "end": 866,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 869,
                    "end": 872,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 872,
                    "end": 875,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 907,
                    "end": 911,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 911,
                    "end": 914,
                    "matchedPaperCorpusId": "245006036"
                },
                {
                    "start": 914,
                    "end": 917,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 917,
                    "end": 920,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 920,
                    "end": 923,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 923,
                    "end": 926,
                    "matchedPaperCorpusId": "235613564"
                },
                {
                    "start": 1088,
                    "end": 1092,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1381,
                    "end": 1385,
                    "matchedPaperCorpusId": "236912875"
                },
                {
                    "start": 1385,
                    "end": 1388,
                    "matchedPaperCorpusId": "263159006"
                },
                {
                    "start": 1388,
                    "end": 1391,
                    "matchedPaperCorpusId": "260334254"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "263159006"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "270870796",
            "title": "From Efficient Multimodal Models to World Models: A Survey",
            "text": "Knowledge Distillation (KD) is a model compression technique that transfers knowledge from a complex model (called the teacher model) to a smaller model (called the student model).This allows the student model to maintain high computational efficiency while achieving the performance of the teacher model.Knowledge distillation was first proposed by Bucilu\u01ce et al., who trained compressed models with pseudodata classifiers to replicate the original classifier's outputs [14].KD can be divided into homomorphic KD and heteromorphic KD.\n\nHomomorphic KD means the student and teacher models have similar or identical structures.In this approach, the student model learns by mimicking the teacher model's outputs (e.g., logits, feature layer outputs).Common homomorphic KD methods include logit-level distillation, feature-level distillation, and module-level distillation.For instance, TinyViT [46] applies distillation during pre-training, storing logits from a large teacher model on hardware to achieve memory and computational efficiency when transferring knowledge to a smaller student Transformer.DeiT-Tiny [47] adopts patchlevel distillation, training a small student model to match the pre-trained teacher model's patch structure, then optimizing with decomposed manifold matching loss to reduce computational costs.Module-level methods like m2mKD [48] separate the teacher module from a pre-trained unified model, combining student modules with modular models, and using a shared meta-model for composition, enabling student modules to mimic teacher module behavior.Feature-level distillation methods like MiniViT [49] combine weights from consecutive Transformer blocks for cross-layer weight sharing, introducing transformations to enhance learning.\n\nHeteromorphic KD refers to student and teacher models with different structures.In this approach, the student model learns by mimicking the teacher model's outputs or intermediate features, despite different architectures.Heteromorphic KD enhances the student model's adaptability, enabling it to learn useful information from the teacher model.Heteromorphic KD includes soft label distillation, where the student model trains by mimicking the teacher model's soft label outputs.",
            "score": 0.5827649429981031,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 25157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 535
                },
                {
                    "start": 537,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 748
                },
                {
                    "start": 748,
                    "end": 870
                },
                {
                    "start": 870,
                    "end": 1101
                },
                {
                    "start": 1101,
                    "end": 1322
                },
                {
                    "start": 1322,
                    "end": 1573
                },
                {
                    "start": 1573,
                    "end": 1758
                },
                {
                    "start": 1760,
                    "end": 1840
                },
                {
                    "start": 1840,
                    "end": 1982
                },
                {
                    "start": 1982,
                    "end": 2105
                },
                {
                    "start": 2105,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 471,
                    "end": 475,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "250920355"
                },
                {
                    "start": 1111,
                    "end": 1115,
                    "matchedPaperCorpusId": "247230104"
                },
                {
                    "start": 1621,
                    "end": 1625,
                    "matchedPaperCorpusId": "248177918"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "265033252",
            "title": "Comparative Knowledge Distillation",
            "text": "In the era of large-scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally-heavy teacher models to lightweight, efficient student models while preserving performance. Yet KD settings often assume readily available access to teacher models capable of performing many in-ferences-a notion increasingly at odds with the realities of costly large-scale models. Addressing this gap, we study an important question: how KD algorithms fare as the number of teacher inferences decreases, a setting we term Reduced-Teacher-Inference Knowledge Distillation (RTI-KD). We observe that the performance of prevalent KD techniques and state-of-the-art data augmentation strategies suffers considerably as the number of teacher inferences is reduced. One class of approaches, termed \u201crelational\u201d knowledge distillation underperforms the rest, yet we hypothesize that they hold promise for reduced dependency on teacher models because they can augment the effective dataset size without additional teacher calls. We find that a simple change - performing high-dimensional comparisons instead of low-dimensional relations, which we term Comparative Knowledge Distillation - vaults performance well over existing KD approaches. We perform empirical evaluation across varied experimental settings and rigorous analysis to understand the learning outcomes of our method. All code is made publicly available.",
            "score": 0.5819804243536113,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9140625
        },
        {
            "corpus_id": "260378128",
            "title": "Structured Knowledge Distillation for Accurate and Efficient Object Detection",
            "text": "Traditional knowldistillation follows a two-stage training pipeline, where large teacher model is initially trained and then distilled to a lightweight student model. Recently, have proposed a one-stage knowledge distillation approach called online knowledge distillation or deep mutual learning. These methods involve training two or multiple student models and distilling their knowledge to each other. In this subsection, we compare the effectiveness of our method in both the two-stage and one-stage knowledge distillation frameworks. Experiments are conducted on MS COCO2017 using Faster RCNN students with ResNet50 backbones. For onestage knowledge distillation experiments, we adopt the deep mutual learning approach and train two student detectors to mimic each other using our methods. For two-stage knowledge distillation experiments, we explore the following two schemes. \n\nr Strong Teacher Scheme: distilling knowledge from a pre- trained Cascade Mask RCNN teacher with ResNetX101-DCN backbone, which achieves 47.3 AP. \n\nr Weak Teacher Scheme: distilling knowledge from a pre- trained teacher which has the identical architecture with the student (i.e., Faster RCNN with ResNet50 backbone) and achieves 38.4 AP. Experimental results are shown in Table X. It is observed that one-stage knowledge distillation leads to 2.3 AP improvements over the baseline, which is 0.2 higher than two-stage knowledge distillation with a weak teacher, but still 0.8 AP lower than the two-stage knowledge distillation with a strong teacher. This observation indicates that one-stage knowledge distillation can achieve comparable and even better performance than two-stage knowledge distillation when no powerful teacher is available. Moreover, two-stage knowledge distillation can make more use of the knowledge from a powerful teacher and achieve better performance.",
            "score": 0.581930633182782,
            "section_title": "5) Two-Stage KD versus One-Stage KD:",
            "char_start_offset": 46007,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 1030
                },
                {
                    "start": 1033,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1861
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "247748812",
            "title": "MKQ-BERT: Quantized BERT with 4-bits Weights and Activations",
            "text": "The basic idea of knowledge distillation is to transfer the knowledge from a powerful model (teacher model) into a weaker model (student model, usually admits a smaller size or few bits than the teacher model). This is done by letting the student model's features to mimic the features from the teacher model, which can be achieved by minimizing the difference between these two sets of features: \u2022 Attention distillation: The output of the self-attention network and the feed-forward network are used for distillation. More specifically, two sets of outputs {A l,a } and {\u00d8 A l,a } for each attention head and attention layers, which can be written as \n\nIn (Jin et al., 2021), authors add ( 6) and ( 7) together with the original training loss \u0141 train into \u0141 f inal = \u0141 output + \u0141 attention + \u0141 train , and use this \u0141 f inal as the final training loss. However, those method can only use teacher models with the same model configuration, which means we cannot use a larger network to further improve the performance of the quantized model.",
            "score": 0.5818913439203117,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 11448,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1040
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "278502445",
            "title": "A Survey on Collaborative Mechanisms Between Large and Small Language Models",
            "text": "Knowledge Distillation (KD) is a key technique used to transfer knowledge and capabilities from a large, often proprietary teacher model (LLM) to a smaller, more efficient student model (SLM) (X. Xu et al., 2024;W. Xu et al., 2024). \n\nKD involves multiple aspects, including the type of knowledge (output probabilities, intermediate representations), distillation algorithms (supervised fine-tuning, divergence minimization), and application domains (model compression, skill transfer, domain specialization). Data augmentation also plays an important role in enhancing the effectiveness of KD for LLMs. Depending on whether the teacher model can provide internal information, KD methods can be divided into white-box KD and black-box KD. KD is also used to transfer specific skills, such as reasoning, instruction following, and tool use (X. Xu et al., 2024;Hendriks et al., 2025;Y. Gu et al., 2023). \n\nKnowledge distillation is a critical mechanism for enabling LLM-level intelligence to be deployed in resource-constrained environments by creating smaller, more efficient models. While LLMs offer superior performance, their size and computational requirements limit their applicability in many practical scenarios. KD allows us to transfer the knowledge learned by these large models into smaller models that can be deployed on edge devices or used in applications with strict latency requirements. The success of knowledge distillation depends on several factors, including the choice of teacher and student models, the quality and quantity of distillation data, and the specific distillation techniques employed. Different LLMs have different strengths, and the choice of teacher model will influence the type of knowledge transferred. The distillation data should be representative of the tasks the student model will perform. Selecting the appropriate distillation algorithm and tuning its parameters are also crucial for achieving optimal performance.",
            "score": 0.5818525075012491,
            "section_title": "Knowledge Distillation-Driven Collaboration",
            "char_start_offset": 25278,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 232
                },
                {
                    "start": 235,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1960
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95556640625
        },
        {
            "corpus_id": "271974282",
            "title": "LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation",
            "text": "teacher. Interestingly, the smaller Qwen-1.5-0.5B student model shows marginal improvement with either the Qwen-1.5-4B or Qwen-1.5-7B teacher. This suggests that an excessively large capacity gap between the teacher and student models can hinder effective knowledge transfer. Utilizing a \"middle teacher\" with an intermediate capacity could bridge this gap, facilitating smoother knowledge transfer and boosting the student model's learning efficiency. \n\nTable 9: Comparison between the strong and weak teachers within the distillation. We set the strong teacher as the LLM is Qwen-1.5-7B and the weak teacher as the LLM is Qwen-1.5-4B. The model configuration is E4T2.",
            "score": 0.5814923945362632,
            "section_title": "IMPACT OF PREFERENCE DISTILLATION",
            "char_start_offset": 31825,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 669
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360595703125
        },
        {
            "corpus_id": "258833648",
            "title": "Lifting the Curse of Capacity Gap in Distilling Language Models",
            "text": "Knowledge Distillation Distillation (Hinton et al., 2015) is a de facto way to compression (Bucila et al., 2006) LMs by transferring the knowledge of LMs to small language models. During the distillation, a small language model serves as a student and treats a LM as a teacher to learn from. There are three lines of work in LM distillation: firstly, task-specific distillation (Sun et al., 2019;Li et al., 2020;Sun et al., 2020a;Park et al., 2021b;Hou et al., 2020;Xia et al., 2022) that conducts distillation on a specific task at finetuning stage; secondly, task-agnostic distillation (Turc et al., 2019;Sanh et al., 2019;Sun et al., 2020b;Wang et al., 2021b) that conducts distillation at pretraining stage; and thirdly, two-stage distillation (Jiao et al., 2020) that combines the power of both task-agnostic andspecific distillation. Though these methods realize promising performance when distilling LMs like BERT base , they can come short of scalability to LMs like BERT large especially when the student is of a small scale. In fact, driven by recent observations Zhang et al., 2022a;Mirzadeh et al., 2020;Cho and Hariharan, 2019), distillation with a small student can be faced with two deficiencies due to the large capacity gap. A few studies including teacher assistantbased (Mirzadeh et al., 2020;Zhang et al., 2022a) and student-friendly (Park et al., 2021a; distillation can alleviate the first but fail to resolve the second. It is noteworthy that some work states they can tackle both deficiencies for vision models (Zhu and Wang, 2021;Zhao et al., 2022), but preliminary studies have found that they are either expensive or not capable of LMs. In our work, we follow the line of task-agnostic distillation of LMs and aims at lifting both efficiencies for the first time.\n\nMixture of Experts",
            "score": 0.581128879986365,
            "section_title": "E Related Work",
            "char_start_offset": 24236,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 112,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 396,
                    "end": 412,
                    "matchedPaperCorpusId": "215745536"
                },
                {
                    "start": 466,
                    "end": 483,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 1116,
                    "end": 1140,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1535,
                    "end": 1555,
                    "matchedPaperCorpusId": "244680427"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.650390625
        },
        {
            "corpus_id": "258832674",
            "title": "Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?",
            "text": "Experiment setup. To validate if relatively weak classifiers are better teachers, we carefully select multiple teacher-student model pairs with different architectures, including ResNet [19], VGG [54], and ShuffleNetV2 (SNV2) [39]. The experiments are conducted on 200K synthetic datasets and the students are trained by 100 epochs. \n\nResults. When training on real datasets, it is common to use a relatively large teacher model to train the student model, such as distilling ResNet34 to ResNet18. In general, smaller teacher models often fail to achieve satisfactory distillation performance compared to larger teacher models. However, when working with synthetic datasets, we observe the opposite phenomenon: relatively weak teacher models can actually achieve better distillation performance than strong ones, as shown in Fig. 3 and Fig. 4. Interestingly, we found that as the capacity of the teacher model increases, a significant drop in performance is observed. Specifically, when training ResNet34 on the synthetic dataset, using ResNet18 as the teacher model leads to a 3% improvement in performance compared to using ResNet50 as the teacher model. These results highlight the importance of carefully selecting a teacher model when performing knowledge distillation and suggest that using a smaller, weaker teacher model may be preferable when working with synthetic datasets.",
            "score": 0.5810071175578377,
            "section_title": "Relatively weak classifiers are better teachers.",
            "char_start_offset": 22185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1384
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "51880435"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8974609375
        },
        {
            "corpus_id": "271227251",
            "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models",
            "text": "Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student) with the goal of retaining much of the teacher model's performance [117]. This process is crucial in scenarios where computational resources are limited or where deployment requires lightweight models. There are various types of traditional knowledge distillation techniques: response-based, feature-based and relation-based and one modern symbolic knowledge distillation, each with its unique approach and area of application: \n\n1) Response-based Knowledge Distillation: Responsebased knowledge distillation involves transferring knowledge from the teacher model's final output layer to the student model, aiming to mimic the teacher's final predictions. This approach is straightforward and has proven effective across various tasks, employing a loss function based on the divergence between the teacher's and student's logits. It's widely applied in model compression and has been adapted for different types of model predictions, including object detection and human pose estimation, where the teacher's output may include additional information like bounding box offsets [118] or heatmaps for landmarks [119]. A key application of responsebased knowledge distillation is in image classification [44], where \"soft targets\" -the probabilities assigned to each class by the teacher model -play a crucial role. These probabilities are adjusted using a temperature factor to control the softness of the targets, allowing the transfer of knowledge from the teacher to the student. The distillation process typically employs the Kullback-Leibler divergence loss to optimize the similarity between the teacher's and student's probability distributions. \n\nThis method is praised for its simplicity and effectiveness, particularly in leveraging knowledge for training. However, its reliance on the final layer's output means it may not fully utilize intermediate-level supervision from the teacher, an aspect crucial for representation learning in deep neural networks. \n\n2) Feature-based Knowledge Distillation: Feature-based knowledge distillation taps into the strength of deep neural networks to learn hierarchical feature representations, a process central to representation learning [120].",
            "score": 0.5806509192219911,
            "section_title": "A. Knowledge Distillation",
            "char_start_offset": 14819,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1796
                },
                {
                    "start": 1799,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2111
                },
                {
                    "start": 2114,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 219,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1223,
                    "end": 1228,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 1255,
                    "end": 1260,
                    "matchedPaperCorpusId": "53292120"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.921875
        },
        {
            "corpus_id": "259262233",
            "title": "SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design",
            "text": "Knowledge distillation, a concept first introduced by Hinton et al. [2015], has emerged as an effective model compression technique whereby a smaller student model is trained to mimic the functionality of a larger, more complex teacher model. Subsequent research has expanded upon this original premise, with distillation methodologies now typically falling into two main categories: general distillation [Sanh et al., 2019, Sun et al., 2020, Wang et al., 2020] and task-specific distillation [Jiao et al., 2019]. \n\nGeneral distillation leverages unlabeled data as a vehicle for knowledge transfer. Despite the computational expense, these methods have shown to be essential in retaining performance levels, particularly when pre-training the student network on an unlabeled corpus [Turc et al., 2019, Jiao et al., 2019]. On the other hand, task-specific distillation exploits task-relevant data to instill the knowledge from the teacher model to the student model. \n\nHowever, the real potential of knowledge distillation has begun to shine through in recent research that combines these two distinct approaches. For instance, the work of Jiao et al. [2019] showed that such hybrid methods could push the performance boundaries even further. \n\nThe advancements in distillation techniques have paved the way for increasingly efficient utilization of large language models. One of the notable milestones in this evolution is the study by Hsieh et al. [2023], which demonstrated that superior performance could be achieved even with less training data and smaller model sizes. \n\nIn summary, the continual progress in knowledge distillation research has opened up new avenues for large language model efficiency. It is this body of work that serves as a valuable foundation for our novel SparseOptimizer, offering a new perspective on model sparsification and optimization.",
            "score": 0.5805861675350821,
            "section_title": "Distillation",
            "char_start_offset": 5275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1573
                },
                {
                    "start": 1576,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1869
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94580078125
        },
        {
            "corpus_id": "271212917",
            "title": "Enhancing Weakly-Supervised Histopathology Image Segmentation with Knowledge Distillation on MIL-Based Pseudo-Labels",
            "text": "Knowledge distillation [23] is a technique that relies on teacher-student (T-S) architecture and targets to transfer knowledge from one model (teacher model) to another (student model).As shown in Fig. 2, beyond labels, the student model can learn various feature representations from the teacher model, such as prediction probabilities, feature vectors, and more [50].Since the student model usually exhibits superior performance over the teacher model [16], methods like momentum update have been widely used to further improve teacher's performance by updating it through the student model [7].Particularly, in iterative knowledge distillation, the student itself acts as the teacher, leveraging its past predictions for more informative supervision during training for generalization capability enhancement [28].To prevent the occurrence of model collapses in such self-supervised learning tasks, various strategies including contrastive loss [46], clustering constraints [6], predictor [17] or batch normalizations [42] are adopted in model optimization process.\n\nIt's important to highlight that, in contrast to the ensemble teacher distillation approach for classification mentioned in [28], our knowledge distillation method maintains the teacher model unchanged during the student's training phase.Once the student model outperforms the teacher, their roles are reversed, and a new cycle of knowledge distillation begins.",
            "score": 0.5803069083996335,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8660,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 185,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 816
                },
                {
                    "start": 816,
                    "end": 1067
                },
                {
                    "start": 1069,
                    "end": 1307
                },
                {
                    "start": 1307,
                    "end": 1430
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "232302458"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 593,
                    "end": 596,
                    "matchedPaperCorpusId": "233444273"
                },
                {
                    "start": 976,
                    "end": 979,
                    "matchedPaperCorpusId": "219721240"
                },
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "219687798"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "273969804",
            "title": "Quantifying Knowledge Distillation Using Partial Information Decomposition",
            "text": "Modern-day machine learning requires large amounts of compute for both training and inference. Knowledge distillation [1,2] can be used to compress a complex machine learning model (the teacher) by distilling it into a relatively simpler model (the student). The term \"distillation\" in this context means obtaining some assistance from the teacher while training the student so that the student performs much better than when trained alone (see Figure 1). In its earliest forms, knowledge distillation involved the student trying to match the output logits of the teacher [1]. More advanced methods focus on distilling multiple intermediate representations of the teacher to the corresponding layers of the student [2][3][4][5]. We also refer the reader to [6,7] for surveys. \n\nInformation theory has been instrumental in both designing [3,4] and explaining [8,9] knowledge distillation techniques. However, less attention has been given to characterizing the fundamental limits of the process from an information-theoretical perspective. Our goal is to bridge this gap by first introducing new measures to quantify the \"transferred knowledge\" and \"knowledge to distill\" for a teacher and a student model given a target downstream task. We bring in an emerging body of work called Partial Information Decomposition (PID) [10][11][12] to explain knowledge distillation. We define the knowledge to distill using the PID measure of \"unique\" information about the task that is available only with the teacher but not the student. As it follows, the transferred knowledge is succinctly quantified by the measure of \"redundant\" information that is common between the teacher and student. \n\nWe propose a multi-level optimization that maximizes redundant information (transferred knowledge) as a regularizer for more effective distillation. While PID has been explored in a few avenues of machine learning, Figure 1: Knowledge Distillation: The teacher (a complex model) assists the student (usually a substantially simpler model) during their training. The learned student can perform much better than an independently trained student without distillation with a similar training setup (i.e., hyperparameters and data). The teacher may or may not have been trained for the same task as the student.",
            "score": 0.5796607254895437,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 123,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 715,
                    "end": 718,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 718,
                    "end": 721,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 721,
                    "end": 724,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 724,
                    "end": 727,
                    "matchedPaperCorpusId": "252693152"
                },
                {
                    "start": 757,
                    "end": 760,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 837,
                    "end": 840,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 840,
                    "end": 842,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 858,
                    "end": 861,
                    "matchedPaperCorpusId": "251643827"
                },
                {
                    "start": 861,
                    "end": 863,
                    "matchedPaperCorpusId": "252846591"
                },
                {
                    "start": 1325,
                    "end": 1329,
                    "matchedPaperCorpusId": "1031742"
                },
                {
                    "start": 1329,
                    "end": 1333,
                    "matchedPaperCorpusId": "10901107"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90234375
        },
        {
            "corpus_id": "263620565",
            "title": "Can a student Large Language Model perform as well as it's teacher?",
            "text": "Knowledge distillation is a technique used in machine learning to transfer knowledge from a larger, more complex model (called the teacher model) to a smaller, simpler model (called the student model). The goal is to create a lightweight, computationally efficient student model that retains as much of the teacher model's performance as possible. This is especially useful for deploying machine learning models on resource-constrained devices or in situations where inference time and energy consumption are critical. \n\nThe knowledge distillation approach entails training a smaller, more focused model to replicate the results of a bigger, more broad language model, like GPT-3 [Brown et al., 2020]. The bigger model's high-level symbolic knowledge should be reduced into a smaller, more effective model that can accurately carry out specific tasks. This method involves training a student Transformer model using knowledge distillation from a larger teacher model. The teacher model provides soft labels for the training data, which are used to train the student model. This allows the student model to learn from the teacher model's rich representation while being more efficient due to its smaller size [Freitag et al., 2017]. In our experiments we used the model introduced by Gholami and Omar [2023] (GPT-Efficio) as the teacher. \n\nHere we study a specific approach called The figurative distiller (FD) process that involves three main components: a teacher model, a student model, and a set of rules. The teacher model is the larger, more general language model, while the student model is the smaller, more specialized model being trained. The rules define the constraints and relationships between different concepts in the modeled domain. There are three steps in the FD procedure. The student model is first trained using a collection of training examples produced by the instructor model. In order to enhance the student model's performance, a task-specific dataset is used to fine-tune it. The output of the student model is then further refined using symbolic rules to ensure it adheres to the limitations and relationships specified by the rules. \n\nBoth supervised, and unsupervised learning can be included in the FD process. The teacher model creates a collection of labeled examples in the supervised environment, subsequently utilized for training the student model.",
            "score": 0.5756277546488937,
            "section_title": "Approach",
            "char_start_offset": 7499,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1336
                },
                {
                    "start": 1339,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2162
                },
                {
                    "start": 2165,
                    "end": 2242
                },
                {
                    "start": 2243,
                    "end": 2386
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "248848394",
            "title": "Bearing Faulty Prediction Method Based on Federated Transfer Learning and Knowledge Distillation",
            "text": "After completing the knowledge distillation process, the three established teacher models and the student model are performed on the testing set of the target datasets of case study II. As shown in Figure 20 and Table 10 in case study II, the student model outperforms the three teacher models in terms of both the average prediction accuracy and the average KL-divergence loss with a smaller parameter size.",
            "score": 0.5749547011654337,
            "section_title": "Model Evaluation",
            "char_start_offset": 51229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 408
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "259378480",
            "title": "Review of Recent Distillation Studies",
            "text": "When knowledge distillation is performed, increasing the model size of the teacher network instead makes the knowledge distillation worse. The results are as follows: the student network is a 2-layer CNN, while the teacher network is a 4-10-layer CNN, respectively, and it can be seen from the results that continuing to increase the depth of the teacher network does not necessarily improve the performance of knowledge distillation. To further prove that the gap does affect knowledge distillation, the authors conducted a small comparison experiment by fixing the teacher network as a 10-layer CNN and changing the number of layers of the student network, and the results are as follows. Based on the above observations, it is not difficult to think that a medium-sized network (teacher assistant [16], TA) should be added between the large network and the small network, and the teacher network should be used to distill the TA first, and then the student network should be distilled by the TA. When TA was added, the distillation effect of the student network was significantly better than the direct distillation of the teacher network. (NOKD in the table indicates no KD, BLKD indicates baseline KD, and TAKD indicates KD with the TA method proposed above) From the analysis of the following figure, we know that the TA distilled from the teacher network (KD-TA) is better than the TA trained directly (FS-TA). The TA results obtained from multiple distillations are as follows, the more multiple stages the better the effect, but the more time and space consuming. In conclusion, the TA approach used, with layers of distillation, where the next layer absorbs the knowledge of the previous layer and passes it to the next layer, is a good solution to the problem that an overly strong teacher network provides some knowledge beyond what the student network can learn, resulting in lower efficiency, and helps us to select a teacher network of the right size.",
            "score": 0.5745507524865882,
            "section_title": "Teacher assistant",
            "char_start_offset": 3552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "278374469",
            "title": "Compressing Neural Networks on Limited Computing Resources",
            "text": "Knowledge distillation algorithm normally consists of three parts: Selection of a teacher network, definition of knowledge, and how to transfer knowledge. Here, the first part, i.e., the selection of the teacher network, has the greatest influence on the entire cost. Early KD methods generally chose the teacher network with the same structure as the student network but with a larger scale [5], [6], [24]. With in-depth studies on knowledge, how to effectively select teacher networks has been reported. One approach is to simply ensemble many teacher networks [16], [29], [30]. The ensemble knowledge provides higher performance because it is more accurate than a single teacher and further densely represents the output space. However, since a teacher network with too high performance may cause over-constraint(s), it is reportedly rather better to select teachers having similar performance [31]. Reference [32] defined a teacher network after the early stopping of learning of a larger network, whereas [31] defined a middle-size network as a teacher assistant to bridge between teacher and student. Furthermore, [33] employed several teacher network assistants that provide smooth knowledge. Despite the performance improvements achieved by sophisticated KD strategies, they often incur substantial computational overhead due to large teacher models or ensemble structures. Yet, few studies have focused on reducing the cost of knowledge distillation itself, especially for lightweight deployment. This motivates us to explore cost-effective alternatives that retain the benefits of teacher guidance while minimizing resource consumption.",
            "score": 0.574265033969415,
            "section_title": "B. KNOWLEDGE DISTILLATION",
            "char_start_offset": 8770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1646
                }
            ],
            "ref_mentions": [
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "235376877"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "7350432"
                },
                {
                    "start": 897,
                    "end": 901,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 913,
                    "end": 917,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1010,
                    "end": 1014,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1120,
                    "end": 1124,
                    "matchedPaperCorpusId": "221802641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69677734375
        },
        {
            "corpus_id": "272593221",
            "title": "What is the Role of Small Models in the LLM Era: A Survey",
            "text": "Scaling models to larger sizes is a straightforward method to enhance performance, but it often proves too computationally expensive for widespread deployment to numerous users. To mitigate this challenge, Knowledge Distillation (KD) (Hinton, 2015;Gou et al., 2021;Zhu et al., 2023;Xu et al., 2024a) offers an effective solution. In KD, a smaller student model is trained to replicate the behavior of a larger teacher model. Typically, this process involves the larger model generating a dataset with pseudo labels, which the smaller model then uses for training. \n\nWhite-box distillation involves using internal states of the teacher model which provides transparency in the training process of the student model. This approach leverages the output distributions, and intermediate features from the teacher LLMs, collectively referred to as feature knowledge (Liang et al., 2023;Gu et al., 2024;Liu et al., 2023b). It enables the development of costeffective yet powerful models, exemplified by Dis-tilBERT (Sanh, 2019) and QuantizedGPT (Yao et al., 2022). \n\nIn contrast, black-box knowledge distillation typically involves generating a distillation dataset through the teacher LLM, which is then used for fine-tuning the student model. For instance, Chain-of-Thought distillation (Wei et al., 2022b) extracts LLM rationales to provide additional supervision, thereby enhancing the reasoning capabilities of smaller models (Li et al., 2022;Hsieh et al., 2023;Shridhar et al., 2023;Magister et al., 2023;Li et al., 2023a;Fu et al., 2023;Tian et al., 2024). Additionally, Instruction Following Distillation aims to improve the zero-shot performance of LLMs by fine-tuning them with a set of instruction-like prompt-response pairs (Jiang et al., 2023c;Li et al., 2024b).",
            "score": 0.574222099088292,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 37344,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1057
                },
                {
                    "start": 1060,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1768
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 248,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 248,
                    "end": 265,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 860,
                    "end": 880,
                    "matchedPaperCorpusId": "252693152"
                },
                {
                    "start": 1038,
                    "end": 1056,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 1282,
                    "end": 1301,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1441,
                    "end": 1460,
                    "matchedPaperCorpusId": "258461606"
                },
                {
                    "start": 1460,
                    "end": 1482,
                    "matchedPaperCorpusId": "258762841"
                },
                {
                    "start": 1482,
                    "end": 1504,
                    "matchedPaperCorpusId": "254823156"
                },
                {
                    "start": 1504,
                    "end": 1521,
                    "matchedPaperCorpusId": "259251773"
                },
                {
                    "start": 1521,
                    "end": 1537,
                    "matchedPaperCorpusId": "256390607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84765625
        },
        {
            "corpus_id": "271162170",
            "title": "Inference Optimization of Foundation Models on AI Accelerators",
            "text": "Knowledge distillation (KD) ( [15,44,76,117]) is a model compression technique in which we train a small model (called student) to match closely the performance of a larger model or an ensemble of models (called teacher). To this end, KD connects a student model with the teacher model by a distillation loss, which penalizes differences in the outputs of the two models at certain layers (see Figure 7). The standard KD approach-also called last-layeronly approach-trains the student to match the performance of the teacher at the last layer (e.g., [44,96]). Another approach-also called layer-wise approach-trains the student to match the hidden representation of the teacher at each layer (e.g., [107]). Layer-wisedistillation approaches report improved results on downstream tasks compared to last-layer-distillation approaches [71], but they stipulate the same number of layers in the student as the teacher. In general, KD approaches are flexible with regard to the exact structure of the student model, which allows optimizing the student for various target hardwares. Another advantage is that the distillation process runs entirely after training the large teacher model. \n\nDistillation does not affect the training of a teacher model, but distillation effort by itself can be a major training effort for the following reasons. First, the number of steps can be similar to pre-training a small model. Second, the distillation loss usually is a combination of the pure student/teacher loss together with an original loss, for which typically the original pre-training data is recommended [44]. To compute the distillation loss, we also need to make a forward pass of a teacher model to get logits. But there  [44], where small student model distills a large teacher model via minimizing a distillation loss. This loss on a transfer dataset is then backpropagated to the student model. is a range of possibilities in selecting the transfer set on which to train the smaller distilled model [87]. For example, symbolic distillation [75,117] approaches synthesize data from the teacher model to this end.",
            "score": 0.573839915883849,
            "section_title": "Distillation",
            "char_start_offset": 33601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2109
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 34,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "251280158",
            "title": "KD-SCFNet: Towards More Accurate and Efficient Salient Object Detection via Knowledge Distillation",
            "text": "To demonstrate the effectiveness of the proposed distillation method and modules, we design the following experiments, and all experiments are based on DUTS-TE and ECSSD datasets in terms of the mean absolute error (M ), Fmeasure (F \u03b2 ) and E-measure (E \u03b2 ) evaluation metrics. \n\nEffectiveness of Proposed Distillation Method. To verify the effectiveness of the proposed distillation method, we distill different students using SCFNet R50 as the teacher and KD-SOD80K as the knowledge distillation dataset. The results are shown in Table 2. It can be seen that the student has a visible improvement in terms of performance, whether it is a lightweight or heavyweight model. In particular, the distilled GCPANet [4] performs better than the teacher model and achieves unprecedented results, which means that the teacher model would not limit the performance of the student model. Fig. 5 shows the comparison between the distillation results and fully supervised results, where the effect of knowledge distillation can be seen more intuitively. \n\nTo further explore the influence of different teachers on the same student, we distill the student SCFNet using different teachers. In detail, we change the backbone network of SCFNet to VGG-16 [33] and Swin Transformer-B [24] to obtain different teachers, and represent them as SCFNet V 16 and SCFNet ST B , respectively. In addition, we also use an existing method CTDNet [50] as the teacher model, which has an absolutely different architecture from SCFNet. As shown in Fig. 6, KD-SCFNet has an obvious promotion with the improvement of the teacher models, which means that better teacher can more accurately deliver the knowledge to student. Besides, even though the CTDNet with an absolutely different structure is used as the teacher, KD-SCFNet still achieves excellent results, which proves the robustness of the proposed distillation method. \n\nTable 2. Distillation results for different models. We show the results of knowledge distillation based models, like KD-U 2 Net, KD-SAMNet, KD-GCPANet and the original results. The best results are shown in red.",
            "score": 0.5723687449245984,
            "section_title": "Ablation Study",
            "char_start_offset": 23691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 280,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1894
                },
                {
                    "start": 1897,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 711,
                    "end": 714,
                    "matchedPaperCorpusId": "211677643"
                },
                {
                    "start": 1239,
                    "end": 1243,
                    "matchedPaperCorpusId": "14124313"
                },
                {
                    "start": 1419,
                    "end": 1423,
                    "matchedPaperCorpusId": "239011600"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8583984375
        },
        {
            "corpus_id": "266335531",
            "title": "Let All be Whitened: Multi-teacher Distillation for Efficient Visual Retrieval",
            "text": "Table 3 summarizes the performance and complexity of original teacher models and distillation models using varying teacher combinations. The first group reports the performance of whitened teacher models. As shown in the table, single-teacher distillation could significantly reduce the model complexity during the inference, but their performance is only comparable to the corresponding teacher models. By contrast, multi-teacher distillation methods including double-teacher and triple-teacher distillation not only reduce the model complexity but also achieve further performance gain. We attribute the performance gain to the complementarity of multiple teacher models and the effectiveness of our proposed multi-teacher distillation framework. Besides, the triple-teacher distillation almost surpassing the double-teacher distillation demonstrates the incremental advantage of fusing more teacher models. It is possible to improve even further by introducing more teacher models. But we can also imagine a performance saturation with endless addition of teachers, due to the student model's limited capacity, less complementarity between teachers, etc. Using more teacher models also means larger training cost. It is a trade-off in practice and we recommend determining the number of teachers based on the expected service time to make the additional training cost more amortized by the improved inference speed. \n\nComparison to baseline approaches In order to further verify the effectiveness of multi-teacher distillation, we compare it to three related baseline approaches, including Ensemble Mean (EM), Embedding Distillation (ED), and Contrastive Learning (CL). EM is a classical ensemble method, which averages the similarity scores produced by teacher models during inference. ED is a feature-based distillation method, which jointly minimizes the Euclidean distances between output embeddings of teacher models and the student model. CL is a representation learning method widely used in recent works (Chen, Xie, and He 2021), which is equivalent to replacing Q in Eq. 2 with an identity matrix. \n\nAs in Table 4, our proposed triple-teacher distillation consistently outperforms the counterparts with a clear margin. Among them, EM fusing knowledge from multiple models performs the best, but it brings heavy computation overhead as every ensembled model must be forwarded to calculate its similarity output.",
            "score": 0.5721200588863555,
            "section_title": "Single-teacher distillation vs. Multi-teacher distillation",
            "char_start_offset": 21379,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1418
                },
                {
                    "start": 1421,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2109
                },
                {
                    "start": 2112,
                    "end": 2230
                },
                {
                    "start": 2231,
                    "end": 2422
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75732421875
        },
        {
            "corpus_id": "264490946",
            "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
            "text": "data privacy, separate test servers, ...), we explore if transfer of complementary knowledge between any teacher and student is possible without performance degradation. Achieving such transfer through any possible model pair unlocks any freely available or self-generated model collection as an auxiliary resource for gains in canonical and problem-specific pretraining. It also avoids the need for model-specific transfer that require expert knowledge, and reduces the reliance on external evaluation measures for model selection. More importantly however, it also enables improvements of larger models by knowledge transfer from weaker, lower-resource models, without the explicit need for additional data & supervision, or sacrifices in e.g. speed, fairness, interpretability or ease-of-use. \n\nWe investigate the limits of knowledge distillation (Hinton et al., 2015;Tian et al., 2020) for this task, which in contrast to data-free approaches (e.g. Wortsman et al. (2022a)), operates independently of model choices. However, standard knowledge distillation frameworks assume information to be distilled to an untrained student. In contrast, we only wish to transfer knowledge not available in an already trained student model, which may even outperform its teacher. This crucially entails a successful trade-off between knowledge gain and retention. Indeed, for knowledge transfer between arbitrary pretrained models, common distillation ( \u00a75.1) exhibits strong model/hyperparameter dependence and performance drops for the majority of student models, particularly for weaker/equiperformant teachers. This can be attributed to catastrophic forgetting (Kirkpatrick et al., 2016;Zenke et al., 2017) outweighing the benefits of complementary knowledge transfer from the teacher. \n\nFor a favorable trade-off between forgetting and knowledge gain, we treat the transfer process as a continual learning problem, where a model is continuously presented with new context for data already seen. To encourage retention, we first study weight interpolation (Stojanovski et al., 2022;Wortsman et al., 2022b). While better than normal distillation, it is often too strong a constraint when the teachers have niche areas of expertise or are overall stronger.",
            "score": 0.5719564866870711,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2248
                }
            ],
            "ref_mentions": [
                {
                    "start": 871,
                    "end": 889,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 953,
                    "end": 976,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 2050,
                    "end": 2076,
                    "matchedPaperCorpusId": "253383803"
                },
                {
                    "start": 2076,
                    "end": 2099,
                    "matchedPaperCorpusId": "237420687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70751953125
        },
        {
            "corpus_id": "254685937",
            "title": "FlexiViT: One Model for All Patch Sizes",
            "text": "Knowledge distillation [23] is a popular technique, where a typically smaller student model is trained to mimic the predictions of a typically larger teacher model. This can significantly improve the performance of the student model compared to standard label-supervised training [5,12,60]. It was recently shown that knowledge distillation corresponds to a much more challenging optimization problem than standard supervised training [5,49], and that initializing the student close to the teacher simplifies alleviates this [49]. Unfortunately, this solution is impractical since the teacher usually has a different (larger) architecture than the student [5]. However, with FlexiViT, we can initialize a student FlexiViT with the weights of a powerful ViT teacher and significantly improve distillation performance.\n\nUnless otherwise stated, the model we use for the remaining experiments in this paper is a FlexiViT-B initialized and distilled from the powerful ViT-B/8 model of [50]. At initialization, we PI-resize the teacher's patch embedding weights to 32 \u00d7 32, and bilinearly resample its position embeddings to 7 \u00d7 7. We then train the student model following the FunMatch [5] approach, minimizing the KLdivergence between the predictions of the teacher and the student FlexiViT with a randomized patch size:\n\nwhere f FlexiViT (x, p) is the distribution over classes for the FlexiViT model on an input x with patch size p, f ViT-B/8 (x) is the predictive distribution of the teacher on the exact same input, D is the training data distribution with random flips, crops, and mixup, and P is the distribution over patch sizes used for training the FlexiViT model. Figure 5 compares the effect of distilling using teacher initialization to random initialization and to supervised training from labels. The comparison was performed for 90 epochs and shows considerable benefits of this unique initialization capability of FlexiViT. Since distillation needs patience [5,54], we additionally run for 300 and 1000 epochs, shown as pale green curves in the figure. FlexiViT matches the teacher's performance at small patch sizes, and teacher initialization provide a large improvement",
            "score": 0.5704092721491871,
            "section_title": "Connection to knowledge distillation",
            "char_start_offset": 13729,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 283,
                    "matchedPaperCorpusId": "235376877"
                },
                {
                    "start": 283,
                    "end": 286,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 286,
                    "end": 289,
                    "matchedPaperCorpusId": "207853355"
                },
                {
                    "start": 435,
                    "end": 438,
                    "matchedPaperCorpusId": "235376877"
                },
                {
                    "start": 438,
                    "end": 441,
                    "matchedPaperCorpusId": "235390933"
                },
                {
                    "start": 525,
                    "end": 529,
                    "matchedPaperCorpusId": "235390933"
                },
                {
                    "start": 656,
                    "end": 659,
                    "matchedPaperCorpusId": "235376877"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "235485156"
                },
                {
                    "start": 1182,
                    "end": 1185,
                    "matchedPaperCorpusId": "235376877"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83837890625
        },
        {
            "corpus_id": "257757091",
            "title": "Mixed-Type Wafer Classification For Low Memory Devices Using Knowledge Distillation",
            "text": "Knowledge distillation (KD) is an important technique in the domain of model compression. In knowledge distillation, a small lightweight student model is trained to mimic the performance of a compute-intensive high-performing teacher model. Surprisingly enough, sometimes the student performs much better than the teacher even though both have the same capacity [23][24] [25]. Large-scale deep models have achieved profuse successes in various fields ranging from computer vision [26] to NLP [27], however, the huge computational complexity and massive storage requirements make it a great challenge to deploy them in real-time applications, especially on devices with limited resources, such as video surveillance and autonomous driving cars. A knowledge distillation system is generally composed of three key components: knowledge, distillation algorithm, and teacher-student architecture. Figure 1 shows a common teacher-student framework for knowledge distillation. Typically, knowledge distillation routines can be classified into the following categories: \n\nResponse-Based Knowledge Distillation: The main idea in response-based knowledge is to directly mimic the outputs of the teacher. Response-based knowledge distillation is a simple yet effective technique for model compression and it is widely used in different applications. Given a teacher model T and student model S, response-based knowledge distillation optimizes the distillation loss between the logits z t = T (x) and z s = S(x) formulated as \n\nMore than often, L KD is employed as Kullback-Leibler loss or even simpler mean-square-loss(MSE) loss. Clearly, minimizing L KD forces the student logits to match the teacher logits, this essentially results in the student model mimicking the teacher model. \n\nFeature-Based Knowledge Distillation Neural Networks are exceptionally well at learning representations of input concepts in increasing levels of abstraction. Feature-based knowledge distillation employs the distillation loss between each feature layer in the teacher and student model respectively. Feature-based distillation is an effective extension of response-based learning, especially for thinner and deeper models. The loss functionally generally takes the form \n\nwhere T (x) and S(x) are the feature maps of the intermediate layers of the teacher and student model.",
            "score": 0.5703915728301256,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1773
                },
                {
                    "start": 1776,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2245
                },
                {
                    "start": 2248,
                    "end": 2350
                }
            ],
            "ref_mentions": [
                {
                    "start": 371,
                    "end": 375,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 480,
                    "end": 484,
                    "matchedPaperCorpusId": "46746239"
                },
                {
                    "start": 492,
                    "end": 496,
                    "matchedPaperCorpusId": "249885739"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "271961991",
            "title": "Knowledge Distillation with Perturbed Loss: From a Vanilla Teacher to a Proxy Teacher",
            "text": "Knowledge distillation is a popular technique to transfer knowledge from a large teacher model to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between this \"distribution closeness'' and the student model generalizability, which enables us to select the PTLoss's perturbation coefficients in a principled way. Extensive experiments on six public benchmark datasets demonstrate the effectiveness of PTLoss with teachers of different scales.",
            "score": 0.5696291559269309,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "271039191",
            "title": "Understanding the Gains from Repeated Self-Distillation",
            "text": "Knowledge distillation [11] was initially proposed as a way to transfer the knowledge learnt by a larger teacher model to a smaller student model, which can then be deployed in limited resource settings.The process is as follows: Train a teacher (T ) model using ground-truth labels, then use its predictions to supervise the training of a student (S) model via a combined per-sample loss, \u03be \u2022 \u2113 \u0177T , y S (\u03b8) + (1 \u2212 \u03be) \u2022 \u2113 y, y S (\u03b8) ,\n\nwhere \u2113 denotes the loss function, y is the ground-truth label, \u0177T denotes the teacher's prediction, and y S (\u03b8) denotes the student's prediction, parameterized by the learnable \u03b8.The extra hyperparameter \u03be is called the imitation parameter [24], generally restricted to \u03be \u2208 [0, 1].It gives additional freedom to the student to balance importance between labels and teacher's predictions.The student trained via this distillation objective (i.e., utilizing the teacher's predictions through \u03be \u0338 = 0) has been widely observed to generalize better than when trained only on the labels (i.e., \u03be = 0).This gain has been attributed to 'dark knowledge' that is (i) impossible to be directly extracted from the training data by the small model, but (ii) easily learnt by the large model and transferred to the small model.\n\nChallenging this interpretation, Li et al. [19] and Furlanello et al. [9] empirically observed performance gains through distillation even when the teacher and student are same-sized models.One can set T and S to have the same architecture, and S trained with the objective in Eq. (1) outperforms T .This is referred to as Born-Again Networks (BANs) or Self-Distillation (SD).Furthermore, repeatedly applying self-distillation on the same training data with a student model having the same architecture provides additional gains on benchmark datasets and architectures [9,35,43].At each step, the student from the previous step acts as the teacher used to train a new student model under the self-distillation loss of Eq. (1).",
            "score": 0.5694442712749914,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 435
                },
                {
                    "start": 437,
                    "end": 617
                },
                {
                    "start": 617,
                    "end": 719
                },
                {
                    "start": 719,
                    "end": 825
                },
                {
                    "start": 825,
                    "end": 1034
                },
                {
                    "start": 1034,
                    "end": 1252
                },
                {
                    "start": 1254,
                    "end": 1444
                },
                {
                    "start": 1444,
                    "end": 1554
                },
                {
                    "start": 1554,
                    "end": 1630
                },
                {
                    "start": 1630,
                    "end": 1833
                },
                {
                    "start": 1833,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1297,
                    "end": 1301,
                    "matchedPaperCorpusId": "14659675"
                },
                {
                    "start": 1324,
                    "end": 1327,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1823,
                    "end": 1826,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1826,
                    "end": 1829,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 1829,
                    "end": 1832,
                    "matchedPaperCorpusId": "219558831"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9443359375
        },
        {
            "corpus_id": "255125239",
            "title": "BD-KD: Balancing the Divergences for Online Knowledge Distillation",
            "text": "Knowledge distillation (KD) is a simple and yet powerful technique for model compression [7]. It has become an accessible tool, which facilitates the creation of compact student networks for deployment onto limited memory and computation edge devices. While distillation is traditionally performed offline, in recent years online methods have gained popularity for their improved performance and computational efficiency. Deep mutual learning [25]is one of the first techniques to propose collaborative learning of two or more networks This technique has opened an opportunity for distillation methods to reduce the training process to a single one-time simultaneous training of teacher and student. Each network is minimizing an objective function which consists of the traditional Cross-Entropy (CE) loss and a forward KL loss which encourages the network to mimic the other network's behaviour. Online Knowledge Distillation with Diverse Peers (OKDDip) is another online method proposed by Chen et al. [2]; a teacher-free two-level distillation method where the first-level happens among multiple students with identical architectures. The group knowledge of these diverse students is then transferred to the \"group leader\" in the second level. In a recent work by Chung et. al. [3] the distillation goes beyond class logit matching and is used to match the feature maps of two networks using an adversarial framework. \n\nNone of these online methods study the impact of a large capacity gap. Guo and colleagues were the first ones to show large gap harm the performance of the networks. To combat this phenomenon, they present Distillation Collaborative learning (KDCL) [6] where a series of ensembling methods is employed to generate high-quality soft targets at once for distillation. Although, KDCL improves the performance of both networks, due to the nature of the loss the focus is mostly on the larger network and therefore the improvement is more enhanced for the teacher. The first work that introduces an online distillation model that attempts to close the accuracy gap during training is Switchable Knowledge distillation model (SWITOKD) [15]. This model mitigates the accuracy gap problem by slowing down the training of the teacher.",
            "score": 0.5694308140691745,
            "section_title": "Related Work",
            "char_start_offset": 5468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2249
                }
            ],
            "ref_mentions": [
                {
                    "start": 443,
                    "end": 447,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1282,
                    "end": 1285,
                    "matchedPaperCorpusId": "209319166"
                },
                {
                    "start": 1673,
                    "end": 1676,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 2153,
                    "end": 2157,
                    "matchedPaperCorpusId": "252198825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7236328125
        },
        {
            "corpus_id": "268681103",
            "title": "ToXCL: A Unified Framework for Toxic Speech Detection and Explanation",
            "text": "Knowledge distillation (Hinton et al., 2015) is a technique that enables a smaller student model to learn from a larger teacher model by transferring knowledge.It has proven effective in improving performance, reducing computational requirements, and increasing efficiency in the field of Computer Vision (Gou et al., 2021).Recently, researchers have explored applying knowledge distillation in Natural Language Processing.For example, Fu et al. (2020) used a contrastive approach to align the intermediate layer outputs of the teacher and student models.Turc et al. (2019) extensively studied the interaction between pre-training, distillation, and fine-tuning, demonstrating the effectiveness of pre-trained distillation in tasks like sentiment analysis.Additionally, Clark et al. (2019) trained a multitasking network by ensembling multiple single-task teachers.In our work, we distill the knowledge from a teacher classifier to our model's classifier (the student classifier), optimizing the Kullback-Leibler distance (Csisz\u00e1r, 1975) between soft labels.",
            "score": 0.5691553067435873,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 160,
                    "end": 324
                },
                {
                    "start": 324,
                    "end": 423
                },
                {
                    "start": 423,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 756
                },
                {
                    "start": 756,
                    "end": 865
                },
                {
                    "start": 865,
                    "end": 1058
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 323,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 770,
                    "end": 789,
                    "matchedPaperCorpusId": "85464175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89892578125
        },
        {
            "corpus_id": "266450524",
            "title": "Topological persistence guided knowledge distillation for wearable sensor data",
            "text": "We tested the proposed method, TPKD, with different datasets including different sizes of window length, the number of classes, and complexity.In most of the cases, TPKD outperformed baselines in classification.Even with smaller or the different number of classes, TPKD showed the best accuracy.This implies that using orthogonality properties to transfer feature relationship can improve performance significantly.The results also showed that larger capacity of teachers does not guarantee the generation of a better student.This corroborates previous studies [12,56].Among different combinations of teachers and students, in most cases, WRN16-3 teachers distilled a superior student.When the window length of a sample is large and the model performs with a smaller number of classes, the smaller network (WRN16-1) showed good performance.This indicates that large-sized models can increase the knowledge gap, and smaller teachers can perform better for easy problems.For different architectures of teachers, in overall cases, the proposed method generated a better student.However, for WRN28-1 Teacher1 and WRN16-3 Teacher2 on GENEActiv, TPKD without Orth.outperformed with Orth.This showed that TPKD with Orth.performs better when the width of teachers is similar to a student among various combinations of teachers.Also, even though improvements for large Teacher1 with TPKD are smaller than other combinations, TPKD achieved better results compared to baselines.Therefore, when two teachers are similar or the width of teacher models is similar to a student, the proposed method performs better than the other combinations.And, TPKD can alleviate the negative effects from the knowledge gap in distillation.\n\nFor TPKD, best cases showed with different \u03b2 and k across datasets.In general, when k is 4, TPKD performs the best.However, the optimal \u03b2 is different for datasets.For dataset including 40 channels (PAMAP2), when \u03b2 is 200, it showed best performance in overall cases, whereas it is 700 for GENEActiv with 3 channels.Finding optimal parameters for training can consume time which is a limitation of the method.",
            "score": 0.5686548351316545,
            "section_title": "Discussion",
            "char_start_offset": 44739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 415
                },
                {
                    "start": 415,
                    "end": 526
                },
                {
                    "start": 526,
                    "end": 569
                },
                {
                    "start": 569,
                    "end": 685
                },
                {
                    "start": 685,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 969
                },
                {
                    "start": 969,
                    "end": 1075
                },
                {
                    "start": 1075,
                    "end": 1158
                },
                {
                    "start": 1158,
                    "end": 1181
                },
                {
                    "start": 1181,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1319
                },
                {
                    "start": 1319,
                    "end": 1467
                },
                {
                    "start": 1467,
                    "end": 1628
                },
                {
                    "start": 1628,
                    "end": 1712
                },
                {
                    "start": 1714,
                    "end": 1781
                },
                {
                    "start": 1781,
                    "end": 1829
                },
                {
                    "start": 1829,
                    "end": 1878
                },
                {
                    "start": 1878,
                    "end": 2030
                },
                {
                    "start": 2030,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 561,
                    "end": 565,
                    "matchedPaperCorpusId": "245604889"
                },
                {
                    "start": 565,
                    "end": 568,
                    "matchedPaperCorpusId": "53473116"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7822265625
        },
        {
            "corpus_id": "274233947",
            "title": "When Babies Teach Babies: Can student knowledge sharing outperform Teacher-Guided Distillation on small datasets?",
            "text": "The vanilla distillation Hinton et al. (2015) method consists of two stages, firstly train a large teacher model, followed by transfer of soft logits to a smaller student model. Also known as Offline distillation, it keeps the teacher fixed, only allowing a one-way knowledge transfer. To reduce memory consumption of training a large teacher model, Zhang et al. (2018) proposed an online distillation framework called mutual learning where a group of student (or student) models were trained simultaneously. Although, online distillation eliminated the teacher model, similar networks in online distillation may prevent the students from learning knowledge from the students Zhang et al.. Recent approaches have attempted to induce diversity in online distillation to improve overall performance. Chen et al. (2020) proposed inducing data diversity by training student models with varying image augmentations. However, this method relies heavily on data augmentations, which can be unpredictable in real-world deployment scenarios. Du et al. (2020) introduced an adaptive ensemble knowledge distillation method using multiple diverse teacher models to train a student model. While this approach shows promise, it requires maintaining several teacher models, leading to increased memory usage and computational overhead. The reported accuracy improvements are also relatively modest, typically ranging from 0.5% to 1% across benchmarks. Our approach closely resembles to that of Zhang et al.. They present a diversity induced weight mutual learning approach for distillation. They introduce diversity by assigning varying pruning ratios to different student models. Although this method reduces memory consumption, the manual assignment of pruning ratios may not generalize well across different architectures and tasks. The reported performance gains are limited, with improvements of less than 0.5% on most benchmarks. As shown by Liu et al. (2017), while pruning induces sparsity within networks and can reduce computational complexity (measured in FLOPs), the relationship between pruning percentage and actual model size reduction is not always linear. Moreover, in Zhang et al., we observe a performance drop when pruning beyond 30%, indicating a trade-off between model compression and accuracy. \n\n3 Diversity Induced Weighted Mutual Learning",
            "score": 0.5684927603812253,
            "section_title": "Related Work",
            "char_start_offset": 2871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2302
                },
                {
                    "start": 2305,
                    "end": 2349
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 45,
                    "matchedPaperCorpusId": "14915449"
                },
                {
                    "start": 350,
                    "end": 369,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 798,
                    "end": 816,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 1033,
                    "end": 1049,
                    "matchedPaperCorpusId": "227276362"
                },
                {
                    "start": 1933,
                    "end": 1950,
                    "matchedPaperCorpusId": "5993328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84228515625
        },
        {
            "corpus_id": "272593472",
            "title": "Applied Federated Model Personalisation in the Industrial Domain: A Comparative Study",
            "text": "Generally speaking, a large difference in model size between the student and instructor networks in (KD) can lead to subpar results. An enhanced KD framework [19] was proposed, which incorporates a teacher assistant and a multistep process. Additionally, the integration of multi-teacher KD technology with dual-stage progressive KD has been suggested [20] to improve the performance of KD under limited data conditions. This approach takes advantage of the benefits provided by multi-teacher KD. \n\nThere have also been attempts to apply self-learning to a model via KD [21]. The aforementioned methodology employs a teacher-student paradigm with identical network structures to derive a distilled student model. This distilled model is then leveraged as a teacher to facilitate the training of a new student model, and this cycle is iteratively repeated to gradually enhance model performance. In an attempt to avoid using exceptionally large models in Neural Machine Translation (NMT) tasks, the paper at [22] utilized KD, introducing two new variations of the technique in the process. \n\nAdditional variations include Relational Knowledge Distillation (RKD) [23], which transfers mutual relations between data examples. Experiments results show that via RKD, student models can often outperform the teacher. Another technique is knows as Similarity-Preserving Knowledge Distillation [24] and it enables the training of a student network by ensuring that input pairs that generate comparable, or distinct, activations in the teacher network yield similar, or dissimilar, activations in the student network. \n\nWhile exploring the field of Logit Distillation, researchers proposed a reformulation of the conventional KD loss [25], splitting it into two components referred to as Target Class Knowledge Distillation (TCKD) and Non-Target Class Knowledge Distillation (NCKD). Also a separate technique dubbed Virtual Knowledge Distillation (VKD) [26] leverages a softened distribution generated by a virtual knowledge generator that is conditioned on the class label, in an attempt to improve the student's performance.",
            "score": 0.5682841629941443,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 11724,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "250034651"
                },
                {
                    "start": 570,
                    "end": 574,
                    "matchedPaperCorpusId": "244805869"
                },
                {
                    "start": 1007,
                    "end": 1011,
                    "matchedPaperCorpusId": "8451212"
                },
                {
                    "start": 1161,
                    "end": 1165,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1725,
                    "end": 1729,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1944,
                    "end": 1948,
                    "matchedPaperCorpusId": "247874670"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8544921875
        },
        {
            "corpus_id": "251460030",
            "title": "Improving Deep Mutual Learning via Knowledge Distillation",
            "text": "Knowledge distillation in [8] is one of the most popular knowledge transfer methods today, and it uses a teacher-student framework. The basic idea of this method is that a pretrained teacher network (i.e., a cumbersome network or the biggest network) using certain hyperparameters are then used to train an untrained student network (i.e., small network) for the purpose of transferring knowledge. This process uses a distillation knowledge equation where a temperature (T) is involved and can be varied to obtain a soft probability output from a class C image which can be calculated as: \n\nSuppose the teacher network is marked as G t and the student network is G s , then the distillation loss can be defined as: \n\nAs a result, the student loss function contained in Figure 2 is minimized during the training process based on ( 6) and (7) as: \n\nwhere \u03bb is a balancing value between the two losses. The main purpose of the teacherstudent framework is to force the student output probability to imitate or match the pre-trained teacher network's probability output.",
            "score": 0.5682726864847274,
            "section_title": "DML and KD",
            "char_start_offset": 10613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1065
                }
            ],
            "ref_mentions": [
                {
                    "start": 837,
                    "end": 840,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79736328125
        },
        {
            "corpus_id": "269137377",
            "title": "Federated Distillation: A Survey",
            "text": "These methods focus on transferring knowledge from the middle layers of the teacher model to guide the training of the student model, helping the student model learn crucial representations and features at an intermediate level.\n\n3) Parameter Knowledge -In this category, partially trained parameters or network modules of the teacher model are directly used as knowledge during distillation training.This method is often combined with other distillation approaches to enhance knowledge transfer [63].Specifically, PESF-KD proposes an efficient approach to transferring knowledge from the teacher network to the student network by updating specific parameters of the pre-trained teacher model [64].FSKD compresses the pre-trained teacher model to obtain a student model and adjusts the feature dimensions using 1 \u00d7 1 convolutions between layers, allowing the student model to achieve performance comparable to traditional fine-tuned distillation methods with a small number of samples [65].Similarly, IAKD [66] and NGFSKD [67] assist distillation by replacing modules of the teacher model and the student model.The distillation loss for individual module replacement is expressed as:\n\nwhere L CE represents the calculation of cross-entropy, and M T o and M S o are the outputs of the corresponding modules in the teacher and student networks.Recently, SAKD proposes merging teacher and student networks of the same style system into a multi-path network.\n\nDuring training, a different teacher network module is dynamically selected to replace the corresponding student network module for each sample, resulting in a student model with superior performance [68].These methods leverage parameter knowledge from the teacher model to guide the training of the student model, either by updating specific parameters or by replacing modules.This enables the student model to learn from the partially trained teacher model, improving its performance and reducing its complexity.",
            "score": 0.568253063083202,
            "section_title": "Soft targets Teacher model Loss",
            "char_start_offset": 14697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 230,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 501
                },
                {
                    "start": 501,
                    "end": 698
                },
                {
                    "start": 698,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1111
                },
                {
                    "start": 1111,
                    "end": 1183
                },
                {
                    "start": 1185,
                    "end": 1342
                },
                {
                    "start": 1342,
                    "end": 1454
                },
                {
                    "start": 1456,
                    "end": 1661
                },
                {
                    "start": 1661,
                    "end": 1834
                },
                {
                    "start": 1834,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "216035835"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "249209818"
                },
                {
                    "start": 1006,
                    "end": 1010,
                    "matchedPaperCorpusId": "234805083"
                },
                {
                    "start": 1022,
                    "end": 1026,
                    "matchedPaperCorpusId": "228063796"
                },
                {
                    "start": 1656,
                    "end": 1660,
                    "matchedPaperCorpusId": "248503415"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73681640625
        },
        {
            "corpus_id": "260704230",
            "title": "Teacher-Student Architecture for Knowledge Distillation: A Survey",
            "text": "In self-distillation, the roles of student and teacher networks are dynamic during the iterative learning process, which indicates that student and teacher networks can be exchanged or student networks are able to learn the knowledge from themselves (without teacher networks). The detailed review of distillation schemes will be provided in Section 5. Furthermore, since the objective of knowledge compression is to construct compact student networks, it is important to note that the size of student networks is smaller than that of teacher networks. The student networks can be designed with a larger or identical size to the teacher networks in the works [5,6,7], providing feasible approaches for knowlegde expansion. To obtain wellgeneralized student networks in knowledge enhancement, multiple teacher networks are constructed across diverse scenarios. The introduction of multi-teacher distillation will be summarized in Section 4. \n\nWe further report the classification performance and computational efficiency of different methods across the multiple distillation objectives in Table 2 and Table 3 of the supplementary material, respectively. By acquiring the distilled knowledge from teacher networks, lightweight student networks are trained in a computationally efficient manner, while consistently demonstrating effective performance across various datasets for all the distillation objectives. Notably, CIFAR100 and CIFAR10 datasets [32] are commonly employed for evaluation purposes. Compared to the vanilla KD [4], which solely relies on logits as knowledge, FitNet [18] introduces intermediate feature maps as additional knowledge, leading to an improved classification accuracy of 0.26%. This suggests the advantage of incorporating intermediate feature maps in knowledge compression. The computational cost can be significantly reduced, particularly when multiple teacher networks are involved. In both the hetero-architecture and homo-architecture designs of teacher networks, student networks can consistently exhibit strong performance across all the objectives.",
            "score": 0.567654680262194,
            "section_title": "Comparison Analysis",
            "char_start_offset": 15731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2085
                }
            ],
            "ref_mentions": [
                {
                    "start": 659,
                    "end": 662,
                    "matchedPaperCorpusId": "207853355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84814453125
        },
        {
            "corpus_id": "254275420",
            "title": "Label-free Knowledge Distillation with Contrastive Loss for Light-weight Speaker Recognition",
            "text": "Similar to the previous work [16], the teacher-student structure is adopted for achieving KD. As shown in Figure 1, given an input speech waveform, the teacher embedding extractor uses filter-bank features (fbanks) of the whole input to generate a per-utterance speaker embedding. A short clip (2s-3s) of the waveform is cropped at a random time point in the utterance to Figure 1: teacher-student training for label-free knowledge distillation generate fbanks as the input to the student embedding extractor. In addition, spectrum augmentation [24] with time-frequency masking and warping of the input fbanks is applied to the student embedding extractor. The teacher model is assumed to be well-trained and its parameters are fixed in KD. The parameters of the student model are randomly initialized and updated by minimizing a distillation loss. The distillation loss is defined as the distance between pairs of embedding generated from the teacher and student models. Through this distillation process, it is expected that the student can learn adequate knowledge from the teacher to generate discriminative speaker embeddings.\n\nA critical difference between our work and the previous [16] is the training loss. The existing distillation method typically employs an auxiliary classification loss. The overall training loss is defined as a weighted combination of the distillation loss and the classification loss. This is because using the distillation loss only may not be sufficient for effective learning of task-specific knowledge. The interpolation could to a certain extent improve the performance of student learning. However, as labelled data are needed for the classification loss, the amount of data available for distillation would be limited. The present study aims to get rid of the classification loss and improve the distillation loss so as to unleash the power of large-scale distillation on unlabeled data.",
            "score": 0.5674397232283422,
            "section_title": "Label-free Knowledge Distillation",
            "char_start_offset": 4061,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 33,
                    "matchedPaperCorpusId": "145859794"
                },
                {
                    "start": 1189,
                    "end": 1193,
                    "matchedPaperCorpusId": "145859794"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8037109375
        },
        {
            "corpus_id": "272987178",
            "title": "Classroom-Inspired Multi-Mentor Distillation with Adaptive Learning Strategies",
            "text": "Knowledge distillation (KD) [1] is a widely adopted model compression technique in deep learning, where a smaller, more efficient student model learns to replicate the behavior of a larger, more complex teacher model. While traditional KD methods [1][2] [3] typically employ a single teacher, multi-teacher (or multi-mentor) distillation has been proposed to further enhance performance by leveraging an ensemble of teachers [4]. This setup is expected to provide richer and more diverse knowledge, improving the student's generalization and robustness. We use the term mentor to describe all networks involved in teaching the student, regardless of their size or role. Despite its potential benefits, multi-mentor distillation faces several significant challenges: \n\nLarge Capacity Gap: Employing multiple large mentors can create a substantial capacity gap between the collective representation power of the mentors and that of the student. This gap can hinder the student's ability to effectively mimic the combined knowledge of the mentors, leading to suboptimal learning outcomes. To bridge this gap, some works [5,6] have introduced intermediate-sized mentors alongside a large teacher. However, smaller mentors may be less effective, potentially introducing additional errors into the student's knowledge. \n\nError Accumulation: The lower performance of smaller mentors can contribute to cumulative errors in the distillation process. This is particularly problematic in sequential distillation frameworks like TAKD (Figure 1(b)), where each mentor teaches only the subsequent smaller model. Such setups can lead to an \"error avalanche,\" where inaccuracies from lower-performing mentors degrade the student's performance [6]. Although DGKD (Figure 1(c)) attempts to mitigate this by allowing each mentor to teach all smaller models and randomly dropping some mentors during training, these strategies can result in valuable information loss and reduced learning efficiency.",
            "score": 0.5674331702381542,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1312
                },
                {
                    "start": 1315,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 31,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 254,
                    "end": 257,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 425,
                    "end": 428,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1117,
                    "end": 1120,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1120,
                    "end": 1122,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 1727,
                    "end": 1730,
                    "matchedPaperCorpusId": "221802641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "263828861",
            "title": "OpenIncrement: A Unified Framework for Open Set Recognition and Deep Class-Incremental Learning",
            "text": "In the context of deep learning, knowledge distillation (KD) was first known in [16] and [33] that enables to transfer the knowledge learned by a larger model (teacher) to a smaller one (student). There are three types of knowledge that can be transferred, namely response-based knowledge, feature-based knowledge, and relation-based knowledge. \n\nResponse-based knowledge distillation is to directly align the predictions between students and teachers. The divergence between the logit layers of teachers and students is encouraged to be minimized. In [16], the logit layer of the teacher network is adapted as soft targets when computing the softmax loss so that the student can be expected to give the same outputs as the teacher. \n\nIn feature-based knowledge distillation, not only the last logit layer but also the intermediate layers of the teachers are utilized to transfer knowledge. The students can therefore output similar features as the teachers. In [33], the neural activations of the first layers in the teacher model are directly used to match the student model. Kim et. al. proposed to extract the factor maps of teachers' and students' layers using convolutional modules and let the student factor mimic the teacher factor [22]. Similarly, Passban et. al. proposed a combinatorial technique that can merge the features of multiple layers in the teacher model using the atten-tion mechanism [29]. \n\nUnlike the previous two categories that align the layer outputs between the teachers and students, relation-based knowledge distillation tends to preserve the relationships among the features in different layers or instances [28][39] [38]. Park et. al. proposed the relational knowledge distillation that transfers the instance relations between teachers and students [28]. Tung et. al. proposed the similarity-preserving knowledge distillation, in which the similarities between the instance features can be transferred to the students [39].",
            "score": 0.5672671181552054,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1957
                }
            ],
            "ref_mentions": [
                {
                    "start": 1640,
                    "end": 1644,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1783,
                    "end": 1787,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1952,
                    "end": 1956,
                    "matchedPaperCorpusId": "198179476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "257078820",
            "title": "Debiased Distillation by Transplanting the Last Layer",
            "text": "KD also plays a significant role outside model compression, showing its power to help training of a similar-sized model (Furlanello et al., 2018). Apart from using the teacher predictions, several other frameworks have been proposed, including the feature distillation (Romero et al., 2014) or distilling the intersample relations (Park et al., 2019). Recently, Chen et al. (2022) distills the feature map with the mean square loss and reuses part of teacher to perfectly mimic the teacher action; this work is most closely related to our method, which also reuses the last layer and distills the feature map. \n\nKnowledge distillation from imbalanced dataset. Several prior works study knowledge distillation under the presence of class imbalance, which is a closely related field to debiasing. Lukasik et al. (2021) shows that KD algorithms can distill not only the 'good knowledge' of the teacher, but also the bad knowledge of the teacher; student networks are likely to boost their performance by reproducing the teacher's behavior in an exaggerated way. This gives rise to the network which is more correct on the samples teacher gets correct, and more wrong on the samples teacher gets wrong. Blakeney et al. (2021) shows that KD can help resolving the inter-class performance discrepancy resulting from pruning the model. Wang et al. (2022) proposes a distillation algorithm that is specialized for distilling on the long-tailed dataset. These works focus primarily on the long-tail problem, which is distinct from the debiasing that we consider. To the best of our knowledge, our work is the first to consider knowledge distillation under the presence of spurious correlation in the training (or distillation) dataset.",
            "score": 0.5670912154214516,
            "section_title": "Related Work",
            "char_start_offset": 8157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1726
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 145,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 331,
                    "end": 350,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 362,
                    "end": 380,
                    "matchedPaperCorpusId": "247762862"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8291015625
        },
        {
            "corpus_id": "264490946",
            "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
            "text": "Knowledge Distillation (KL distillation) was pioneered by Hinton et al. (2015), which suggests minimizing the KL divergence between a teacher and a student model's soft targets \u03c3(z t ) and \u03c3(z s ): with temperature T . We use Eq. 1 as our base transfer objective (KL-Dist transfer), as it still remains popular and competitive (Beyer et al., 2022;Rajasegaran et al., 2020;Tian et al., 2020) (see Supp. for other objectives). KL distillation is often extended with auxiliary classification losses (e.g. crossentropy L XE , (Hinton et al., 2015;Tian et al., 2020;Rajasegaran et al., 2020;Beyer et al., 2022)) to stabilize the distillation process. We denote the \u03bb-weighted combination as XE-KL distillation, and the associated transfer as XE-KL-Dist. transfer or XE-KL: \n\nMost knowledge distillation research considers the distillation from a trained teacher to an untrained student, in stark contrast to our goal of knowledge transfer between pretrained models while retaining student knowledge already gained a priori. And indeed, when applied to knowledge transfer between pretrained models in \u00a75.1, standard knowledge distillation struggles to transfer knowledge without performance drops for most teacher-student pairs. We measure this via the transfer delta \u2206 transf = acc(f s kt ) \u2212 acc(f s ), which quantifies the change in the student's top-1 accuracy, with f s kt being the student model following the knowledge transfer.",
            "score": 0.566967068524697,
            "section_title": "KNOWLEDGE TRANSFER THROUGH KNOWLEDGE DISTILLATION",
            "char_start_offset": 14411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 372,
                    "matchedPaperCorpusId": "219720852"
                },
                {
                    "start": 372,
                    "end": 390,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 543,
                    "end": 561,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 561,
                    "end": 586,
                    "matchedPaperCorpusId": "219720852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78955078125
        },
        {
            "corpus_id": "254815049",
            "title": "Sound Event Detection System Based on VGGSKCCT Model Architecture with Knowledge Distillation",
            "text": "We next disclose the effect of using knowledge distillation and incorporating both the mean teacher method and knowledge distillation.The pre-trained model in knowledge distillation is similar to the aforementioned VGGSK system.The difference is it replaces the SK blocks with RepVGG (Reparameterized VGG) blocks (Ding et al. 2021) in order to decrease the number of model parameters from 2.5 m to 943k.This pre-trained model is also trained on the same DESED data.The first two rows in Tables 4 and 5 list the PSDS scores of VGGSKCCT system and pre-trained system with mean teacher framework.The third row is the knowledge distillation model with VGGSKCCT as a student model.This is referred to as 2-pass approach, since mean-teacher method and knowledge distillation method are applied in separate passes.The last row is a result of the model that includes a mean teacher model of VGGSKCCT and a pre-trained teacher model.This is referred to as 1-pass approach, since mean-teacher method and knowledge distillation method are applied simultaneously.The results in these tables indicate that when employing a pre-trained model to distill a VGGSKCCT model, there is no significant improvement.As we can see, the score of the student model and the pre-trained teacher model is roughly the same.The student model can only predict like a pre-trained teacher model if only knowledge distillation is used.However, the 1-pass approach, which incorporates both the mean teacher method and knowledge distillation, does improve performance.Here we also show the supervised losses of using 1-pass approach or not (mean teacher model with/without knowledge distillation) in Figure 5.The lighter line in the background is the true value of the supervised loss between strong labels and strong predictions.The darker line is the trend of the loss value.We can see that the pre-trained teacher model can assist the student model with the mean teacher structure in learning better, as the model that incorporates knowledge The terms \"Baseline,\" \"VGGSK\" and \"VGGSKCCT\" denote the three systems in our experiment.Details can be found in Section Materials and methods.",
            "score": 0.5666559872063308,
            "section_title": "Comparison of Different Training Methods",
            "char_start_offset": 21429,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 465
                },
                {
                    "start": 465,
                    "end": 593
                },
                {
                    "start": 593,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 924
                },
                {
                    "start": 924,
                    "end": 1051
                },
                {
                    "start": 1051,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1400
                },
                {
                    "start": 1400,
                    "end": 1531
                },
                {
                    "start": 1531,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1793
                },
                {
                    "start": 1793,
                    "end": 1840
                },
                {
                    "start": 1840,
                    "end": 2096
                },
                {
                    "start": 2096,
                    "end": 2150
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73291015625
        },
        {
            "corpus_id": "254338873",
            "title": "LPAI\u2014A Complete AIoT Framework Based on LPWAN Applicable to Acoustic Scene Classification Scenarios",
            "text": "Calculate the total loss in Equation ( 8). The teacher model is transferred to a new student model M t after retraining. The identical model compression method mentioned in Equation ( 4) is applied here. Beyer et al. [41] showed that knowledge distillation is more effective when the inputs to the teacher and student models are consistent, so X retrain is still used as the input to the student for distillation. The complete compression process is in Algorithm 2.",
            "score": 0.5665700386294592,
            "section_title": "3:",
            "char_start_offset": 15071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 465
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 221,
                    "matchedPaperCorpusId": "235376877"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44287109375
        },
        {
            "corpus_id": "256460933",
            "title": "Calibrating Student Models for Emotion-related Tasks",
            "text": "Knowledge Distillation: Knowledge distillation (KD) is an efficient method broadly used for transferring knowledge from a teacher network to a student network. In the knowledge distillation setting, a student model is trained to obtain the knowledge of a deeper or more complex teacher model and can therefore estimate the capacity of the powerful teacher model by incorporating the extra knowledge. KD was first introduced as an approach to compress large networks into smaller networks (Ba and Caruana, 2014;Bucilu\u01ce et al., 2006) for computational efficiency. The advances of KD, however, go beyond model compression. Zhang and Sabuncu (2020) empirically explained the reason behind the enhanced performance of self-distillation and proposed a framework that employs instance-specific regularization for teacher predictions. Phuong and Lampert (2019) examined the impact of distillation on student models by analyzing linear and deep linear classifiers. Unlike previous works, we are interested in analyzing the impact of knowledge distillation on the calibration of the models. Thus, we examine the calibration of large-scale pre-trained models through knowledge distillation. We further analyze the impact of dataset shift on calibration for all these settings. We evaluate the predictive uncertainty on both in-domain and out-of-domain test sets from known and unknown distributions on emotion-related datasets. \n\nMixup: Mixup (Zhang et al., 2018) was first proposed to improve the generalization of deep neural networks in computer vision. Since then, many studies have explored mixup in natural language processing tasks (Guo et al., 2019;Guo, 2020;Chen et al., 2020;Yin et al., 2021;Kong et al., 2020;Liang et al., 2021). Liang et al. (2021) proposed a data-agnostic distillation framework that leverages mixup to confer the student model with better generalization ability. Kong et al. (2020) examined BERT calibration using mixup by generating augmented samples based on a cosine distance of extracted features.",
            "score": 0.566334965145884,
            "section_title": "Related Work",
            "char_start_offset": 7664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 488,
                    "end": 510,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 510,
                    "end": 531,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1432,
                    "end": 1452,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 1646,
                    "end": 1656,
                    "matchedPaperCorpusId": "212928508"
                },
                {
                    "start": 1656,
                    "end": 1674,
                    "matchedPaperCorpusId": "216553182"
                },
                {
                    "start": 1674,
                    "end": 1691,
                    "matchedPaperCorpusId": "236477688"
                },
                {
                    "start": 1709,
                    "end": 1728,
                    "matchedPaperCorpusId": "226226888"
                },
                {
                    "start": 1730,
                    "end": 1749,
                    "matchedPaperCorpusId": "226226888"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "272827138",
            "title": "On Importance of Pruning and Distillation for Efficient Low Resource NLP",
            "text": "Knowledge distillation [4], [5] is a compression technique, where a larger model (teacher) is used to train a smaller model (student). The student model tries to replicate the behavior of the teacher model. The goal is to distill the knowledge of the teacher into the student, allowing the student to achieve performance similar to the teacher while being more computationally efficient. \n\n[20] explores using knowledge distillation to improve performance in machine translation for low-resource languages. Instead of a single teacher, the authors propose using an ensemble of multilingual models as teachers. This ensemble approach potentially captures a wider range of knowledge beneficial for the student model. Their research finds that the student model trained with knowledge distillation can even outperform the teacher models and the original ensemble. \n\nIn the last 3 experiments, we apply Block Movement Pruning followed by Knowledge Distillation. In this case, the teacher model is our base model, marathi-topic-all-doc-v2 and the student model is the pruned model. [14] is a technique that aims to optimize the computational efficiency of training models by utilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point precision (fp32 or float32) to represent and process variables. By reducing the precision of certain variables to lower numerical formats like 16-bit floating point (fp16 or float16), we can speed up the computations.",
            "score": 0.5659958639621921,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 13662,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 387
                },
                {
                    "start": 390,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1507
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "272367233",
            "title": "Efficient Point Cloud Classification via Offline Distillation Framework and Negative-Weight Self-Distillation Technique",
            "text": "This section investigates the impact of distillation weights on the classification performance of the student model. The experimental design follows conventional settings in knowledge distillation, where the teacher-student distillation weight \u03b1 is set to a high value without self-distillation. This ensures that the weight is comparable to the classification loss weight, providing strong guidance for the student model. Once the teacher-student distillation parameters are established, the model further introduces a self-distillation with a negative weight \u03b2 of low magnitude to prevent interference with the training of the student model. This section explores the effects of varying weights on model performance through a series of experiments. The results are presented in Tab. 5 and 6, corresponding to the accuracy of the student model under different \u03b1 and \u03b2 values, respectively. As shown in Tab. 5, without self-distillation, the accuracy varies with different \u03b1 values, indicating that the strength of guidance has a direct impact on the knowledge transfer from the teacher model and the learning of the student model. The table demonstrates that an appropriately high \u03b1 is crucial for effective knowledge distillation and student model guidance. \n\nTab. 6 illustrates that when the teacher model provides strong guidance, the model's performance is generally better with a negative self-distillation weight \u03b2 than with a positive \u03b2. This confirms that negative-weight selfdistillation serves a regularizing role, enhancing the model's ability to generalize from training data and preventing rapid convergence to suboptimal solutions under the teacher model's guidance.",
            "score": 0.5651215255031641,
            "section_title": "Effects of Distillation Weights on Model Performance:",
            "char_start_offset": 18652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1681
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "252110723",
            "title": "On the effectiveness of compact biomedical transformers",
            "text": "Knowledge distillation (Hinton et al., 2015) is the process of transferring knowledge from a larger model called \"teacher\" to a smaller one called \"student\" using the larger model's outputs as soft labels. Distillation can be done in a task-specific way where the pre-trained model is first fine-tuned on a task and then the student attempts to imitate the teacher network. This is an effective method, however, fine-tuning of a pre-trained model can be computationally expensive. Task-agnostic distillation, on the other hand, allows the student to mimic the teacher by looking at its masked language predictions or intermediate representations. The student can subsequently be directly fine-tuned on the final task (Wang et al., 2020;Yao et al., 2021). \n\nDistilBERT is a well-known example of a compressed model that uses knowledge distillation to transfer the knowledge within the BERT base model to a much smaller student network which is about 40% smaller and 60% faster. It uses a triple loss which is a linear combination of language modeling, distillation and cosine-distance losses.",
            "score": 0.5646684756874176,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 13990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1091
                }
            ],
            "ref_mentions": [
                {
                    "start": 717,
                    "end": 736,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 736,
                    "end": 753,
                    "matchedPaperCorpusId": "235652233"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85107421875
        },
        {
            "corpus_id": "268063805",
            "title": "T3DNet: Compressing Point Cloud Models for Lightweight 3-D Recognition",
            "text": "We have explored different kinds of distillation and combination methods. \n\nFeature level distillation: To improve knowledge distillation (KD) at the feature level, the output of a teacher's hidden layer (hint [27]) is used. This method is different from prediction layer distillation, as it uses Mean Squared Error (MSE) loss to compute the difference between the teacher and student. To match the size of the student's output with the teacher's output size, a linear mapping layer is added. \n\nThe formulation for this method is as follows: \n\nwhere u t is the teacher's feature output, u s is the student's feature output, r(\u2022) is the linear mapping operation. The parameters of r(\u2022) are also updated during training. However, the results of our experiments (shown in Table V) indicate that feature-level distillation does not improve the convergence of the model. This is because the hint-based learning method is more suitable for scenarios where the student network is deep and thin, and requires more supervision from the middle layers. The PointNet++ model used in our work does not have this property. This distillation method still requires the original model to work as the teacher. \n\nMutual learning: Deep mutual learning [28] is an online distillation method. Different from traditional one-way distillation from a static teacher to a student, mutual learning allows two students to learn from each other collaboratively and train the peer networks in one learning process. The idea of two networks' mutual learning can be applied to the training process of tiny network augmentation, i.e., training the tiny network and augmented network at the same time. Although the purpose of tiny network augmentation is only to obtain a well-trained tiny model, the training process will also make augmented networks converge at last because the model optimizes the whole parameters of the largest model. So we can introduce distillation loss for both the tiny model and augmented model to let them be each other's teachers in the training stage. The formulation of this process can be shown as: \n\nwhere L KD 1 is the loss for the tiny model, while L KD 2 is the loss for the augmented model.",
            "score": 0.5642536645670418,
            "section_title": "4) Exploratory Experiments and Ablation Studies:",
            "char_start_offset": 34026,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 76,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2096
                },
                {
                    "start": 2099,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 1232,
                    "end": 1236,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "259378480",
            "title": "Review of Recent Distillation Studies",
            "text": "A new temperature scaling method, the asymmetric temperature scale (ATS), is proposed here to make the distribution of error classes during distillation more discriminable. the ATS applies different temperatures to logits in correct and incorrect classes, allowing for more flexible and differentiated distillation instruction during instruction. This approach is particularly beneficial when teachers are overconfident, as it reduces the size of correct category logits, increases the diversity of error category logits, and ultimately increases the variance and accuracy of the derivations. Experiments using different datasets and network architectures are described here, including ResNet, WideResNet, ResNeXt, VGG, ShuffleNet, AlexNet, MobileNet, and DSCNN. the training setup mainly follows previous studies with slight modifications based on the dataset. The learning rate decayed every 30 cycles, with a batch size of 128 for CIFAR and 64 for the other datasets. The researchers investigated the phenomenon that having more accurate teacher models does not necessarily lead to better student model learning in knowledge distillation. They identified three factors of successful teachers:correct instruction, smooth regularization, and class differentiation. They propose a solution called asymmetric temperature scaling (ATS) to enhance the derived variance of large teachers to make their distillation labels more discriminatory when teaching. Experiments have shown that the method is effective. So far, researchers are studying why large teachers have difficulty teaching knowledge distillation (KD) effectively. They hope their research provides a new perspective on the KD field, but they do not anticipate any negative social impact from their work.",
            "score": 0.5619100305241642,
            "section_title": "Symmetric temperature scaling",
            "char_start_offset": 17771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1763
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7353515625
        },
        {
            "corpus_id": "266335424",
            "title": "Weight subcloning: direct initialization of transformers using larger pretrained ones",
            "text": "Knowledge distillation is a technique used to create a smaller student model from a larger teacher model, with the goal of reducing model size and computational complexity (Gou et al., 2021;Lin et al., 2022;Park et al., 2022). In this approach, the student model is trained to imitate the teacher model, either at the output layer or by replicating intermediate hidden features. This process can be time-consuming because it involves the large teacher model during training. In a closely related work to our paper, (Lin et al., 2020) propose a method called \"weight distillation\". \n\nIn weight distillation, in addition to logit alignment, the parameters of the teacher are mapped to the student network using a transformation, which is learned in the presence of both teacher and student networks during training. \n\nIn the context of knowledge and weight distillation, we can view the teacher and student models as the parent and des-tination architectures, respectively, in our scenario. What distinguishes our approach from traditional distillation techniques is our focus on achieving faster training. Instead of actively involving the teacher model in the training process, we directly transfer the weights from the parent model to the destination model. In comparison to weight distillation, we demonstrate that the function used to map parent to destination parameters can be as straightforward as cloning with minor modifications, eliminating the need for additional learning during this parameter transfer. As a direct consequence of this approach, the training loop remains unchanged, making it significantly more adaptable for a wide range of training tasks.",
            "score": 0.5616429538856667,
            "section_title": "Distillation",
            "char_start_offset": 3815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 813
                },
                {
                    "start": 816,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1668
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 190,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 190,
                    "end": 207,
                    "matchedPaperCorpusId": "248986384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81494140625
        },
        {
            "corpus_id": "251107546",
            "title": "Self-Supervision and Self-Distillation with Multilayer Feature Contrast for Supervision Collapse in Few-Shot Remote Sensing Scene Classification",
            "text": "Knowledge distillation [50] can transfer the knowledge learned in one network to another. The structures of the two networks can be the same or different. Knowledge distillation usually trains a selected network as a teacher first and then uses the output of the teacher network and labels to train a network as a student, so it is also called \"teacherstudent learning\". Knowledge distillation can reduce the volume of the network and maintain a performance level close to that of the original network. Two networks with different performances can also be combined through knowledge distillation. Han [51] showed that for a given neural network, reducing the network weight by more than 85% through knowledge distillation would not significantly damage the performance of the neural network. According to the object of distillation, knowledge distillation has many forms. KD [50] realized knowledge transfer by minimizing the difference between teachers' and students' classified output labels. Fitness [52] extracted features from the middle layer of the teacher network to guide the student to learn useful knowledge, which can make the student network deeper and narrower than the teacher network. In [53], knowledge was transferred to students in the form of an attention map. IRG [54] constrained the similarity of multiple samples and proposed to constrain the instance relationship graph of students and teachers.",
            "score": 0.560944742032325,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 14701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1420
                }
            ],
            "ref_mentions": [
                {
                    "start": 1204,
                    "end": 1208,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1285,
                    "end": 1289,
                    "matchedPaperCorpusId": "198185886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86279296875
        },
        {
            "corpus_id": "272987178",
            "title": "Classroom-Inspired Multi-Mentor Distillation with Adaptive Learning Strategies",
            "text": "Knowledge distillation (KD) [1] is a widely adopted technique for compressing deep neural networks, where a smaller student model learns from a larger teacher model by minimizing the distance between their output probability distributions, or soft labels. Traditional KD methods primarily focus on logit-based distillation, where the student learns directly from the teacher's output logits. Notable methods include PKT [8], which employs probabilistic knowledge transfer, FT [9], which transfers factorized feature representations, and AB [10], which leverages activation boundaries formed by hidden neurons. \n\nFeature-based distillation methods transfer knowledge by aligning intermediate representations between the teacher and student. FitNets [3] introduced this approach using intermediate feature maps for training. Later methods like AT [2], VID [11], and CRD [12] enhance knowledge transfer by matching attention maps, utilizing variational information distillation, and employing contrastive learning, respectively. \n\nRelation-based methods focus on preserving the structural relationships within the teacher's feature maps. RKD [13] maintains data point structures through relational knowledge distillation, while SP [14] and SRRL [15] optimize for similarity-preserving objectives. DIST [16] addresses large capacity gaps by applying a correlation-based loss to maintain both inter-class and intra-class relationships, enhancing distillation efficiency. \n\nRecent approaches have explored more specialized distillation techniques. WSLD [17] introduces weighted soft labels to balance bias-variance trade-offs, while One-to-All Spatial Matching KD [18] focuses on spatial matching techniques. OFA [7] optimizes feature-based KD by projecting features onto the logit space, significantly improving performance for heterogeneous models. To enhance distillation effectiveness, several methods have incorporated adaptive strategies. CTKD [19] dynamically learns the temperature during training to gradually increase learning difficulty, and DTKD [20] employs real-time temperature scaling to improve knowledge transfer efficiency.",
            "score": 0.5602742233124574,
            "section_title": "Knowledge Distillation Approaches",
            "char_start_offset": 4393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1465
                },
                {
                    "start": 1468,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2136
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 31,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 420,
                    "end": 423,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 476,
                    "end": 479,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 540,
                    "end": 544,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 748,
                    "end": 751,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 854,
                    "end": 858,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 868,
                    "end": 872,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1139,
                    "end": 1143,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1228,
                    "end": 1232,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1242,
                    "end": 1246,
                    "matchedPaperCorpusId": "235613564"
                },
                {
                    "start": 1299,
                    "end": 1303,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 1547,
                    "end": 1551,
                    "matchedPaperCorpusId": "231740588"
                },
                {
                    "start": 1658,
                    "end": 1662,
                    "matchedPaperCorpusId": "248986384"
                },
                {
                    "start": 1707,
                    "end": 1710,
                    "matchedPaperCorpusId": "265871679"
                },
                {
                    "start": 1944,
                    "end": 1948,
                    "matchedPaperCorpusId": "254069919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "253116921",
            "title": "Semi-Supervised Learning Based on Reference Model for Low-resource TTS",
            "text": "Knowledge distillation (KD) [34] can make student modle get the information from the teacher model. Its success is usually attributed to the privileged information of similarity between the class distribution of the teacher model and the student model. It was first proposed by Hinton et al. [34] transfer knowledge from large teacher networks to smaller student networks. It works by training students to predict target classification labels and imitate teachers' class probabilities because these features contain additional information about how teachers generalize [34].\n\nLiu et al. [35] tried the method of the teacher-student model for resolving the problem of exposure bias. There is an existing problem of exposure bias of autoregressive, due to the unmatched training and inference phase. This problem cloud leads to an unpredictable error for the model during the inference and accumulates the error frame by frame along the time axis.",
            "score": 0.5597979108989537,
            "section_title": "A. Knowledge Distillation",
            "char_start_offset": 4599,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 587,
                    "end": 591,
                    "matchedPaperCorpusId": "207847470"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78857421875
        },
        {
            "corpus_id": "250096410",
            "title": "An Industrial-Grade Solution for Crop Disease Image Detection Tasks",
            "text": "After model pruning, the accuracy of the model generally decreases. Even if the pruned model is fine-tuned again, the accuracy may still have a large gap with the model before pruning. Therefore, this article can solve this problem by minimizing the accuracy loss by Knowledge Distillation (KD). Knowledge distillation uses transfer learning to supplement specific parameters missing in the small model to achieve the recognition accuracy of the large model as much as possible. Knowledge distillation can be regarded as a model compression method, where the large model is the teacher and the miniature model is the student. \n\nUsually, the traditional training process finds the excellent likelihood for the ground truth under Hard Label. In contrast, the training process of KD uses the category probabilities of the teacher model as soft targets (Labels With Probabilities) to guide the training of the student model. The knowledge describing the similarity of different categories of information can be transferred from these soft targets (Hinton et al., 2015) to improve the performance of the student model. \n\nFigure 4 shows the primary technical process of knowledge distillation. The teacher model is the original model with high training accuracy in the knowledge extraction process. The pruned original model is the student model, with a small number of parameters and a relatively simple model structure. The teacher model uses a series of hyperparameters to converge to the optimal state according to the established principles. Then, the same hyperparameters of the teacher model are used to train the student model for knowledge distillation. The distillation loss is corrected by coefficients \u03b2 for the distillation loss of the teacher model and the student model where the Hard Label (Ground Truth) can effectively reduce the possibility of errors being propagated to the student model. Measuring the similarity of student and teacher models can be expressed in Equation ( 6), L R is a function that can measure the similarity, expressed explicitly in softmax. In general, when the entropy value of the probability distribution output from softmax \n\nis relatively small, the value of negative labels is very close to 0, which contributes very little to the loss function, which leads to a reduction in the attention of the student model for negative labels during distillation, which is addressed by the temperature coefficient T in Equation ( 7).",
            "score": 0.5592018877670053,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 18368,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1113
                },
                {
                    "start": 1116,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2163
                },
                {
                    "start": 2166,
                    "end": 2463
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "268060109",
            "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems",
            "text": "Knowledge distillation is a technique where a compact model (student) is trained to mimic the behavior of a larger, more complex model (teacher). This allows the student model to inherit the teacher's capabilities without incurring the computational overhead. The schematic can be referred to Fig. 2: \n\nRationale behind distillation Concept. In many engineering scenarios, deploying gargantuan models is infeasible due to resource constraints. However, these large models often possess superior performance. Knowledge distillation bridges this gap, enabling smaller models to emulate the performance of their larger counterparts.",
            "score": 0.5580545878835483,
            "section_title": "Knowledge distillation",
            "char_start_offset": 21974,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 629
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "267061245",
            "title": "Knowledge Fusion of Large Language Models",
            "text": "In contrast, the approach proposed in this paper supports the fusion of multiple LLMs with diverse architectures by explicitly transferring their knowledge and capabilities to a target LLM. \n\nKnowledge Distillation Knowledge distillation (Hinton et al., 2015), initially proposed for model compression, involves training a student model under the guidance of one or more teacher models. In the NLP community, knowledge distillation has been widely applied to text classification tasks. These applications include training the student model to replicate the teacher's output distribution (Sanh et al., 2019;Turc et al., 2019), as well as features (Sun et al., 2019;Jiao et al., 2020) and relations (Wang et al., 2020) derived from intermediate layers of the teacher model. In the realm of text generation, the conventional approach focuses on minimizing the KL divergence between the student and teacher generation distributions. This is achieved by using the teacher's probability distributions at each time step as supervision (Khanuja et al., 2021;Gu et al., 2023;Agarwal et al., 2023) or by directly training on the teacher's generated texts (Peng et al., 2023;Xu et al., 2023). \n\nWhile our method shares a framework similar to multi-teacher knowledge distillation, there are two significant distinctions. First, in traditional knowledge distillation, the student models are typically constrained to be smaller in size than the teachers. In our scenario, however, there are no limitations on the size of the target model. Second, traditional knowledge distillation often results in the student models lagging behind the teachers in performance after distillation. In contrast, we anticipate that after the fusion, the target model will surpass any of the source models in performance.",
            "score": 0.5576658510540632,
            "section_title": "RELATED WORK",
            "char_start_offset": 7492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 192,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1787
                }
            ],
            "ref_mentions": [
                {
                    "start": 646,
                    "end": 664,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 664,
                    "end": 682,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1028,
                    "end": 1050,
                    "matchedPaperCorpusId": "236477925"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "268247468",
            "title": "Logit Standardization in Knowledge Distillation",
            "text": "The phenomenon implies that the transfer of knowledge from a large teacher is not always as smooth as a small one. They explain the observation with the existence of a capacity gap between cumbersome teachers and lightweight students. We interpret the issue as the difficulty of students in mimicking logits of comparable range and variance as teachers. As shown in the bivariate histogram of Fig. 5a, the bigger teachers like ResNet50 and VGG13 generates more condensed logits of mean closer to zero and smaller standard deviation. The tendency shows that smaller models intrinsically predicts the outputs of larger bias from zero mean and larger variance. Therefore, when students are small, the same tendency remains and they are difficult to produce as compact logits as large teachers. \n\nWe alleviate the problem by our pre-process. As shown in Tab. 5, our pre-process consistently improves the distillation performance for various teachers of different sizes and capacities. We also show the mimicking goodness of students by a bivariate histogram plot in Fig. 5b. The student  distilled by vanilla KD predicts logits apparently deviated from teachers' in terms of logit mean and standard deviation. In contrast, our pre-process enables the student to perfectly matching the teacher in terms of standardized logit mean and standard deviation (see the zoomed-in region at bottom right corner). \n\nMore experiments of distilling vision Transformers [6,20,21,38,42] are attached in the supplements.",
            "score": 0.5576591588020567,
            "section_title": "Extensions",
            "char_start_offset": 23080,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 790
                },
                {
                    "start": 793,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1398
                },
                {
                    "start": 1401,
                    "end": 1500
                }
            ],
            "ref_mentions": [
                {
                    "start": 1452,
                    "end": 1455,
                    "matchedPaperCorpusId": "248406101"
                },
                {
                    "start": 1455,
                    "end": 1458,
                    "matchedPaperCorpusId": "250699338"
                },
                {
                    "start": 1458,
                    "end": 1461,
                    "matchedPaperCorpusId": "267005248"
                },
                {
                    "start": 1461,
                    "end": 1464,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 1464,
                    "end": 1467,
                    "matchedPaperCorpusId": "250920355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65869140625
        },
        {
            "corpus_id": "268820185",
            "title": "A Comprehensive Review of Knowledge Distillation in Computer Vision",
            "text": "The Knowledge Distillation (KD) process involves transferring knowledge from a teacher model to a student model, and is divided mainly into two categories Homogenous Architecture KD and Cross-Architecture KD.KD based on same architecture includes methods such as Attention-based KD, which guides student focus using teacher attention, and Channel-wise KD, which distills knowledge from specific channels.Additionally, Relational KD teaches feature relationships while Progressive KD progressively increases knowledge complexity, and Teacher-guided Self-Distillation guides self-distillation with predictions from the teacher.A cross-architecture KD method such as AdaMix, which dynamically selects the best teacher for each sample, and CAKD, which projects teacher features into a space that is compatible with students, are examples of cross-architecture KD methods.Multi-Teacher Knowledge Distillation incorporates predictions from multiple teachers, Feature-wise Attention Knowledge Distillation filters knowledge by using attention, and Knowledge Ensemble Distillation distills knowledge from student ensembles.\n\nBased on advice and insights from the teacher models, these strategies seek to develop more effective student models' while preserving or enhancing performance [80][81][82][83][84][85][86][87][88][89][90].\n\nVarious computer vision tasks have been improved by the use of knowledge distillation, where small student models are trained to mimic large teacher models.Some of the important applications of KD in Computer Vision are discussed in detail as: With the use of a vast quantity of high-resolution training data, a teacher model was pre-trained before being put through its paces.The second phase is to apply the information gained after the teacher model has been trained to train a student model without the need for further data.The authors provide a novel loss function that accounts for the output variation between both teacher and student models as well as the reconstruction error.As a result of this loss function, the student model can learn from the output of the teacher model while avoiding overfitting.The results of the experiments reveal that the suggested strategy outperforms existing methods that do not require additional data.\n\nFor various benchmark datasets, including Set5, Set14, and B100, the approach produces cutting-edge results.",
            "score": 0.5572320000484767,
            "section_title": "Application of Knowledge Distillation in Computer Vision",
            "char_start_offset": 31399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 208,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 625
                },
                {
                    "start": 625,
                    "end": 867
                },
                {
                    "start": 867,
                    "end": 1115
                },
                {
                    "start": 1117,
                    "end": 1322
                },
                {
                    "start": 1324,
                    "end": 1480
                },
                {
                    "start": 1480,
                    "end": 1701
                },
                {
                    "start": 1701,
                    "end": 1853
                },
                {
                    "start": 1853,
                    "end": 2010
                },
                {
                    "start": 2010,
                    "end": 2137
                },
                {
                    "start": 2137,
                    "end": 2268
                },
                {
                    "start": 2270,
                    "end": 2378
                }
            ],
            "ref_mentions": [
                {
                    "start": 1313,
                    "end": 1317,
                    "matchedPaperCorpusId": "52290108"
                },
                {
                    "start": 1317,
                    "end": 1321,
                    "matchedPaperCorpusId": "236159551"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "272987178",
            "title": "Classroom-Inspired Multi-Mentor Distillation with Adaptive Learning Strategies",
            "text": "Multi-teacher distillation methods aim to further enhance student performance by leveraging an ensemble of mentors [4]. \n\nOnline knowledge distillation has been particularly successful in this context. Deep Mutual Learning (DML) [21] introduces a framework where multiple peer models learn from each other simultaneously during training, fostering collaborative learning among smaller networks and outperforming traditional oneway (offline) distillation. Other online methods include ONE [22], OKDDip [23], and FFM [24], which often outperform offline methods. Online distillation has also been extended to pose estimation tasks [25]. SHAKE [26] proposed using proxy teachers with shadow heads to use the benefits of online distillation in offline settings. \n\nTo address the capacity gap in multi-teacher setups, Teacher-Assistant KD (TAKD) [5] employs intermediate-sized teacher assistants (TAs) to bridge the gap between the largest teacher and the student. However, sequential distillation through TAs can result in an \"error avalanche\", where errors propagate at each step, reducing final performance. Adaptive Ensemble Knowledge Distillation (AEKD) [27] mitigates this issue by using an adaptive dynamic weighting strategy to reduce error propagation in the gradient space. Densely Guided KD (DGKD) [6] further improves upon these methods by guiding each TA with both larger TAs and the main teacher, enabling a more gradual and effective transfer of knowledge. Additionally, DGKD introduces a strategy of randomly dropping mentors during training to expose the student to diverse learning sources, enhancing overall learning robustness. \n\nWhile existing multi-teacher methods offer various mechanisms for knowledge distillation, they still grapple with challenges such as managing the capacity gap, mitigating error accumulation, and adapting to dynamic mentor-student relationships.",
            "score": 0.5571835517892016,
            "section_title": "Multi-Teacher Knowledge Distillation",
            "char_start_offset": 6570,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 122,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1642
                },
                {
                    "start": 1645,
                    "end": 1889
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 118,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 229,
                    "end": 233,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 488,
                    "end": 492,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 501,
                    "end": 505,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 515,
                    "end": 519,
                    "matchedPaperCorpusId": "222124879"
                },
                {
                    "start": 629,
                    "end": 633,
                    "matchedPaperCorpusId": "236912875"
                },
                {
                    "start": 641,
                    "end": 645,
                    "matchedPaperCorpusId": "258509275"
                },
                {
                    "start": 841,
                    "end": 844,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1154,
                    "end": 1158,
                    "matchedPaperCorpusId": "227276362"
                },
                {
                    "start": 1304,
                    "end": 1307,
                    "matchedPaperCorpusId": "221802641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74462890625
        },
        {
            "corpus_id": "264490946",
            "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
            "text": "Contrastive distillation for knowledge transfer between arbitrary models To understand how well contrastive distillation techniques are suited for knowledge transfer between arbitrary  pretrained models, we measure the average transfer success rate for both CD and CRD distillation transfer ( \u00a7A.2), with results shown in Fig. 6. We leverage the same experimental setup on 10% ImageNet as for the other transfer approaches (see \u00a75.1). The experimental results clearly show the contrastive distillation approaches to be unable to improve the student model for most teacher models. On closer examination of the results we can see that the contrastive distillation approaches result in similar levels of knowledge transfer from the teacher to the student, but appear to also incur much stronger overall overwriting, causing the student to lose large portions of its previous knowledge. \n\nWhile very suitable for distillation to untrained students, this behaviour is unfortunately not well suited for knowledge transfer between already trained expert models.",
            "score": 0.5571354709968668,
            "section_title": "XE-KL Distillation",
            "char_start_offset": 42672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 1054
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7099609375
        },
        {
            "corpus_id": "249209742",
            "title": "What Knowledge Gets Distilled in Knowledge Distillation?",
            "text": "The need for such answers has become particularly relevant because there have been studies which present some surprising findings about the distillation process. Cho & Hariharan (2019) showed that performing knowledge distillation with a bigger teacher does not necessarily improve the student's performance over that with a smaller teacher, and thus raised questions about the effectiveness of the distillation procedure in such cases. Stanton et al. (2021) showed that the agreement between Preprint the teacher and distilled student's predictions on test images is not that different to the agreement of those between the teacher and an independently trained student, raising further doubts about how knowledge distillation works, if it works at all. \n\nOur goal is hence to shed some light on the 'dark knowledge' (Hinton et al., 2014a) that gets distilled in knowledge distillation. In this work, we present a comprehensive study tackling the above questions. We use the task of image classification as a case study, and analyze three popular state-of-the-art knowledge distillation methods (Hinton et al., 2014b;Romero et al., 2015;Tian et al., 2020). Many of our findings are quite surprising. For example, by simply mimicking the teacher's output on ImageNet images using the method of Hinton et al. (2014b), the student can inherit many implicit properties of the teacher. It can gain the adversarial vulnerability that the teacher has. If the teacher is invariant to color, the student also improves its invariance to color, even without explicitly being trained to be that way. While the specific transferred properties and their extent differ across different distillation methods and teacher-student architectures, our study reveals that there is much more happening in the background of the distillation process than what meets the eye: distillation makes the student extract similar intermediate representations as the teacher, something we study quantitatively. We show that this has practical implications, a good one and a bad one. The good is that a student can inherit many useful properties of a teacher even without being exposed to it explicitly, which saves computation. The bad is that an otherwise fair student can inherit biases from an unfair teacher.",
            "score": 0.5570921142335242,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1874,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2193
                },
                {
                    "start": 2194,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 184,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1095,
                    "end": 1117,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1137,
                    "end": 1155,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1293,
                    "end": 1314,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "274023578",
            "title": "Dual-Head Knowledge Distillation: Enhancing Logits Utilization with an Auxiliary Head",
            "text": "2.1 KNOWLEDGE DISTILLATION Knowledge distillation (KD) (Hinton et al., 2015) aims to transfer knowledge from a large teacher network to a small student network. Existing works can be roughly divided into two groups: featurebased methods and logit-based methods. \n\nFeature-based methods focus on distilling knowledge from intermediate feature layers. FitNet (Romero et al., 2015) is the first approach to distill knowledge from intermediate features by measuring the distance between feature maps. RKD (Park et al., 2019) utilizes the relations among instances to guide the training process of the student model. CRD (Tian et al., 2019) incorporates contrastive learning into knowledge distillation. OFD (Heo et al., 2019) contains a new distance function to distill significant information between the teacher and student using marginal ReLU. \n\nReviewKD (Chen et al., 2021) proposes a review mechanism that uses multiple layers in the teacher to supervise one layer in the student. Other papers (Passalis & Tefas, 2018;Kim et al., 2018;Koratana et al., 2019;Li, 2022;Liu et al., 2023;Wang et al., 2023;Roy Miles & Deng, 2024;Miles & Mikolajczyk, 2024) enforce various criteria based on features. Most feature-based methods can attain superior performance, yet involving considerably high computational and storage costs. \n\nLogit-based methods mainly concentrate on distilling knowledge from logits and softmax scores after logits. DML (Zhang et al., 2018b) introduces a mutual learning method to train both teachers and students simultaneously. DKD (Zhao et al., 2022) proposes a novel logit-based method to reformulate the classical KD loss into two parts and achieves state-of-the-art performance by adjusting weights for these two parts. DIST (Huang et al., 2022) relaxes the exact matching in previous KL divergence loss with a correlation-based loss and performs better when the discrepancy between the teacher and the student is large.",
            "score": 0.5561697848151186,
            "section_title": "RELATED WORK",
            "char_start_offset": 4677,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 501,
                    "end": 520,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 616,
                    "end": 635,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 703,
                    "end": 721,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 854,
                    "end": 873,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 995,
                    "end": 1019,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 1036,
                    "end": 1058,
                    "matchedPaperCorpusId": "172133986"
                },
                {
                    "start": 1102,
                    "end": 1125,
                    "matchedPaperCorpusId": "268358422"
                },
                {
                    "start": 1125,
                    "end": 1151,
                    "matchedPaperCorpusId": "257632008"
                },
                {
                    "start": 1435,
                    "end": 1456,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75732421875
        },
        {
            "corpus_id": "273186993",
            "title": "Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher",
            "text": "In contrast, a weaker teacher, together with a smaller performance gap from the student, has been shown to be a better choice (Son et al., 2021;Yang et al., 2019b;Mirzadeh et al., 2020;Gao et al., 2020;Wang et al., 2022). In fact, it can be theoretically proved by (Wang et al., 2022) that weak models \"have higher mutual information regarding the input\" compared to stronger teacher models, which can enhance knowledge distillation. Interestingly, this phenomenon can be intuitively understood, just like it is hard for a child to learn advanced mathematics directly from a senior professor. Instead, it would be better to teach him/her to count or learn elementary mathematics first. In other words, for distillation, a student model should learn from a teacher that has an appropriate knowledge/performance gap over it. Nevertheless, even with a relatively weak teacher that has a small performance gap, a fixed model that is commonly used would eventually become useless since the student will gradually improve and surpass the teacher at some time. Thus, how to adaptively control the performance gap within a reasonable range throughout the whole distillation process is a critical problem. \n\nTo address this, we seek to explicitly control the performance gap to boost knowledge transfer. However, it is non-trivial to determine what performance gap should be regarded as \"too-large\". Although the gap can be measured by some criteria, e.g., accuracy for classification, the threshold to distinguish a too-large gap is still ambiguous. To solve this, we cast the problem of determining a threshold into measuring the gap in terms of model size between student and teacher. We propose to introduce a learnable dynamic teacher (DT) model besides the pre-trained static teacher model, training it from scratch together with the student. Since DT is a larger model and often converges faster than the student, we are able to keep a promising performance gap between DT and student during training. In addition, we hope to build a stronger connection between DT and student to transfer knowledge in a more explicit way, in order to further enhance the performance. To achieve this, we develop a hard strategy for distillation that enforces DT and the student to share their parameters and encourage parameter inheritance.",
            "score": 0.5560220538042939,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1438,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1196
                },
                {
                    "start": 1199,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2322
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 144,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 144,
                    "end": 163,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 163,
                    "end": 185,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 202,
                    "end": 220,
                    "matchedPaperCorpusId": "252846591"
                },
                {
                    "start": 265,
                    "end": 283,
                    "matchedPaperCorpusId": "252846591"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.744140625
        },
        {
            "corpus_id": "247446679",
            "title": "CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification",
            "text": "We first explore the optimal teacher model for each student model. Specifically, we use the KD setting mentioned in Section 2 and test a set of (teacher, student) model pairs. Specifically, we consider the following models: EfficientNet B0, B2, B6 [21] and DenseNet-121 [22] (CNN models); and AST Tiny, Small, Base [11] (Transformer models). The reason for selecting these models are: 1) EfficientNet-B2 and AST-Base are the best CNN and Transformer models, and we want to see if KD can further improve their performance; 2) AST-Base is larger than EfficientNet-B2 in terms of the number of parameters. We include models with various sizes to clearly identify whether the improvement of KD is due to different model sizes or different model architecture; 3) We also include one different CNN architecture (DenseNet) to check if the cross-model KD framework generalizes to different CNN architectures. \n\nFor each (teacher, student) pair, we select the temperature \u03c4 \u2208 {1.0, 2.5} based on the performance on the validation set, all other knowledge distillation settings are identical for all (teacher, student) pairs. The mean average precisions (mAPs) of the student model of various (teacher,student) pairs on the FSD50K evaluation set are shown in Table 3. Key observations are as follows: \n\n1. CNNs and ASTs are good teachers for each other. While knowledge distillation improves the student model performance in almost all settings (denoted by \u2020 in Table 3), we find that models always prefer a different teacher. Specifically, all CNN models (including DenseNet) achieve the best performance when ASTs are the teacher; all AST models achieve the best performance when CNNs are the teacher. Note that even though DenseNet and EfficientNet models also have different architectures, knowledge distillation between these two models leads to limited improvement. This demonstrates that the inherent differences between CNNs and ASTs is crucial for the success of knowledge distillation. 2. For both directions, the student model matches or outperforms its teacher. Usually, in knowledge distillation, the student model gets closer to, but is still weaker than, its teacher model.",
            "score": 0.5553194282295588,
            "section_title": "Which model is a good teacher?",
            "char_start_offset": 14718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 252,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 270,
                    "end": 274,
                    "matchedPaperCorpusId": "9433631"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "254564348",
            "title": "Vertical Layering of Quantized Neural Networks for Heterogeneous Inference",
            "text": "Knowledge distillation transfers the knowledge learned by large models (i.e., teacher) to a compact one (i.e., student) via a weighted combination of original loss using ground-truth labels and distillation loss calculated with the soft output of a teacher model [47]. There is a broad consensus that knowledge distillation can help improve the performance of low-bit-width quantized networks by transferring the knowledge of a full precision network to quantized ones [48], [49], [50]. QKD [49] proposes a quantization-aware knowledge distillation scheme by co-training a low-bit-width student and a larger teacher model. SPEQ [50] introduces a novel self-training paradigm where the high-precision teacher and the low-bit-width student from the same full precision source model are trained collaboratively. In our training process, where different bit-width networks are enclosed, we design a self-knowledge transfer that utilizes a relatively higher precision network's output as soft labels for the cooperative training of the lower precision ones.",
            "score": 0.5550604263043402,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 14280,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1052
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "222066738"
                },
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "222066738"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "257279830",
            "title": "Distillation from Heterogeneous Models for Top-K Recommendation",
            "text": "We present our analysis showing the difficulty of our task that compresses the ensemble knowledge of heterogeneous teacher models. Further, we show that a clue that helps to ease the difficulty can be obtained from the teachers' intermediate training states. Here, we use MF with embedding size 6 as the student model, and all teacher models have embedding size 64. Similar results are also observed with other types of students. We train the student solely with distillation (Eq.1). Please refer to Sec.5 for the detailed setup.",
            "score": 0.5548225047493942,
            "section_title": "Study on Ranking Knowledge Distillation",
            "char_start_offset": 12679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 529
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70263671875
        },
        {
            "corpus_id": "264490946",
            "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
            "text": "While better than normal distillation, it is often too strong a constraint when the teachers have niche areas of expertise or are overall stronger. We thus propose to constrain distillation at the data level by partitioning it into two sets -one with samples where transfer from a teacher is desired, and one where we wish to retain the student behavior. This introduces significantly fewer constraints on the model weights to learn from arbitrary teacher context, while reducing forgetting by retaining initial performance on samples where the teacher has limited positive (even detrimental) impact. Moreover, our data partitioning can be achieved without any supervision. \n\nDoing so, we see significant increases in the success rate (non-zero gains of the student) for all teacher-student pairings -from 32.5% with normal distillation to 92.5% with data partitioning. Our data-level regularization is the only setting which allows for consistently positive transfer from weaker teachers, while retaining the transfer performance of normal distillation for much stronger teachers and even outperforming normal distillation for equiperformant ones. In addition, it allows for the transfer of specialized knowledge ( \u00a75.1) and requires no pairing-specific hyperparameters. Unlike ensembling methods (Lakshminarayanan et al., 2017;Gontijo-Lopes et al., 2022;Sinha et al., 2021;Dietterich, 2000), our approach maintains original inference costs and handles high performance differences. Finally, we study architectural properties and their impact on the transfer process ( \u00a75.1) beyond the transfer method, and look into scalability to knowledge transfer from multiple models, where we find that simple sequential transfer can perform favorably when leveraging our transfer method, achieving clear improvements over transfer from just the single best teacher model. Overall, our contributions can be summarized as: (1) We discover the consistent existence of complementary knowledge between arbitrary models pretrained on the same dataset -even if model families or performances differ. (2) We conduct extensive, exploratory studies to investigate the possibility of guaranteed model-and performance-independent transfer of the complementary knowledge without performance degradation.",
            "score": 0.5543328901235365,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2281
                }
            ],
            "ref_mentions": [
                {
                    "start": 1298,
                    "end": 1329,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 1375,
                    "end": 1392,
                    "matchedPaperCorpusId": "10765854"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.720703125
        },
        {
            "corpus_id": "259378480",
            "title": "Review of Recent Distillation Studies",
            "text": "Knowledge distillation [7] is a method of transferring the knowledge from a complex model, called the teacher, to a smaller and simpler model, called the student. In recent years, several variants of knowledge distillation have been proposed, including teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation. This literature review summarizes the recent developments in these variants of knowledge distillation and discusses their strengths and limitations. Knowledge distillation is a method of compressing a complex deep neural network (DNN) into a smaller and faster DNN while preserving its accuracy. The process of knowledge distillation involves training a smaller DNN, called the student, to imitate the predictions of a larger and more complex DNN, called the teacher. The student network is trained to produce similar results as the teacher network, but with fewer parameters and lower computational cost. Knowledge distillation has been widely used for model compression and acceleration, and has shown great promise in various applications [5,8,13,18], such as computer vision and natural language processing. In recent years, several variants of knowledge distillation [3,4,21] have been proposed and explored to improve the performance of knowledge distillation [6,9]. One of these variants is teaching assistant distillation, which introduces an intermediate model, called the teaching assistant, between the teacher and the student. \n\nThe teaching assistant is trained to mimic the behavior of the teacher, and the student is trained to imitate the outputs of the teaching assistant. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better capture the knowledge learned by the teacher. Another variant of knowledge distillation [10,11] is curriculum distillation, which designs the learning process to follow a curriculum, similar to human education. The curriculum is designed to present easy examples first and gradually increase the difficulty of the examples as the student improves. This approach has been shown to provide better performance than traditional knowledge distillation, especially for tasks that require a lot of prior knowledge. Mask distillation is a variant of knowledge distillation [12,20] that focuses on transferring the attention mechanism learned by the teacher to the student.",
            "score": 0.5539827561954486,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2273
                },
                {
                    "start": 2274,
                    "end": 2430
                }
            ],
            "ref_mentions": [
                {
                    "start": 1104,
                    "end": 1106,
                    "matchedPaperCorpusId": "235677396"
                },
                {
                    "start": 1328,
                    "end": 1330,
                    "matchedPaperCorpusId": "253270253"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "248503415",
            "title": "Spot-Adaptive Knowledge Distillation",
            "text": "In this section, we evaluate the proposed method under homogeneous distillation. To this end, we adopt various teacher-student combinations, where the teacher models and the student models are in the samestyle architectures, including C-  layer for L KL , and some intermediate layers for L KD . Note that all competitors are combined with KD [1] to enhance their performance in the standard distillation scheme, and thus the softmax layer is always a candidate distillation spot in the proposed adaptive scheme. However, for intermediate layers, different distillers conduct knowledge distillation at different number of layers (one-spot distillation or multi-spot distillation). The proposed adaptive distillation strategy only determines where or not to conduct distillation at these distillation spots. It does not add any other candidate distillation spots to the standard distillation methods. Experimental results on CIFAR-100 are shown in Table III, and results on tiny-ImageNet are shown in Table V. From these results, it can be seen that the proposed adaptive distillation strategy consistently outperforms the standard distillation strategy with nearly all distillers, under all teacher-student architectures, and on both CIFAR-100 and tiny-ImageNet datasets. For example, when combined with PKT [52] for distilling knowledge from C-ResNet56 to C-ResNet20, the proposed adaptive distillation strategy improves the accuracy by 0.84% (from 70.72% to 71.56%) and 0.21% (from 54.29% to 54.50%), respectively. The standard distillation strategy imposes distillation constraints on all distillation spots and throughout the whole distillation process, which leads to overregularization on the student and thus the performance degradation. Similar results are also observed on other distillation methods and student-teacher combinations. The highly consistent results demonstrate the effectiveness of the proposed method for improving performance of existing distillation methods. \n\n2) Heterogeneous Distillation: The proposed adaptive distillation strategy is naturally compatible with homogeneous distillation. However, it seems not so compatible with heterogeneous distillation.",
            "score": 0.5538005635517493,
            "section_title": "B. Benchmark Comparisons 1) Homogeneous Distillation:",
            "char_start_offset": 26859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1985
                },
                {
                    "start": 1988,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2186
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 346,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1308,
                    "end": 1312,
                    "matchedPaperCorpusId": "52012952"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6591796875
        },
        {
            "corpus_id": "258048760",
            "title": "A Survey on Recent Teacher-student Learning Studies",
            "text": "Knowledge distillation [7] is a method of transferring the knowledge from a complex model, called the teacher, to a smaller and simpler model, called the student. In recent years, several variants of knowledge distillation have been proposed, including teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation. This literature review summarizes the recent developments in these variants of knowledge distillation and discusses their strengths and limitations. \n\nKnowledge distillation is a method of compressing a complex deep neural network (DNN) into a smaller and faster DNN while preserving its accuracy. The process of knowledge distillation involves training a smaller DNN, called the student, to imitate the predictions of a larger and more complex DNN, called the teacher. The student network is trained to produce similar results as the teacher network, but with fewer parameters and lower computational cost. Knowledge distillation has been widely used for model compression and acceleration, and has shown great promise in various applications [5,8,13,18], such as computer vision and natural language processing. \n\nIn recent years, several variants of knowledge distillation [3,4,21] have been proposed and explored to improve the performance of knowledge distillation [6,9]. One of these variants is teaching assistant distillation, which introduces an intermediate model, called the teaching assistant, between the teacher and the student. The teaching assistant is trained to mimic the behavior of the teacher, and the student is trained to imitate the outputs of the teaching assistant. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better capture the knowledge learned by the teacher. \n\nAnother variant of knowledge distillation [10,11] is curriculum distillation, which designs the learning process to follow a curriculum, similar to human education. The curriculum is designed to present easy examples first and gradually increase the difficulty of the examples as the student improves. This approach has been shown to provide better performance than traditional knowledge distillation, especially for tasks that require a lot of prior knowledge. \n\nMask distillation is a variant of knowledge distillation [12,20] that focuses on transferring the attention mechanism learned by the teacher to the student.",
            "score": 0.5537844811146724,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1813
                },
                {
                    "start": 1816,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2277
                },
                {
                    "start": 2280,
                    "end": 2436
                }
            ],
            "ref_mentions": [
                {
                    "start": 1106,
                    "end": 1108,
                    "matchedPaperCorpusId": "235677396"
                },
                {
                    "start": 1111,
                    "end": 1114,
                    "matchedPaperCorpusId": "245218794"
                },
                {
                    "start": 1235,
                    "end": 1238,
                    "matchedPaperCorpusId": "257771511"
                },
                {
                    "start": 1240,
                    "end": 1243,
                    "matchedPaperCorpusId": "258841675"
                },
                {
                    "start": 1332,
                    "end": 1334,
                    "matchedPaperCorpusId": "253270253"
                },
                {
                    "start": 1862,
                    "end": 1865,
                    "matchedPaperCorpusId": "252625376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "273963855",
            "title": "Over-parameterized Student Model via Tensor Decomposition Boosted Knowledge Distillation",
            "text": "Revisiting Prediction Match of Knowledge Distillation Traditional knowledge distillation involves two stages: fine-tuning the teacher model for a specific task, followed by training strategies to constrain the student model to closely approximate the teacher model. These processes aim to transfer the knowledge from the teacher to the student model. Recent studies have mainly focused on directly learning from the features and logits of the teacher model to transfer crucial knowledge [23,56]. \n\nHowever, these methods are limited by the capacity of the student model due to the limitation of total parameters. Moreover, this distillation approach based on cross-entropy loss constraints may lead to the student model losing its ability to learn independently. We aim to design a novel model distillation framework to enable the student model not only to effectively learn the knowledge from the teacher model but also to maintain its ability to learn independently. \n\nDistillation Loss for Auxilary Tensors. To achieve the goal of \"learning knowledge from the teacher model while maintaining the ability of the student model to learn independently,\" we introduce a high-order tensor alignment training method based on the MPO decomposition. A crucial merit of MPO decomposition is its ability to reorganize and aggregate the core information, decomposing the weight matrices into a central tensor (containing a large number of parameters and important information) and auxiliary tensors (containing fewer parameters and additional information to the central tensor) [35,36]. Therefore, in the knowledge distillation, in addition to minimizing the cross-entropy loss concerning the ground truth, we add a loss constraint for aligning the auxiliary tensors between the student and teacher models: \n\nwhere the matrices A t,k and A s,k refer to the auxiliary tensor of student and teacher models with the same dimensions respectively. MSE means the mean-square error loss function. To ensure that the student model learns from the teacher while preserving its central tensor for independent learning, we minimize the mean-square error loss between the auxiliary tensors of both the student and teacher models. Since this distillation framework is based on improvements to the weight matrices, it is orthogonal to most current distillation methods. Therefore, it can further enhance the distillation effectiveness based on existing distillation methods (as thoroughly discussed in the experimental section).",
            "score": 0.5535549570548477,
            "section_title": "Assisted Constraints for Knowledge Distillation",
            "char_start_offset": 13536,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1797
                },
                {
                    "start": 1800,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2346
                },
                {
                    "start": 2347,
                    "end": 2505
                }
            ],
            "ref_mentions": [
                {
                    "start": 487,
                    "end": 491,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 491,
                    "end": 494,
                    "matchedPaperCorpusId": "211066200"
                },
                {
                    "start": 1569,
                    "end": 1573,
                    "matchedPaperCorpusId": "235352554"
                },
                {
                    "start": 1573,
                    "end": 1576,
                    "matchedPaperCorpusId": "247218249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87353515625
        },
        {
            "corpus_id": "271208532",
            "title": "Efficient image classification through collaborative knowledge distillation: A novel AlexNet modification approach",
            "text": "A key innovation is the introduction of bottleneck units at the split point, transforming intermediate layer features into a more compression-friendly, lower-dimensional representation. Implementing a split-DNN architecture with bottleneck units is more complex than traditional neural network models. This complexity poses challenges in terms of deployment and maintenance, especially on edge devices with limited computational resources [17]. introduces a simplified knowledge distillation technique to compress a large teacher model into a smaller, efficient student model without significant performance loss. The method involves reusing the teacher classifier and aligning the student features with the teacher using a single \u21132 loss, achieving performance parity when perfectly aligned. The need for an additional projector to ensure compatibility between different architectures introduces extra complexity and computational overhead, somewhat negating the simplicity of the approach [18]. addresses a key challenge in knowledge distillation: the semantic mismatch in manual layer associations between teacher and student models in feature-map-based distillation methods [18]. proposed solution, Semantic Calibration for Interactive Knowledge Distillation (SemCKD), automatically determines appropriate target layers in the teacher model for each student layer using an attention mechanism. This approach allows each student layer to distill knowledge from multiple layers of the teacher model, rather than being restricted to a single intermediate layer. While SemCKD aims to address semantic mismatches, its adaptability and effectiveness across a wide variety of teacher-student network architectures are not fully established. Non-interactive knowledge distillation faces a significant challenge the student network limited capacity for feature transformation compared to the teacher means that the student cannot perfectly replicate the teacher knowledge [19]. This discrepancy can hinder the effectiveness of the knowledge distillation process. In response to the limitations found in existing knowledge distillation and semi-supervised learning methods, our research has led to the creation of a unique model that synthesizes the strengths of various established techniques while addressing their key weaknesses. Our model simplifies the knowledge distillation process by streamlining the transfer of knowledge from the teacher to the student model, avoiding the complexities and interpretational challenges of traditional methods. \n\nThe proposed work is the pilot study to introduce an effective, lightweight model for image classification leveraging knowledge distillation.",
            "score": 0.5533615104446109,
            "section_title": "Introduction",
            "char_start_offset": 9491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2326
                },
                {
                    "start": 2327,
                    "end": 2545
                },
                {
                    "start": 2548,
                    "end": 2689
                }
            ],
            "ref_mentions": [
                {
                    "start": 439,
                    "end": 443,
                    "matchedPaperCorpusId": "247762862"
                },
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "248598127"
                },
                {
                    "start": 1178,
                    "end": 1182,
                    "matchedPaperCorpusId": "248598127"
                },
                {
                    "start": 1967,
                    "end": 1971,
                    "matchedPaperCorpusId": "234805083"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "268820185",
            "title": "A Comprehensive Review of Knowledge Distillation in Computer Vision",
            "text": "Online and offline distillation, and self-distillation are three common distillation schemes used in knowledge distillation.Here's a brief overview of each:\n\nOffline distillation is an increasingly popular technique in knowledge distillation for training a smaller student model to mimic the behaviors of a larger and more experienced teacher model.This approach involves training the teacher model on a huge dataset while the student model picks up the information using an offline training procedure.\n\n[55] Offline distillation provides several advantages.To start, it could reduce and simplify the model, increasing its computational efficiency and deployment speed.Second, it can enhance the student model's generalization performance, particularly when there is a lack of training data.Finally, it may be used to teach the student model the knowledge the teacher model has acquired, enhancing the student model's performance on a specific task.Traditionally, knowledge distillation has been aimed at defining and transferring knowledge from teacher to student models effectively, but recent self-supervised contrastive methods can adversely affect the original task's representation learning.The authors in this paper [55] proposed a novel approach mitigates this by using an alternative self-supervised augmented task, which guides the network to learn the joint distribution of the original recognition task and the auxiliary task, thereby enhancing representation power without sacrificing classification capability.The use of auxiliary classifiers to intermediate feature maps generates diverse self-supervised knowledge for thorough oneto-one transfer, unlike previous methods that only use probabilistic knowledge from the final layers.The proposed approach [55] significantly outperforms previous state-of-the-art SSKD methods across various network pairs, with average improvements of 2.56% on CIFAR-100 and 0.77% on ImageNet.\n\nIn the original design, knowledge distillation was introduced as a way to transfer knowledge between teacher and student prediction logits by minimizing the KL-Divergence between teacher and student prediction logits.Distillation methods based on features outperform logit-based methods, but are computationally expensive.",
            "score": 0.5530428179343247,
            "section_title": "Distillation Schemes in Knowledge Distillation",
            "char_start_offset": 16681,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 124,
                    "end": 156
                },
                {
                    "start": 158,
                    "end": 349
                },
                {
                    "start": 349,
                    "end": 502
                },
                {
                    "start": 504,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 949
                },
                {
                    "start": 949,
                    "end": 1197
                },
                {
                    "start": 1197,
                    "end": 1524
                },
                {
                    "start": 1524,
                    "end": 1747
                },
                {
                    "start": 1747,
                    "end": 1939
                },
                {
                    "start": 1941,
                    "end": 2158
                },
                {
                    "start": 2158,
                    "end": 2263
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8876953125
        },
        {
            "corpus_id": "270726811",
            "title": "Balancing Speed and Precision: Lightweight and Accurate Depth Estimation for Light Field Image",
            "text": "To design a knowledge distillation framework, adapting the optimal student model is a key step.Cho and Hariharan [29] indicated that when there is a large disparity in representation power between the teacher model and the student model, knowledge transfer is ineffective.In order to explore the extent of the difference that can be tolerated between teacher and student models in knowledge distillation, two variations of the student model were tested: one with a single convolution block removed, referred to as the 'compact model', and another with three convolution blocks removed, referred to as the 'ultra-compact model'.Using these lightweight student models and the same teacher model, the students were trained through the knowledge distillation framework.Subsequently, they were evaluated to assess the effectiveness of the training.The accuracy results of each model are presented in Table 3 and visualized in Figure 8.\n\nFrom these results, it was observed that while the compact model showed improvements in both MSE and BadPix through knowledge distillation, the ultra-compact model, on the other hand, experienced a decrease in accuracy.This indicates that in the case of the EPINET architecture, knowledge distillation does not work effectively for enhancing the accuracy when there is a difference of three 92158 VOLUME 12, 2024 Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.convolution blocks between the teacher and student models.In other words, it was found that the acceptable difference in convolution blocks is less than three.To investigate how the difference of three convolution blocks influences the effectiveness of knowledge distillation, an examination of the accuracy trends over epochs was conducted.The models compared include one with three blocks removed (middle) and another with one block removed (bottom).Red square areas highlight the significant difference in error distribution between the compact model and ultra-compact model.Inference was conducted on the light field synthetic dataset [22].That is, the student model, with three convolutional blocks removed, ceased to improve its accuracy at an early stage.These findings indicate that the large model discrepancy between the student and teacher models likely made it difficult for the student model to appropriately learn the abundant information held by the teacher model.",
            "score": 0.5528484924644379,
            "section_title": "B. ACCURACY ENHANCEMENT EXPERIMENTS 1) THE EVALUATION OF THE LIMITS OF MODEL DISPARITY BETWEEN TEACHER AND STUDENT",
            "char_start_offset": 27663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 627
                },
                {
                    "start": 627,
                    "end": 765
                },
                {
                    "start": 765,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 930
                },
                {
                    "start": 932,
                    "end": 1151
                },
                {
                    "start": 1151,
                    "end": 1436
                },
                {
                    "start": 1436,
                    "end": 1455
                },
                {
                    "start": 1455,
                    "end": 1513
                },
                {
                    "start": 1513,
                    "end": 1614
                },
                {
                    "start": 1614,
                    "end": 1796
                },
                {
                    "start": 1796,
                    "end": 1907
                },
                {
                    "start": 1907,
                    "end": 2033
                },
                {
                    "start": 2033,
                    "end": 2099
                },
                {
                    "start": 2099,
                    "end": 2217
                },
                {
                    "start": 2217,
                    "end": 2434
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 117,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 2094,
                    "end": 2098,
                    "matchedPaperCorpusId": "26221504"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8037109375
        },
        {
            "corpus_id": "272753230",
            "title": "Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights",
            "text": "Our research demonstrates an effective and straightforward approach for generating rationales from a large model to enhance a smaller model. However, a significant performance gap between the teacher model (the large model) and the student model (the small model) still exists. As illustrated in table 6, the flan-t5-3b, which serves as the teacher model, significantly outperforms the smaller t5-small model across all four datasets we tested. \n\nThis limitation underscores the inherent challenge in knowledge distillation and model scaling. While our method effectively transfers knowledge from a large to a small model, the reduced capacity of the smaller model limits its ability to fully replicate the performance of its larger counterpart. This performance discrepancy highlights the trade-offs involved in model downsizing, where gains in efficiency and deployability often come at the cost of reduced accuracy and overall capability. \n\nAddressing this gap remains a key area for future research. Efforts could focus on developing more advanced distillation techniques or optimizing small models to better capture and utilize the knowledge transferred from larger models. Our findings lay the groundwork for such exploration, offering insights into the dynamics of knowledge transfer between models of varying sizes and complexities.",
            "score": 0.5525754044095452,
            "section_title": "Limitation",
            "char_start_offset": 23879,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 941
                },
                {
                    "start": 944,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1340
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "258564700",
            "title": "Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation",
            "text": "Knowledge distillation is a popular technique to transfer knowledge from large teacher models to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between this\"distribution closeness\"and the student model generalizability, which enables us to select the PTLoss's perturbation coefficients in a principled way. Extensive experiments on five datasets demonstrate PTLoss can significantly improve the distillation effectiveness for teachers of various scales.",
            "score": 0.5523331213916316,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "250492247",
            "title": "SlimSeg: Slimmable Semantic Segmentation with Boundary Supervision",
            "text": "We compare the mIoUs of training the slimmable model with and without stepwise downward distillation in Table 3. For the smallest subnetwork with width\u00d70.25, the mIoUs consistently improve with distillation under all combinations of loss functions. With the distillation strategy proposed by our work, mIoUs improve on all subnetworks, and among them, the smallest subnetwork with width\u00d70.25 has the largest increase (0.8%) from 73.6% to 74.4%. \n\nWhich is the best teacher for small submodels? We train our slimmable model with soft targets predicted by different models as teachers in knowledge distillation. For the student subnetwork S(   ), 'prev', 'largest', 'mean' indicates that the soft target is the predicted probability  +1 of the last larger subnetwork S(  +1 ),   of the largest subnetwork S(   ) [56] and the average of all the predictions 1  \u2212  =+1   by the subnetwork larger than the current model S(  +1 ), ..., S(   ), respectively. Different from the setting of 'mean', 'larger' represents using the average loss of all the larger submodels' distillation. The mIoU of our slimmable model under different teacher settings are reported in Table 4. Note that all the models are trained with the sum of the three losses proposed. Our 'prev' setting, the stepwise downward distillation, outperform others by higher mIoU 74.4% and 77.37% on width \u00d70.25 and \u00d70.5. Using the average loss of all larger submodels results in better mIoUs on the larger submodels with width \u00d70.75 and \u00d71.0, but even lower mIoU than models trained without distillation on width \u00d70.25 and \u00d70.5. The results are consistent with the phenomenon that student network's performance degrades when the gap between student and teacher is too large [36].",
            "score": 0.5519083542963849,
            "section_title": "Does inplace knowledge distillation work?",
            "char_start_offset": 24229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1734
                }
            ],
            "ref_mentions": [
                {
                    "start": 810,
                    "end": 814,
                    "matchedPaperCorpusId": "76660361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69482421875
        },
        {
            "corpus_id": "263830209",
            "title": "What do larger image classifiers memorise?",
            "text": "Despite the impressive performance of large neural models on a range of challenging tasks in vision and NLP, practical deployment of such models is often infeasible due to their high inference cost. Recently, knowledge distillation [18,41] has emerged as a promising approach to compress these large models into more tractable models. Here, one feeds a large (\"teacher\") model's predicted distribution over labels as the prediction targets for a small (\"student\") model. Compared to the standard training on raw labels (as considered in earlier sections), distillation can provide significant performance gains; these are informally attributed to distillation performing \"knowledge transfer\". Knowledge distillation has been successfully applied across many applications, including: computer vision [10], language modeling [72], information retrieval [51], machine translation [87], and ads recommendation [1,52]. \n\nCompared to standard training, distillation presents us with an interesting setup where models of different sizes interact during the training procedure. This prompts us to ask: how does coupling between models of different sizes during distillation affect, if at all, the memorisation behavior of the resulting student model? Interestingly, while distillation has been shown to yield significant performance gains on average, training accuracy has been shown to be systematically harmed [24]. Previous work also showed how distillation can lead to worsened accuracy on a subset of hard examples [54]. In a related study, model compression has been shown to harm accuracy on tail classes [42]. These observations indeed hint at the potential impact of distillation procedure on student's memorisation behavior, which we systematically explore in this section. \n\nTowards this, we consider knowledge distillation as conducted using logit matching, and where the teacher is trained on the same sub-sample as the student for estimating the memorisation scores per Equation 2. We provide the hyperparameter details in Appendix D. \n\nDistillation inhibits memorisation on average. We begin by investigating what happens to average memorisation under distillation.",
            "score": 0.5513621752965363,
            "section_title": "Distillation lowers memorisation",
            "char_start_offset": 25608,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1775
                },
                {
                    "start": 1778,
                    "end": 2040
                },
                {
                    "start": 2043,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 236,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 851,
                    "end": 855,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "207847275"
                },
                {
                    "start": 1404,
                    "end": 1408,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.921875
        },
        {
            "corpus_id": "253107531",
            "title": "Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation",
            "text": "Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge of (commonly) a larger and better performing teacher model to a student model (Buciluundefined et al., 2006). The idea is to mix the ground-truth label with the model probability distribution of a teacher model, resulting in an adaptive version of label smoothing (Tang et al., 2021).\n\n(2) \u03d5 and \u03b8 denote the parameters of a teacher model and a student model respectively.P indicates a probability distribution smoothed with a temperature. Similar to label smoothing, \u03d5 remains unchanged in training; thus a student model is learned to minimize the KL divergence between its probability distribution and that of the teacher model. WhenP \u03d5 follows a uniform distribution with the temperature set to 1, the loss function of knowledge distillation is identical to that of uniform label smoothing.\n\nTraining a large teacher model can be computationally expensive; for this reason, there have been attempts to replace the teacher model with the student model itself, called self-knowledge distillation (Zhang et al., 2019;Yuan et al., 2020;Kim et al., 2021;Zhang and Sabuncu, 2020). TF-KD (Yuan et al., 2020) trains a student with a pre-trained teacher that is identical to the student in terms of structure. SKD-PRT (Kim et al., 2021) utilizes the previous epoch checkpoint as a teacher with linear increase in \u03b1. (Zhang and Sabuncu, 2020) incorporates beta distribution sampling (BETA) and self-knowledge distillation (SD), and introduce instance-specific prior label distribution. (Yun et al., 2020) utilizes self-knowledge distillation to minimize the predictive distribution of samples with the same class, encouraging consistent probability distribution within the same class.",
            "score": 0.5500255477273367,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6578,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 44,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 159,
                    "end": 189,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1098,
                    "end": 1116,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 1116,
                    "end": 1133,
                    "matchedPaperCorpusId": "233714221"
                },
                {
                    "start": 1133,
                    "end": 1157,
                    "matchedPaperCorpusId": "219558831"
                },
                {
                    "start": 1165,
                    "end": 1184,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 1293,
                    "end": 1311,
                    "matchedPaperCorpusId": "233714221"
                },
                {
                    "start": 1391,
                    "end": 1416,
                    "matchedPaperCorpusId": "219558831"
                },
                {
                    "start": 1560,
                    "end": 1578,
                    "matchedPaperCorpusId": "214727822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74462890625
        },
        {
            "corpus_id": "253236895",
            "title": "Teacher-Student Architecture for Knowledge Learning: A Survey",
            "text": "Knowledge distillation focuses on training a student model, using predictions from a larger-sized teacher model. The purpose of knowledge distillation is to have a compact student model while maintaining comparable performance with a teacher model. Hinton et al. [4] first propose to distill knowledge from multiple models to a single student model for the task of model compression and transfer learning. Tang et al. [14] compress BERT [15] to a much light-weight Bi-LSTM [16] for the task of natural language processing. Romero et al. [17] suggest the success of deep neural nets is largely attributed to the deep hierarch. Thus they propose to compress wide (a large number of neurons in each layer) and deep teacher models into much narrower (fewer neurons in each layer) and deeper student models. Yim et al. [18], design the architectures of students and teachers as a N -part module, where each module contains various numbers of convolutional layers. Student models generally have a simpler design, and the task for students is to learn each layer output of the teacher. \n\nIn a different approach, Wang et al. [19] argue that in vanilla Knowledge Distillation, it is hard for students to learn all knowledge from teachers, thus students normally show worse performance compared to the teacher. The authors adopt generative adversarial networks [20] to simulate the distillation process. The generator (student model with fewer parameters) learns the distribution of the data, whereas the discriminator (teacher model with more parameters) learns to differentiate if the input is from a student or real. \n\nTang et al. [21] propose to distill complicated models in information retrieval or recommendation systems where when a query is given, the model predicts the top K relevant information. Zhang et al. [22] utilize multiple students to learn from each other during training. The purpose is for performance improvement instead of model compression. Furlane et al. [23] propose a novel ensemble learning approach, where students and teachers share the same architecture. The N th student is responsible to train the N + 1 th student. Predictions are averaged in the end.",
            "score": 0.5499503148128446,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1610
                },
                {
                    "start": 1613,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 814,
                    "end": 818,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1118,
                    "end": 1122,
                    "matchedPaperCorpusId": "53976534"
                },
                {
                    "start": 1352,
                    "end": 1356,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 1625,
                    "end": 1629,
                    "matchedPaperCorpusId": "50778760"
                },
                {
                    "start": 1812,
                    "end": 1816,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1973,
                    "end": 1977,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "254815049",
            "title": "Sound Event Detection System Based on VGGSKCCT Model Architecture with Knowledge Distillation",
            "text": "The mean teacher model is combined with knowledge distillation to overcome the limitation that the pre-trained teacher model (in knowledge distillation) is difficult to surpass.The teacher model (of the mean teacher model) can therefore be more effective than the pre-trained teacher model (of knowledge distillation).A similar work (Endo and Nishizaki 2022) was published in ICASSP Endo and Nishizaki (2022) at the time we prepared this paper.They also combined the mean teacher model with knowledge distillation.The main difference is that they used knowledge distillation between an ensemble model and its component models, while we used a pretrained model for distillation.They used distillation to improve component models and then further improve the ensemble model.We used a pre-trained model to enhance the prediction of a mean teacher model.Another difference is that the model used for distillation would be tuned while training in their approach, but would not be tuned in ours.Thus, their work is in line with a common knowledge distillation for model compression, while ours is in line with traditional training.Finally, we implement second-stage consistency training in our system.\n\nThe rest of this paper is organized as follows: We first introduce the data set and method used in our experiment in Section \"Dataset\" and \"Materials and methods,\" respectively.Then we describe the experiment setup and show the outcome in Section \"Results and discussion.\"Finally, concluding remarks are drawn in Section \"Conclusion.\"",
            "score": 0.5492264398035547,
            "section_title": "Introduction",
            "char_start_offset": 3634,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 177,
                    "end": 318
                },
                {
                    "start": 318,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 677
                },
                {
                    "start": 677,
                    "end": 772
                },
                {
                    "start": 772,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 989
                },
                {
                    "start": 989,
                    "end": 1125
                },
                {
                    "start": 1125,
                    "end": 1195
                },
                {
                    "start": 1197,
                    "end": 1374
                },
                {
                    "start": 1374,
                    "end": 1469
                },
                {
                    "start": 1469,
                    "end": 1531
                }
            ],
            "ref_mentions": [
                {
                    "start": 333,
                    "end": 358,
                    "matchedPaperCorpusId": "238418978"
                },
                {
                    "start": 383,
                    "end": 408,
                    "matchedPaperCorpusId": "238418978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59521484375
        },
        {
            "corpus_id": "264490946",
            "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
            "text": "Effectiveness of standard knowledge distillation for knowledge transfer. To study the suitability of standard KL distillation for general-purpose knowledge transfer, we select 400 teacher-student pairs (Tab. 5 in Supp. for details), all of which exhibit a significant percentage of complementary knowledge (c.f. \u00a73). Across these pairs for each student model, we measure the percentage of teachers for which a positive transfer delta \u2206 transf. is obtained. Results are visualized in Fig. 4a, and reveal that for the majority of students there are less than 40% of teachers with performance increases. An   additional classification loss (XE-KL, \u00a74.1) can raise the median success rate slightly above 40%. \n\nIn both cases, however, overwriting pre-existent knowledge more often than not overshadows the benefits gained from actual knowledge transfer, particularly when transferring from a weaker teacher model as shown in Fig. 4b (KL-Dist. Transfer), where absolute performance changes after transfer are visualized against initial teacher-student performance differences (as measured on the separate validation data). In addition, we find that these limits also hold when deploying simple additional regularizers such as label smoothing (54%, Yuan et al. (2020b)), with consistently negative transfer for weaker teachers, and reduced effectiveness for stronger ones. \n\nLeveraging continual learning. Treating general knowledge transfer as a continual learning task through weight regularization (XE-KL+MCL) raises median success rates significantly (80%, Fig. 4a). However, we find a lack of efficacy when knowledge is specialized to areas of expertise, and when teachers are stronger, which we address with data-level regularization (KL+DP), raising success rates to 92.5%. As shown in Fig. 4b, these gains can be attributed to positive transfer deltas even for much weaker teachers (see performance differences much lower than zero), and, unlike strict weight-level regularization in e.g. MCL, barely limit gains from much stronger teachers. Indeed, we find that for a number of stronger teachers, particularly where performance differences are not as striking, data-level regularization can even offer an edge over normal distillation. \n\nFull ImageNet experiments.",
            "score": 0.5479707283603334,
            "section_title": "EVALUATION OF DIFFERENT APPROACHES FOR KNOWLEDGE TRANSFER",
            "char_start_offset": 21846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 704
                },
                {
                    "start": 707,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2238
                },
                {
                    "start": 2241,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 1243,
                    "end": 1262,
                    "matchedPaperCorpusId": "219962714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6953125
        },
        {
            "corpus_id": "264952263",
            "title": "A lightweight speech recognition method with target-swap knowledge distillation for Mandarin air traffic control communications",
            "text": "Experiments were carried out on the Mandarin ATC communications dataset to investigate the effectiveness of the TSKD training strategy proposed in this article. As shown in Table 3, two distinct teacher-student network structures, including homogeneous and heterogeneous architectures, were employed. The left set used the Conformer teacher model to distill the Conformer student model within homogeneous architectures, whereas the right group made use of the Transformer teacher model to distill the Conformer student model within heterogeneous network structures. The findings suggest that the effectiveness of knowledge distillation can be boosted by employing either TKD or SKD, with TKD bringing greater benefits. It also supports that non-target class knowledge distillation (NCKD) is the primary reason why classical logits-based distillation is effective but severely restricted, which coincides with the efficiency analysis of DKD (Zhao et al., 2022). Furthermore, integrating two training strategies into one, namely TSKD, can further strengthen the performance of knowledge distillation. In this article, TSKD focuses on the logit output exchange of the target class between the student and teacher models, which can effectively improve the distillation performance. Apart from the target class, the top-k information predicted by the teacher model for each segment of acoustic features is also a sort of valuable information (Reddi et al., 2021); it reflects which k characters the acoustic features input should be transcribed into, from the   (Hinton, Vinyals & Dean, 2015) 6.30 11.76 6.94 \n\nFitNets (Romero et al., 2015) 6.22 10.92 6.68 DKD (Zhao et al., 2022) 6.11 11.39 6.30 TSKD (Ours) 6.00 10.65 6.11 \n\nNotes. \n\nResults of the proposed model are in bold. \n\nperspective of the teacher model. On the basis of this assumption, another comparative experiment was conducted to verify the effectiveness of the TSKD, with the predictive top-k output of the teacher model for each segment as the information exchange.",
            "score": 0.5479320330298127,
            "section_title": "Effectiveness of TSKD in homogeneous and heterogeneous architectures",
            "char_start_offset": 17657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1719
                },
                {
                    "start": 1722,
                    "end": 1728
                },
                {
                    "start": 1731,
                    "end": 1773
                },
                {
                    "start": 1776,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2028
                }
            ],
            "ref_mentions": [
                {
                    "start": 940,
                    "end": 959,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1437,
                    "end": 1457,
                    "matchedPaperCorpusId": "233237266"
                },
                {
                    "start": 1557,
                    "end": 1587,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1614,
                    "end": 1635,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1656,
                    "end": 1675,
                    "matchedPaperCorpusId": "247476179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73974609375
        },
        {
            "corpus_id": "270878760",
            "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
            "text": "Secondly, for the OPT model, we discovered that the most straightforward KD algorithm, which employs the teacher distribution as supervision for each token step to fine-tune the student model, achieved the best overall performance.Likewise, MINILLM outperformed other distillation algorithms and even exceeded the performance of teacher  models for OPTs of any size on the Flipkart dataset.Finally, for LLaMA, SeqKD demonstrated a comparatively better distillation effect, whereas for LLaMA2, JS showed a relatively superior performance.This suggests that even when the model size is identical and the model structure is similar, the effectiveness of the same distillation algorithm can vary significantly.",
            "score": 0.5470208524073643,
            "section_title": "Robustness Evaluation of White-box KD",
            "char_start_offset": 26110,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 231,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 706
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83935546875
        },
        {
            "corpus_id": "270755154",
            "title": "RSMoDM: Multimodal Momentum Distillation Model for Remote Sensing Visual Question Answering",
            "text": "Knowledge distillation aims to extract knowledge from complex, large-scale teacher models and transfer it to smaller student models. This process typically involves minimizing the discrepancy between the output distributions of the teacher and student models to enhance the performance of the student model. According to existing research [54], knowledge can be categorized into different types, including response-based knowledge [55], feature-based knowledge [56], and relation-based knowledge [57]. These types target different aspects of neural network architecture. For instance, response-based knowledge focuses on the output of the final layer, while feature-based knowledge emphasizes the feature representation of intermediate layers to capture higher-order correlations within the network structure [58]. \n\nThe learning schemes for knowledge distillation can be divided into three main categories based on whether the teacher and student models are updated synchronously: offline distillation, online distillation, and self-distillation [54]. Offline distillation is a traditional method where a pretrained teacher model transfers its knowledge to the student model in the form of logits or intermediate features [59]. Online distillation is an end-to-end training framework where both teacher and student models are updated simultaneously, particularly suitable when largecapacity, high-performance teacher models are unavailable [60]. Self-distillation is a special form of online distillation, involving the transfer of knowledge within the same network from deeper to shallower layers or from early to later training stages, thus achieving knowledge transfer and training optimization [61]. \n\nIn this article, we propose a momentum distillation method that can be considered a form of online self-distillation. Inspired by the ALBEF [24], we maintain a momentum version of the model, which generates more stable and reliable pseudolabels for additional supervision. This approach aims to prevent the model from being penalized for producing reasonable outputs that differ from the ground truth. Through momentum distillation, we effectively achieve model generalization across a broader data distribution while reducing reliance on noisy or imprecise labels.",
            "score": 0.5468729304039444,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 13127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1704
                },
                {
                    "start": 1707,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2272
                }
            ],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 343,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 431,
                    "end": 435,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 461,
                    "end": 465,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 809,
                    "end": 813,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1047,
                    "end": 1051,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1223,
                    "end": 1227,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1441,
                    "end": 1445,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 1699,
                    "end": 1703,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1847,
                    "end": 1851,
                    "matchedPaperCorpusId": "236034189"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92578125
        },
        {
            "corpus_id": "269921267",
            "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
            "text": "This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.\n\nThe core idea of knowledge distillation is that while large teacher models may have high predictive accuracy due to the large number of parameters, these advantages are not always directly applicable to resource-constrained environments.Knowledge distillation therefore aims to extract the \"dark knowledge\" of the teacher model -not just the final predictions, but also its decision-making processes and uncertainty estimates for the input data -and then infuse this knowledge into a more concise student model:  The knowledge distillation technique adopts a typical teacher-student training framework, as shown in Figure 2. In this framework, the \"teacher\" model represents a highly complex deep neural network.In contrast, the Student model is designed to be more streamlined and aims to achieve a similar level of performance with fewer parameters.The process begins with the pre-training of the Teacher model, a step that allows it to accumulate broad and deep knowledge on the specified data set.Instead of directly replicating the weights of the \"teacher\" model, the subsequent knowledge transfer phase uses its output -particularly soft probability distributions (rather than hard classification labels).This process combines the rich experience of the \"teacher\" model with direct guidance from real-world data to form a dual oversight mechanism designed to efficiently extract and transfer deep learning capabilities, while reducing resource burdens through model compression techniques and enhancing the feasibility and flexibility of the model in practical applications.\n\nKnowledge distillation is generally categorized into two main types: output-based and feature-based distillation.In this approach, the student model acquires similar predictive abilities by assimilating the output probability distribution from the teacher model, essentially grasping and transforming high-level abstract concepts.This process is achieved by designing specialized distillation loss functions that ensure that the student model can capture and mimic the high-level transformation logic of the teacher model to the input information, thus maintaining consistent or close performance to the teacher model on the prediction task.\n\nFeature-based distillation emphasizes the assimilation of intermediate layer features from the teacher model.",
            "score": 0.5467840743454149,
            "section_title": "B. Knowledge distillation",
            "char_start_offset": 14355,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 182,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 894
                },
                {
                    "start": 894,
                    "end": 1033
                },
                {
                    "start": 1033,
                    "end": 1183
                },
                {
                    "start": 1183,
                    "end": 1393
                },
                {
                    "start": 1393,
                    "end": 1762
                },
                {
                    "start": 1764,
                    "end": 1877
                },
                {
                    "start": 1877,
                    "end": 2405
                },
                {
                    "start": 2407,
                    "end": 2516
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91162109375
        },
        {
            "corpus_id": "270077283",
            "title": "Self-Knowledge Distillation via Progressive Associative Learning",
            "text": "In this section, we first briefly introduce the most related works of knowledge distillation.Then we specifically review recent self-distillation works.\n\nKnowledge distillation is a widely used paradigm for model compression, which transfers knowledge from a complex teacher model to a compact student model.To be specific, the teacher network has high accuracy and huge parameters, while the student network is not as accurate as the teacher network but has fewer parameters.Through knowledge distillation, we hope that the student network can approach or exceed the teacher network as much as possible.In this way, we obtain a compact student network with a similar prediction effect as the teacher network.Ba et al. [30] first proposed a method that uses the teacher's logits before the softmax as the regression target to train the student network, which completes the imitation of the teacher network by forcing the student network to mimic the teacher network's logits.Hinton et al. [12] first proposed to use the soft outputs of the pretrained teacher network as dark knowledge to supervise the training of the student network.They introduced a temperature hyperparameter T and formulated the problem as \"knowledge distillation\".The student network is forced to learn the soft targets of the teacher network, which are obtained through using a high temperature T on the softmax inputs.In the process of knowledge transfer, soft targets often contain richer information than one-hot targets.Romero et al. [13] extended the knowledge distillation method proposed by Hinton et al.In their method, the student network can be deeper and narrower than the teacher network and improve the performance by learning the outputs of the teacher network and the features of the middle layer.All the above methods are offline distillation methods [31,32], which need a pretrained teacher network.\n\nIn contrast to these methods, online knowledge distillation trains the student network under the supervision of a teacher from scratch.For example, Zhang et al. [33] proposed a mutual learning method, which uses multiple neural networks.Zhao et al. [9] proposed a collaborative training method, which uses both an expert teacher and a from-scratch teacher to supervise the student.To reduce the computational cost, Zhou et al. [34] proposed to employ two different networks which share some low parameters and train separately.",
            "score": 0.5467511765829022,
            "section_title": "Related Work",
            "char_start_offset": 5661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 152
                },
                {
                    "start": 154,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 975
                },
                {
                    "start": 975,
                    "end": 1134
                },
                {
                    "start": 1134,
                    "end": 1236
                },
                {
                    "start": 1236,
                    "end": 1392
                },
                {
                    "start": 1392,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1584
                },
                {
                    "start": 1584,
                    "end": 1785
                },
                {
                    "start": 1785,
                    "end": 1889
                },
                {
                    "start": 1891,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2128
                },
                {
                    "start": 2128,
                    "end": 2272
                },
                {
                    "start": 2272,
                    "end": 2418
                }
            ],
            "ref_mentions": [
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 989,
                    "end": 993,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1840,
                    "end": 1844,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1844,
                    "end": 1847,
                    "matchedPaperCorpusId": "258888057"
                },
                {
                    "start": 2052,
                    "end": 2056,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 2140,
                    "end": 2143,
                    "matchedPaperCorpusId": "198179767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "253499102",
            "title": "PILE: Pairwise Iterative Logits Ensemble for Multi-Teacher Labeled Distillation",
            "text": "The idea of knowledge distillation was first introduced by Cristian et al. (2006) to train small and fast models to mimic cumbersome and complex models, without much loss in performance. Hinton et al. (2015a) developed this idea further by minimizing the difference between their soft target distribution. With the rise of the pre-training and fine-tuning paradigm, various work has later extended this idea to large-scale pretrained models and shown impressive results on multiple NLP tasks (Wang et al., 2019;Rajpurkar et al., 2018;Lai et al., 2017) with a significant gain in training efficiency. Sanh et al. (2019) conducted knowledge transfer during the pre-training phase, also known as a task-agnostic way. Sun et al. (2019a) proposed an approach to transfer knowledge between intermediate layers in the fine-tuning stage. Jiao et al. (2020) additionally uses attentionbased distillation and hidden states-based distillation for students to imitate teachers' behaviors in intermediate layers. Wang et al. (2020) introduced self-attention relation-based transfer and teacher assistants (Mirzadeh et al., 2020) to further improve the performance of students. \n\nEnsemble Knowledge Distillation There is also some other work exploring the issues of multiteacher distillation. For example, Du et al. (2020) adaptively ensemble knowledge distillation to find a better optimizing direction for the student network. Wu et al. (2021) designed a co-finetuning framework to jointly finetune multiple teachers for better collaborative knowledge distillation. Li et al. (2021) explored the influence of teacher model adoption which is promising for improving student performance. Different from the above work, we investigate the problem of ensemble knowledge distillation in ranking tasks and use the golden label to supervise the ensemble process.",
            "score": 0.5461559049449125,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1843
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 81,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 187,
                    "end": 208,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 492,
                    "end": 511,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 511,
                    "end": 534,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 534,
                    "end": 551,
                    "matchedPaperCorpusId": "6826032"
                },
                {
                    "start": 714,
                    "end": 732,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 830,
                    "end": 848,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1000,
                    "end": 1018,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 1092,
                    "end": 1115,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1292,
                    "end": 1308,
                    "matchedPaperCorpusId": "227276362"
                },
                {
                    "start": 1554,
                    "end": 1570,
                    "matchedPaperCorpusId": "237605152"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79833984375
        },
        {
            "corpus_id": "253080510",
            "title": "Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation",
            "text": "Knowledge distillation is a method to distill knowledge from large-scale models to small-scale models. Formally speaking, considering an NLP classification task, we have a corpus D = {(x i , y i )} N i=1 that contains N input-output pairs, where x i is an input sentence with tokens \n\nis the vocabulary, n i is the number of tokens in x i . y i is the output label for x i . We use plain texts rather than bold texts for x because language inputs are a sequence of tokens, which is different from images. Then we distill knowledge from a large-scale model f (\u2022, \u03b8 T ) with parameter \u03b8 T (i.e., a teacher model) to a small-scale model g(\u2022, \u03b8 S ) with parameter \u03b8 S (i.e., a student model). In practice, \u03b8 T has much more parameters than \u03b8 S . The distillation process can be divided into two stages: \n\n\u2022 Teacher training. Optimize \u03b8 T on the dataset D. In classification problems, we use cross-entropy loss to do empirical risk minimization on \u03b8 T : \n\n\u2022 Student training. Optimize \u03b8 S on the dataset D with both ground-truth labels and outputs from teachers. In classification problems, \n\nwhere d(\u2022, \u2022) is a distance function. In practice, d(\u2022, \u2022) could be cross-entropy or mean square error. \n\nEmpirical results from former studies (Hinton et al., 2015;Sun et al., 2020b;Sanh et al., 2019;Sun et al., 2019) show that knowledge distillation will train a better \u03b8 S because the student model not only learns from ground-truth labels but also learns the generality from the teacher model. \n\nNote for the student training, we can combine knowledge distillation and data augmentation together: \n\nwhere L Aug denotes the knowledge distillation loss on augmented data which leads to different variants of methods. As one important way to help the student learn more effectively, how to generate new data with augmentation loss is the key and major discussion topic in the remaining sections.",
            "score": 0.5454585637942153,
            "section_title": "KNOWLEDGE DISTILLATION",
            "char_start_offset": 9881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1588
                },
                {
                    "start": 1591,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 1289,
                    "end": 1306,
                    "matchedPaperCorpusId": "201670719"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.841796875
        },
        {
            "corpus_id": "260802774",
            "title": "Efficient RGB-T Tracking via Cross-Modality Distillation",
            "text": "fusion strategy as that in the teacher model [35]. In particular, each layer adopts the same fusion strategy in the student model. It can be seen that the exploitation of the multi-path fusion strategy can well improve the performance of the student model. In addition, the performance gap between 'Student-MPSD' and 'Student-TF' is much smaller, which indicates that our proposed MPSD module can better mimic the fused features in the teacher model to compensate for the performance penalty from simple fusion operations.\n\nTeacher-Student knowledge distillation experiments. Table 4 shows the performance of using some other knowledge distillation methods in the feature extraction and feature fusion stages for comparisons, including KD [8], Fit-Nets [21], ReviewKD [3] and MD [29]. It is observed from Table 4 that the proposed distillation strategy performs the best. Due to the absence of cross-modal interactions, these existing knowledge distillation methods usually achieve some modest performance gains. In addition, we notice that the student model with a single-stream feature extractor performs obviously well than the student model with a two-stream feature extractor after knowledge distillation. This may be due to the fact that the single-stream network can narrow the modality difference to a certain extent and better acquire the knowledge from the teacher model.",
            "score": 0.5454010351111294,
            "section_title": "Ablation Experiments and Analyses",
            "char_start_offset": 25851,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 49,
                    "matchedPaperCorpusId": "249800120"
                },
                {
                    "start": 768,
                    "end": 771,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 779,
                    "end": 783,
                    "matchedPaperCorpusId": "246868573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "277217930",
            "title": "Distributed Collaborative Learning with Representative Knowledge Sharing",
            "text": "Knowledge distillation, a form of transfer learning, facilitates knowledge sharing by compressing complex models into smaller, efficient ones. This technique has proven particularly valuable in large-scale distributed training, as demonstrated by DeepSeek, which integrates knowledge distillation to optimize model convergence and maintain high performance while reducing computational overhead [1]. In a collaborative learning setting, distillation enables nodes to transfer knowledge without sharing raw data or full model parameters, making it a strong alternative to traditional model aggregation in federated learning. \n\nRecent advances have demonstrated the importance of adaptive weighting in knowledge transfer, where confidence-based or similarity-based methods improve the effectiveness of model distillation [14,15]. This aligns with importance sampling techniques in transfer learning and domain adaptation, where distillation weights reflect the reliability of each teacher node [16,17]. Prior works [2,18] emphasize that knowledge transfer effectiveness depends on teacher-student alignment, motivating our approach of using Energy Coefficients for adaptive distillation weighting. \n\nIn federated learning, dynamic model importance weighting has been explored in heterogeneous FL settings [19,20], where similarity-based aggregation improves personalization. \n\nA notable related work is FedED (Federated Ensemble Distillation) [21], which transfers knowledge via ensembles of local models instead of a single teacher-student relationship. While FedED aggregates predictions from multiple clients, our method personalizes distillation weights using Energy Coefficients, ensuring knowledge transfer is selectively tailored to each node's task distribution. This distinction is particularly critical in heterogeneous settings where generalization beyond local training data is essential. \n\nHinton et al. [2] first introduced this concept, demonstrating how teacher-student frameworks could retain high performance. Subsequent works refined this approach: Passalis and Tefas [22] matched probability distributions of teacher and student models in feature space, Huang and Wang [23] introduced loss functions minimizing distributional discrepancies, and Tian et al. [18] incorporated contrastive learning to align teacher-student representations effectively. \n\nIn distributed settings, the use of representative datasets provides a practical alternative to raw data or parameter sharing. Li and Yang [24] constructed representative data points for large datasets to enable efficient training and alignment.",
            "score": 0.5451661738884008,
            "section_title": "Knowledge Distillation and Representative Datasets",
            "char_start_offset": 6845,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1898
                },
                {
                    "start": 1901,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2367
                },
                {
                    "start": 2370,
                    "end": 2496
                },
                {
                    "start": 2497,
                    "end": 2615
                }
            ],
            "ref_mentions": [
                {
                    "start": 823,
                    "end": 826,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 996,
                    "end": 999,
                    "matchedPaperCorpusId": "740063"
                },
                {
                    "start": 1303,
                    "end": 1307,
                    "matchedPaperCorpusId": "235446706"
                },
                {
                    "start": 1441,
                    "end": 1445,
                    "matchedPaperCorpusId": "226262346"
                },
                {
                    "start": 2085,
                    "end": 2089,
                    "matchedPaperCorpusId": "52012952"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9248046875
        },
        {
            "corpus_id": "268032671",
            "title": "m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers",
            "text": "Knowledge distillation. Knowledge distillation is a model compression technique that transfers knowledge from a large teacher network to a smaller student model [11]. Common knowledge distillation approaches involve mimicking the teacher model's softened output [1,11,16]. In scenarios with multiple teachers, the outputs of all teacher models can be averaged [11], or other methods can be used [5,19,41]. Traditional knowledge distillation methods often struggle when a significant capacity gap exists between the student and the teacher models. Recent approaches have attempted to address this gap by using teacher assistants [25,35]. In addition, some existing works conduct distillation at the module level instead of distilling entire models [23,39,43]. While previous works primarily focus on compressing model sizes within the context of monolithic models, we propose a module-to-module distillation technique to enhance the performance of modular architectures, especially in unconventional distillation scenarios. Particularly, the distillation of modules is mutually independent and can be executed in a distributed manner. \n\nModularization. Modular deep learning involves decomposing neural architectures into independent and parameter-efficient modules, where samples are conditionally routed to subsets of modules, and their outputs are aggregated. Modularization has been widely applied in various areas such as transfer learning [12,13,21,30], modular training [27], and scaling up model size [15,20]. \n\nIn transfer learning, pre-trained models are often used as modules in an assembled model for new tasks. This assembled model can adapt to new data by adjusting or adding modules to enhance performance in different scenarios. For instance, Brown et al. [2] added a prompt module to the input of a pre-trained model, while Rebuffi et al. [31] introduced adapter modules into the model architecture. Modular training approaches like Deep Incubation [27] incubate modules in individual nodes to avoid communication overhead and accelerate convergence. Conditional computation enables the scaling of model size while maintaining inference complexity. V-MoE [15] scales up vision models with only half the computation required during inference. The MoE framework introduces experts to enable modularization at the FFN layer.",
            "score": 0.545007099108101,
            "section_title": "Related Work",
            "char_start_offset": 5289,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1133
                },
                {
                    "start": 1136,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1516
                },
                {
                    "start": 1519,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2257
                },
                {
                    "start": 2258,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 262,
                    "end": 265,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 268,
                    "end": 271,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 398,
                    "end": 401,
                    "matchedPaperCorpusId": "216265175"
                },
                {
                    "start": 401,
                    "end": 404,
                    "matchedPaperCorpusId": "228376532"
                },
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 632,
                    "end": 635,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 747,
                    "end": 751,
                    "matchedPaperCorpusId": "263830428"
                },
                {
                    "start": 1444,
                    "end": 1448,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 1476,
                    "end": 1480,
                    "matchedPaperCorpusId": "257557698"
                },
                {
                    "start": 1508,
                    "end": 1512,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 1771,
                    "end": 1774,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1855,
                    "end": 1859,
                    "matchedPaperCorpusId": "215822736"
                },
                {
                    "start": 1965,
                    "end": 1969,
                    "matchedPaperCorpusId": "257557698"
                },
                {
                    "start": 2171,
                    "end": 2175,
                    "matchedPaperCorpusId": "231879586"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81396484375
        },
        {
            "corpus_id": "270878760",
            "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
            "text": "This facilitates easier calculation.This method was evaluated on four datasets: DART for data-to-text generation [92], XSum for summarization [93], WMT16 EN-RO for machine translation [11], and Commonsense Dialogue [167].The experiments demonstrated that  -DISTILL variants outperformed existing distribution-matching KD methods, leading to performance improvements when combined with representation-matching KD methods.Additionally, the results indicated that symmetric distillation loss is superior to asymmetric distillation loss, confirming that extreme mode averaging or collapse is suboptimal.MiniMA [161] found that the optimal distillation effect occurs when the student model is approximately 40% the size of the teacher model.It combines structured pruning with logit-based knowledge distillation, using LLaMA2-7B [130] as the teacher model to train the 3B MiniMA model.The results showed that MiniMA achieved impressive performance in knowledge, reasoning, and encoding, while using a similar or even fewer number of tokens than the teacher model.",
            "score": 0.5447200042439968,
            "section_title": "Logits-based KD.",
            "char_start_offset": 15407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 36,
                    "end": 221
                },
                {
                    "start": 221,
                    "end": 599
                },
                {
                    "start": 599,
                    "end": 736
                },
                {
                    "start": 736,
                    "end": 880
                },
                {
                    "start": 880,
                    "end": 1058
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 117,
                    "matchedPaperCorpusId": "220364230"
                },
                {
                    "start": 142,
                    "end": 146,
                    "matchedPaperCorpusId": "215768182"
                },
                {
                    "start": 215,
                    "end": 220,
                    "matchedPaperCorpusId": "237099281"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "258480404",
            "title": "Minimal Distillation Schedule for Extreme Language Model Compression",
            "text": "Pretrained language models (LMs) (Devlin et al., 2019;Liu et al., 2019;Radford et al., 2019;Brown et al., 2020;Raffel et al., 2020) have achieved promising results in various downstream tasks (Wang et al., 2019;Rajpurkar et al., 2018), but are inapplicable to those requiring limited computational resources (Liu et al., 2021b). To address * Dawei Song is the corresponding author. In the study, a BERT base model is used as the teacher and distilled to a pruned student (10% parameters of the teacher) via different teacher assistants (Mirzadeh et al., 2020) on MRPC and QQP. There are several observations: (1) The blue curve shows that the performance of the teacher assistant degrades with the decreasing of its scale, which is obvious. \n\n(2) The green curve validates that the performance of the student varies with different teacher assistants. \n\n(3) The red curve represents \u03bb-tradeoff of the teacher assistant, which is positively correlated with the performance of the student. \n\nthis issue, LMs can be compressed using a range of strategies such as model quantization (Zafrir et al., 2019;Bai et al., 2021), pruning (Michel et al., 2019;Hou et al., 2020), etc., among which knowledge distillation (Sun et al., 2019;Wang et al., 2020) has gained significant attention. It operates within the teacher-student framework, where a large model acts as the teacher, transferring its knowledge to a smaller student model. Recent advances (Mirzadeh et al., 2020) have shown a significant performance decline in conventional distillation methods when dealing with a substantial capacity gap between the teacher and the student models. To alleviate this, teacher assistantbased distillation (Son et al., 2021) has been proposed. This approach involves distilling the teacher model into an intermediate-scale teacher assistant, which then serves as an intermediary to transfer knowledge to the student model.",
            "score": 0.5440510422601199,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1906
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 54,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 92,
                    "end": 111,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 111,
                    "end": 131,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 192,
                    "end": 211,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 211,
                    "end": 234,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 536,
                    "end": 559,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1078,
                    "end": 1099,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 1099,
                    "end": 1116,
                    "matchedPaperCorpusId": "229923538"
                },
                {
                    "start": 1126,
                    "end": 1147,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 1147,
                    "end": 1164,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 1207,
                    "end": 1225,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 1225,
                    "end": 1243,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 1440,
                    "end": 1463,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1690,
                    "end": 1708,
                    "matchedPaperCorpusId": "221802641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64599609375
        },
        {
            "corpus_id": "276782339",
            "title": "Targeted Distillation for Sentiment Analysis",
            "text": "Effect of Teacher LLMs. We experiment with different teacher LLMs in our distillation framework to analyze their impact. The results in Table 7 reveal that teacher quality significantly influences distillation effectiveness, as larger teacher LLMs generally lead to more substantial improvements. Furthermore, we make two noteworthy observations. First, even when using identical models for both teacher and student, distillation has the potential to enhance the student's sentiment analysis performance. This result suggests the potential for leveraging distillation to achieve self-improvement in specialized domains. Second, higher teacher quality does not always correlate with better student performance, as evidenced by the FSA performance when using 8B and 70B models as teachers. This may be related to the capability gap between teachers and students, warranting further exploration in future work. Results on MMLU. A potential concern of targeted distillation towards specialized capabilities is the possible degradation of the model's general abilities. To investigate this concern, we conduct evaluations on the Massive Multitask Language Understanding (MMLU) benchmark (Hendrycks et al., 2021). As shown in Table 8, we find that our distillation approach not only avoids any deteriora-tion but also results in a slight improvement. This indicates that our distillation approach can enhance specialized capabilities without compromising general capabilities.",
            "score": 0.5438146000500745,
            "section_title": "Discussions",
            "char_start_offset": 21255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1470
                }
            ],
            "ref_mentions": [
                {
                    "start": 1182,
                    "end": 1206,
                    "matchedPaperCorpusId": "221516475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86376953125
        },
        {
            "corpus_id": "264516404",
            "title": "Attention and feature transfer based knowledge distillation",
            "text": "The concept of knowledge distillation (KD) was proposed by Hinton et al. 21 , which forced the student network to extract knowledge from the soft labels and ground truth labels provided by teachers. In order to make full use of the \"dark knowledge\" contained in soft labels, the concept of temperature was introduced. The existing KD methods can be mainly divided into three types: logic-based 20,21,[31][32][33][34] , feature-based 18,[22][23][24][25][26][27][28][29]35 , and attention maps-based 19,30 . \n\nLogic distillation transfers the knowledge implicit in the output logic of the teacher model to the student network. BAN 32 obtained superior performance to the teacher model by directing the same parameterized network as the teacher. DKD 20 reformulates KD loss into target-class knowledge distillation (TCKD) and non-target-class knowledge distillation (NCKD), revealing that KD's coupling formula limits the effectiveness and flexibility of knowledge transfer. CrossKD 34 passes intermediate features of the student network to the teacher's detection head, resulting in cross predictions, which are then forced to mimic the teacher's predictions. In addition, there are several articles on logical distillation methods 21,33,34 . \n\nFeature-based KD methods tend to have better performance, forcing students to extract valid content from intermediate features of the teacher network at the cost of requiring more computation than logical distillation. RKD 25 can transform the relationship of data examples to punish differences in teacher and student relevance, similar to the transfer of sample relevance studies from teacher and student networks 26,27 . PKT 35 models the teacher's knowledge as a probability distribution and uses KL divergence to measure distance. RKD 25 uses multicase relationships to guide students' learning. CRD 22 combines comparative learning with knowledge distillation, and uses comparative objectives to carry out knowledge transfer. ReviewKD 18 uses cross-layer connection paths to integrate the knowledge implied by features at different levels. \n\nKD method based on attention diagram instructs students what information the network should pay attention to in reasoning.",
            "score": 0.5435253318593065,
            "section_title": "Related work",
            "char_start_offset": 2593,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2088
                },
                {
                    "start": 2091,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 397,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 400,
                    "end": 404,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 433,
                    "end": 436,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 440,
                    "end": 444,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "232232777"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "229220499"
                },
                {
                    "start": 498,
                    "end": 501,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 629,
                    "end": 631,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 747,
                    "end": 749,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1233,
                    "end": 1236,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1466,
                    "end": 1468,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1659,
                    "end": 1662,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1662,
                    "end": 1664,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1783,
                    "end": 1785,
                    "matchedPaperCorpusId": "131765296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79833984375
        },
        {
            "corpus_id": "271720097",
            "title": "Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations",
            "text": "They found a lack of available small-scale distilling datasets, and available datasets need to provide more diversity to produce a high-quality model. Moreover, student evaluation focuses too narrowly on one model family, and the evaluation process needs to be more precise. To improve dataset diversity, a new distilling dataset is proposed. Furthermore, trained models on this dataset are thoroughly evaluated using a comprehensive testing methodology. \n\nChallenges. There are significant challenges that impact distillation quality. In order to achieve more students with the teacher's abilities, the reduction of the distribution mismatch is actively investigated [61] [66]. The difficulty in reduction could stem from the lack of publicly available diverse distilling datasets [65]. This results in either manually creating or augmenting existing datasets. However, the diversity and detail of current datasets could also be a challenge in distilling capable students [65]. Also, reaching specific quality aspects for the student is challenging. The authors of [66] found that it is challenging distilling a student that has similar distribution and expressive power to the teacher. One possible reason investigated [78], which criticized that the student can only be as good as the used teacher. That means that the student learns hallucinations, bias, and toxicity behavior already present in the teacher. If only Black-Box Knowledge Distillation is available, the risk of only imitating the teacher's style without actually learning the behavior is present [78]. \n\nPractical Constraints. In many knowledge distillation strategies, a student-teacher setup is deployed. This results in two models loaded into memory during white-box and local black-box distillation, which requires a memory-potent environment [61] [66] [67]. Running two models is costly because computing and memory are expensive. These costs also apply to APIs, where memory costs are factored into API usage costs. Therefore, it has to be considered if fine-tuning is needed given the resource requirements and if it is preferred over simple optimization techniques like quantization. \n\nIn Conclusion. \n\n\u2022 Knowledge distillation is a technique for fine-tuning and compressing models, with the goal of transferring knowledge from larger models to smaller ones.",
            "score": 0.5434322261974724,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 30741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2160
                },
                {
                    "start": 2163,
                    "end": 2177
                },
                {
                    "start": 2180,
                    "end": 2335
                }
            ],
            "ref_mentions": [
                {
                    "start": 668,
                    "end": 672,
                    "matchedPaperCorpusId": "259164722"
                },
                {
                    "start": 782,
                    "end": 786,
                    "matchedPaperCorpusId": "258352678"
                },
                {
                    "start": 973,
                    "end": 977,
                    "matchedPaperCorpusId": "258352678"
                },
                {
                    "start": 1221,
                    "end": 1225,
                    "matchedPaperCorpusId": "258887629"
                },
                {
                    "start": 1565,
                    "end": 1569,
                    "matchedPaperCorpusId": "258887629"
                },
                {
                    "start": 1816,
                    "end": 1820,
                    "matchedPaperCorpusId": "259164722"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68603515625
        },
        {
            "corpus_id": "271441660",
            "title": "Knowledge Distillation in Image Classification: The Impact of Datasets",
            "text": "Several works have explored various aspects of knowledge distillation in deep neural networks [26], including teacher and student model architectures, regularization techniques, and optimization methods. \n\nFor example, Li et al. proposed a transferred attention method to improve the performance of convolutional neural networks [27], while Yazdanbakhsh et al. studied the application of knowledge distillation in specific domains such as healthcare [19]. However, despite these significant advances, little attention has been paid to the impact of data on this knowledge transfer process. The authors demonstrated the effectiveness of the distillation on various tasks and highlighted its potential for model compression. The FitNets paper [20] proposed a specific form of knowledge distillation called FitNets, where a student network is guided not only by the output probabilities of a teacher network but also by intermediate representations (or hints). This work aimed to improve the transfer of information in the training process. Ref. [27] introduces attention transfer as a form of knowledge distillation. It focuses on transferring attention maps from a teacher to a student network to improve the student's performance. Attention transfer has proven effective in enhancing the generalization capabilities of the student model. To address the limitations of traditional knowledge distillation, ref. [31] introduces Jacobian matching, a novel method that aims to transfer not only the output probabilities but also the derivatives of the teacher model's predictions. This approach provides a more comprehensive form of knowledge transfer. Ref. [30] explores the benefits of knowledge distillation beyond model compression. The authors show that the knowledge distillation process not only compresses models but also accelerates the optimization process, enabling faster convergence during training. Ref. [32] introduces the concept of a \"teacher assistant\" by proposing an extension to traditional knowledge distillation. The teacher assistant helps bridge the performance gap between the teacher and the student, leading to enhanced knowledge transfer.",
            "score": 0.5433954586082324,
            "section_title": "Knowledge Distillation in the Literature",
            "char_start_offset": 6239,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 206,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 98,
                    "matchedPaperCorpusId": "249209742"
                },
                {
                    "start": 450,
                    "end": 454,
                    "matchedPaperCorpusId": "259389158"
                },
                {
                    "start": 1409,
                    "end": 1413,
                    "matchedPaperCorpusId": "3603145"
                },
                {
                    "start": 1653,
                    "end": 1657,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1913,
                    "end": 1917,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93115234375
        },
        {
            "corpus_id": "259378480",
            "title": "Review of Recent Distillation Studies",
            "text": "The use of Simple Knowledge Distillation (SimKD) allows the compression of powerful teacher models into lightweight student models without sacrificing performance. This is achieved by reusing the discriminative classifier of the teacher model for student inference and training the student encoder by feature alignment with a single loss. A projector was also developed to help match the student encoder with the teacher classifier, making the technique applicable to a variety of teacher and student architectures. Experiments show that with the addition of the projector, the technique achieves state-of-the-art results at the cost of a reduced compression ratio. Vanilla KD aligns the logit or class projections of the two models, but the performance gap between the original teacher model and the refined student model is still large. Various methods have been proposed to overcome this problem, but they require detailed knowledge representation and optimized hyperparameters. The SimKD technique is proposed, which trains the student model by feature alignment in the previous layer of the classifier and directly replicates the teacher classifier for student inference. On a standard benchmark dataset, SimKD outperforms all state-of-the-art methods compared with various combinations of teacher-student architectures. The concept of knowledge distillation (KD), a technique for compressing knowledge from a powerful teacher model into a smaller student model, is discussed here. Transferred knowledge, usually in the form of soft targets, is thought to capture the relationships between different categories and serve as effective regularization during student training. Feature distillation is a common solution to prevent performance degradation in teacherto-student compression, which utilizes more information from the middle layer of the teacher model. One of the proposed methods, SimKD, is related to hypothetical transfer learning (HTL) and aims to close the performance gap between teachers and students on the same dataset. Here, by analyzing the limitations of the existing method and focusing on its improvement, a new method, logit distillation, is proposed, and the knowledge distillation loss function is re-represented as a weighted sum of two components, target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD).",
            "score": 0.5426898072940937,
            "section_title": "Simple knowledge distillation",
            "char_start_offset": 36280,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2374
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87939453125
        },
        {
            "corpus_id": "273691083",
            "title": "Larger models yield better results? Streamlined severity classification of ADHD-related concerns using BERT-based knowledge distillation",
            "text": "Knowledge distillation process between a teacher model (BERT Large Uncased) and a student model (BERT Base Uncased/LastBERT) using the WikiText-2-raw transfer set. The teacher model, pre-trained on masked language modeling (MLM), generates soft labels by applying a softmax function at a high temperature T . The student model learns by mimicking these soft predictions and also computing hard predictions against the ground truth labels. The training process minimizes a weighted sum of the distillation loss (between teacher and student soft predictions) and the student loss (between student predictions and ground truth). The hyperparameter \u03b1 balances the contributions of these two losses. This setup enables the student model to retain performance comparable to the teacher while significantly reducing the model size and computational cost. The detailed top-level overview of our approach is presented in Fig 2.",
            "score": 0.5426660843713802,
            "section_title": "Top-level overview of the proposed knowledge distillation approach between student and teacher BERT models",
            "char_start_offset": 22395,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 918
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84423828125
        },
        {
            "corpus_id": "258567300",
            "title": "Inherit With Distillation and Evolve With Contrast: Exploring Class Incremental Semantic Segmentation Without Exemplar Memory",
            "text": "In deep learning, knowledge distillation (KD) aims to transfer information learned from one model to another whilst training constructively. KD was first defined by [74] and generalized by [6]. The common characteristic of KD is symbolized by its Student-Teacher (S-T) framework [75]. In recent years, KD has been applied in model compression [76], [77], knowledge transfer [78], [79] and semantic segmentation [80]. And for dense prediction tasks, knowledge distillation has been demonstrated feasible and effective [75]. For example, pixel-wise similarity distillation [81] and channel-wise distillation [82] are proposed to improve the distillation efficiency. Wang et al. [83] propose to transfer the intra-class feature variation from teacher to student by constructing densely pairwise relations. In IL tasks, Zhou et al. [84] propose a multi-level knowledge distillation strategy by leveraging all previous model snapshots. In CISS, KD has been proven as an effective way to preserve the capability of classifying old classes without storing past data in incremental steps. A typical KD approach is to use the outputs from the old model to guide the new model by a distillation loss [20]. Further, Michieli et al. [23] explores distillation in intermediate feature space and indicates that L2-norm is superior to cross-entropy or L1-norm. Qiu et al. [85] use self-attention to capture both within-class and between-class knowledge.",
            "score": 0.5425234832648212,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 9859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1438
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 279,
                    "end": 283,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 343,
                    "end": 347,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 349,
                    "end": 353,
                    "matchedPaperCorpusId": "204788964"
                },
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 411,
                    "end": 415,
                    "matchedPaperCorpusId": "208109903"
                },
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 571,
                    "end": 575,
                    "matchedPaperCorpusId": "235241865"
                },
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "236882796"
                },
                {
                    "start": 676,
                    "end": 680,
                    "matchedPaperCorpusId": "226292143"
                },
                {
                    "start": 1190,
                    "end": 1194,
                    "matchedPaperCorpusId": "211011110"
                },
                {
                    "start": 1221,
                    "end": 1225,
                    "matchedPaperCorpusId": "199000946"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88134765625
        },
        {
            "corpus_id": "267657497",
            "title": "Maximizing discrimination capability of knowledge distillation with energy function",
            "text": "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks. Over the years, KD methods have evolved to narrow the performance gap between student and teacher models by utilizing both final predictions, known as logits-based distillation [10,15,16,11,17], and intermediate features, known as features-based distillation [18,19,20,21,22,23,24,12,25,26,27,28]. \n\nPrevious works on logits-based distillations include the following: DML [15] proposed a mutual learning strategy for collaboratively teaching and learning between student and teacher models; TAKD [16] introduced a multi-step method with an intermediate-size network (i.e., assistant network) to bridge the gap between teachers and students; DKD [11] decomposed the soft-label distillation loss into two components: target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD), enabling each part to independently harness its effectiveness; Multi KD [17] proposed multi-level prediction alignment, containing instance, batch, and class levels, and prediction augmentation. While these approaches emphasize effective knowledge transfer, they do not consider dividing entire datasets or provide mechanisms to distinguish and transfer knowledge from specific samples. \n\nFitNet [18] was groundbreaking as it leveraged not only the final outputs but also intermediate representations. Since the introduction of FitNet, various feature-based KD methods have emerged as follows: AT [22] prompted the student to mimic the attention map of the teacher network; PKT [19] employed various kernels to estimate the probability distributions, employing different divergence metrics for distillation; RKD [20] focused on transferring the mutual relations of data samples; CRD [21] framed the objective as contrastive learning for distillation; VID [23] took a different approach by maximizing mutual information; OFD [24] introduced a novel loss function incorporating teacher transform and a new distance function; Review KD [12] introduced a review mechanism that leverages past features for guiding current ones through residual learning.",
            "score": 0.5424806733062693,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 3764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 366,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 439,
                    "end": 442,
                    "matchedPaperCorpusId": "219169868"
                },
                {
                    "start": 442,
                    "end": 445,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 451,
                    "end": 454,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 454,
                    "end": 457,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 457,
                    "end": 460,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 460,
                    "end": 463,
                    "matchedPaperCorpusId": "258298441"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "269167845"
                },
                {
                    "start": 469,
                    "end": 472,
                    "matchedPaperCorpusId": "269206209"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1058,
                    "end": 1062,
                    "matchedPaperCorpusId": "260933721"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "271227251",
            "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models",
            "text": "2) Feature-based Knowledge Distillation: Feature-based knowledge distillation taps into the strength of deep neural networks to learn hierarchical feature representations, a process central to representation learning [120]. Unlike responsebased knowledge distillation, which focuses on the outputs of the last layer, feature-based distillation utilizes the outputs from intermediate layers, or feature maps, to guide the student model. This approach is particularly beneficial for training models that are both narrower and deeper, as it provides a richer set of training signals. \n\nThe concept was first introduced with Fitnets [49], aiming to improve student model training by matching feature activations between the teacher and student directly. Following this, several methodologies have been developed to facilitate this matching process, either directly or indirectly [121]. Notable contributions include the derivation of \"attention maps\" to express the use of neuron selectivity transfer [122], matching probability distributions in feature space [123], and introducing \"factors\" for more interpretable intermediate representations [54]. Techniques like route constrained hint learning [124] and the use of activation boundaries [125] have been proposed to minimize the performance gap between teacher and student models, alongside innovative strategies like cross-layer knowledge distillation [121] which adaptively matches teacher and student layers. \n\nDespite the effectiveness of feature-based knowledge transfer in enriching the student model's learning, challenges remain in selecting appropriate layers for hints and guidance due to the size discrepancies between teacher and student models. This necessitates further exploration into how best to match the feature representations between teacher and student models effectively. \n\n3) Relation-based Knowledge Distillation: Relation-based knowledge distillation goes beyond the scope of responsebased and feature-based methods by examining the relationships between different layers or data samples within the teacher model. This approach delves into the dynamics between feature maps, layers, and even the relationships between different teachers or data samples, offering a more nuanced form of knowledge transfer. \n\nFlow of solution process (FSP) [53] utilizes the Gram matrix between two layers to encapsulate the relationships between pairs of feature maps through inner product calculations. Knowledge distillation via singular value decomposition [126] distill essential information from these relationships.",
            "score": 0.5424488063617605,
            "section_title": "A. Knowledge Distillation",
            "char_start_offset": 16933,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1844
                },
                {
                    "start": 1847,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2281
                },
                {
                    "start": 2284,
                    "end": 2462
                },
                {
                    "start": 2463,
                    "end": 2580
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 222,
                    "matchedPaperCorpusId": "393948"
                },
                {
                    "start": 875,
                    "end": 880,
                    "matchedPaperCorpusId": "227335337"
                },
                {
                    "start": 1056,
                    "end": 1061,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 1141,
                    "end": 1145,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 1195,
                    "end": 1200,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 1238,
                    "end": 1243,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1403,
                    "end": 1408,
                    "matchedPaperCorpusId": "227335337"
                },
                {
                    "start": 2315,
                    "end": 2319,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9111328125
        },
        {
            "corpus_id": "268650296",
            "title": "Comparative analysis of strategies of knowledge distillation on BERT for text matching",
            "text": "With the proposal of the pre-training model Bidirectional Encoder Representations from Transformers (BERT), its effectiveness on NLP has been constantly refreshed [1]. However, the huge overhead of space and time associated with its large number of parameters limits its application to downstream tasks. Based on this, there is a desire to find a smaller BERT model that takes into account the capabilities of BERT and can have a smaller size. Combining the previously proposed idea of distillation, knowledge distillation and BERT are combined to obtain a desired model. \n\nKnowledge distillation as a model compression technique consists of two models. The former is named teacher model. It has larger volume and is more capable. The latter one is called student model, which has small number of parameters. During training, the student model is allowed to learn to mimic the teacher's behaviour to learn the pre-trained teacher model's capability. The loss in the distillation process consists of the cross-entropy of the predicted probability of the two models and the cross-entropy of the student model and the true value [2,3]. BERT distillation that is, the parameters and volume of the BERT shrunk by the distillation method under the condition of ensuring that the model ability does not lose too much.",
            "score": 0.5420521784113237,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1310
                }
            ],
            "ref_mentions": [
                {
                    "start": 1126,
                    "end": 1129,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1129,
                    "end": 1131,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8095703125
        },
        {
            "corpus_id": "273812187",
            "title": "On the Impact of White-box Deployment Strategies for Edge AI on Latency and Model Performance",
            "text": "Motivation This question aims to empirically assess the latency of the Distilled operator and compare it with the Identity, SPTQ, QAT, and Pruned operators in the corresponding tiers (i.e., mobile, edge, and cloud). The accuracy of the Distilled operator is validated in the monolithic tiers and compared with the Identity, SPTQ, QAT, and pruned operators to assess the performance deviation. \n\nApproach In our study, the distillation process involves replicating the behavior of a teacher model (larger and complex architecture) by training a student model (smaller and simpler architecture) to achieve accuracy comparable to the teacher model. The student model is often designed to have a similar architecture to the teacher model. The Knowledge Distillation was performed using the INC tool from teacher to student model in Pytorch format due to its support for distillation. The generated distilled student models were then exported to ONNX format for measuring the latency and accuracy results. Table 3 shows the size and parameters of the teacher and the student models considered for knowledge distillation. Both the student and parent models contain pretrained weights. These pretrained weights provide the student model with foundational knowledge about the domain and allow quicker convergence during knowledge distillation. This helps in improving the performance during knowledge distillation as the representations learned by both student and teacher models are more likely to be compatible, making it easier for the student model to mimic the teacher's knowledge. Considering that pretrained weights of the student model are quite common in scenarios when their architecture is similar to the parent model, the training hyper-parameters listed in the INC repository 1617 again were considered. For the distillation of ResNet and ResNext subject models, distributed training with multiple GPUs (i.e., 8 GPUs) again was used. In contrast, for the distillation of Bert and Roberta subject models, a single CPU was enough to provide quick training. Approach For the DSPTQ operator, the distillation process was performed on the subject models in Pytorch format using INC as explained in the RQ3 approach.",
            "score": 0.5418127156360181,
            "section_title": "RQ3:What is the impact of the Knowledge Distillation-based white box operator on latency and accuracy?",
            "char_start_offset": 39953,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2215
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76708984375
        },
        {
            "corpus_id": "269982742",
            "title": "GIFT: Unlocking Full Potential of Labels in Distilled Dataset at Near-zero Cost",
            "text": "A straightforward approach to combining soft labels and hard labels is knowledge distillation (Hinton et al., 2015), which transfers knowledge from a teacher model to a student model using hard labels (cross-entropy loss) and soft labels provided by a strong teacher model (KL divergence loss). In Table 5 , we compare our proposed method, GIFT, with the state-of-the-art knowledge distillation techniques across synthetic datasets distilled via RDED (Sun et al., 2024) 6 . \n\nIt can be observed that GIFT outperforms all knowledge distillation methods. We attribute the failure of knowledge distillation methods to the extremely small size of synthetic datasets, which significantly hampers the performance of knowledge distillation methods, as corroborated by (Stanton et al., 2021). Therefore, knowledge distillation is not well-suited for our problem, further highlighting the effectiveness of GIFT.",
            "score": 0.5413123534368991,
            "section_title": "CAN KNOWLEDGE DISTILLATION WORK?",
            "char_start_offset": 21556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 902
                }
            ],
            "ref_mentions": [
                {
                    "start": 451,
                    "end": 469,
                    "matchedPaperCorpusId": "265694675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85302734375
        },
        {
            "corpus_id": "258486778",
            "title": "A First Look at the Impact of Distillation Hyper-Parameters in Federated Knowledge Distillation",
            "text": "Knowledge Distillation (KD) is a technique that was initially proposed for model compression, where a larger teacher model is used to train a more compact student model. A smaller student model learns a high-fidelity representation of a larger teacher model through the teacher's output (soft targets) [4,10,23]. The teacher is usually pre-trained while the student model learns by mimicking the teacher's soft targets on the same training set or a separate transfer set. \n\nBesides model compression settings that employ offline distillation (i.e., from a trained teacher to an untrained student), knowledge distillation has been recently employed to transfer knowledge between already pre-trained models in distributed training settings [5, 11-13, 16, 17, 22]. These models might be trained using different data samples, might have diverse performances resulting from heterogeneity, including statistical heterogeneity (i.e., non-IID data distributions), where participants typically have different distributions and quantities of local data, and system heterogeneity, where participants might have different amounts of bandwidth and computational power. Learning across these systems and unbalanced datasets under those constraints can be challenging. We call knowledge distillation between already pre-trained models \"joint distillation. \" \n\nJoint distillation can be useful for distributed training and federated learning (FL) and can deliver great advantages, including (1) significant communication reduction [12,16,21,22], and (2) model architecture flexibility [5,11,13,17], as participants can pick the model architecture that suits their capabilities. The communication reduction acquired from knowledge distillation can be obtained either by communicating the pre-trained models only once instead of communicating the models updates at every round [9,16], or by communicating the model outputs only, which have negligible size compared to the model updates. This was found to reduce the communication overheads by up to 99% while achieving similar model performance relative to a FL benchmark, even with non-IID data distributions [12]. However, joint distillation is still not well studied and understood in the literature, hindering the possible gains.",
            "score": 0.5400713008791259,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1342
                },
                {
                    "start": 1345,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2265
                }
            ],
            "ref_mentions": [
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1515,
                    "end": 1519,
                    "matchedPaperCorpusId": "221135969"
                },
                {
                    "start": 1519,
                    "end": 1522,
                    "matchedPaperCorpusId": "229371278"
                },
                {
                    "start": 1522,
                    "end": 1525,
                    "matchedPaperCorpusId": "231801970"
                },
                {
                    "start": 1525,
                    "end": 1528,
                    "matchedPaperCorpusId": "236330611"
                },
                {
                    "start": 1572,
                    "end": 1575,
                    "matchedPaperCorpusId": "233313136"
                },
                {
                    "start": 1862,
                    "end": 1865,
                    "matchedPaperCorpusId": "229371278"
                },
                {
                    "start": 2142,
                    "end": 2146,
                    "matchedPaperCorpusId": "221135969"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84326171875
        },
        {
            "corpus_id": "267312283",
            "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
            "text": "Knowledge distillation (KD) is a technique that transfers useful knowledge from a large teacher model T to a small student model S to improve the accuracy of S. This process encourages the student model to generate outputs that are similar to that of the teacher model to obtain the teacher model's generalized knowledge. KD matches the outputs of classifiers, embedding layers, and sublayers in T and S to distill diverse types of knowledge within different distillation sources. We explain diverse types of distillation sources used for KD in Section 5.1.1. \n\nWhen we apply KD for language models, we need to consider how to match sublayers in T and S since they have different numbers of sublayers. We categorize the sublayer matching strategies into three groups: 1:1, many:1, and many:many strategies. We explain more details about each of the sublayer matching strategies in Section 5.1.2. \n\nWe summarize the performance of diverse KD algorithms for encoder-only Transformers in Table 6. We denote the distillation sources and the sublayer-matching strategies of each KD algorithm in columns 2 and 3, respectively. We Table 6. Comparison of the performance of knowledge distillation (KD) algorithms for encoder-only Transformers on MNLI, QQP, and SQuAD 1.1 (SQD) benchmarks.",
            "score": 0.539894395702395,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 53658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1280
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "273811396",
            "title": "Decoupling Dark Knowledge via Block-Wise Logit Distillation for Feature-Level Alignment",
            "text": "Another failure scenario for traditional knowledge distillation arises when there is a significant disparity in capacity between the student and teacher, particularly when \u03b1 st is minimal. Leveraging their insights and analytical framework, let us introduce an intermediate stepping-stone model, donated as f n , between the student and teacher: \n\nand subsequently, this stepping-stone model assimilates knowledge from the teacher: \n\nHere, \u03b1 sn , \u03f5 sn , \u03b1 nt , and \u03f5 nt are defined to elucidate the learning dynamics within this tripartite framework. It is pertinent to note that the incorporation of an intermediary simplifies the learning process, whether it is the student learning from \n\nAdditionally, under the assumptions made by [16], \u03f5 nt +\u03f5 sn \u2264 \u03f5 st , we have, \n\nThese relationships corroborate that the upper bound of error with a stepping-stone model is smaller than in vanilla knowledge distillation. In scenarios characterized by significant performance disparities between the student and the teacher, the integration of a stepping-stone model intermediary effectively partitions the modest \u03b1 st into two more tractable components, \u03b1 sn and \u03b1 nt . This strategic division substantially enhances the efficiency of the knowledge distillation process, optimizing the learning trajectory for the student. \n\nMoreover, the principles underlying Block-KD's branch structure are reminiscent of self-distillation, and its benefits can be understood through a multi-view hypothesis [2]. Introducing L cross , which averages votes during the training process, often leads to more effective teaching labels. This approach aligns with constructing an output ensemble [51]. \n\nDuring training, it can yield performance that surpasses even the initial teacher model, thereby elevating the overall framework's upper bound. These observations generally align with empirical experiments [51].",
            "score": 0.5395548176118918,
            "section_title": "IV. THEORETICAL ANALYSIS",
            "char_start_offset": 18905,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 345
                },
                {
                    "start": 348,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 1669,
                    "end": 1673,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 1883,
                    "end": 1887,
                    "matchedPaperCorpusId": "215745611"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58349609375
        },
        {
            "corpus_id": "272827040",
            "title": "Simple Unsupervised Knowledge Distillation With Space Similarity",
            "text": "Early solutions for distillation have been designed for a fully supervised teacher and leverage the output space of the teacher model. Soft-label distillation [29] is amongst the first work towards training smaller networks with guidance from larger teacher networks. Apart from supervised loss from the ground-truth labels, it minimises cross-entropy between the teacher's and the student's output logit distribution. Many methods since then have utilised the output logits to improve the knowledge distillation performance [10,34,43,71,78]. \n\nHuang et al. [31] propose a correlation based formulation which is very similar to ours. The key difference apart from the teacher (supervised vs. unsupervised) is that they normalise the logits (via. softmax) and then compute inter and intra-class similarities. Whereas, we independently normalize spatial and feature dimensions of the embedding features. From the perspective of computing intraclass similarity, it is logical to apply the softmax beforehand for generating classwise scores, however, when operating on the embedding space, any normalisation on the features alters the space information as well.",
            "score": 0.5395129968599635,
            "section_title": "Logit Based Distillation",
            "char_start_offset": 4309,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1157
                }
            ],
            "ref_mentions": [
                {
                    "start": 525,
                    "end": 529,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 529,
                    "end": 532,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 532,
                    "end": 535,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 535,
                    "end": 538,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 538,
                    "end": 541,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 558,
                    "end": 562,
                    "matchedPaperCorpusId": "248986690"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "257806487",
            "title": "CoCoID: Learning Contrastive Representations and Compact Clusters for Semi-Supervised Intent Discovery",
            "text": "Knowledge distillation usually refers to training a large teacher model and distilling its knowledge into a small student model (Hinton et al., 2015). Zhang et al. (2018) propose a strategy where an ensemble of students learn collaboratively and then teach each other. Yuan et al. (2019) find that knowledge distillation is a more general label smoothing regularization and present a teacher-free knowledge distillation framework where a student model learns from itself. Yun et al. (2020) propose a class-wise knowledge distillation method which distills the predictive distribution between different samples of the same label during training, which is similar to our intra-cluster knowledge distillation in the sense of distilling predictive distribution between samples from the same group. The significant difference is that we distill knowledge between user utterances in the same cluster under the semi-supervised setting rather than in the supervised condition for images (Yun et al., 2020).",
            "score": 0.5393287229037176,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6572,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 998
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 170,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 472,
                    "end": 489,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 979,
                    "end": 997,
                    "matchedPaperCorpusId": "214727822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "277940190",
            "title": "Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models",
            "text": "Knowledge distillation (KD) for natural language processing (NLP) tasks has garnered significant attention in recent years (Huang et al., 2024b) with a plethora of research focused on improving the efficiency and accuracy of large language models (Yang et al., 2024a). The introduction of open-source toolkits such as TextBrewer (Yang et al., 2020) has facilitated the setup of distillation experiments across various neural network models and NLP tasks, offering flexibility and adaptability in the KD process. Surveys on recent teacher-student learning studies (Gao, 2023) and knowledge distillation for large language models (Yang et al., 2024b) have highlighted the diversity of methods being explored, including whitebox (Nguyen et al., 2022), black-box knowledge distillation (Liang et al., 2021), teaching-assistant distillation, curriculum distillation, mask distillation, and decoupling distillation. \n\nA key theme emerging from these studies is the importance of balancing the efficiency and accuracy of the KD. Researchers have sought to improve the efficiency of large language models by reducing their size or improving inference speed without significantly compromising their accuracy (Anderson and G\u00f3mez-Rodr\u00edguez, 2020;Mukherjee et al., 2021). The development of novel distillation methods, such as zero-shot knowledge distillation (Rashid et al., 2020), has also addressed the challenges related to data accessibility and privacy, enabling student networks to learn from teachers without access to task-specific data. \n\nThe choice between sentence-level and token-level distillation methods has been a subject of debate, with studies suggesting that token-level distillation is more suitable for simple scenarios, whereas sentence-level distillation excels in complex scenarios (Wei et al., 2024a). Hybrid approaches combining both sentence-level and token-level distillation have also been proposed, offering a potential solution to scenariodependent preferences (Wei et al., 2024a).",
            "score": 0.5382306232714962,
            "section_title": "V. KNOWLEDGE DISTILLATION FOR NATURAL LANGUAGE PROCESSING TASKS",
            "char_start_offset": 16204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1534
                },
                {
                    "start": 1537,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2001
                }
            ],
            "ref_mentions": [
                {
                    "start": 247,
                    "end": 267,
                    "matchedPaperCorpusId": "270878760"
                },
                {
                    "start": 628,
                    "end": 648,
                    "matchedPaperCorpusId": "270878760"
                },
                {
                    "start": 726,
                    "end": 747,
                    "matchedPaperCorpusId": "251040524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84716796875
        },
        {
            "corpus_id": "257308816",
            "title": "A Survey on Deep-Learning-Based Real-Time SAR Ship Detection",
            "text": "Hinton put forward the concept of knowledge distillation for the first time in distilling the knowledge in a neural network. And he introduced the soft targets of teacher to induce the training of students' network. The knowledge distillation is classified into three categories, they are logits transfer, teacher assistant, and domain adaptation [109]. \n\nGenerally speaking, the teacher model has strong ability and performance, while the student model is compact. The knowledge distillation methods transfer the generalization ability of the teacher model to the compact student model to improve its performance with less complexity. The basic idea of knowledge distillation is to transfer the dark knowledge in the complex teacher model to the simple student model. These methods match or outperform the teacher's performance, while requiring notably fewer parameters and multiplications [110], [111], [112], as shown in Fig. 7. \n\nThe parameter T represents temperature. Generally, T is 1. When T is larger, a softer probability distribution will be obtained. There are two loss functions. The first loss function requires that the student model and the teacher model use the same T when calculating the softmax layer. The second loss function requires the student model T to be taken as 1, and the loss function is the weighted average of the two objective functions. Soft prediction carries more and more useful information than hard prediction. The knowledge distillation can get a lightweight CNN model with high accuracy [113]. \n\nThe softmax function is formulated as follows:",
            "score": 0.5375857118718886,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 14030,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1584
                }
            ],
            "ref_mentions": [
                {
                    "start": 891,
                    "end": 896,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 898,
                    "end": 903,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 905,
                    "end": 910,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1529,
                    "end": 1534,
                    "matchedPaperCorpusId": "102483463"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69091796875
        },
        {
            "corpus_id": "275117509",
            "title": "Leveraging logit uncertainty for better knowledge distillation",
            "text": "Knowledge distillation improves student model performance. However, using a larger teacher model does not necessarily result in better distillation gains due to significant architecture and output gaps with smaller student networks. To address this issue, we reconsider teacher outputs and find that categories with strong teacher confidence benefit distillation more, while those with weaker certainty contribute less. Thus, we propose Logits Uncertainty Distillation (LUD) to bridge this gap. We introduce category uncertainty weighting to consider the uncertainty in the teacher model\u2019s predictions. A confidence threshold, based on the teacher\u2019s predictions, helps construct a mask that discounts uncertain classes during distillation. Furthermore, we incorporate two Spearman correlation loss functions to align the logits of the teacher and student models. These loss functions measure the discrepancy between the models\u2019 outputs at the category and sample levels. We also introduce adaptive dynamic temperature factors to optimize the distillation process. By combining these techniques, we enhance knowledge distillation results and facilitate effective knowledge transfer between teacher and student models, even when architectural differences exist. Extensive experiments on multiple datasets demonstrate the effectiveness of our method.",
            "score": 0.5374426228853173,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "258298441",
            "title": "Function-Consistent Feature Distillation",
            "text": "Original KD: Hinton et al. (2015) first clarified the concept of knowledge distillation (KD). In their work, probability distribution softened by a temperature parameter \u03c4 is used to transfer the knowledge from teacher to student: \n\nwhere x is input image, \u03c4 represents temperature used to soften the output distributions, \u03c3 denotes the softmax function, and z is logit scores output by the penultimate layer of a neural network. KL-Divergence is then used to make the student learn from the teacher: \n\nwhere t and s denote teacher and student, respectively. \n\nFeature Distillation: Instead of using final probability distributions, feature distillation utilizes the knowledge in the intermediate features to guide the student. Compared with probability distribution, the intermediate feature has much larger volume, and theoretically it could provide more supervision. Considering a pair of teacher and student models, each of which is divided into N sequential modules (stages) and a final linear classifier. The forward process of the networks is as follows: \n\n(3) \n\ns denote the modules in teacher and student networks, C t and C s are their respective classifiers. Now consider the intermediate features: \n\nWith a pre-defined list of distillation positions K \u2286 {1, 2, \u2022 \u2022 \u2022 , N }, at each position k \u2208 K, the teacher feature F k t (hint layer) and the student feature F k s (guided layer) are matched for distillation. With a bridge module B k st , which is a simple combination of one convolution and one BatchNorm (Ioffe & Szegedy, 2015) layer, as student transformation to match the shape of F k t , the simplest form of feature distillation can be achieved with the following appearance loss: \n\n(5)",
            "score": 0.5368709517891153,
            "section_title": "PRELIMINARIES",
            "char_start_offset": 7983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 558
                },
                {
                    "start": 561,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1701
                },
                {
                    "start": 1704,
                    "end": 1707
                }
            ],
            "ref_mentions": [
                {
                    "start": 1521,
                    "end": 1544,
                    "matchedPaperCorpusId": "5808102"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "252185104",
            "title": "F-coref: Fast, Accurate and Easy to Use Coreference Resolution",
            "text": "Knowledge Distillation is the process of learning a small student model from a large teacher model. \n\nTeacher model We use the state-of-the-art LINGMESS model of Otmazgin et al. (2022) as the teacher model. \n\nStudent model we build our student model as a variant of the s2e model with fewer layers and parameters. The \"expensive\" Longformer (Beltagy et al., 2020) encoder was replaced with Dis-tilRoBERTa (Sanh et al., 2019), which is on average \u00d78 faster than Longformer. The number of parameters of the mention and the antecedent pairwise scorers was reduced by a factor of 6. This reduces the total number of parameters from 494M to 91M. In addition, the number of sequential layers in the network reduced from 26 layers to only 8 layers (6 encoder layers, 1 mention scorer and 1 antecedent scorer). As a result, our student combines the strengths of the s2e model by not constructing span representation with a lightweight encoder and substantially less model parameters. \n\nHard distillation Traditional approaches for knowledge distillation trains the student on the logits of the teacher model's predictions on unlabeled data (Gou et al., 2021). However, as we will further elaborate in Section \u00a75.1, applying such an approach to a coreference model with all its components (i.e. encoder, mention scorer, pruning, antecedent scorer) achieves poor performance. To remedy this issue, we employ hard target knowledge distillation, where the teacher model acts as an annotator for the unlabeled data and the student model learns from these \"silver\" annotations.",
            "score": 0.5368109499854726,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8576,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 102,
                    "end": 206
                },
                {
                    "start": 209,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1563
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8046875
        },
        {
            "corpus_id": "257038997",
            "title": "HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers",
            "text": "Pre-trained language models have demonstrated powerful generalizability in various downstream applications (Wang et al., 2018;Rajpurkar et al., 2016a). However, the number of parameters in such models has grown over hundreds of millions (Devlin et al., 2018;Raffel et al., 2019;Brown et al., 2020). This poses a significant challenge to deploying such models in applications with latency and storage requirements. \n\nKnowledge distillation (Hinton et al., 2015) has been shown to be a powerful technique to compress a large model (i.e., teacher model) into a small one (i.e., student model) with acceptable performance degradation. It transfers knowledge from the teacher model to the student model through regularizing the consistency between their output predictions. In language models, many efforts have been devoted to task-specific knowledge distillation (Tang et al., 2019;Turc et al., 2019;Sun et al., 2019;Aguilar et al., 2020). In this case, a large pre-trained model is first fine-tuned on a downstream task, and then serves as the teacher to distill a student during fine-tuning. However, task-specific distillation is computational costly because switching to a new task always requires the training of a task-specific teacher. Therefore, recent research has started to pay more attention to task-agnostic distillation (Sanh et al., 2019;Sun et al., 2020;Jiao et al., 2019;Wang et al., 2020b;Khanuja et al., 2021;Chen et al., 2021), where a student is distilled from a teacher pre-trained on open-domain data and can be efficiently fine-tuned on various downstream tasks. \n\nDespite the practical benefits, task-agnostic distillation is challenging. The teacher model has a significantly larger capacity and a much stronger representation power than the student model. As a result, it is very difficult for the student model to produce predictions that match the teacher's Figure 1: Left: In HomoDistil, the student is initialized from the teacher and is iteratively pruned through the distillation process.",
            "score": 0.536700980891451,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1583
                },
                {
                    "start": 1586,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 914,
                    "end": 935,
                    "matchedPaperCorpusId": "203953149"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "255125462",
            "title": "Prototype-guided Cross-task Knowledge Distillation for Large-scale Models",
            "text": "Unlike other model compression methods, knowledge distillation can reduce the size of the network and improve the performance of small models on downstream tasks regardless of the structural differences between teacher and student networks. Its success has been witnessed in a wide range of applications such as computer vision [17], [22]- [27], speech recognition [28]- [30], and natural language processing [31]- [33]. \n\nHowever, these knowledge distillation approaches mainly require the teacher model and the student model to be the same task, e.g., the student model and teacher model share the same label space, which limits their application in the real scenario such as downstream tasks in different label spaces (as shown in Fig. 1 (a)). \n\nCross-task knowledge distillation can transfer the knowledge of the teacher model to downstream tasks in different label spaces, which expands the application of the teacher model on a variety of downstream tasks. Existing Sametask knowledge distillation works mainly to transfer the final prediction logits or the intermediate-layers knowledge of the teacher model, which are global-level knowledge alignments and can not be applied to cross-task knowledge distillation directly. Earlier cross-task knowledge distillation work [34] aligns the high-order comparison relationship between models in a local manner, while, this method lag in the representation power of the invariant intrinsic object and is a two-stage distillation method. \n\nUnder the context of cross-task knowledge distillation, the intrinsic object characteristics can give benefit guidance to the training of the student model, for example, the shape arXiv:2212.13180v1 [cs.CV] 26 Dec 2022 features of the legs of a cattle and a horse when the cattle and horse belong to the datasets of the teacher model and the student model, respectively. Considering the complexity and variability of real scene tasks and the generalization capability of large-scale pre-trained models, in this paper, we propose a Prototype-guided Cross-task Knowledge Distillation (ProC-KD) approach to transfer the local intrinsic knowledge of a large-scale teacher network to various task scenarios (as shown in Fig. 1 (b)). And our method of obtaining the downstream tasks small model is a one-stage training process.",
            "score": 0.536684414137033,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 1836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 420
                },
                {
                    "start": 423,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1486
                },
                {
                    "start": 1489,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2216
                },
                {
                    "start": 2217,
                    "end": 2310
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 338,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 340,
                    "end": 344,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 365,
                    "end": 369,
                    "matchedPaperCorpusId": "18195425"
                },
                {
                    "start": 371,
                    "end": 375,
                    "matchedPaperCorpusId": "220935864"
                },
                {
                    "start": 415,
                    "end": 419,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1277,
                    "end": 1281,
                    "matchedPaperCorpusId": "219965004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85400390625
        },
        {
            "corpus_id": "251463676",
            "title": "Study of Deep Learning-Based Legal Judgment Prediction in Internet of Things Era",
            "text": "In natural language processing area, using a pretraining model with a huge amount of data can effectively improve the performance of the model in the target task. However, the huge number of parameters of the pretraining model also makes it difficult to directly apply it to online tasks. Using knowledge distillation for the pretraining model can effectively improve the practicability of the model. e purpose of knowledge distillation is to achieve knowledge transfer between models by letting the untrained student model learn the trained teacher model. Generally, the structure of the student model is simpler than that of the teacher model, and it has fewer layers or parameters.\n\nrough knowledge distillation, the student model can obtain similar performance to the teacher model, accelerate the model inference, and reduce the memory usage of the model. e structure of knowledge distillation is shown in Figure 1.\n\nAs shown in the figure, knowledge distillation generally uses the output layer distribution of the teacher model as a soft label, and the labels in the dataset as hard labels. Imitation of the teacher model. In order to further narrow the gap between the teacher model and the student model, the intermediate layer distribution loss of the teacher model and the student model can also be added in the distillation process. e loss function L of knowledge distillation can be expressed as follows:\n\n(1)\n\nAmong them, L qa , L 1 , and L 2 represent the distillation loss from the soft label, the supervision loss from the dataset label, and the intermediate layer distribution loss of the teacher model and the student model, respectively, and \u03bc, \u03c0, and \u03c1 represent the weights of each loss function.\n\nOn the premise of ensuring the performance, we try to use the knowledge distillation strategy. e BERTmodel is used as the teacher model for knowledge distillation to reduce the redundant parameters and enhance the inference speed.\n\nReferring to the existing research, we tried to use two mainstream knowledge distillation strategies for pretraining models: knowledge distillation and BERT knowledge distillation strategy to compress the model and combined the characteristics of legal judgment prediction to further improve the predicition accuracy.",
            "score": 0.53611746483459,
            "section_title": "BERT Knowledge Distillation-Based Legal Decision Prediction",
            "char_start_offset": 10225,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75439453125
        },
        {
            "corpus_id": "261101164",
            "title": "DLIP: Distilling Language-Image Pre-training",
            "text": "Knowledge distillation (KD) [18,57,37,3] is one effective method for transferring the knowledge in a larger teacher model h t to a smaller student model h s . The student model is trained to mimic the behaviour of the teacher network. The objective of KD can be formulated as follows: \n\nwhere L task is the task-specific loss function for the target task, and L Dist is a loss function that penalizes the difference between the teacher and the student. Several works leverage knowledge distillation to improve the performance of student models or compressing student models [57,26,13,42,53]. ViTKD [53] explores the way of feature-based distillation for vision transformers. Dis-tillBERT [42] leverages knowledge distillation to train a small BERT by mimicking the teacher's output probability of masked language prediction and the embedding features. In addition, [57,20,45,13] leverage attention-based distillation to compress the model, which forces the student to mimic the attention maps of a powerful teacher model to improve the performance of the student model significantly. \n\nIn this work, we systematically explore the importance of different modules and the impacts of different information in distillation to build a light multimodal model.",
            "score": 0.5361133332424051,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6305,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1253
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 38,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 38,
                    "end": 40,
                    "matchedPaperCorpusId": "235376877"
                },
                {
                    "start": 581,
                    "end": 584,
                    "matchedPaperCorpusId": "233033712"
                },
                {
                    "start": 875,
                    "end": 878,
                    "matchedPaperCorpusId": "233033712"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85791015625
        },
        {
            "corpus_id": "267028052",
            "title": "Multi-view Distillation based on Multi-modal Fusion for Few-shot Action Recognition(CLIP-M2DF)",
            "text": "Knowledge distillation is the classic model compression method, with the core idea of guiding lightweight student models to mimic better-performing and more structurally complex teacher models. Optimization strategies, such as mutual learning and self-learning through neural networks and data resources, such as unlabelled and crossmodal, significantly enhance model performance. Knowledge Amalgamation [Shen et al., 2019] is the migration of multiple tasks into a single student model to make it capable of handling multiple tasks. Mutual distillation [Zhang et al., 2018] addresses using student models to learn from each other to improve performance without a robust teacher network, avoiding the reliance on large-scale teacher models. [Albanie et al., 2018] proposed cross-modal affective recognition with the distillation of data features from different modalities. AMFAR [Wanyan et al., 2023] is related to the mutual distillation of Multi-modal. Our work is based on the Multi-view distillation.",
            "score": 0.535978984779189,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8377,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1004
                }
            ],
            "ref_mentions": [
                {
                    "start": 404,
                    "end": 423,
                    "matchedPaperCorpusId": "53231387"
                },
                {
                    "start": 741,
                    "end": 763,
                    "matchedPaperCorpusId": "52018690"
                },
                {
                    "start": 879,
                    "end": 900,
                    "matchedPaperCorpusId": "250581305"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "271270180",
            "title": "Towards Optimal Trade-Offs in Knowledge Distillation for CNNs and Vision Transformers at the Edge",
            "text": "The gap of model capacity between the teacher and the student is a critical consideration in the KD process.An optimal balance must be struck to ensure effective distillation outcomes.When the student model's capacity is too low relative to the teacher, the student struggles to effectively incorporate the logits information provided by the teacher.This limitation impedes the student's ability to capture the distilled knowledge, thus hindering its performance.\n\nConversely, when the student model's capacity is excessively large, the expected improvements in distillation efficacy may not materialize.Larger student models tend to exhibit slower learning rates and are more susceptible to overfitting, resulting in diminished generalization performance.Furthermore, the computational and memory resources required for training and deploying larger student models pose significant challenges, particularly in resource-constrained environments such as edge devices.The increased computational demands, higher memory footprint, and slower inference speeds associated with larger models render them less practical for deployment in real-world edge scenarios.Therefore, determining the optimal student size also requires experimental investigation.",
            "score": 0.5358932322514787,
            "section_title": "B. Student Model Capacity",
            "char_start_offset": 9530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 108,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 350
                },
                {
                    "start": 350,
                    "end": 463
                },
                {
                    "start": 465,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 756
                },
                {
                    "start": 756,
                    "end": 966
                },
                {
                    "start": 966,
                    "end": 1157
                },
                {
                    "start": 1157,
                    "end": 1246
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "261696730",
            "title": "Self-training and multi-task learning for limited data: evaluation study on object detection",
            "text": "In this experiment, we increase the student unlabeled training data using the complementary subset with the teachers' as shown in Fig. 2B2, C2. For the sake of completion, we also include the performance of student trained on the 3quarter subset while the teacher is trained on the first 1-eighth . \n\nFrom Table 4, there is a gradual diminution in the student performance along with the shrinkage of the teacher's training size despite the expansion of the students' training size, showing the importance of targets' accuracy and, subsequently, the teacher generalizability. Interestingly, a small reducing of the teacher training size, from to for the same student size , results in a large gap in the student performance, suggesting a toleration threshold for data deficiency of the teachers. The use of knowledge distillation also helps improve the student, even to surpass the teacher. . The teachers' IOU scores are provided for reference. Although the teachers had not been trained on the same data nor the same task with the students, the results with knowledge distillation are constantly better than without by large margins.",
            "score": 0.5355186941028665,
            "section_title": "Increasing student unlabeled data",
            "char_start_offset": 13365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 298
                },
                {
                    "start": 301,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1134
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5966796875
        },
        {
            "corpus_id": "267938070",
            "title": "Knowledge Fusion of Chat LLMs: A Preliminary Technical Report",
            "text": "In comparison to the model ensemble method, which requires the parallel deployment of multiple models, and the model merging approach, which is generally limited to models with identical architectures, FUSELLM supports the fusion of multiple source LLMs with different architectures into a target LLM. \n\nKnowledge Distillation Knowledge distillation (Hinton et al., 2015), initially proposed for model compression, involves training a student model under the guidance of one or more teacher models. In the NLP community, knowledge distillation has been widely applied to text classification tasks. These applications include training the student model to replicate the teacher's output distribution (Sanh et al., 2019;Turc et al., 2019), as well as features (Sun et al., 2019;Jiao et al., 2020) and relations (Wang et al., 2020) derived from intermediate layers of the teacher model. In the realm of text generation, the conventional approach focuses on minimizing the KL divergence between the student and teacher generation distributions. This is achieved by using the teacher's probability distributions at each time step as supervision (Khanuja et al., 2021;Gu et al., 2023;Agarwal et al., 2023) or by directly training on the teacher's generated texts (Peng et al., 2023;Xu et al., 2023).",
            "score": 0.5353450293353136,
            "section_title": "Related Work",
            "char_start_offset": 7110,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 304,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1293
                }
            ],
            "ref_mentions": [
                {
                    "start": 758,
                    "end": 776,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 776,
                    "end": 794,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 809,
                    "end": 828,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 1140,
                    "end": 1162,
                    "matchedPaperCorpusId": "236477925"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7666015625
        },
        {
            "corpus_id": "252355060",
            "title": "CES-KD: Curriculum-based Expert Selection for Guided Knowledge Distillation",
            "text": "Knowledge distillation is a training method that is based on teacher-student learning. It was firstly introduced as a mode of model compression by Bucilua et al. [12] then further popularized by Hinton et al. [7]. The goal of KD is to increase the accuracy of a student network due to transfer of information from the pre-trained teacher network during training. Distillation methods rely on different techniques to capture the knowledge of the teacher that is transferred to the student. The traditional KD [7] uses the soft targets produced by the teacher network as the knowledge to transfer to the student. Some recent works focus on feature-based knowledge distillation and train the student to match the intermediate layers of the teacher [13]. In addition, some approaches [14] transfer spatial attention maps, where the student attends to similar parts of the image as the teacher network. In our work we exploit the soft targets of the teacher network for our distillation process.",
            "score": 0.5347868736993693,
            "section_title": "II. RELATED WORKS A. Knowledge Distillation",
            "char_start_offset": 4326,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 780,
                    "end": 784,
                    "matchedPaperCorpusId": "198179476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "258298461",
            "title": "Improving Knowledge Distillation Via Transferring Learning Ability",
            "text": "In most knowledge distillation methods, the initial step is to typically search among all pre-trained models to identify the optimal teacher model for the distillation process. However, it is not possible to select the optimal SL-T model in a teacher-like manner due to the synchronized learning of the SL-T and student. The method of multi-teacher knowledge distillation integrates the output of multiple teachers as supervision information and has been shown to yield superior distillation results compared to conventional single-teacher methods. Considering the accuracy and effectiveness of the supervision information provided by the SL-T, we adopt the concept of the multi-teacher knowledge distillation [35; 36; 37; 38; 39] methods and introduce multiple completely identical SL-Ts, which outputs are weighted fused and provided to the student as supervision information. The learning trajectory enhancement strategy can enable SL-Ts to provide the student with more reasonable approximation goals in each iteration. Specifically, in the process of training SL-T, we introduce two fully isomorphic and untrained deep neural networks SL-T 1 (SLT 1 ) and SL-T 2 (SLT 2 ), which learn from the teacher as student individually during the entire process of knowledge distillation: \n\nwhere \u03b2 1 (i) and \u03b2 2 (i) represent the parameter of SL-T 1 and SL-T 2 at epoch i. We then weighted fuse the output of these two SL-T networks : SLT (X; \n\n) where \u03c1 is the weight coefficient, the fused output is provided to the student network as supervision information(Eq3). In this way, we hope it can improve the reliability of the learning trajectory provided by SL-T, thereby improving the overall performance of knowledge distillation (as shown in Figure4).",
            "score": 0.5343836795591524,
            "section_title": "Learning Trajectory Enhancement",
            "char_start_offset": 11485,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1749
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79541015625
        },
        {
            "corpus_id": "262046518",
            "title": "Distilling Hubert with LSTMs via Decoupled Knowledge Distillation",
            "text": "The method of knowledge distillation proposed in [4] consists of transferring knowledge from a potentially large and cumber-arXiv:2309.09920v1 [eess.AS] 18 Sep 2023 some classification model to a distilled one using a soft target distribution for each sample in a transfer dataset, calculated for each of C classes as \n\nwhere l c represents the logit of class c, and \u03c4 is a temperature scaling. The authors argue that soft targets provide much more information than hard labels and show that they are an effective way of communicating patterns discovered by a teacher model, being leveraged as guidance for training smaller models even with less data than the teacher. The knowledge distillation loss [4,5] is defined as \n\nwhere KL(\u2022||\u2022) is the Kullback-Leibler divergence, and p T and p S are the softmax-probability scores output by the teacher and the student model, respectively. The final loss is a linear combination of L KD and the cross-entropy loss between the student's class probabilities and the ground-truth hard labels. More recently, Knowledge Distillation (DKD) [5] was proposed. The authors first show that the KD loss can be split into two parts: Target Class Knowledge Distillation (TCKD) and Non-Target Class Knowledge Distillation (NCKD). The first one is argued to transfer the knowledge about the \"difficulty\" of the training samples, while the latter contains valuable knowledge about the non-target classes. The reformulated loss reads as \n\nwhere b \u2208 R 2 are the binary probabilities of the target class and all the other non-target classes together, and p \u2208 R C\u22121 represents the probabilities among non-target classes, discarding the target one. The NCKD term is shown to be coupled and inversely tied to the teacher's confidence on the target output, p T . This means that a good teacher would have a high p T , suppressing the NCKD component. As a consequence, the effectiveness of distillation of valuable information about the non-target classes would be limited for well-predicted samples.",
            "score": 0.5343514000172024,
            "section_title": "KNOWLEDGE DISTILLATION",
            "char_start_offset": 3750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 52,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 701,
                    "end": 704,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 704,
                    "end": 706,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1078,
                    "end": 1081,
                    "matchedPaperCorpusId": "247476179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75390625
        },
        {
            "corpus_id": "271270565",
            "title": "DFMSD: Dual Feature Masking Stage-wise Knowledge Distillation for Object Detection",
            "text": "It is well-known that knowledge distillation (KD) can help transfer knowledge from a complex model (teacher) to a compact network (student), so that the latter can achieve improved performance at a much lower cost.It is considered to be an effective means of model compression for a variety of downstream tasks including object detection and semantic segmentation [1,2,3,4,5,6].Primarily focusing on the output head of the network, early distillation algorithms aim at transferring implicit knowledge learned in the complex teacher network to the lightweight student model.This distillation scheme is also known as logit-based classification distillation [1,2,6,7].In addition, the feature-based distillation approach has received increasing attention.It helps the student network to mimic feature maps from the teacher model in the distillation process, allowing the generated student features to enjoy improved representational capability [8,9].More recently, a popular distillation paradigm has emerged as feature-masking distillation.\n\nIn contrast to feature distillation in which the student's feature directly mimics the counterpart of the teacher [10,11], feature-masking distillation operates by masking selective regions of the student feature map and reconstructing the masked regions for distillation [12].In this sense, feature-masking distillation essentially reconstructs the transferred knowledge from the teacher instead of transferring knowledge directly.Consequently, it can help the student learn better from the teacher.In particular, recent efforts are devoted to taking advantage of feature attention for uncovering object-aware spatially important regions and channel-wise clues such that the student features are reconstructed with sufficient descriptive power comparable to teacher features [13].As a result, this attention-directed feature masking strategy enormously contributes to improving the performance of the student model [14].\n\nAlthough dramatic progress has been made in recent years, most feature-masking distillation methods are developed mainly to address homogeneous distillation, which assumes that teacher and student models share roughly similar structures except that the former usually adopts a stronger backbone.",
            "score": 0.5342868005538264,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 214,
                    "end": 378
                },
                {
                    "start": 378,
                    "end": 573
                },
                {
                    "start": 573,
                    "end": 665
                },
                {
                    "start": 665,
                    "end": 752
                },
                {
                    "start": 752,
                    "end": 947
                },
                {
                    "start": 947,
                    "end": 1038
                },
                {
                    "start": 1040,
                    "end": 1317
                },
                {
                    "start": 1317,
                    "end": 1472
                },
                {
                    "start": 1472,
                    "end": 1540
                },
                {
                    "start": 1540,
                    "end": 1821
                },
                {
                    "start": 1821,
                    "end": 1961
                },
                {
                    "start": 1963,
                    "end": 2258
                }
            ],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 369,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 371,
                    "end": 373,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 373,
                    "end": 375,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 375,
                    "end": 377,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 658,
                    "end": 660,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 660,
                    "end": 662,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 941,
                    "end": 944,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1154,
                    "end": 1158,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1158,
                    "end": 1161,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1312,
                    "end": 1316,
                    "matchedPaperCorpusId": "261556970"
                },
                {
                    "start": 1816,
                    "end": 1820,
                    "matchedPaperCorpusId": "256889908"
                },
                {
                    "start": 1956,
                    "end": 1960,
                    "matchedPaperCorpusId": "102483181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8759765625
        },
        {
            "corpus_id": "277940190",
            "title": "Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models",
            "text": "Language modeling with knowledge distillation (KD) has emerged as a crucial technique for compressing large language models while maintaining their accuracy, as demonstrated in various studies (Yang et al., 2024b;Wei et al., 2024a;Zhang et al., 2024b). The core idea behind KD is to transfer knowledge from a pretrained teacher model to a smaller student model, thereby reducing computational requirements and improving inference speed without significantly compromising performance. A comprehensive survey of KD for large language models (Yang et al., 2024b) provides an overview of the techniques, evaluation tasks, and applications, highlighting the effectiveness of this approach. \n\nRecent studies have explored different distillation methods, including sentence-level, token-level, and hybrid approaches. For instance, (Wei et al., 2024a) introduced a novel hybrid method that combines token-level and sentence-level distillation, outperforming the individual methods and previous studies. (Zhang et al., 2024b) proposed a multi-level KD method that improves model performance and allows for flexible student attention head number settings. Furthermore, (Park et al., 2021) presented a new KD objective that transfers contextual knowledge via word relations and layer transforming relations without restrictions on architectural changes between teacher and student models. \n\nThe importance of selecting an appropriate distillation method depending on scenario complexity has also been emphasized in the literature. For example, (Wei et al., 2024a) argued that token-level distillation is better suited to simple scenarios, whereas sentence-level distillation excels in complex scenarios. However, other studies have not explicitly made this distinction, highlighting the need for further research on task-agnostic distillation and its applications (Liang et al., 2023). The use of pretrained language models as teachers and the exploration of various student models, including those with smaller sizes or different architectures, are also a common theme in the literature (Yang et al., 2024b;Wang et al., 2022). \n\nTechnical details and methodologies vary across studies, with KD techniques including sentence-level, token-level, hybrid, multi-level, and task-agnostic distillation.",
            "score": 0.534177817483777,
            "section_title": "XIX. LANGUAGE MODELING",
            "char_start_offset": 72228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2116
                },
                {
                    "start": 2119,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 213,
                    "matchedPaperCorpusId": "270878760"
                },
                {
                    "start": 539,
                    "end": 559,
                    "matchedPaperCorpusId": "270878760"
                },
                {
                    "start": 2077,
                    "end": 2097,
                    "matchedPaperCorpusId": "270878760"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "252565150",
            "title": "Selective Cross-Task Distillation",
            "text": "Standard knowledge distillation is a special case of generalized knowledge distillation. In this part, we show that our proposed method can achieve competitive performance on standard knowledge distillation. We compare our method to several distillation methods on CIFAR-100 and CUB. Architectures of the teachers and the students are same as those in the previous part. Results are listed in Tab. 4 and Tab. 5. Our method achieves best results in most cases. Specifically, when teacher and student share a same architecture, we also try to use self-distillation [39] to learn a student. SEED [94] and SSKD [40] are recently proposed  self-supervised distillation methods, and we can see that our method is better because instance labels are used. Another interesting topic is cross-family distillation, which means the architectures of teacher and student come from different families. Specifically, we set teacher to ResNet-50 [8] for both CIFAR-100 and CUB. Results are shown in Tab. 6 and Tab. 7. In Tab. 6, we can see that the accuracy of teacher (72.17) is lower than that of a WideResNet-(40,2) student (74.44). Thus, the improvements of all the distillation methods in the first column are limited. Our proposed method achieves best accuracy in most cases, showing that it can distill the knowledge of a cross-family teacher. Similar phenomenon can be observed in Tab. 7.",
            "score": 0.5341293227627062,
            "section_title": "Standard Knowledge Distillation",
            "char_start_offset": 43070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1379
                }
            ],
            "ref_mentions": [
                {
                    "start": 563,
                    "end": 567,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 593,
                    "end": 597,
                    "matchedPaperCorpusId": "231592453"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "219636179"
                },
                {
                    "start": 929,
                    "end": 932,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69482421875
        },
        {
            "corpus_id": "271310496",
            "title": "Continual Distillation Learning",
            "text": "Logit distillation is one of the most classic forms of knowledge distillation. It aims to have a smaller student model learn the logits output of a larger or more accurate teacher model. In our CDL setup, we conducted experiments using two types of logit knowledge distillation methods: normal knowledge distillation (KD) [13] and Decoupled Knowledge Distillation (DKD) [40]. \n\nNormal Knowledge Distillation (KD): This approach was initially proposed by Hinton et al. [13], \n\nwhere distillation transfers the knowledge from a large, complex teacher model to a smaller student model, helping the latter to approximate the teacher model in terms of performance and accuracy. To achieve this, Hinton et al. designed a method where the logit output of the teacher model serves as \"soft labels\", guiding the student model's training. After passing through the softmax layer, the output values provide probabilities for each class. \n\nwhere z i is the logit, and p i is the predicted probability for class i. The soft target is the class probability distribution of teacher model. In the distillation process, a temperature parameter \u03c4 , is introduced to smooth the output distribution of the model, making it easier for the student model to learn the subtle differences between classes. \n\nIn the teacher-student model shown in Fig. 2, the class tokens of the teacher model and the student model processed by the pre-trained ViT backbones are connected to the teacher classifier and student classifier to output their logits, respectively. The loss function of the student model for logit distillation is defined as: \n\nwhere L(g \u03d5 (f b (x)), y) represents the cross-entropy classification loss used for learning with true label y, g \u03d5 is the student classifier, and f b denotes the pre-trained ViT backbone and we only use the final class token into the classifier. L KD represents the knowledge distillation loss, i.e., the KL divergence between the teacher's probability distribution p T i and the student's probability distribution p S i . The L pool is a loss function specific to the prompt pool. Different prompt-based continual learning methods have their respective prompt pool loss functions.",
            "score": 0.5338111635507257,
            "section_title": "D Logit Distillation Details",
            "char_start_offset": 31877,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2194
                }
            ],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 374,
                    "matchedPaperCorpusId": "247476179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "251719031",
            "title": "Tree-structured Auxiliary Online Knowledge Distillation",
            "text": "Knowledge distillation is a ubiquitous technique for model compression and acceleration. It trains a smaller student model to imitate the output distribution of a larger teacher model via a L 2 loss on logits (i.e. before Softmax) [3] or cross-entropy [12]. Recently, many researches attempt to further improve the performance by discovering different knowledge from the teacher, such as intermediate representations [21], attention maps [28], and flow of solution process (FSP) [26]. However, the limitation to these methods is that they need a pre-trained teacher and conduct a two-stage distillation process.",
            "score": 0.5337346487255257,
            "section_title": "II. RELATED WORK A. Knowledge Distillation",
            "char_start_offset": 4608,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 479,
                    "end": 483,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "273969804",
            "title": "Quantifying Knowledge Distillation Using Partial Information Decomposition",
            "text": "In this section, we propose information theoretic metrics to quantify both the task-relevant information that is available in the teacher for distillation, and the amount of information that has already been transferred to the student. We mathematically demonstrate favorable properties of our proposed measures in comparison to other candidate measures. Our mathematical results highlight the limitations of existing knowledge distillation frameworks that naively align the student with the teacher with no regard for task-relevance. We leverage the unique and redundant information definitions given by [12] for an exact quantification of these quantities. \n\nDefinition 3.3 (Unique and redundant information [12]). Let P be the joint distribution of Y, T and S, and \u2206 be the set of all joint distributions over Y \u00d7 T \u00d7 S. Then, U ni(Y : T \\S) := min \n\nRed(Y : T, S) := I(Y ; T ) \u2212 min \n\nwhere \n\nT and s \u2208 S} i.e., \u2206 P is the set of all joint distributions with marginals of the pairs (Y, T ) and (Y, S) equal to that of P . \n\nComparison to Existing Approaches for Knowledge Distillation: A multitude of knowledge distillation frameworks exist which are based on maximizing the mutual information between the teacher and the student (i.e., I(T ; S)) [3,4,14,15]. While a distillation loss that maximizes I(T ; S) can be helpful to the student when the teacher possesses task-related information, we show that it creates a tension with the ordinary loss when the teacher has little or no task-relevant information. Moreover, even though the teacher contains task-related information, the limited capacity of the student may hinder a proper distillation when this kind of framework is used. The following examples provide critical insights, exposing the limitation of I(T ; S). \n\nOur proposed measure Red(Y : T, S) resolves these cases in an explainable manner by succinctly capturing task-relevant knowledge.",
            "score": 0.5334871901171966,
            "section_title": "Explaining Knowledge Distillation",
            "char_start_offset": 10601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 886
                },
                {
                    "start": 889,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1776
                },
                {
                    "start": 1779,
                    "end": 1908
                }
            ],
            "ref_mentions": [
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "10901107"
                },
                {
                    "start": 710,
                    "end": 714,
                    "matchedPaperCorpusId": "10901107"
                },
                {
                    "start": 1251,
                    "end": 1254,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1254,
                    "end": 1256,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1256,
                    "end": 1259,
                    "matchedPaperCorpusId": "229220499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60107421875
        },
        {
            "corpus_id": "273291149",
            "title": "DeLTran15: A Deep Lightweight Transformer-Based Framework for Multiclass Classification of Disaster Posts on X",
            "text": "BERT-tiny and BERT-mini are compact variants of the BERT model, offering a lightweight and resource-efficient solution for NLP tasks. Knowledge distillation is applied, here, the teacher is the pretrained BERT-LARGE model. The student models are trained to replicate the teacher model, which has already been fine-tuned for a particular task. This process, known as task-specific distillation, utilizes the soft logits from the teacher model to distill knowledge into the student models. It presupposes the availability of human-labeled data to fine-tune the teacher model for the given task, subsequently providing the student model with corresponding logits for learning. Its key features include a significantly reduced number of parameters compared to larger versions of BERT, maintaining a simplified architecture while retaining the fundamental capabilities of the original model. In comparison to BERT-tiny, BERT-mini typically has a slightly larger model size and parameter count, providing a balance between model efficiency and performance [53], [54].",
            "score": 0.5334340844489203,
            "section_title": "A. BERT-TINY AND BERT-MINI",
            "char_start_offset": 20237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1061
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "252545066",
            "title": "PROD: Progressive Distillation for Dense Retrieval",
            "text": "The knowledge distillation [11] has been widely studied for decades. There are a variety of off-the-shelf practical techniques, such as the response-based method [19], the feature-based method [37] and distilling the attention scores [49] with Kullback-Leibler divergence (KLD) or mean squared error (MSE). Recently, a rising group of work focuses on the relationships between the teacher model and the student model. Some elaborate the knowledge transmission methods [4,56], some introduce middle models [28], and others work on distillation with multiple teachers [45]. It can be noted that how to use a more reasonable teacher model to guide the learning of the student model has gradually become the current key direction. \n\nAmong the studies on knowledge distillation, there is a branch of work on progressive distillation, in which teachers are dynamically adjusted to distill students. In spite of the success on the workflow of pretraining [43], progressive distillation methods are widely proven to be effective in down-stream tasks, such as image classification [36], image generation [39], GLUE [14,36], question answering [36], and cross-modal representation learning [1]. In each task, the main idea of the existing progressive distillation work is to use stronger teachers while educating students. Different from the above studies, PROD is adopt to the research field of dense retrieval, using a progressive method from two perspectives, teacher and data, in which teachers with different architectures are applied in a progressive order and more and more confusing data is mined to fill the performance gap between the teacher and the student.",
            "score": 0.5330772846634415,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1659
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "234777937"
                },
                {
                    "start": 468,
                    "end": 471,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "237250417"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1106,
                    "end": 1110,
                    "matchedPaperCorpusId": "239009625"
                },
                {
                    "start": 1180,
                    "end": 1183,
                    "matchedPaperCorpusId": "248085000"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84912109375
        },
        {
            "corpus_id": "253517580",
            "title": "Leveraging angular distributions for improved knowledge distillation",
            "text": "Knowledge distillation. Knowledge distillation, a transfer learning method, trains a smaller model by shifting knowledge from a larger model. KD is firstly introduced by Bucilu\u01ce et al. [18] and is further explored by Hinton et al. [8]. The main concept of KD is using soft labels by a trained teacher network. That is, mimicking soft probabilities helps students get knowledge of teachers, which improves beyond using hard labels (training labels) alone. Cho et al. [2] explore which combination of student-teacher is good to obtain the better performance. They show that using a teacher trained by early stopping the training improves the efficacy of KD. KD can be categorized into two approaches that use the outputs of the teacher [1]. One is response-based KD, which uses the posterior probabilities with softmax loss. The other is feature-based KD using the intermediate features with normalization. Feature-based methods can be performed with the response-based method to complement traditional KD [1]. Recently, featurebased distillation methods for KD have been studied to learn richer information from the teacher for bettermimicking and performance improvement [1,13,19]. Romero et al. [20] firstly introduced the use of intermediate representations in FitNets using feature-based distillation. This method enables the student to mimic the teacher's feature maps in intermediate layers. \n\nAttention transfer. To capture the better knowledge of a teacher network, attention transfer [1,21,22,23] has been utilized, which is one of the popular methods for feature-based distillation. Zagoruyko et al. [21] suggest activation-based attention transfer (AT), which uses a sum of squared attention mapping function computing statistics across the channel dimension. Although the depth of teacher and student is different, knowledge can be transferred by the attention mapping function, which matches the depth size as one. The activation-based spatial attention maps are used as the source of knowledge for distillation with intermediate layers, where the maps are created as: \n\n, where f is a computed attention map, A is an output of a layer, c is the number of channels for the output, j is the number for the channel, and d > 1.",
            "score": 0.5330677110520741,
            "section_title": "Related Work",
            "char_start_offset": 3725,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2080
                },
                {
                    "start": 2083,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 189,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 734,
                    "end": 737,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1004,
                    "end": 1007,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1171,
                    "end": 1174,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1174,
                    "end": 1177,
                    "matchedPaperCorpusId": "208139041"
                },
                {
                    "start": 1177,
                    "end": 1180,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 1492,
                    "end": 1495,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1495,
                    "end": 1498,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1498,
                    "end": 1501,
                    "matchedPaperCorpusId": "211116043"
                },
                {
                    "start": 1501,
                    "end": 1504,
                    "matchedPaperCorpusId": "231839582"
                },
                {
                    "start": 1609,
                    "end": 1613,
                    "matchedPaperCorpusId": "829159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "272987178",
            "title": "Classroom-Inspired Multi-Mentor Distillation with Adaptive Learning Strategies",
            "text": "We conduct a series of ablation studies to understand the individual contributions of different components of our ClassroomKD framework, providing insights into our design choices. Role of System Components. In Table 3a, we observe a significant improvement when moving from single-teacher distillation (row 2) to a multi-mentor setup (row 3). The presence of multiple mentors, specifically the intermediatesized peers, bridges the capacity gap between the large teacher and the small student. This gap is a well-known limitation in traditional KD, where the student struggles to fully comprehend the knowledge transferred from a much larger teacher. Introducing peers, which have capacities between the teacher and student, effectively provides a smoother learning gradient for the student, facilitating a more gradual and interpretable knowledge transfer. \n\nThe adaptive distillation strategy (row 4) boosts accuracy by 2.56%, highlighting the limitations of static distillation methods. By adjusting distillation based on the student's progress and mentor outputs, ClassroomKD ensures more efficient learning, especially during critical phases where mentor usefulness varies. Table 3b shows that the KF Module improves accuracy from 65.96% to 68.49% by filtering out irrelevant knowledge, while the Mentoring Module dynamically adapts teaching strategies, raising performance to 67.25%. Together, these modules achieve the highest accuracy of 68.52%, ensuring both quality and adaptability in knowledge transfer. \n\nWe examine the classroom composition and further analyze our framework in the following sections.",
            "score": 0.5330474824408413,
            "section_title": "Ablation Studies",
            "char_start_offset": 20085,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1615
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71337890625
        },
        {
            "corpus_id": "268857025",
            "title": "Task Integration Distillation for Object Detectors",
            "text": "Knowledge distillation is an effective model compression technique that facilitates the transfer of knowledge from a large model to a smaller one, enabling the smaller model to achieve, or even approximate, the performance of the larger model.Initially introduced by Hinton et al. [12], knowledge distillation has been predominantly applied in the image classification domain.The technique primarily transfers \"dark knowledge\" to the student model through the soft labels of the teacher model.To smoothly extract this \"dark knowledge\", a hyperparameter known as temperature is introduced.\n\nKnowledge distillation was initially proposed in the domain of image classification and has since been widely applied across various fields.Currently, knowledge distillation can be categorized into three types based on the source of knowledge: response-based knowledge distillation [3,5,22,35,42], relation-based knowledge distillation [23,24], and feature-based knowledge distillation [10,11,13,14].Response-based knowledge distillation, the earliest proposed method, involves extracting the output of the teacher network's last layer and directly mimicking the teacher's final prediction.Relation-based knowledge distillation extracts relationships between different layers or data samples within the teacher network, transferring such relational knowledge to the student to achieve distillation.Feature-based knowledge distillation involves extracting and aligning feature maps from both the teacher and student models to the same size, then having the student model's feature maps fit those of the teacher model, facilitating the transfer of knowledge.Due to its ease of handling knowledge sources and almost uniform performance across most domains, feature-based knowledge distillation has garnered widespread attention from scholars for its strong versatility.\n\nKnowledge distillation methods are not only widely applied in the field of computer vision, such as image retrieval [34] and face recognition [17], but also in natural language processing [20,30], speech recognition [15], recommendation systems [28], information security [44], multimodal data [43], and finance and securities [8].Knowledge distillation, with its capability to transfer knowledge between different models, has garnered significant attention across multiple domains due to its versatility.",
            "score": 0.5328204401026782,
            "section_title": "Knowledge distillation",
            "char_start_offset": 7226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 493
                },
                {
                    "start": 493,
                    "end": 588
                },
                {
                    "start": 590,
                    "end": 730
                },
                {
                    "start": 730,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1180
                },
                {
                    "start": 1180,
                    "end": 1388
                },
                {
                    "start": 1388,
                    "end": 1646
                },
                {
                    "start": 1646,
                    "end": 1856
                },
                {
                    "start": 1858,
                    "end": 2189
                },
                {
                    "start": 2189,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 872,
                    "end": 875,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 875,
                    "end": 877,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 877,
                    "end": 880,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 880,
                    "end": 883,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 883,
                    "end": 886,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 926,
                    "end": 930,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 930,
                    "end": 933,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 976,
                    "end": 980,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 980,
                    "end": 983,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 986,
                    "end": 989,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 1974,
                    "end": 1978,
                    "matchedPaperCorpusId": "250581330"
                },
                {
                    "start": 2000,
                    "end": 2004,
                    "matchedPaperCorpusId": "260068567"
                },
                {
                    "start": 2074,
                    "end": 2078,
                    "matchedPaperCorpusId": "235417286"
                },
                {
                    "start": 2103,
                    "end": 2107,
                    "matchedPaperCorpusId": "2552056"
                },
                {
                    "start": 2130,
                    "end": 2134,
                    "matchedPaperCorpusId": "245693198"
                },
                {
                    "start": 2152,
                    "end": 2156,
                    "matchedPaperCorpusId": "237490408"
                },
                {
                    "start": 2185,
                    "end": 2188,
                    "matchedPaperCorpusId": "203605465"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91748046875
        },
        {
            "corpus_id": "246485656",
            "title": "How to Improve Deep Learning for Software Analytics (a case study with code smell detection)",
            "text": "This leads to simpler models that run faster. We now discuss yet another motivation for using feedforward networks: knowledge distillation. \n\nKnowledge distillation [23] is a technique that allows a \"teacher\" model to train a \"student\" model. Specifically, a large number of inputs is generated and fed to the originally trained, more complex model (a deep learner), and the outputs are captured. These input-output pairs are used to train a simpler, often faster or more interpretable model (such as a smaller deep learner or a decision tree). Surprisingly, the student model often matches or even outperforms the teacher model. Knowledge distillation has been widely studied [27,38,41], and shown to be useful for various tasks [10,21]. Knowledge distillation also has the benefits of yielding simpler, faster, models, and being amenable to transfer learning [55]. \n\nHowever, there have been shown to be limitations to knowledge distillation [11]. Specifically, a key insight of Cho and Hariharan [11] is that larger models may not be effective teachers, especially for smaller students, and typical solutions such as multiple rounds of distillation fail. From this, we infer that because convolutional models are typically significantly larger than their feedforward counterparts (due to the additional parameters introduced by the convolutional layers), they may be less effective teachers for decision tree learners. Therefore, we advocate instead for obtaining performance by leveraging hyper-parameter optimization on feedforward networks, which yield fast, simple models; then, if an interpretable model is required, knowledge distillation can be used to obtain a decision tree with similar performance. This is because our feedforward networks are significantly smaller than deep learners such as convolutional neural networks, and so can be effective teacher models in knowledge distillation.",
            "score": 0.5325155611502863,
            "section_title": "BROADER IMPLICATIONS",
            "char_start_offset": 31324,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1902
                }
            ],
            "ref_mentions": [
                {
                    "start": 681,
                    "end": 684,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 684,
                    "end": 687,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 734,
                    "end": 737,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 861,
                    "end": 865,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 944,
                    "end": 948,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 999,
                    "end": 1003,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8662109375
        },
        {
            "corpus_id": "254366823",
            "title": "Adaptive Mixing of Auxiliary Losses in Supervised Learning",
            "text": "Effect of optimal \u03bbs on Knowledge Distillation In the first experiment, we examine effective transfer of learned knowledge from various teachers to a student model which has fewer parameters. We compares test accuracies obtained with KD, AMAL , TAKD (Mirzadeh et al. 2019) KD with ResNet (20,32,56,110) as teacher and ResNet8 as student models on CIFAR100, Wide Residual Networks (WRN-16-3,WRN-16-4,WRN-16-6,WRN-16-8) as teacher and WRN-16-1 as student models on Stanford Cars and with Wide Residual Networks (WRN-16-3,WRN-16-4,WRN-16-6,WRN-16-8) as teachers and Resnet8 as student on FGVC-Aircraft. For TAKD and DGKD we use ResNet14 for CI-FAR100 and WRN-16-2 for Stanford Cars and FGVC-Aircraft as teaching assistant models. In all our knowledge distillation experiments we use temperature \u03c4 = 4 and \u03bb a = 0.9 (weights associated with KD loss) except in case of AMAL. For DGKD we use set the binomial variable to be 0.75, best reported in the paper. Figure 3 shows that AMAL consistently outperforms other techniques when a much smaller model learns from large teacher model (CIFAR100, Stanford Cars) and is comparable to DGKD in FGVC-Aircraft dataset. The figure shows plot relative test accuracies (w.r.t. non-KD students) vs model compression ratio1 .Interestingly, methods such as KD, SSKD and TAKD actually perform worse than training a student model with standard cross entropy loss. This observation is consistent with (Cho and Hariharan 2019), where authors claim KD may fail if the student is too weak. This problem gets worse when techniques such as SSKD bring even more additional information for the student model to learn. TAKD tries to address this issue by bring in teaching assistant model, which have already gone through with knowledge distillation from the teacher model.",
            "score": 0.5323907728182815,
            "section_title": "Results with Knowledge Distillation",
            "char_start_offset": 20265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1792
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 272,
                    "matchedPaperCorpusId": "211259075"
                },
                {
                    "start": 1428,
                    "end": 1452,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "271924207",
            "title": "PRG: Prompt-Based Distillation Without Annotation via Proxy Relational Graph",
            "text": "Knowledge distillation (KD) is a common approach to model compression, which involves training a student model using knowledge from a high-capacity teacher model to achieve better performance and accuracy.This is a process of knowledge transfer, transferring from the teacher network to the student network, training the student model to mimic the behavior and predictions of the teacher model by transferring knowledge.Based on the different transfer methods,it can be broadly classified into three main directions: logitbased, feature-based, and relation-based. Early works like [2], [4]- [8] used soft logit outputs from teachers for additional supervision. Subsequent feature distillation methods [9]- [14] focused on intermediate feature representations. Relation distillation methods [15]- [18] explored the structural information in teachers' logits or features. Other approaches [19]- [23] have investigated different training paradigms and teacher model generation strategies. Among them, the first knowledge distillation method [2]employs softened labels to learn the class Fig. 2. Overview of our method: Text prompts are encoded via the CLIP text encoder to generate classification weights T , and sample images are processed via the CLIP image encoder to obtain teacher image features I f . Each T i multiplies I f to produce logits W i for corresponding prompts. W i are weighted by their max logit and averaged to create weighted logits W , which concatenated with I f to form the teacher's sample node F t . Simultaneously, the student model processes images to produce features F ori that match the dimensions of I f and classification logits W s , which are then concatenated to form the student sample node F s . Class proxy nodes P t and P s are updated by sample nodes. Pearson correlation coefficients between sample and class proxy nodes form relational graph edges E t and E s . Finally, node and edge alignments transfer task-relevant knowledge to student model, optimizing distillation effectiveness. distribution. It proposes a variation of softmax and introduces a variable T to generate softened labels.DML [4]breaks the predefined \"strong-weak relationship\" of traditional distillation models, enabling a group of student networks to learn from and guide each other during training.",
            "score": 0.5320164996219455,
            "section_title": "II. RELATED WORKS",
            "char_start_offset": 7695,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2312
                }
            ],
            "ref_mentions": [
                {
                    "start": 586,
                    "end": 589,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 591,
                    "end": 594,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 701,
                    "end": 704,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 706,
                    "end": 710,
                    "matchedPaperCorpusId": "236882796"
                },
                {
                    "start": 790,
                    "end": 794,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "198185886"
                },
                {
                    "start": 887,
                    "end": 891,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "245011127"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.794921875
        },
        {
            "corpus_id": "257757091",
            "title": "Mixed-Type Wafer Classification For Low Memory Devices Using Knowledge Distillation",
            "text": "where T (x) and S(x) are the feature maps of the intermediate layers of the teacher and student model. The transformation \u03a6 T and \u03a6 S are applied when the feature map has different shapes. \n\nRelation-Based Knowledge Distillation: Relation-based knowledge distillation further explores the relationships between the different layers in the teacher and student model. In general, the distillation loss of relation-based knowledge based on the relations of feature maps can be formulated as \n\nwhere T (x) and S(x) are feature maps from different layers in teacher and student models, respectively. Pairs of feature maps are picked from both teacher and student models, T (x) and \u0164 (x) from the teacher and \u015c(x), \u0160(x) from the student. \u03a8 T and \u03a8 S are the similarity functions for the pair of feature maps sampled and L R is the correlation function between the teacher and student network.",
            "score": 0.5316681618842171,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 188
                },
                {
                    "start": 191,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 886
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53759765625
        },
        {
            "corpus_id": "277940190",
            "title": "Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models",
            "text": "Feature-based knowledge distillation (KD) is a variant of the KD technique that focuses on transferring knowledge through intermediate features rather than just output logits. This approach has gained significant attention in recent years, with several papers proposing novel methods to improve feature alignment between teacher and student models. For instance, (Yu et al., 2024) proposed a block-wise logit distillation framework that combines the strengths of logit-based and feature-based methods, demonstrating comparable or superior results to those of state-of-the-art distillation methods. Similarly, (Zhang et al., 2022b) revisited logit-based KD and proposed a class-aware logit knowledge distillation method that extended logit distillation to both the instance-level and class-level, outperforming several prevailing logit-based and feature-based methods. A common theme among these studies is the importance of improving the feature alignment between teacher and student models. A standard approach involves minimizing the L 2 distance (Dao et al., 2021b) between intermediate features: \n\nwhere \n\ns denote the feature representations from the teacher and student, respectively, for the ith training instance. (Wang et al., 2023c) propose a novel loss term, ND loss, that encourages the student model to produce large-norm features and aligns the direction of student features with teacher class-means, leading to state-of-the-art performance on several benchmarks. This highlights the significance of regularizing feature norms and directions in knowledge distillation. Furthermore, (Ji et al., 2021) proposed an attention-based feature matching approach, demonstrating the effectiveness of using attention mechanisms to align features between teacher and student models. (Ruffy and Chahal, 2019) examined various KD strategies for simple classification tasks and found that many feature-based methods are difficult to reproduce and lack generalizability. This suggests a need for simpler, more effective, and reproducible distillation methods that can be applied to a wide range of tasks and datasets.",
            "score": 0.5315760330346548,
            "section_title": "XV. FEATURE-BASED KNOWLEDGE DISTILLATION",
            "char_start_offset": 56616,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1099
                },
                {
                    "start": 1102,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2115
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 380,
                    "matchedPaperCorpusId": "273811396"
                },
                {
                    "start": 1596,
                    "end": 1613,
                    "matchedPaperCorpusId": "231839582"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89794921875
        },
        {
            "corpus_id": "269009659",
            "title": "CLIP-Embed-KD: Computationally Efficient Knowledge Distillation Using Embeddings as Teachers",
            "text": "Knowledge distillation (KD) involves three components: a) teacher model which is typically a large model exhibiting the behavior that we would like to distill into a smaller model; b) the smaller student model; c) a distillation loss that captures the difference between the teacher and student outputs with the goal of minimizing their differences:\n\nHere, L D is the overall distillation loss over the output logits of the student, z s and the output logits of the teacher, z t .\n\nThe term L CE is the typical cross-entropy supervision loss of the student logits and ground truth labels \u0177.Additionally, L KD represents the distillation term that captures the difference between teacher and student logits, and is commonly the KL-divergence between z s and z t .The scalars \u03b1 1 , \u03b1 2 represent a weighting of the two losses.Minimizing L D aligns the student and teacher model behaviors while minimizing the student loss with respect to the ground truth \u0177.Follow up research has proposed several variations on the loss functions [5].However, in all cases of KD, even if the teacher parameters are frozen (i.e., no backward pass is needed), computing the logits z t for the loss function requires forward passes through the teacher model which is often much larger than the student model [5].Thus, a significant proportion of the computational resources in KD are allocated towards the operation of the teacher.\n\nOur preliminary work is motivated to address this issue and we present our approach in two parts: CLIP-Teacher-KD evaluates the performance of using CLIP's pre-training objective within distillation, but requires running repeated forward passes through the teacher model.We subsequently adapt this to CLIP-Embed-KD which uses pre-computed embeddings in place of the full teacher model.",
            "score": 0.5315438941275455,
            "section_title": "Knowledge distillation",
            "char_start_offset": 4181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 349
                },
                {
                    "start": 351,
                    "end": 480
                },
                {
                    "start": 482,
                    "end": 590
                },
                {
                    "start": 590,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 824
                },
                {
                    "start": 824,
                    "end": 955
                },
                {
                    "start": 955,
                    "end": 1032
                },
                {
                    "start": 1032,
                    "end": 1290
                },
                {
                    "start": 1290,
                    "end": 1409
                },
                {
                    "start": 1411,
                    "end": 1682
                },
                {
                    "start": 1682,
                    "end": 1796
                }
            ],
            "ref_mentions": [
                {
                    "start": 1028,
                    "end": 1031,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1286,
                    "end": 1289,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8642578125
        },
        {
            "corpus_id": "268819314",
            "title": "MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks",
            "text": "Model distillation (Hinton et al., 2015;Gou et al., 2021) is the process of transferring knowledge from large models (teacher models) to small ones (student models).Traditional distillation optimizes the similarity between logits produced by the teacher and student models (Hinton et al., 2015;Kim et al., 2019;Mirzadeh et al., 2020).Symbolic distillation (West et al., 2022;Li et al., 2023;West et al., 2023) for language models learns a student model on texts generated by the teacher model.In comparison with traditional distillation, symbolic distillation allows the student model to focus on one aspect of the teacher model (West et al., 2022), which can be some high-level ability, such as chain-of-thought reasoning (Li et al., 2023), with much smaller model size.For IE, symbolic model distillation has been successfully applied for an IE subtask, NER (Zhou et al., 2023), which distills an NER model that can extract entities in a broad domain.This paper aims to distill the cross-IE task ability of LLMs, i.e., meta-understanding of IE and proposes a meta-model that can effectively learn IE tasks with few examples.",
            "score": 0.5315187168606774,
            "section_title": "Model Distillation",
            "char_start_offset": 6392,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 334
                },
                {
                    "start": 334,
                    "end": 493
                },
                {
                    "start": 493,
                    "end": 771
                },
                {
                    "start": 771,
                    "end": 953
                },
                {
                    "start": 953,
                    "end": 1126
                }
            ],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 57,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 356,
                    "end": 375,
                    "matchedPaperCorpusId": "238857304"
                },
                {
                    "start": 375,
                    "end": 391,
                    "matchedPaperCorpusId": "259251773"
                },
                {
                    "start": 391,
                    "end": 409,
                    "matchedPaperCorpusId": "266163405"
                },
                {
                    "start": 629,
                    "end": 648,
                    "matchedPaperCorpusId": "238857304"
                },
                {
                    "start": 723,
                    "end": 740,
                    "matchedPaperCorpusId": "259251773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "258187531",
            "title": "Deep Collective Knowledge Distillation",
            "text": "Knowledge distillation [8] is an effective method for compressing a heavy teacher model to a lighter student model. The main idea of knowledge distillation is that a teacher model with higher capacity and better performance distills the softened output distribution into a student model as knowledge. Hinton et al. [8] explained that softened outputs with higher entropy than hard labels provide much richer information. In previous studies [1,8,17], assigning the probabilities to other classes, which leads to increased entropy, was effective in generalizing a network. These probabilities provided valuable information about the correlation between other classes and Dubey et al. [4] showed that maximum entropy training with these correlation information is effective. Many studies [7,10,[14][15][16]18,21,24] on knowledge distillation have focused on efficiently transferring teacher's knowledge to students. We took a step further by adopting a different approach to knowledge enrichment. If a student model learns from only a teacher's soft targets, then the student will only imitate the teacher. However, if the knowledge of peer students is additionally given, it can help a student model outperform the student model learned only from a teacher. Our work focuses on creating a student model that can have rich representation by training not only with the knowledge provided by the teacher but also with additional knowledge of various correlations between classes provided by the peer students. As a method for generating additional knowledge, we propose gathering knowledge from multiple students. \n\nMutual learning methods [2,5,12,26] aim to train powerful student models using ensembled knowledge of multiple untrained students without a pretrained teacher model. These methods train multistudent models with knowledge generated on-the-fly from students' logits, and this online knowledge is often generated to better represent the ground truth or soft targets of the teacher model. However, we propose an approach for generating additional online knowledge containing diverse correlation information from multistudent models, not similar to the ground truth or the teacher's soft targets. Since the teacher model learned with the supervised learning manner becomes overconfident [20], it may overlook other correlation information. Thus, we believe that the additional knowledge, including correlational information between classes, can assist the teacher model.",
            "score": 0.5312958961445524,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2346
                },
                {
                    "start": 2347,
                    "end": 2477
                }
            ],
            "ref_mentions": [
                {
                    "start": 441,
                    "end": 444,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 446,
                    "end": 449,
                    "matchedPaperCorpusId": "9545399"
                },
                {
                    "start": 789,
                    "end": 792,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 792,
                    "end": 796,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 800,
                    "end": 804,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 804,
                    "end": 807,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 807,
                    "end": 810,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 810,
                    "end": 813,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1636,
                    "end": 1639,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 1639,
                    "end": 1641,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1641,
                    "end": 1644,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1644,
                    "end": 1647,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "266939439",
            "title": "DSP-KD: Dual-Stage Progressive Knowledge Distillation for Skin Disease Classification",
            "text": "Broadly speaking, knowledge distillation refers to extracting knowledge from a larger deep neural network and transferring it into a smaller neural network model by aligning two different types of information sources [30]: (a) those utilizing the teacher's output probabilities (logit-based). In 2015, Hinton et al. formally popularized vanilla knowledge distillation, with the core idea of employing the teacher's prediction logits as soft labels to guide the student [9]. Zhou et al. addressed the bias-variance tradeoff by proposing weighted soft labels and validated its effectiveness through experiments with standard evaluation benchmarks [31]. Chi et al. introduced Normalized Knowledge Distillation (NormKD), aiming to customize the temperature for each sample based on the characteristic of the sample's logit distribution. It demonstrated significantly better classification performance than vanilla knowledge distillation with almost no additional computational or storage costs [11]. (b) Those utilizing the teacher's intermediate representations (featurebased). Romero et al. first introduced the idea that activations, neurons, or features from intermediate layers can serve as knowledge to guide the student model [32]. Inspired by this, Chen et al. enhanced the effectiveness of knowledge distillation by learning feature maps from the teacher network through two main techniques-shared classifier and generative adversarial network [12]. Kim et al. utilized a paraphraser to perform teacher factor extraction and transferred the learned factor to the student through convolutional operations [33]. Yang et al. proposed Masked Generative Distillation, a versatile feature-based distillation method applicable to diverse tasks. It involved masking random pixels of the student's feature, compelling it to generate the teacher's full feature [34]. \n\nIn the human world, we integrate information from various sources to gain cognition. Similarly, in the task of knowledge distillation, a student model can also acquire multi-level knowledge. Based on this concept, most recent research has focused on multi-teacher distillation, using several distinct deep learning models as teachers to impart knowledge to the student.",
            "score": 0.5306584576826459,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 12932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2233
                }
            ],
            "ref_mentions": [
                {
                    "start": 469,
                    "end": 472,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 645,
                    "end": 649,
                    "matchedPaperCorpusId": "231740588"
                },
                {
                    "start": 1449,
                    "end": 1453,
                    "matchedPaperCorpusId": "54441502"
                },
                {
                    "start": 1609,
                    "end": 1613,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 1856,
                    "end": 1860,
                    "matchedPaperCorpusId": "248506080"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "248243663",
            "title": "Optimizing Performance of Federated Person Re-identification: Benchmarking and Analysis",
            "text": "Besides client clustering, we adopt knowledge distillation (KD) to elevate performance and improve the convergence of distillation is a method proposed by Hinton et, al. [14] to transfer knowledge from a teacher model to a student model, where the teacher model contains more knowledge than the student model. We adopt knowledge distillation to better transfer knowledge from local models to the global model, regarding clients as teachers and the server as the student. \n\nAfter clients finish local training and upload models, we apply knowledge distillation with a public shared dataset D \u210e in the server. Figure 3b illustrates the additional steps required from knowledge distillation: (1) The server uses each trained model   of client  to generate soft labels3 \u2113  using samples of D \u210e . These soft labels represent the knowledge of clients' models. (2) Apart from model aggregation, the server aggregates these soft labels with \n\nThe server fine-tunes the global model with D \u210e and corresponding labels \u2113 to learn the distilled knowledge.",
            "score": 0.5303372473898508,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 18063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 932
                },
                {
                    "start": 935,
                    "end": 1043
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77783203125
        },
        {
            "corpus_id": "247318671",
            "title": "Efficient Sub-structured Knowledge Distillation",
            "text": "Our idea is to locally match all sub-structured predictions of the teacher model and those of its student model, which avoids adopting time-consuming techniques like DP to globally search for output structures. Intuitively, similar predictions over the sub-structures lead to an accurate approximation of complete structures. Furthermore, mimicking the sub-structured predictions allows the student model to better imitate the internal behavior of the teacher model (Ba and Caruana, 2014). For example, if a label sequences [AA] is given score 10 and \u221210 by the teacher model while its student predicts all 0 Figure 1: An example of our efficient sub-structured knowledge distillation. KD is short for knowledge distillation. Traditional KD matches output distributions directly. However, the output space of structured prediction models is exponential in size, and thus infeasible to directly match output distributions. By matching sub-structured scores, we efficiently transfer the structured knowledge from the teacher to the student. \n\nfor both of A, the student learns nothing from the teacher if the globally structured knowledge distillation is adopted as the training objective since they all sum up to 0. Performing sub-structured knowledge distillation, however, enables the student model to learn the detailed behavior of the teacher. Besides, our simple formulation benefits in parallel matrix computation, which further reduces the training time. \n\nWe have performed experiments on two structured prediction tasks, named entity recognition (NER) and syntactic chunking, showing that our approach outperforms previous methods with far less training time.",
            "score": 0.5292472427221704,
            "section_title": "Introduction",
            "char_start_offset": 2067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1667
                }
            ],
            "ref_mentions": [
                {
                    "start": 466,
                    "end": 488,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "276533262",
            "title": "Decoupled Classifier Knowledge Distillation",
            "text": "SKD serves to retain the accurate knowledge obtained from feature alignment. According to Eq 17, DCKD fixes this knowledge by restructuring the output. To validate SKD's effectiveness in suppressing repetitive learning. These experiments Table 1 aim to demonstrate that traditional, undifferentiated KD is incompatible with the current feature-based knowledge distillation approach. \n\nAs shown in Table 2, combining either traditional KD methods or our PKD approach directly with current feature-based knowledge distillation leads to a significant performance drop. This decline even exceeds previous improvements; for example, when using ResNet324 as the teacher model and ResNet84 as the student model, applying traditional KD for output distillation results in a substantial performance decrease of 1.91. According to the formula, SKD prunes portions of knowledge to avoid redundant learning of already mastered information. In contrast, traditional KD and PKD lack adaptive adjustments, resulting in redundant knowledge learning. Output-based distillation and feature-based distillation focus on different aspects of the model: the former primarily targets the output layer, while the latter emphasizes the underlying structure. The DCKD method enhances the learning capacity of the student model by adjusting the output structure to incorporate knowledge that has not been fully processed into the distillation process. According to the results in Table 2, SKD performs excellently in most scenarios, but when combined with PKD, the overall model performance is further improved. This improvement is evident across multiple teacher-student model combinations, particularly when WRN-40-2 is used as the teacher model and ResNet8x4 or ShuffleNet-V1 as the student model, where performance varies in degree. \n\nUsing SKD or PKD alone has limited effect and does not significantly improve model performance, but when combined, performance is noticeably enhanced, demonstrating their synergistic effect in knowledge extraction and transfer. As shown in Eq 8, SKD decouples the output to provide a clearer knowledge structure for the student model's learning. It also introduces the same pruning operation in both the student and teacher models to prevent redundant learning content during knowledge transfer.",
            "score": 0.5287734443746619,
            "section_title": "Effects of SKD and PKD",
            "char_start_offset": 17779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2307
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85546875
        },
        {
            "corpus_id": "273811396",
            "title": "Decoupling Dark Knowledge via Block-Wise Logit Distillation for Feature-Level Alignment",
            "text": "Here, O(\u2022) symbolizes the estimation error associated with the statistical learning processes given a specified dataset size, while \u03f5 sr delineates the approximation error reflective of the model's inherent capability. Within this framework, f r \u2208 F r represents the authentic (ground truth) target function, and f s \u2208 F s denotes the learner or student function. R specifies the error magnitude, |\u2022| C quantifies the function class capacity, n denotes the dataset size, and \u03b1 sr ranges between 1/2 to 1, indicating a learning rate which approaches 1/2 for more complex tasks and nears 1 for simpler tasks. Similarly, assuming f t \u2208 F t as the teacher function, we have: \n\nwhere \u03b1 tr and \u03f5 tr are correspondingly defined to encapsulate the learning dynamics when the teacher is educated from scratch. Building upon the groundwork established by [32], and assuming that training is conducted through pure distillation (\u03bb = 1), we facilitate direct knowledge transfer from the teacher to the student, thereby establishing a baseline for knowledge distillation. From this baseline, we derive: \n\nin which \u03b1 st and \u03f5 st relate to the student's learning process from the teacher. [32] highlighted the necessity for |F t | C to remain minimal to ensure that traditional knowledge distillation surpasses rudimentary learning processes. Furthermore, it is imperative to recognize that \u03b1 sr \u2264 \u03b1 st and \u03b1 sr \u2264 \u03b1 tr , with a larger gap indicating a reduced rate of learning. Additionally, according to assumptions made by [16], \u03f5 tr + \u03f5 st \u2264 \u03f5 sr . This setup confirms effective knowledge distillation: (32) This relationship suggests that the aggregate error bound in traditional distillation is less than or equal to that in scratch learning, valid asymptotically as n \u2192 \u221e. In scenarios where the sample size is finite and |F t | C is excessively large, these inequalities might fail, indicating potential shortcomings in the distillation process. Another failure scenario for traditional knowledge distillation arises when there is a significant disparity in capacity between the student and teacher, particularly when \u03b1 st is minimal.",
            "score": 0.5287001482707556,
            "section_title": "IV. THEORETICAL ANALYSIS",
            "char_start_offset": 16967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1089
                },
                {
                    "start": 1092,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2126
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6787109375
        },
        {
            "corpus_id": "264952263",
            "title": "A lightweight speech recognition method with target-swap knowledge distillation for Mandarin air traffic control communications",
            "text": "On the basis of this assumption, another comparative experiment was conducted to verify the effectiveness of the TSKD, with the predictive top-k output of the teacher model for each segment as the information exchange. As shown in Table 4, the experimental results show that exchanging top-k information is an effective knowledge distillation approach, nevertheless, its performance does not surpass that of the target class, i.e., TSKD. \n\nTo further validate the effectiveness of TSKD proposed in this article, several typical knowledge distillation methods were implemented in homogeneous and heterogeneous network structures. The experimental results are presented in Tables 5 and 6, respectively. The results demonstrate that under these six homogeneous or heterogeneous architectures, the TSKD enhanced the recognition accuracy of all student models by an average of 1.13%, achieving the best distillation results in most experiments. \n\nSpecifically, the TSKD dramatically boosts the Transformer student model under heterogeneous architectures, reducing the SER by 2.13%. A plausible explanation is that the Conformer teacher model may enable the Transformer student model to capture",
            "score": 0.5284688424664687,
            "section_title": "Effectiveness of TSKD in homogeneous and heterogeneous architectures",
            "char_start_offset": 19467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68408203125
        },
        {
            "corpus_id": "269187844",
            "title": "Dynamic Self-adaptive Multiscale Distillation from Pre-trained Multimodal Large Model for Efficient Cross-modal Representation Learning",
            "text": "The successful application of deep learning models across various domains has highlighted the challenge of deploying these large models in resource-constrained environments.Knowledge distillation emerges as a pivotal technique for this, efficiently compressing models by transferring knowledge from a complex teacher model to a simpler, lightweight student model, thus preserving the teacher's performance within a reduced model size.\n\nExisting knowledge distillation methods are categorized into three types: response-based, relationship-based, and feature-based.KD [18] employs response-based distillation using the teacher's final classifier outputs for richer soft labels.DKD [59] categorizes logits into target and non-target classes, emphasizing the importance of non-target information.BAN [14] uses target class logits as weighting factors for samples.In relationship-based methods, FSP [56] and RKD [38] construct relationship matrices between teacher and student features to capture and distill structural knowledge, enhancing understanding through differences in sample relationships and geometrical configurations within the same batch.\n\nIn feature-based distillation, FitNets [44] uses middle layer outputs of the teacher model as \"Hints\" to guide the student model, noting differences in feature activations.FT [21] enhances knowledge transfer by pre-training connectors in the teacher model.AB [17] employs a binarization technique to filter features between the teacher and student models, while AT [57] transforms features into attention values to optimize distillation.CRD [46] utilizes contrastive learning to align corresponding features between teacher and student as positive examples and separate non-corresponding ones as negatives.OFD [16] reviews these techniques, analyzing how feature location, connector composition, and loss functions influence the efficiency of knowledge transfer.\n\nHowever, previous distillation methods are tailored for unimodal scenarios, operate at a single scale, and often require additional features or information, limiting their applicability to multimodal Two people in the photo are playing the guitar and the other is poking at him.",
            "score": 0.528259452452516,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7292,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 173,
                    "end": 434
                },
                {
                    "start": 436,
                    "end": 564
                },
                {
                    "start": 564,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 793
                },
                {
                    "start": 793,
                    "end": 860
                },
                {
                    "start": 860,
                    "end": 1148
                },
                {
                    "start": 1150,
                    "end": 1322
                },
                {
                    "start": 1322,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1587
                },
                {
                    "start": 1587,
                    "end": 1756
                },
                {
                    "start": 1756,
                    "end": 1912
                },
                {
                    "start": 1914,
                    "end": 2192
                }
            ],
            "ref_mentions": [
                {
                    "start": 680,
                    "end": 684,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 895,
                    "end": 899,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 908,
                    "end": 912,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1325,
                    "end": 1329,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 1409,
                    "end": 1413,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1515,
                    "end": 1519,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1591,
                    "end": 1595,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1760,
                    "end": 1764,
                    "matchedPaperCorpusId": "102483181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "273100519",
            "title": "Ensemble of vision transformer architectures for efficient Alzheimer\u2019s Disease classification",
            "text": "Touvron et al. [9] introduced DeiT to mitigate ViT's reliance on extensive datasets. DeiT facilitates ViT training on smaller datasets such as ImageNet-1k and employs a teacher-student framework to train a more compact transformer model. A new distillation token was introduced to interact with the class and patch tokens in addition to the token utilized by ViT. Distillation can be categorized into two types: soft and hard. The teacher model supplies probability distributions in soft distillation as \"soft\" labels. In hard distillation, \"hard\" labels are the class predictions or decisions the teacher model provides. The difference between the outputs of the teacher (1) and student models is minimized in soft distillation using Equation 3. \n\nIn hard distillation, the teacher's decision is taken as the true label. Equation 4 yields the hard distillation using the teacher's true label, y t = argmax c Z t (c). \n\nSoft and hard distillation are training methods used to transfer knowledge from a larger teacher model to a smaller student model. The selection between soft and hard distillation hinges on the model's specific objectives and the desired characteristics in the final predictions. These strategies aim to enhance the predictions of the \n\nstudent model by utilizing the knowledge distilled from a larger teacher model. Once the distillation process is complete, the student model can autonomously predict new, unseen data. Essentially, the distillation process represents a form of transfer learning where the student model acquires knowledge from the teacher model. The final classification utilizes only the trained student model, which independently makes predictions following the knowledge transfer process.",
            "score": 0.5281481726578044,
            "section_title": "Data-efficient image transformer (DeiT)",
            "char_start_offset": 15150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1730
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 18,
                    "matchedPaperCorpusId": "229363322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81689453125
        },
        {
            "corpus_id": "247446679",
            "title": "CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification",
            "text": "Nevertheless, the student models still outperform the no-KD baselines even when their teacher models are trained with only 5 epochs with poor performance. This further confirms that the student is not simply copying the teacher's behavior as copying the behavior of a teacher weaker than the student itself can only cause a performance drop, but our experiments show that cross-model KD still leads to a performance improvement in this case. 4. Self-KD leads to smaller or no improvement. We observe that performance improvement can also sometimes be achieved by using the same teacher and student architecture (the main diagonal of Table 3), particularly for AST models. This is as expected because 1) the soft labels helps optimization of the student model, and 2) since we use strong data augmentation (e.g., SpecAugment, mixup, random noise, and time shift) during training, knowledge distillation with the same model architecture can have a similar effect to selftraining [38]. Nevertheless, the improvement led by selfknowledge distillation is much smaller than cross-model knowledge distillation, which demonstrates the importance of using a different teacher for knowledge distillation. \n\n5. Iterative knowledge distillation does not further improve model performance. Since cross-model knowledge distillation works bi-directionally, the performance of both KD-CNN and KD-AST are better than no-KD baselines. Will using KD models instead of no-KD models as teachers lead to even better students? The answer is no, as shown in Table 4. While KD models (with their optimal teachers) are stronger, they are worse teachers than the no-KD models. This again proves our finding that a stronger teacher is not necessarily a better teacher as the student is not simply copying the teacher's behavior. \n\nTo summarize, in this section, we find both CNN and AST models prefer a small and different teacher. Cross-model knowledge distillation is beyond simple behavior copying but more about transferring and combining the strength of the teacher and student models as the performance of the teacher is surprisingly not important.",
            "score": 0.528128516792237,
            "section_title": "Which model is a good teacher?",
            "char_start_offset": 18482,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1800
                },
                {
                    "start": 1803,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 977,
                    "end": 981,
                    "matchedPaperCorpusId": "207853355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8349609375
        },
        {
            "corpus_id": "256615641",
            "title": "Knowledge Distillation in Vision Transformers: A Critical Review",
            "text": "Sucheng et al. [48] proposed another novel solution named Cross Inductive Bias Distillation (Coadvice). The authors in the paper [48] stated that teacher accuracy is not the dominant factor in student accuracy, but inductive teacher bias is. Co-advising the student transformer can be done well with a lightweight teacher with a different architectural inductive bias providing outstanding results. As a result, teachers with different inductive biases possess diverse knowledge despite being trained on the same dataset, and models with different inductive biases tend to focus on diverse patterns. During distillation, the student gained a more accurate and comprehensive understanding of the data and compounds because of distilled diverse knowledge by the multi-teacher network. \n\nA token inductive bias alignment was also proposed to align the inductive bias of the token with its target teacher model. Proposed vision transformers (CiT) in [48] outperformed all existing ViTs using only lightweight teachers with cross-inductive bias distillation methods. Kan Wu et al. [49] proposed a different solution known as Tiny-ViT compared to the previously mentioned methods focused most of the time on finetuning distillation. They observed that small pre-trained models could benefit from knowledge transferred from large to small pre-trained models using massive pretraining data. In particular, the authors employed distillation during pretraining to transfer knowledge. It is possible to save memory and computation overhead by sparsifying and storing logits in advance for large teacher models. A large pre-trained model with computation and parameter constraints is automatically scaled down to produce tiny student transformers. Extensive experiments have demonstrated the effectiveness of Tiny-ViT. It achieved top-1 Accuracy of 84.8% using only 21M parameters, comparable to Swin-B without using as many parameters as Swin-B. Tiny-ViT also achieved 86.5% accuracy with increased image resolutions, slightly exceeding Swin-L's Accuracy with only 11% parameters. The proposed work in the paper [49] focused on pretraining distillation, making small models more capable of learning from larger models and transferring those skills to downstream tasks.",
            "score": 0.5280173756984009,
            "section_title": "4.3) Cross Inductive Bias Distillation (Coadvice)",
            "char_start_offset": 30914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 782
                },
                {
                    "start": 785,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 19,
                    "matchedPaperCorpusId": "235606268"
                },
                {
                    "start": 129,
                    "end": 133,
                    "matchedPaperCorpusId": "235606268"
                },
                {
                    "start": 946,
                    "end": 950,
                    "matchedPaperCorpusId": "235606268"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83056640625
        },
        {
            "corpus_id": "252918755",
            "title": "PTDE: Personalized Training with Distilled Execution for Multi-Agent Reinforcement Learning",
            "text": "Knowledge distillation [Hinton et al., 2015] is proposed to compress big models. It distills the knowledge generated from a larger network into a smaller network. Policy distillation [Rusu et al., 2015] presents a novel knowledge distillation method which can be used in reinforcement learning to extract the policy of agent and train a new network with an expert level performance and better efficiency. CTDS [Zhao et al., 2022] proposes a novel Centralized Teacher with Decentralized Student framework which consists of a teacher model and a student model to alleviate the inefficiency caused by the limitation of local observability.",
            "score": 0.5279887785161057,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 636
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7529296875
        },
        {
            "corpus_id": "261244235",
            "title": "Boosting Residual Networks with Group Knowledge",
            "text": "Conventional Knowledge distillation (KD) (Hinton, Vinyals, and Dean 2015) transfers knowledge from a teacher network to a student network via logits (Kim, Park, and Kwak 2018;Shen and Xing 2022) or features (Bai et al. 2020;Jung et al. 2021), aiming at obtaining a compact and accurate student network. It usually requires additional cost because of training a larger teacher network. As a comparison, we do not require larger teachers or additional structures. And our target is to improve any given residual network effectively and efficiently by training its subnets well. Most related concepts among KD are described in detail as follows. Ensemble Distillation. As the ensemble method (Dietterich 2000) is a useful technique to improve the performance of deep learning models, it is generally considered that, an ensemble of multiple teacher models can commonly provide supervision with higher quality compared with a single teacher. (Du et al. 2020) studies the conflicts and competitions among teachers and introduces a dynamic weighting method for better fusing teachers' knowledge. However, typical ensemble distillation methods need additional teacher models to provide supervision for a single student model. It requires huge computation cost and is not suitable for multiple coupled subnet students. Differently, we do not need extra teacher models or inference. For multiple unique coupled subnet students, we specifically provide suitable supervision by aggregating the hierarchical subnet group knowledge. Self Distillation. To save the cost introduced by a larger teacher network, the self distillation (SD) attempts to provide supervision within the student network itself in training. (Yun et al. 2020) narrows down the predictive distribution deviation between different samples of the same label to provide the regularization supervision. (Deng and Zhang 2021;Kim et al. 2021) utilize the snapshot of the previous output logits as supervision to learn from prior experience. (Shen et al. 2022) rearranges the data sampling by including mini-batch from previous iteration, and uses the on-the-fly soft targets generated in the previous iteration to supervise the network. Similarly, we also consider that the historic information during the training contains abundant knowledge. However, previous SD methods only utilize the intermediate features or output logits of a single main net.",
            "score": 0.5274043529740646,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6136,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2189
                },
                {
                    "start": 2190,
                    "end": 2296
                },
                {
                    "start": 2297,
                    "end": 2403
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 73,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 149,
                    "end": 175,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 175,
                    "end": 194,
                    "matchedPaperCorpusId": "244799371"
                },
                {
                    "start": 224,
                    "end": 241,
                    "matchedPaperCorpusId": "235368045"
                },
                {
                    "start": 689,
                    "end": 706,
                    "matchedPaperCorpusId": "10765854"
                },
                {
                    "start": 1702,
                    "end": 1718,
                    "matchedPaperCorpusId": "252780856"
                },
                {
                    "start": 1858,
                    "end": 1879,
                    "matchedPaperCorpusId": "229371332"
                },
                {
                    "start": 1879,
                    "end": 1894,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 1994,
                    "end": 2012,
                    "matchedPaperCorpusId": "244799371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81689453125
        },
        {
            "corpus_id": "258048760",
            "title": "A Survey on Recent Teacher-student Learning Studies",
            "text": "Mask distillation is a variant of knowledge distillation [12,20] that focuses on transferring the attention mechanism learned by the teacher to the student. In mask distillation, the teacher is trained to produce a mask that indicates the importance of each input feature for a particular prediction. The student is then trained to imitate the predictions of the teacher while using the mask to weigh the importance of the input features. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better capture the attention mechanism learned by the teacher. \n\nFinally, decoupling distillation is a variant of knowledge distillation [1] that decouples the distillation loss from the task loss. In decoupling distillation, the student is trained to imitate the outputs of the teacher on a validation set, while being trained on the task loss on the training set. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better balance the trade-off between preserving the knowledge of the teacher and adapting to the task. \n\nIn conclusion, knowledge distillation is a widely used method for compressing and accelerating deep neural networks 2 TEACHER ASSISTANT When knowledge distillation is performed, increasing the model size of the teacher network instead makes the knowledge distillation worse. The results are as follows: the student network is a 2-layer CNN, while the teacher network is a 4-10-layer CNN, respectively, and it can be seen from the results that continuing to increase the depth of the teacher network does not necessarily improve the performance of knowledge distillation. \n\nTo further prove that the gap does affect knowledge distillation, the authors conducted a small comparison experiment by fixing the teacher network as a 10-layer CNN and changing the number of layers of the student network, and the results are as follows. \n\nBased on the above observations, it is not difficult to think that a medium-sized network (teacher assistant [16], TA) should be added between the large network and the small network, and the teacher network should be used to distill the TA first, and then the student network should be distilled by the TA. When TA was added, the distillation effect of the student network was significantly better than the direct distillation of the teacher network.",
            "score": 0.5273725185097047,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1958
                },
                {
                    "start": 1961,
                    "end": 2268
                },
                {
                    "start": 2269,
                    "end": 2412
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "252626689"
                },
                {
                    "start": 686,
                    "end": 689,
                    "matchedPaperCorpusId": "256105379"
                },
                {
                    "start": 2070,
                    "end": 2074,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "265680820",
            "title": "Learn by Yourself: A Feature-Augmented Self-Distillation Convolutional Neural Network for Remote Sensing Scene Image Classification",
            "text": "In the process of knowledge distillation, a large teacher model is usually use training first, and then the teacher model is adopted to generate soft labels, which are together with the output of the student model to train the student model. During the ing process, different weights can be used to balance the relative importance of har soft objects. Choosing an appropriate teacher model is crucial to the effect of know distillation. In general, the teacher model should be complex and accurate enough to vide high-quality soft targets. A commonly used teacher model is a pre-trained deep ral network model. The student model is usually more lightweight and simplified the teacher model for deployment where computing resources are constrained. common student model design strategies include using shallow network structure reducing the number of network parameters. With the deepening of research, man proved and extended knowledge distillation methods have emerged. For exampl FitNets method [39] introduced the concept of intermediate layer alignment to alig intermediate layer outputs of the teacher model and the student model. The atte transfer method [40] learned knowledge from the teacher network by having the stu network imitate the attention map of the teacher network. The relational knowledg tillation method [41] exploited relational modeling to improve knowledge distillati comprehensive overhaul of the feature distillation method [42] adopted the feature lation, designed a new distillation loss, distilled features before the ReLU function retained negative values before distillation. Ahn et al. [43] proposed a variational mation distillation framework, which transfers the knowledge learned by Soft labels are the probability distributions output by the teacher model, which can provide richer information to help the student model learn. To transfer knowledge effectively, an appropriate loss function needs to be defined to measure the difference between the output of the student model and the output of the teacher model. Commonly used loss functions include the mean squared error [37], cross-entropy loss [37], and KL divergence [38]. The mean square error loss function measures the numerical difference of the output, while the cross-entropy loss function measures the difference in the probability distribution of the output. KL divergence (Kullback-Leibler divergence), also known as relative entropy, is an indicator used to measure the difference between two probability distributions.",
            "score": 0.5270434092510297,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 16621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2359
                },
                {
                    "start": 2360,
                    "end": 2522
                }
            ],
            "ref_mentions": [
                {
                    "start": 1326,
                    "end": 1330,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1451,
                    "end": 1455,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1619,
                    "end": 1623,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 2160,
                    "end": 2164,
                    "matchedPaperCorpusId": "116908168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8330078125
        },
        {
            "corpus_id": "256460933",
            "title": "Calibrating Student Models for Emotion-related Tasks",
            "text": "Knowledge and Self-distillation with Vanilla Models \n\nFigure 3 compares knowledge and self-distillation with vanilla models and illustrates that distillation settings yield better-calibrated models with lower ECE in all of the experiments with our setup (with the self-distillation being the best setting). Figure 4 shows that the errors obtained with our mixup method are much smaller in general compared to the other settings and yields best calibrated models. Table 3: Accuracy in percentage (%) for in-domain (GoEmotions, Yelp, EPITOME) and out-of-domain (Meld, SST, NewsEmpathy) datasets. LS, and TS refer to the label smoothing, and temperature scaling, respectively. KD refers to knowledge distillation from a teacher model that is specified at the subscript and a student model with a different architecture (for example, in KD RoBERTa , RoBERTa is used as the teacher in the knowledge distillation setting and BERT is used a the student model). For space restrictions, we do not include the student model. All reported values for the methods are mean \u00b1 std of three repetitions. \n\nIn",
            "score": 0.5268883340930895,
            "section_title": "C Visualisations of Comparison of the",
            "char_start_offset": 30661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 54,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1092
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5556640625
        },
        {
            "corpus_id": "258833199",
            "title": "DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining",
            "text": "A.1 Background and Related Work Knowledge Distillation (KD). The KD (Hinton et al., 2015) is one of the promising ways to transfer from a powerful large network or ensemble to a small network to meet the low-memory or fast execution requirements. BANs (Furlanello et al., 2018) sequentially distill the teacher model into multiple generations of student models with identical architecture to achieve better performance. BERT-PKD (Sun et al., 2019) distills patiently from multiple intermediate layers of the teacher model at the fine-tuning stage. DistilBERT (Sanh et al., 2019) and MiniLM (Wang et al., 2020) leverage knowledge distillation during the pre-training stage. TinyBERT (Jiao et al., 2020) sets a twostage knowledge distillation procedure that contains general-domain and tasks-specific distillation in Transformer (Vaswani et al., 2017). Despite their success, they may encounter difficulties affecting the sub-optimal performance in language understanding tasks due to the trade-off between model compression and performance loss. \n\nSemi-supervised Learning (SSL). The majority of SSL algorithms are primarily concentrated in the field of computer vision, including Pseudo Labeling (Lee et al., 2013), Mean Teacher (Tarvainen and Valpola, 2017), VAT (Miyato et al., 2019), Mix-Match (Berthelot et al., 2019), FixMatch (Sohn et al., 2020), CRMatch (Fan et al., 2023), Flex-Match (Zhang et al., 2021), AdaMatch (Berthelot et al., 2022), and SimMatch (Zheng et al., 2022), all of which exploit unlabeled data by encouraging invariant predictions to input perturbations (Sajjadi et al., 2016). The success of semi-supervised learning methods in the visual area motivates research in the NLP community. Typical techniques include VAMPIRE (Gururangan et al., 2019), Mix-Text (Chen et al., 2020) and UDA (Xie et al., 2020).",
            "score": 0.5267181267953249,
            "section_title": "A Appendix",
            "char_start_offset": 22836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1830
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 89,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 252,
                    "end": 277,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 429,
                    "end": 447,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 590,
                    "end": 609,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 682,
                    "end": 701,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 827,
                    "end": 849,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1196,
                    "end": 1214,
                    "matchedPaperCorpusId": "18507866"
                },
                {
                    "start": 1229,
                    "end": 1258,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 1264,
                    "end": 1285,
                    "matchedPaperCorpusId": "17504174"
                },
                {
                    "start": 1297,
                    "end": 1321,
                    "matchedPaperCorpusId": "146808485"
                },
                {
                    "start": 1332,
                    "end": 1350,
                    "matchedPaperCorpusId": "210839228"
                },
                {
                    "start": 1392,
                    "end": 1412,
                    "matchedPaperCorpusId": "239016453"
                },
                {
                    "start": 1423,
                    "end": 1447,
                    "matchedPaperCorpusId": "235376822"
                },
                {
                    "start": 1462,
                    "end": 1482,
                    "matchedPaperCorpusId": "247446931"
                },
                {
                    "start": 1580,
                    "end": 1602,
                    "matchedPaperCorpusId": "8102341"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65478515625
        },
        {
            "corpus_id": "270878760",
            "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
            "text": "The distillation of Bidirectional Long Short-Term Memory Networks (BiL-STM) [126] marks the earliest attempt to apply knowledge distillation to BERT [60].The distillation objective is to minimize the mean squared error loss between the logits of the student network and those of the teacher.This approach has been tested on three tasks: sentence classification and sentence matching.Experimental results show that the shallow BiLSTM-based model achieves performance comparable to the ELMo language model [104], but with approximately 100 times fewer parameters and a 15-fold increase in inference speed.Similarly, DistillBERT [111] initializes a shallower student model using the teacher's parameters and minimizes the difference in soft target probabilities between the teacher and student, a technique known as word-level knowledge distillation.It introduced a triple loss that combines language modeling, distillation, and cosine distance loss to leverage the inductive bias learned by the pre-trained model.DistilBERT achieved performance equivalent to or exceeding the ELMo baseline in nine tasks.Compared to BERT, DistilBERT maintains 97% of the performance while reducing the number of parameters by 40%.MixKD [75] extends the concept of encouraging students to mimic teachers' logits by using linear interpolation of example pairs.It improves the effectiveness of knowledge distillation by using data augmentation to create additional samples from the available task-specific data.This approach mirrors students learning more effectively from teachers by asking further questions to explore their answers and concepts in depth, providing more data for student models to extract insights from large-scale language models.Evaluation results across six datasets show that MixKD significantly outperforms traditional knowledge distillation and previous methods in compressing large language models.ReAugKD [163] includes both an inference stage and a training stage.In the inference stage, it aggregates soft labels generated by teachers that closely resemble student embeddings.\n\nDuring the training phase, a novel relationship KD loss is used to minimize the differences between teacher-student embeddings and their distributions.",
            "score": 0.5266861923405708,
            "section_title": "Logits-based KD.",
            "char_start_offset": 8921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 154,
                    "end": 291
                },
                {
                    "start": 291,
                    "end": 383
                },
                {
                    "start": 383,
                    "end": 603
                },
                {
                    "start": 603,
                    "end": 847
                },
                {
                    "start": 847,
                    "end": 1011
                },
                {
                    "start": 1011,
                    "end": 1102
                },
                {
                    "start": 1102,
                    "end": 1211
                },
                {
                    "start": 1211,
                    "end": 1339
                },
                {
                    "start": 1339,
                    "end": 1489
                },
                {
                    "start": 1489,
                    "end": 1728
                },
                {
                    "start": 1728,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 1970
                },
                {
                    "start": 1970,
                    "end": 2083
                },
                {
                    "start": 2085,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 153,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 504,
                    "end": 509,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "226226888"
                },
                {
                    "start": 1910,
                    "end": 1915,
                    "matchedPaperCorpusId": "259370551"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90625
        },
        {
            "corpus_id": "253098943",
            "title": "Respecting Transfer Gap in Knowledge Distillation",
            "text": "Knowledge distillation (KD) [21] transfers knowledge from a teacher model, e.g., a big, cumbersome, and energy-inefficient network, to a student model, e.g., a small, light, and energy-efficient network, to improve the performance of the student model. A common intuition is that a teacher with better performance will teach a stronger student. However, recent studies find that the teacher's accuracy is not a good indicator of the resultant student performance [8]. For example, a poorly-trained teacher with early stopping can still teach a better student [8,11,77]; or, a teacher with a smaller model size than the student is also a good teacher [77]; or, a teacher with the same architecture as the student helps to improve the student-self-distillation [13,82,81,27]. \n\nShould we view KD in a perspective of domain transfer [12,63], we would better understand the above counter-intuitive findings. From Figure 1, we can see that teacher predictions and ground-truth labels indeed behave differently. Although the teacher is trained on the balanced dataset, its predicted probability distribution over the dataset is imbalanced. Even on the same training set with the same model parameter, teachers with different temperature \u03c4 yield different \"soft label\" distributions from the ground-truth ones. This implies that human and teacher knowledge are from different domains, and there is a transfer gap that drives the \"dark knowledge\" [21] transferring from teacher to student-regardless of \"strong\" or \"weak\" teachers, it is a valid transfer as long as there is a gap. However, the transfer gap affects the distillation performance on the under-represented classes, i.e., classes on the tail of teacher predictions, which is overlooked in recent studies. Take CIFAR-100 as an example. We rank and divide the 100 classes into 4 groups according to the ranks of predicted probability. As shown in Table 1, compared to vanilla training, KD achieves better performance in all the subgroups. However, the increase in the top 25 classes is much higher than that in the last 25 classes, i.e., averagely 5.14% vs. 0.85%.",
            "score": 0.5263653859296779,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 559,
                    "end": 562,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 565,
                    "end": 568,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 650,
                    "end": 654,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 759,
                    "end": 763,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 763,
                    "end": 766,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 766,
                    "end": 769,
                    "matchedPaperCorpusId": "232302458"
                },
                {
                    "start": 769,
                    "end": 772,
                    "matchedPaperCorpusId": "232232777"
                },
                {
                    "start": 830,
                    "end": 834,
                    "matchedPaperCorpusId": "8548594"
                },
                {
                    "start": 834,
                    "end": 837,
                    "matchedPaperCorpusId": "29160099"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "259766297",
            "title": "The Staged Knowledge Distillation in Video Classification: Harmonizing Student Progress by a Complementary Weakly Supervised Framework",
            "text": "The final distillation result of our formalized RSKD is given as follows: \n\nwhere (R S , Y t ) represents the standard knowledge distillation loss, and \u03b7, \u03be, and \u03c4 are hyperparameters that control the importance of the consistency losses. The Rp\u2212CS , F e\u2212CS , and Re\u2212CS superscripts refer to the response-based, feature-based, and response-feature-based consistency losses, respectively. \n\nWe believe that the effectiveness of RSKD for knowledge transfer may depend on the complexity of the datasets involved. Similar to the learning process of humans, when the knowledge to be mastered is elementary, the learning process is short and resembles the response-based substage. On the other hand, when the knowledge to be learned is complex, the learning process is long and resembles the feature-based and response-based substages. Therefore, inappropriate ratios of \u03b7, \u03be, and \u03c4 may compromise the accuracy of the student model's predictions. It is worth noting that while RSKD does not always achieve the optimal value in some teacher-student combinations when the teacher and student network styles are the same, it outperforms other distillation methods in experiments of different network architectures. This phenomenon can be explained by the fact that the small size of CIFAR-100 data may not fully demonstrate the advantages of stage learning in the same style of network architecture. Further discussion on this topic can be found in IV, V, IX, and VII.",
            "score": 0.5259220576849877,
            "section_title": "B. Details about RSKD a) Formulation:",
            "char_start_offset": 25539,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 76,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 387
                },
                {
                    "start": 390,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1459
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66455078125
        },
        {
            "corpus_id": "269921267",
            "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
            "text": "Knowledge distillation, a core strategy in modern machine learning, focuses on solving the problem of balance between model size and computational efficiency.The core idea is to effectively transfer the deep knowledge and experience accumulated in large-scale, complex models (often referred to as \"teacher models\") to \"student models\" with smaller numbers of participants and lower computational requirements.\n\nSince then, this technology has rapidly attracted widespread attention from academia and industry, and has been expanded and deepened in several ways: Early stage (2015-2018) : Initial research has focused on simplifying network structure, reducing model volume and computational requirements, while maintaining the predictive performance of the model.In this period, the basic framework of knowledge distillation and the design of loss function were established.Technical deepening (2019-2021) : Researchers began to explore more refined distillation methods, including multi-teacher distillation, feature-stage distillation, relational distillation, etc.\n\nIn the field of deep learning, model knowledge lies in the configuration of parameters it learns through training, which guides the model on how best to extract features from input data and make predictions.Large networks, thanks to their large number of parameters and complex structure design, can capture deeper feature associations on large-scale data sets, showing superior learning ability and generalization performance.However, this advantage is often difficult to play directly in resource-constrained real-world application scenarios, because they have high requirements for computing resources and storage space.Knowledge distillation technology is born to solve this contradiction, it focuses not only on the final classification or regression results of the model output, but also on the teaching of the teacher model's confidence distribution (i.e., soft label) that each sample belongs to various categories.By designing a specific training mechanism, the student model tries to imitate the soft decision-making process while learning the real label, so that it can also \"inherit\" the decision logic and deep understanding of the data of the teacher model under limited parameters.This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.",
            "score": 0.5258158445654759,
            "section_title": "B. Knowledge distillation",
            "char_start_offset": 12089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 410
                },
                {
                    "start": 412,
                    "end": 764
                },
                {
                    "start": 764,
                    "end": 875
                },
                {
                    "start": 875,
                    "end": 1068
                },
                {
                    "start": 1070,
                    "end": 1277
                },
                {
                    "start": 1277,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1693
                },
                {
                    "start": 1693,
                    "end": 1993
                },
                {
                    "start": 1993,
                    "end": 2266
                },
                {
                    "start": 2266,
                    "end": 2446
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89208984375
        },
        {
            "corpus_id": "248683566",
            "title": "Teacher-student collaborative knowledge distillation for image classification",
            "text": "Ensembles have been widely used to improve model performance [27][28][29]. Since different models could be complementary, the outputs of multiple models with the same structure and different initialization training ensembles can be used to improve test performance. Several studies have found that ensembles are also effective in improving knowledge distillation performance. [30] found that ensemble teacher networks can effectively improve student network classification performance. To overcome offline-distillation issues, a strong teacher network is needed, [31] combined knowledge distillation and an ensemble approach to train a multi-branch network and then built a strong teacher model based on the branches of the ensemble to enhance the learning capabilities of the target network. However, this approach undoubtedly leads to a complex teacher model. In contrast with multiple teacher network ensembles [32], our student network constructs multiple exit outputs from shallow to deep, and only a small number of parameters are added to achieve the effect of multiple model ensembles. \n\nFinally we use ensembles to vote on the classification results of multiple sub-models in the student network, and obtain a strong classifier.",
            "score": 0.5256125023242231,
            "section_title": "Ensemble",
            "char_start_offset": 8904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1237
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 65,
                    "matchedPaperCorpusId": "10765854"
                },
                {
                    "start": 65,
                    "end": 69,
                    "matchedPaperCorpusId": "15455464"
                },
                {
                    "start": 69,
                    "end": 73,
                    "matchedPaperCorpusId": "11149239"
                },
                {
                    "start": 914,
                    "end": 918,
                    "matchedPaperCorpusId": "224818016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8330078125
        },
        {
            "corpus_id": "271855826",
            "title": "Low-Dimensional Federated Knowledge Graph Embedding via Knowledge Distillation",
            "text": "Knowledge distillation (KD) is a prominent technique employed in model compression [9,30,39]. In this approach, a larger, pretrained model, referred to as the teacher model, provides guidance to a smaller model, known as the student model. The objective is for the student model to approximate the outputs of the teacher model, thereby achieving similar performance [11]. \n\nMany embedding methods demonstrate enhanced performance with higher embedding dimensions. However, the trade-off includes longer inference times and larger model sizes for high-dimensional Knowledge Graph Embedding. Recently, several knowledge distillation methods [17,27,40] have been proposed to compress highdimensional embeddings into lower-dimensional representations for knowledge graphs. MulDE [27] employs multi-teacher distillation techniques aimed at improving the efficacy of low-dimensional models. However, it does not adequately address the dimension gap issue, which can significantly affect the effectiveness of the distillation process. DualDE [40] presents an innovative two-stage distillation approach where only a high-dimensional teacher model is pre-trained. Initially, the teacher model remains fixed while only the student model is updated under its guidance during the first stage. Subsequently, in the second stage, both the teacher and student models reciprocally guide each other's embedding updates. Despite involving only a pre-trained teacher model, DualDE still encounters significant training costs due to the necessity of updating the teacher model during the second stage [17]. IteDE [17] introduces an iterative knowledge distillation approach aiming at progressively reducing model sizes to a predefined target. Throughout this process, multiple models of intermediate sizes are trained under the guidance of adjacent higher-dimensional models. Essentially, it remains within the realm of multiple-teacher strategies. These methods typically prioritize enhancing the performance of low-dimensional embeddings with limited regard for the associated training costs. \n\nHowever, in the context of FKGE, the training procedure typically involves iterative communication between clients and a central server. Given constraints such as limited network bandwidth and client-side data usage, minimizing parameter transmission during FKGE training is crucial.",
            "score": 0.525551960630215,
            "section_title": "Knowledge Graph Embedding Compression with Knowledge Distillation",
            "char_start_offset": 8578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 371
                },
                {
                    "start": 374,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2074
                },
                {
                    "start": 2077,
                    "end": 2213
                },
                {
                    "start": 2214,
                    "end": 2360
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 86,
                    "matchedPaperCorpusId": "252846538"
                },
                {
                    "start": 86,
                    "end": 89,
                    "matchedPaperCorpusId": "257687846"
                },
                {
                    "start": 89,
                    "end": 92,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 366,
                    "end": 370,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 639,
                    "end": 643,
                    "matchedPaperCorpusId": "259629787"
                },
                {
                    "start": 643,
                    "end": 646,
                    "matchedPaperCorpusId": "222341253"
                },
                {
                    "start": 646,
                    "end": 649,
                    "matchedPaperCorpusId": "245124050"
                },
                {
                    "start": 775,
                    "end": 779,
                    "matchedPaperCorpusId": "222341253"
                },
                {
                    "start": 1035,
                    "end": 1039,
                    "matchedPaperCorpusId": "245124050"
                },
                {
                    "start": 1581,
                    "end": 1585,
                    "matchedPaperCorpusId": "259629787"
                },
                {
                    "start": 1593,
                    "end": 1597,
                    "matchedPaperCorpusId": "259629787"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8740234375
        },
        {
            "corpus_id": "275007781",
            "title": "Knowledge Reasoning- and Progressive Distillation-Integrated Detection of Electrical Construction Violations",
            "text": "To verify the effectiveness of progressive distillation, we conducted experiments using different teacher models. The results are shown in Table 5, and further demonstrate the impact of selecting various teacher models on the performance of the student network. This comparison highlights how different teacher models contribute to guiding the student network in learning more effectively, improving overall detection accuracy. It can be observed that compared to directly using a single advanced-teacher or primary-teacher model for knowledge distillation, the progressive distillation strategy achieves better results. Through phased learning, progressive distillation first leverages the rich expressive capabilities of the advanced teacher model to establish a solid knowledge foundation for the primary-teacher model. Subsequently, the primary teacher gradually guides the student model, enabling effective knowledge transfer. \n\nThis multi-stage process ensures that the student model can effectively absorb knowledge from both teacher models, leading to improved performance in complex scenarios such as detecting safety behaviors in power construction environments. The gradual knowledge transfer helps mitigate the gap between the advanced and student models, optimizing both the learning efficiency and detection accuracy.",
            "score": 0.5255225204364067,
            "section_title": "Public Dataset Experiments",
            "char_start_offset": 48734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1331
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84228515625
        },
        {
            "corpus_id": "247521335",
            "title": "Generalized Knowledge Distillation via Relationship Matching",
            "text": "Although knowledge distillation makes it easier to transfer learning experiences between related heterogeneous models, reusing models learned from a general label space is still difficult. We propose generalized knowledge distillation where the student is not restricted to having identical classes with the teacher. Our REFILLED improves the learning efficiency of the target model with the help of two stages, i.e., comparison matching and adaptive local knowledge distillation. REFILLED aligns the comparison ability w.r.t. embeddings, removing the label space constraint while simultaneously capturing high order relationships among instances. Then, emphasizing the teacher's confident supervision makes REFILLED automatically match the predictions between two models locally. Experiments validate that REFILLED improves classification performance in a variety of tasks, including general and standard knowledge distillation.",
            "score": 0.5255011035179851,
            "section_title": "CONCLUSION",
            "char_start_offset": 60947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 929
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83154296875
        },
        {
            "corpus_id": "274982528",
            "title": "LMD-PGN: Cross-Modal Knowledge Distillation from First-Person-View Images to Third-Person-View BEV Maps for Universal Point Goal Navigation",
            "text": "In existing knowledge distillation studies, teacher and student models often share the same architecture [3]. This similarity simplifies knowledge transfer and enhances the distillation effect. In contrast to these standard setups, this paper addresses the challenging setup of cross-architecture knowledge distillation, where the teacher and student models have different architectures. Further, the teacher model is a non-differentiable reinforcement learning action planner model (NNQL), while the student model is a differentiable image-based action classifier model (MLP). Few studies have explored cross-architecture knowledge distillation, especially cases like ours where the teacher and student models differ significantly. \n\nInstead of assuming that class-specific probability maps are provided by the teacher as in typical knowledge distillation, we introduce a novel framework, conceptually similar to the one used in our previous work [12], where only class-specific rank values are provided. The only difference between the standard knowledge distillation framework proposed by Hinton et al. and our framework is that we replace class-specific probability vectors with L1-normalized classspecific reciprocal rank vectors. The Kullback-Leibler (KL) divergence loss is employed as the loss function to align the output probability distribution of the student model with that of the teacher model. The Adam optimizer with a learning rate of 0.001 is used for optimization. A batch size of 32 is adopted to balance computational efficiency and training stability.",
            "score": 0.5254781031084017,
            "section_title": "C. Teacher Student Knowledge Distillation",
            "char_start_offset": 12673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1573
                }
            ],
            "ref_mentions": [
                {
                    "start": 948,
                    "end": 952,
                    "matchedPaperCorpusId": "239040859"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70751953125
        },
        {
            "corpus_id": "247315450",
            "title": "Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation",
            "text": "The knowledge of n complementary teachers can be transfered to the student through word-level knowledge distillation:\n\nwhere p is the prediction of student S and q i is the prediction of teacher T i . We use a hyperparameter \u03b1 to interpolate the distillation loss and the crossentropy loss:\n\nIn this way, the student model learns both new knowledge from the training set and complementary knowledge from teacher models. With an appropriate \u03b1, we can achieve a balance between the Algorithm 1 COKD Input: training set D, the number of teachers n Output: student model S 1: randomly initialize student S and teachers T 1:n 2: while not converge do for i = 1 to n do T i \u2190 S 9: return student model S two kinds of knowledge and alleviate the problem of imbalanced training. However, this method is based on knowledge distillation where knowledge is transferred unidirectionally from teachers to the student. Though the student can benefit from balanced training, these complementary teachers also set an upperbound to the student and prevent it from performing better.\n\nTo overcome this limitation, we follow the underlying idea of two-way knowledge transfer where the knowledge is also transferred from the student to teachers (Zhang et al., 2018;Zhu et al., 2018). We use a simple reinitialization method to achieve the two-way knowledge transfer. At the end of each epoch, we reinitialize teacher models with the parameters of the student model:\n\nThrough the reinitialization, the student and teachers are exactly the same at the beginning of each epoch. In this way, both the student and teachers are iteratively improved so the student performance is no longer limited by the fixed ability of teachers. We summarize the training process of COKD in Algorithm 1.",
            "score": 0.5254711297209637,
            "section_title": "Complementary Training",
            "char_start_offset": 17853,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1225,
                    "end": 1245,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1245,
                    "end": 1262,
                    "matchedPaperCorpusId": "48352434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.724609375
        },
        {
            "corpus_id": "273233176",
            "title": "SNN-PAR: Energy Efficient Pedestrian Attribute Recognition via Spiking Neural Networks",
            "text": "Knowledge Distillation is a technique for model compression that facilitates a smaller student model in learning from a larger teacher model. The student acquires knowledge by imitating various aspects of the teacher, such as its intermediate features [41], prediction logits [22], or activation boundaries [20]]. This approach was originally put forth by Hinton et al. [21] to supervise students based on the hard and soft label's output by the teacher, and nowadays there is a lot of work on using distillation for knowledge transfer to help the model get better performance. Earlier knowledge distillation (KD) techniques can be classified into three distinct categories: distillation from logits, distillation from features, and distillation based on attention. In terms of logit distillation, DIST [24] employs the Pearson correlation coefficient in place of KL divergence, combining both inter-and intra-class relationships. SRRL [62] ensures that the logits output from the teacher and the features of the student, after the teacher's linear layer, are identical. WSLD [75] examines soft labels and assigns varying weights to them based on the bias-variance trade-off. In addition to logit distillation, Several studies [5,43,64,66] concentrate on transferring knowledge through intermediate features. FitNet [41] directly distills semantic information from these intermediate features. AT [68] shifts the attention from feature maps to the student model. RKD [40] extracts relationships from the feature maps. MGD [65] masks the features of the student model, compelling it to replicate the features of the teacher model. To our knowledge, AT [68] is the sole knowledge distillation method that focuses on transferring attention, defining the attention map as a spatial representation that highlights the areas of the input that the model concentrates on the most. Wang et al. propose the HDETrack [56] which employs a hierarchical knowledge distillation strategy to augment the student tracking network from multi-modal or multi-view teacher network. In this paper, we employ both logits and intermediate features for knowledge distillation, believing that the integration of these two methods can greatly enhance knowledge transfer and improve the effectiveness of the student model. 3 Our Proposed Approach",
            "score": 0.5250732104363938,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2293
                },
                {
                    "start": 2294,
                    "end": 2317
                }
            ],
            "ref_mentions": [
                {
                    "start": 307,
                    "end": 311,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 803,
                    "end": 807,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "235613564"
                },
                {
                    "start": 1227,
                    "end": 1230,
                    "matchedPaperCorpusId": "250279758"
                },
                {
                    "start": 1230,
                    "end": 1233,
                    "matchedPaperCorpusId": "236882796"
                },
                {
                    "start": 1233,
                    "end": 1236,
                    "matchedPaperCorpusId": "244488341"
                },
                {
                    "start": 1467,
                    "end": 1471,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1522,
                    "end": 1526,
                    "matchedPaperCorpusId": "248506080"
                },
                {
                    "start": 1906,
                    "end": 1910,
                    "matchedPaperCorpusId": "262822525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86962890625
        },
        {
            "corpus_id": "259766297",
            "title": "The Staged Knowledge Distillation in Video Classification: Harmonizing Student Progress by a Complementary Weakly Supervised Framework",
            "text": "Knowledge distillation (KD) was first proposed in [57] and has since been widely used to transfer knowledge from a large teacher network to a smaller student network [30]. Direct response-based distillation methods use softened label outputs to match the teacher and student predictions. However, they can be challenging to converge in some cases, leading to the development of feature-based distillation methods. \n\nFitNets [31] proposed a two-stage method based on the middle layer to improve the performance of student networks. However, introducing an intermediate layer for dimension conversion may introduce additional parameters. Attention Transfer [32] introduced the attention mechanism to transform the feature information of each layer inside the teacher network to the student network. \n\nRelation-based distillation explores the relationship between different layers in the teacher and student networks. In [33], the authors define the matrix correlation to measure the feature correlations between input and output layers of teachers and students. This method requires teachers and students to have the same structure, which limits the model's generalization ability.",
            "score": 0.5250309474160967,
            "section_title": "B. Types of Knowledge Distillation",
            "char_start_offset": 6663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1179
                }
            ],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 54,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 918,
                    "end": 922,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "251718732",
            "title": "Defensive Distillation-Based Adversarial Attack Mitigation Method for Channel Estimation Using Deep Learning Models in Next-Generation Wireless Networks",
            "text": "However, deep neural networks (DNNs) require large amounts of training data, which is not always available for new tasks or domains. Several knowledge distillation methods have been proposed to address this issue that can train a smaller student network to mimic the prediction of a more extensive and more accurate teacher network. \n\nDistillation has been applied in the field of intelligent systems, such as knowledge-based systems and rule-based systems, to reduce the system's size and improve the system's performance by improving the quality of the system's knowledge. The teacher and student models' difference can be considered a form of regularization, which is crucial to prevent overfitting. \n\nAlgorithm 1 shows the pseudocode.",
            "score": 0.5250000807687897,
            "section_title": "III. DEFENSIVE DISTILLATION",
            "char_start_offset": 26662,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 738
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.763671875
        },
        {
            "corpus_id": "268591082",
            "title": "Knowledge distillation based on projector integration and classifier sharing",
            "text": "Knowledge distillation can transfer the knowledge from the pre-trained teacher model to the student model, thus effectively accomplishing model compression. Previous studies have carefully crafted knowledge representation, targeting loss function design, and distillation location selection, but there have been few studies on the role of classifiers in distillation. Previous experiences have shown that the final classifier of the model has an essential role in making inferences, so this paper attempts to narrow the gap in performance between models by having the student model directly use the classifier of the teacher model for the final inference, which requires an additional projector to help match features of the student encoder with the teacher's classifier. However, a single projector cannot fully align the features, and integrating multiple projectors may result in better performance. Considering the balance between projector size and performance, through experiments, we obtain the size of projectors for different network combinations and propose a simple method for projector integration. In this way, the student model undergoes feature projection and then uses the classifiers of the teacher model for inference, obtaining a similar performance to the teacher model. Through extensive experiments on the CIFAR-100 and Tiny-ImageNet datasets, we show that our approach applies to various teacher\u2013student frameworks simply and effectively.",
            "score": 0.5247328351537517,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90478515625
        },
        {
            "corpus_id": "267200066",
            "title": "Progressive Distillation Based on Masked Generation Feature Method for Knowledge Graph Completion",
            "text": "Compared with the traditional logits distillation, masked generation feature distillation method we propose can ensure that the teacher model can transfer knowledge more efficiently, so that students can learn more enriched teacher knowledge. At the same time, combined with the progressive distillation strategy, it can ensure that the model parameters is significantly reduced while maintaining the model performance as much as possible.",
            "score": 0.5247089811532899,
            "section_title": "Related Work",
            "char_start_offset": 7661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 439
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "276533262",
            "title": "Decoupled Classifier Knowledge Distillation",
            "text": "A powerful network is typically built upon a vast model foundation, but this also entails substantial computational costs. With the advancement of technology, such models are progressively transitioning towards being lightweight, with the goal of reducing costs. One potential direction for cost reduction is knowledge distillation (KD). KD refers to the process of aligning or approximating the logic or class predictions of a powerful, large-parameter teacher model on a smaller student model, given the same output [1,2]. The aim is to achieve similar or better performance compared to the large teacher model. Thanks to the robust practical effectiveness of knowledge distillation, it has achieved tremendous success in multiple fields, including object detection [3], semantic segmentation [4], and transformer training [5]. \n\nSince the developments presented in [6], most current research has primarily focused on two directions: distilling knowledge from deep features and methods based on outputs. Feature knowledge distillation has proposed many methods in recent years 7,8, most of which are built on additional supervision of a pre-trained teacher model, especially the intermediate layers [6][7][8][9][10][11][12]. However, even though output-based distillation is less performant compared to feature knowledge distillation, it focuses on a deeper semantic level [13]. Both of these methods have their respective advantages. Feature distillation demonstrates significant performance benefits in various tasks, but due to its complexity, it incurs additional computational costs. On the other hand, output-based distillation requires comparatively fewer computational resources and storage costs, but its performance significantly lags behind that of feature distillation. The differences between the two methods are not just in terms of final performance and computational costs; the core distinction lies in the focus areas of the models they target. Generally, output-based distillation concentrates more on the real labels and the model's outputs, whereas feature distillation emphasizes deep feature extraction. Each method has its own set of advantages that are difficult for the other to match. However, because they focus on different aspects, the knowledge learned by the student model also varies.",
            "score": 0.5245787588751447,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 795,
                    "end": 798,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 825,
                    "end": 828,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 1204,
                    "end": 1207,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1207,
                    "end": 1210,
                    "matchedPaperCorpusId": "227335337"
                },
                {
                    "start": 1213,
                    "end": 1217,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "235613564"
                },
                {
                    "start": 1375,
                    "end": 1379,
                    "matchedPaperCorpusId": "247476179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "251953448",
            "title": "Membership Inference Attacks by Exploiting Loss Trajectory",
            "text": "The notion of transferring knowledge from larger models (teacher models) to smaller ones (student models) emerges quite early. After forming a framework under the name of knowledge distillation (KD), this line of work is extended by either finding new approaches for KD or applying KD in different domains. For the former direction, Romero et al. [34] use additional linear projection to train a thinner and deeper student model. Zagoruykoet et al. [54] adopt attention mechanism to advance the performance of knowledge distillation. Yim et al. [52] propose a new method that adds additional losses to enhance the performance but also speeds up the optimization process. Xu et al. [48] utilize conditional adversarial networks to learn a loss function for KD. Regarding the applications of knowledge distillation, many studies have explored KD in other domains, such as semisupervised learning [44], sequence modeling [23] and multimodal learning [14]. \n\nOur work is more related to a specific direction of knowledge distillation, that is self-distillation [13]. In this direction, the teacher model and student model have identical model architectures, and the distillation is used to improve the performance of the student over the teacher. However, there is still a core difference between this direction and our work, that is we adopt the knowledge distillation to extract the membership information represented by the loss trajectory, but care less about the general performance of the model.",
            "score": 0.5243096308396437,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 42869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1497
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 351,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 449,
                    "end": 453,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 545,
                    "end": 549,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 681,
                    "end": 685,
                    "matchedPaperCorpusId": "4916078"
                },
                {
                    "start": 894,
                    "end": 898,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 918,
                    "end": 922,
                    "matchedPaperCorpusId": "8451212"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "6832420"
                },
                {
                    "start": 1057,
                    "end": 1061,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8681640625
        },
        {
            "corpus_id": "258035138",
            "title": "Progressive multi-level distillation learning for pruning network",
            "text": "The initial knowledge distillation [25] argued that one-hot labels limit the performance of the network model, and that the soft labels of a more robust network would provide more abundant information, which would allow the transfer of knowledge from a larger teacher network to a smaller one, thereby bridging the gap. Moreover, besides focusing on extracting logits output knowledge, intermediate representations of knowledge within the teacher in the form of feature maps can also be learned by the student model. FitNet [26] first proposed distillation learning for a single intermediate layer of knowledge. AT [27] extended this idea by extracting multiple intermediate layers knowledge of the teacher model to guide student learning, and by using L2-regularization on each feature map to ensure consistent dimensions for each pair of feature maps. However, knowledge from deeper intermediate layers may provide students with overly standardized guidance, while knowledge from shallower layers may not serve as a guiding role [12], which results in the inefficient transfer of knowledge. In relation-based distillation learning, knowledge transfer relationships between different layers or data are further explored. Yim et al. [28] used the relationship between layers of the teacher's network as the goal of student model learning. SP [29] aimed to preserve the student's pairwise similarity rather than mimicking the teacher's representation space, so that students could better understand the relationships between instances. Furthermore, in addition to the applications mentioned above in classification tasks, knowledge distillation methods have also proven their effectiveness in more complex tasks such as object detection [30,31]. \n\nModel pruning and knowledge distillation are two independent parts of model compression. How to combine these two methods is one of the problems worth discussing. The simplest way to combine them is to use knowledge distillation after the completion of pruning [15,32]. However, we have shown that the use of distillation learning in the fine-tuning process of pruning can yield better results, as demonstrated in \"Two combined strategies\". Furthermore, it is also necessary to validate the efficacy of distillation learning for structured pruning networks on various model architectures and public datasets.",
            "score": 0.5239708504431092,
            "section_title": "Knowledge distillation",
            "char_start_offset": 5673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1744
                },
                {
                    "start": 1747,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2355
                }
            ],
            "ref_mentions": [
                {
                    "start": 615,
                    "end": 619,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1031,
                    "end": 1035,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1233,
                    "end": 1237,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1342,
                    "end": 1346,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1736,
                    "end": 1740,
                    "matchedPaperCorpusId": "267213"
                },
                {
                    "start": 1740,
                    "end": 1743,
                    "matchedPaperCorpusId": "235613518"
                },
                {
                    "start": 2008,
                    "end": 2012,
                    "matchedPaperCorpusId": "236386992"
                },
                {
                    "start": 2012,
                    "end": 2015,
                    "matchedPaperCorpusId": "225595342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89794921875
        },
        {
            "corpus_id": "251719031",
            "title": "Tree-structured Auxiliary Online Knowledge Distillation",
            "text": "Deep neural networks have led to a series of successes in computer vision and natural language processing [17]. They show superiority in representing complex concepts due to the large size of parameters [15]. However, the cumbersome neural networks are computationally expensive, hindering their applications in real-world problems with limited resources. To *Corresponding author. 1 Code is available at https://github.com/Linwenye/Tree-Supervised.\n\nthis end, knowledge distillation [12] is proposed to transfer the knowledge from a large teacher model to a compact student model. Traditional knowledge distillation requires a two-stage training process, in which a high-capacity teacher model is pretrained in the first stage, and then transfers the knowledge of the teacher to a compact student model in the second stage [3], [12]. This two-stage process increases the pipeline complexity and training cost. To simplify the distillation procedure, online knowledge distillation [16], [32] is proposed, which simultaneously trains a set of student models and distills their knowledge from each other in a peer-teaching manner. This approach requires a one-stage learning procedure and leverages peer network to provide the teacher knowledge. Recent researches on online knowledge distillation aim at improving the quality of the knowledge learned from peers. ONE [16] and CL-ILR [23] introduces an ensemble teacher which gathers the students' knowledge with a gate mechanism. OKDDip [6] further boosts the performance by maintaining the peer variety through an attention mechanism. These approaches mainly focus on the design of the distillation objective.\n\nBy contrast, in this work, we demonstrate that the design of the overall architecture for online knowledge distillation is another key factor for the performance of the student model. We adopt the original distillation objective as that in DML [32] without bells and whistles, and show that the tree structure for online knowledge distillation is the key to our state-ofthe-art performance. We propose a unified framework, the tree-structured auxiliary online knowledge distillation (TSA). During training, TSA hierarchically adds more auxiliary peers in later layers, which naturally forms a tree structure (See Fig. 1). Different branches construct different views of the inputs, which",
            "score": 0.5232483355829779,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 207,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 484,
                    "end": 488,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 824,
                    "end": 827,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 829,
                    "end": 833,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 987,
                    "end": 991,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1365,
                    "end": 1369,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1381,
                    "end": 1385,
                    "matchedPaperCorpusId": "44119099"
                },
                {
                    "start": 1485,
                    "end": 1488,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 1904,
                    "end": 1908,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8291015625
        },
        {
            "corpus_id": "271956542",
            "title": "Rethinking Knowledge Transfer in Learning Using Privileged Information",
            "text": "In Figure 1, we show that the reported effects are indeed visible around 50 epochs but quickly disappear when we allow all models to continue training. \n\nImportant to note, is that given the teacher-student setup, when the no-PI and student model performances are reported at 50 epochs in Figure 1, the student model actually requires a teacher model that had already completed 50 epochs, thus combined requiring 100 training epochs. Taking this into consideration, there is no evidence of either improved sample efficiency or computational efficiency by using Generalised distillation in this setting. \n\nFurther discussion on knowledge distillation using PI While Generalized distillation shows preliminary evidence of knowledge transfer, we can see that it takes place only for low data regimes and in highly styled examples. To address these gaps, several attempts have been made from the application side [16,6,12,29], with [29] applying generalized distillation to recommendations with privileged information in e-commerce. Admittedly, all of these works report marginal improvement over the no-PI model.",
            "score": 0.5230407524871192,
            "section_title": "Generalized distillation",
            "char_start_offset": 18426,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 154,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1109
                }
            ],
            "ref_mentions": [
                {
                    "start": 909,
                    "end": 913,
                    "matchedPaperCorpusId": "11940370"
                },
                {
                    "start": 913,
                    "end": 915,
                    "matchedPaperCorpusId": "53044539"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72021484375
        },
        {
            "corpus_id": "255998274",
            "title": "Human-Timescale Adaptation in an Open-Ended Task Space",
            "text": "All of the scaling comparison experiments shown in the previous section use an identical distillation teacher for the first frames of training, as detailed in Appendix D.6. Now, we look at the role distillation plays in scaling. In short, we find that kickstarting training with a distillation period is crucial when scaling up model size. As shown in Figure 15, training a 265M parameter Transformer model without distillation results in poor performance compared to a much smaller 23M parameter Transformer trained in the same way. However, when training with distillation from a 23M parameter teacher for the first 4 billion training frames, the 265M model clearly outperforms the 23M variant. See experiment details in Appendix D.11.\n\nAdditionally, we find that even when the model size is the same for both student and teacher, we observe large gains from distillation, for a constant total frame budget ( Figure 16). We speculate that this is due to bad representations learned early on by the student agent (Cetin et al., 2022;Nikishin et al., 2022), which can be avoided by using distillation. This is also consistent with findings in offline RL, where additional data is often required to effectively scale the model (Reid et al., 2022). The effect is largest for the first round of distillation, with diminishing returns in subsequent rounds of distillation ( Figure E.5).",
            "score": 0.5230192666336384,
            "section_title": "Distillation improves performance and enables scaling agents",
            "char_start_offset": 40779,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "270737691",
            "title": "MAGIC: Meta-Ability Guided Interactive Chain-of-Distillation for Effective-and-Efficient Vision-and-Language Navigation",
            "text": "Knowledge Distillation (KD) is a model compression technique that facilitates the training of a smaller \"student\" network under the guidance of a larger \"teacher\" network.This method was initially introduced by Bucilua et al. [43] and later gained popularity through the work of Hinton et al. [5], who introduced a temperature scaling method to soften the output targets.Additionally, Romero et al. [44] introduced the concept of hint learning, utilizing the outputs from the teacher's hidden layers to supervise the student.Zagoruyko et al. [45] highlighted the use of attention mechanisms as another effective strategy for knowledge transfer.Mirzadeh et al. [46] employed a teacher assistant to bridge the gap between the teacher and the student.\n\nStudies have extensively explored knowledge distillation within single modalities, such as vision [47,48] and language [49,50].Recently, there has been a growing interest in cross-modal distillation.For example, ALBEF [51] proposed a momentum distillation approach that leverages pseudo-targets generated by a momentum model.DLIP [8] comprehensively analyzed the distinct roles of visual and linguistic modes in KD.TinyCLIP [9] introduced techniques like affinity mimicking and weight inheritance to enhance the distillation for large-scale language-image pre-trained models.WSD [52] proposed a whitened similarity distillation method to distill cross-modal features.Guo et al. [53] proposed a context-aware graph method to improve the comprehension of the semantic dependencies among implicit contexts.It is important to note that these methods have primarily focused on straightforward single-stage classification tasks.In contrast, tasks in E-AI, such as VLN, demand a more complex, multi-stage prediction capability.Therefore, we dissect the required capabilities of embodied agents and propose a novel meta-ability guided knowledge distillation framework with the adaptive weighting strategy.Additionally, we move beyond the traditional singlestep teacher-student pipeline by introducing an interactive chain-of-distillation learning approach to improve the performance of both teachers and students iteratively.",
            "score": 0.5227429579555054,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9215,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 525
                },
                {
                    "start": 525,
                    "end": 644
                },
                {
                    "start": 644,
                    "end": 748
                },
                {
                    "start": 750,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 949
                },
                {
                    "start": 949,
                    "end": 1075
                },
                {
                    "start": 1075,
                    "end": 1165
                },
                {
                    "start": 1165,
                    "end": 1325
                },
                {
                    "start": 1325,
                    "end": 1417
                },
                {
                    "start": 1417,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1770
                },
                {
                    "start": 1770,
                    "end": 1947
                },
                {
                    "start": 1947,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 399,
                    "end": 403,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 542,
                    "end": 546,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 660,
                    "end": 664,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 848,
                    "end": 852,
                    "matchedPaperCorpusId": "208109903"
                },
                {
                    "start": 852,
                    "end": 855,
                    "matchedPaperCorpusId": "250920355"
                },
                {
                    "start": 869,
                    "end": 873,
                    "matchedPaperCorpusId": "203626972"
                },
                {
                    "start": 968,
                    "end": 972,
                    "matchedPaperCorpusId": "236034189"
                },
                {
                    "start": 1174,
                    "end": 1177,
                    "matchedPaperCorpusId": "262083905"
                },
                {
                    "start": 1329,
                    "end": 1333,
                    "matchedPaperCorpusId": "249095312"
                },
                {
                    "start": 1428,
                    "end": 1432,
                    "matchedPaperCorpusId": "235298813"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8212890625
        },
        {
            "corpus_id": "259378480",
            "title": "Review of Recent Distillation Studies",
            "text": "The classical KD method uses the Kullback-Leibler scatter to minimize the difference between the output probabilities of teachers and students. However, recent studies have shown that more accurate teachers do not necessarily teach better, and the reasons for this remain unknown. In the following, a new asymmetric temperature scaling (ATS) method is proposed to improve the clarity of teacher-provided error class probabilities and make large teachers teach better. There are various approaches to knowledge distillation, which include transferring knowledge from complex teachers to smaller students through a combination of cross-entropy and distillation loss. Some studies have examined the dependence of knowledge transfer on student and teacher architecture and found that larger models are not always better teachers. Other work has focused on understanding the advantages of knowledge distillation from a principles-based perspective. The paper also explains the notation used, such as softmax functions, logarithms, and probabilities. In addition, a particular phenomenon where disadvantaged students are unable to fully imitate good teachers due to a mismatch of abilities is discussed and explained in detail. Knowledge distillation can be decomposed into three parts, namely, correct guidance, smooth regularization, and category discrimination. These three terms are measured quantitatively using the target category probability, the mean of the error category probability, and the variance of the error category probability. An appropriate temperature scaling method that incorporates the validity of these three terms simultaneously is presented here. Category defensibility is considered to be the basis of KD. A theoretical analysis of the application of temperature scaling to knowledge distillation is presented below. The softening probability vector used in KD is analyzed, showing that an increase in temperature leads to a more uniform probability distribution, and the concepts of intrinsic variance (IV) and derived variance (DV) are also introduced to analyze the class-discriminable terms. And further explains why larger teachers cannot teach better, showing that the effectiveness of KD is more related to differences between error classes than to all classes. A new temperature scaling method, the asymmetric temperature scale (ATS), is proposed here to make the distribution of error classes during distillation more discriminable.",
            "score": 0.522568297696143,
            "section_title": "Symmetric temperature scaling",
            "char_start_offset": 15480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2290
                },
                {
                    "start": 2291,
                    "end": 2463
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70556640625
        },
        {
            "corpus_id": "258048760",
            "title": "A Survey on Recent Teacher-student Learning Studies",
            "text": "The classical KD method uses the Kullback-Leibler scatter to minimize the difference between the output probabilities of teachers and students. However, recent studies have shown that more accurate teachers do not necessarily teach better, and the reasons for this remain unknown. In the following, a new asymmetric temperature scaling (ATS) method is proposed to improve the clarity of teacher-provided error class probabilities and make large teachers teach better. There are various approaches to knowledge distillation, which include transferring knowledge from complex teachers to smaller students through a combination of cross-entropy and distillation loss. Some studies have examined the dependence of knowledge transfer on student and teacher architecture and found that larger models are not always better teachers. Other work has focused on understanding the advantages of knowledge distillation from a principles-based perspective. The paper also explains the notation used, such as softmax functions, logarithms, and probabilities. In addition, a particular phenomenon where disadvantaged students are unable to fully imitate good teachers due to a mismatch of abilities is discussed and explained in detail. Knowledge distillation can be decomposed into three parts, namely, correct guidance, smooth regularization, and category discrimination. These three terms are measured quantitatively using the target category probability, the mean of the error category probability, and the variance of the error category probability. An appropriate temperature scaling method that incorporates the validity of these three terms simultaneously is presented here. Category defensibility is considered to be the basis of KD. A theoretical analysis of the application of temperature scaling to knowledge distillation is presented below. The softening probability vector used in KD is analyzed, showing that an increase in temperature leads to a more uniform probability distribution, and the concepts of intrinsic variance (IV) and derived variance (DV) are also introduced to analyze the class-discriminable terms. And further explains why larger teachers cannot teach better, showing that the effectiveness of KD is more related to differences between error classes than to all classes. A new temperature scaling method, the asymmetric temperature scale (ATS), is proposed here to make the distribution of error classes during distillation more discriminable.",
            "score": 0.5224502382411015,
            "section_title": "Symmetric temperature scaling",
            "char_start_offset": 15531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2290
                },
                {
                    "start": 2291,
                    "end": 2463
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.705078125
        },
        {
            "corpus_id": "252668439",
            "title": "Towards a Unified View of Affinity-Based Knowledge Distillation",
            "text": "Large-scale deep neural networks often have advantage in their performance over the smaller counterparts [11,32] thanks to their capacity. The size of networks can however be an issue in their deployment for production; due to a large memory footprint or substantial computational cost, those models may not necessarily fit well to small devices with limited power capacity, e.g. a mobile phone. While different approaches have been proposed to cope with this problem, such as designing effective architectures [13,34], model pruning [10,21], low-rank approximation [8,14], and model quantization [4,20], research based on knowledge transfer have shown promising directions in terms of knowledge distillation (KD), or distillation in short, which this paper is concerned with. \n\nThe basic idea of knowledge transfer is to encode useful knowledge of data by a teacher model and use that for training a smaller student network. Bucilu\u01ce et al. [3] were among the first that introduced knowledge transfer in the context of artificial neural networks. They employed a large ensemble to label training data, and the labelled data were used to train a single neural network. Similar ideas were utilised in [1,2,12,26,33] where a bulkier model was used to assist training of a smaller, shallower network. All these methods use individual knowledge from single sample encoding/embedding which can be referred to as instance-based knowledge. \n\nMore recent work looked at distillation using intersample similarity, i.e. preserving the relation-based knowledge derived from intra-batch relationships among features of samples [22,23,25,28]. Those methods are compelling as they exploit somewhat richer information, and have shown improved performance over instance-based approach. Yet, as observed in [16,27,30], they are largely outperformed by a few latest work including contrastive representation distillation. This could be due to the possible need for a more effective way for distilling the abundant information, including the challenge of defining proper distance between representations in the teacher and student networks [5].",
            "score": 0.5221035305903432,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 109,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 515,
                    "end": 518,
                    "matchedPaperCorpusId": "24982157"
                },
                {
                    "start": 534,
                    "end": 538,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 538,
                    "end": 541,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 566,
                    "end": 569,
                    "matchedPaperCorpusId": "7340116"
                },
                {
                    "start": 597,
                    "end": 600,
                    "matchedPaperCorpusId": "9376794"
                },
                {
                    "start": 600,
                    "end": 603,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 941,
                    "end": 944,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1199,
                    "end": 1202,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1202,
                    "end": 1204,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1207,
                    "end": 1210,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1210,
                    "end": 1213,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1614,
                    "end": 1618,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1618,
                    "end": 1621,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 1621,
                    "end": 1624,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1624,
                    "end": 1627,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1789,
                    "end": 1793,
                    "matchedPaperCorpusId": "231839582"
                },
                {
                    "start": 1793,
                    "end": 1796,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1796,
                    "end": 1799,
                    "matchedPaperCorpusId": "235613564"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83837890625
        },
        {
            "corpus_id": "271843206",
            "title": "UNIC: Universal Classification Models via Multi-teacher Distillation",
            "text": "Knowledge distillation (KD) was initially introduced as a model compression technique [7], where the goal is to train a smaller student model from the output of a teacher model [23]. While early work focused on predicting the final outputs of a classification model, the idea was rapidly extended to other forms of distillation, such as distilling intermediate representations [1,21,22,49,73,75,79]. These methods perform well but require careful layer selection and loss balancing [21]. In our work, instead of matching layer-wise representations between the student and teacher architectures, we add shortcut connections from intermediate layers of the student to the loss of each teacher. \n\nMulti-teacher knowledge distillation. KD can naturally be extended to an ensemble of teachers so that student can benefit from their potential complementarity. While the final outputs of teachers trained for the same task can simply be averaged [3,15,23,75], multi-teacher distillation with teachers trained for different tasks is more challenging. UDON [76] first trains domain-specialist teachers which are subsequently distilled in a student model using adaptive data sampling for balancing the different domains. In [60], contrastive learning is used for ensemble distillation while [56] proposes a framework tailored for teachers trained with masked image modeling and contrastive learning. But such approaches are not straightforward to extend to teachers learned differently. Similarly, [71] combines self-supervised teachers from arbitrary heterogeneous pretext tasks. [13,16,51] focus on jointly utilizing pseudo-and true labels for multi-teacher distillation. Roth et al. [51] formulate multi-teacher distillation as continual learning and further propose a novel method for data partitioning based on confidence. Here we develop a more generic method for combining teachers, that is not limited to certain types of teachers or losses, and, unlike [30,51], does not require labeled data, nor classifiers associated with each teacher for obtaining pseudo-labels. \n\nLoss balancing is shown to be crucial in multi-task learning [11,24,26,78]. Similar strategies to automatically balance losses have also been proposed for multi-teacher distillation [15,32].",
            "score": 0.5219425267432165,
            "section_title": "Related Work",
            "char_start_offset": 4470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 2065
                },
                {
                    "start": 2068,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2258
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 181,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 377,
                    "end": 380,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 380,
                    "end": 383,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 383,
                    "end": 386,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 389,
                    "end": 392,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 392,
                    "end": 395,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 395,
                    "end": 398,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 482,
                    "end": 486,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 939,
                    "end": 942,
                    "matchedPaperCorpusId": "202660953"
                },
                {
                    "start": 945,
                    "end": 948,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 948,
                    "end": 951,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1214,
                    "end": 1218,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1281,
                    "end": 1285,
                    "matchedPaperCorpusId": "259274820"
                },
                {
                    "start": 1571,
                    "end": 1575,
                    "matchedPaperCorpusId": "85464175"
                },
                {
                    "start": 1575,
                    "end": 1578,
                    "matchedPaperCorpusId": "237291521"
                },
                {
                    "start": 1578,
                    "end": 1581,
                    "matchedPaperCorpusId": "264490946"
                },
                {
                    "start": 1676,
                    "end": 1680,
                    "matchedPaperCorpusId": "264490946"
                },
                {
                    "start": 1956,
                    "end": 1959,
                    "matchedPaperCorpusId": "264490946"
                },
                {
                    "start": 2129,
                    "end": 2133,
                    "matchedPaperCorpusId": "4703661"
                },
                {
                    "start": 2133,
                    "end": 2136,
                    "matchedPaperCorpusId": "44079150"
                },
                {
                    "start": 2136,
                    "end": 2139,
                    "matchedPaperCorpusId": "41794936"
                },
                {
                    "start": 2139,
                    "end": 2142,
                    "matchedPaperCorpusId": "267024006"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "273901289",
            "title": "SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models",
            "text": "Knowledge distillation aims to leverage the teacher model's performance and generalize it to the student model (Hinton et al., 2015;Park et al., 2019;Guo et al., 2023). For language models, Sanh et al., 2019 uses the teacher model's generation distribution for each token as a supervision signal for the student model, and some other works (Wang et al., 2020b,a) distill the attention scores of one layer to transfer the knowledge of larger LMs into smaller models. Unlike their objectives of transferring knowledge between models of different sizes, we use attention distillation for knowledge retention. Both our teacher and student models share a similar architecture and are derived from the same pre-trained LLM, which enables head-by-head and layer-by-layer distillation.",
            "score": 0.5218024895266848,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 23791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 777
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 150,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 150,
                    "end": 167,
                    "matchedPaperCorpusId": "258548765"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89697265625
        },
        {
            "corpus_id": "271050980",
            "title": "Leveraging Topological Guidance for Improved Knowledge Distillation",
            "text": "Conventional Knowledge Distillation.Knowledge distillation obtains a smaller model by utilizing the learned knowledge of a larger model, which was first introduced by Bucilu\u01ce et al. (Bucilu\u01ce et al., 2006) and explored more by Hinton et al. (Hinton et al., 2015).In KD, soft labels are utilized for knowledge transfer from a teacher to a student, which provide richer supervision signals and reduce overfitting.This also leads to better transferability of learned representations.The loss function of conventional KD for training a student is:\n\nwhere, L CE is the standard cross entropy loss, L K is KD loss, and \u03bb is a hyperparameter; 0 < \u03bb < 1.The difference between the output of the softmax layer for a student network and the ground-truth label is minimized by the cross-entropy loss:\n\nwhere, H(\u2022) is a cross entropy loss function, l S is the logits of a student, and y g is a ground truth label.The gap between outputs of student and teacher are minimized by KL-divergence loss:\n\nwhere, z T = sof tmax(l T /\u03c4 ) and z S = sof tmax(l S /\u03c4 ) are softened outputs of a teacher and student, respectively, and \u03c4 is a hyperparameter; \u03c4 > 1.To obtain the best performance, we adopt early stopping for KD (ESKD) which improves the efficacy of KD (Cho & Hariharan, 2019).\n\nFeature-based Knowledge Distillation.Features from intermediate layers of a network can be utilized in knowledge transfer (Gou et al., 2021;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019).Zagoruyko et al. (Zagoruyko & Komodakis, 2017) suggested activation-based attention transfer (AT), which is computed by a sum of squared attention mapping function, and calculating statistics across the channel dimension.Tung et al. (Tung & Mori, 2019) introduced similarity-preserving knowledge distillation, matching similarity within a mini-batch of samples between a teacher and a student.",
            "score": 0.5216529516368972,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7489,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 36,
                    "end": 262
                },
                {
                    "start": 262,
                    "end": 410
                },
                {
                    "start": 410,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 542
                },
                {
                    "start": 544,
                    "end": 645
                },
                {
                    "start": 645,
                    "end": 788
                },
                {
                    "start": 790,
                    "end": 900
                },
                {
                    "start": 900,
                    "end": 983
                },
                {
                    "start": 985,
                    "end": 1138
                },
                {
                    "start": 1138,
                    "end": 1266
                },
                {
                    "start": 1268,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1455
                },
                {
                    "start": 1455,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1848
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 204,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 240,
                    "end": 261,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1242,
                    "end": 1265,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1390,
                    "end": 1408,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1408,
                    "end": 1436,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1436,
                    "end": 1454,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1472,
                    "end": 1501,
                    "matchedPaperCorpusId": "829159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "258887701",
            "title": "On the Impact of Knowledge Distillation for Model Interpretability",
            "text": "Several recent studies have elucidated why knowledge distillation (KD) improves model performance. However, few have researched the other advantages of KD in addition to its improving model performance. In this study, we have attempted to show that KD enhances the interpretability as well as the accuracy of models. We measured the number of concept detectors identified in network dissection for a quantitative comparison of model interpretability. We attributed the improvement in interpretability to the class-similarity information transferred from the teacher to student models. First, we confirmed the transfer of class-similarity information from the teacher to student model via logit distillation. Then, we analyzed how class-similarity information affects model interpretability in terms of its presence or absence and degree of similarity information. We conducted various quantitative and qualitative experiments and examined the results on different datasets, different KD methods, and according to different measures of interpretability. Our research showed that KD models by large models could be used more reliably in various fields.",
            "score": 0.5215732502788866,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.921875
        },
        {
            "corpus_id": "258564799",
            "title": "DynamicKD: An Effective Knowledge Distillation via Dynamic Entropy Correction-Based Distillation for Gap Optimizing",
            "text": "Therefore, this paper focuses on knowledge distillation-based model compression. \n\nIn the knowledge distillation, how the teacher network better guides the student network training has become vital research. Several researchers have studied various knowledge used in the distillation process. \n\nRomero et al. applied the middle layer features of the teacher network as knowledge to guide the student network learning [26]. Komodakis et al. transferred attention from the large-scale teacher network to the student network [27]. Damiano et al. viewed the knowledge transfer between teacher and student networks as maximizing the mutual information between teacher and student networks [28]. Zagoruyko et al. used the attention mechanism as a learnable knowledge, and they used the teacher network's attention knowledge to guide the student network's training. In neural networks, the convolutional layers map one feature to another. Yim et al. treated the mapping processing of features between layers as knowledge and used the FSP matrix to describe this knowledge so that the student network could imitate it [29]. The rich and varied knowledge exchange between the teacher and student networks helps the student network training. \n\nBut, these methods do not focus on the performance gap between the teacher and student networks, which may affect student network learning. \n\nSeveral works have studied the performance gap and proposed corresponding improvements. Cho et al. found that the underperformance teacher with early-stop training benefits the student [30]. Mirzadeh et al. found that when the gap between the teacher network and the student network is large, the student trained by a lower-performance lightweight teacher network performs better than the one taught by a higher-performance large-scale teacher network [16]. For this reason, he utilized a medium-sized neural network (called Teacher Assistant) to help the student network cross the large performance gap. Both methods mentioned above show that reducing the performance gap can improve distillation performance. \n\nHowever, these static methods do not correct the performance gap further during the distillation. The performance gap keeps changing with the performance improvement of the student network, so these strategies may still hinder the student network from imitating the high-performance teacher network. \n\nVarious knowledge distillation algorithms are available to continuously update the knowledge applied for the student network training during the distillation.",
            "score": 0.5214618948682588,
            "section_title": "Introduction",
            "char_start_offset": 2212,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 83,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2086
                },
                {
                    "start": 2089,
                    "end": 2186
                },
                {
                    "start": 2187,
                    "end": 2388
                },
                {
                    "start": 2391,
                    "end": 2549
                }
            ],
            "ref_mentions": [
                {
                    "start": 522,
                    "end": 526,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 684,
                    "end": 688,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1561,
                    "end": 1565,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1828,
                    "end": 1832,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "259858962",
            "title": "Cost-effective Distillation of Large Language Models",
            "text": "Knowledge distillation (KD) involves training a small \u201cstudent\u201d model to replicate the strong performance of a high-capacity \u201cteacher\u201d model, enabling efficient deployment in resource-constrained settings. Top-performing methods tend to be task-or architecture-specific and lack generalizability. Several existing approaches require pretraining of the teacher on task-specific datasets, which can be costly for large and unstable for small datasets. Here we propose an approach for improving KD through a novel distillation loss agnostic to the task and model architecture. We successfully apply our method to the distillation of the BERT-base and achieve highly competitive results from the distilled student across a range of GLUE tasks, especially for tasks with smaller datasets. 1",
            "score": 0.5213947316995684,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "272987375",
            "title": "Linear Projections of Teacher Embeddings for Few-Class Distillation",
            "text": "We focus our experiments on a variety of classification tasks, using a standard distillation setup as in [14]. In order to focus solely on the effect of the distillation loss of each method, we always set \u03b1 = 0 in (1). This reduces the variance between methods which may have different optimal values of \u03b1, and reduces the hyperparameter search space. Furthermore, in the important case of the semi-supervised setting one does not have access to ground-truth labels. \n\nThe Architectures Given that the specific combination of student and teacher architectures is known to influence the effectiveness of knowledge distillation, we have chosen to evaluate various student-teacher pairings to ensure the robustness of our method. \n\nFor Vision datasets, the \"ideal\" distillation scenario is given by distilling ResNet-92 to ResNet-56, where both architectures and embeddings dimensions match (D = 256). We also consider the case of a smaller and a larger dimension with ResNet-92 (D = 256) and MobileNet with width and depth multiplier equal to 2 (D = 2048), respectively, distilling to MobileNet (D = 1024). The latter cases address scenarios where the embedding dimensions differ, with one scenario involving the same architecture family and the other involving different architectures. For smaller NLP datasets (LMRD, GLUE/cola, and GLUE/sst2), we consider distillation from an ALBERT-Large model (D = 1024) to an ALBERT-Base model (D = 768). For the larger scale NLP datasets (based on Amazon US reviews and Sentiment 140) we consider distillation from an ALBERT-XXL model (D = 4096) and an ALBERT-XL model (D = 2048) to (i) an ALBERT-Base model (D = 768); (ii) and a two-layer-MLP of width (D = 4096) that operates over representations generated by a (frozen) sentence-T5 encoder model of 11 billion parameters [23]. The latter case addresses the scenario involving different teacher-student architectures but with the same embedding dimension.",
            "score": 0.5212720264060654,
            "section_title": "The Setup",
            "char_start_offset": 18613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 466
                },
                {
                    "start": 469,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1945
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "274625091",
            "title": "A Holistic Review of the TinyML Stack for Predictive Maintenance",
            "text": "Larger NN models often perform better than smaller models on complicated datasets. However, deploying such large models on TinyML devices might be infeasible. A solution is to ''distil'' the knowledge of a large model into a smaller model, which, due to its reduced resource consumption, can be deployed on TinyML devices. \n\nThis technique is known as knowledge distillation. The idea behind this method is to train a large model and then use the predictions generated by this model to train a smaller model. Thus, the small model trains both using the ground truths and the predictions generated by the large model [112]. \n\nConsider the following example. We want to create a small model that can classify the contents of an image. Normally, we train the network to make the same classification as the ground truth labels. In knowledge distillation, we first train a larger model on our data. We then have the larger model classify all images in the dataset. Then, we train the small classifier not just to classify the ground truth but also to make similar classifications as the larger model. This training can be achieved by altering the loss function of the smaller model. Often, the larger model is referred to as the ''teacher'', and the small model as the ''student''. \n\nAs knowledge distillation aims to remove parts of a larger model that is not necessary to achieve some level of predictive performance, it is closely related to ablation studies. In ablation studies, tests are carried out on which model components are necessary for the model's predictive performance and which components can be left out to reduce model complexity and resource consumption.",
            "score": 0.5211743809241,
            "section_title": "5) KNOWLEDGE DISTILLATION",
            "char_start_offset": 50600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 322
                },
                {
                    "start": 325,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1276
                },
                {
                    "start": 1279,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1669
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.697265625
        },
        {
            "corpus_id": "252917729",
            "title": "RoS-KD: A Robust Stochastic Knowledge Distillation Approach for Noisy Medical Imaging",
            "text": "Knowledge Distillation (KD) [14] is an effective way to compress large models into smaller ones with comparable performance. KD is based on a teacher-student learning paradigm in which the student learns from the soft-targets of the teacher network. Recently, some methods employ multiple teachers and show great promises in further boosting student model performance effectively [10]- [12]. Most of these existing methods using multiple teachers simply assign equal weight to all teacher models during the whole distillation process. Moreover, they primarily use the same training data to train each participating teacher network which make all teachers prone to learning the similar noise pattern available in the training data. Recently, [15] identified that individual teacher models may perform differently on different data-points due to optimization strategy and parameter initialization. This encourages us to assign different weights to teacher models for different training instances during training. RoS-KD provides a noisetolerant perspective of knowledge distillation and introduces smoothening as a key to improving its performance. Compared to previous knowledge distillation methods using multi-teacher models usually fix the same weight for a teacher model on all training examples, RoS-KD design allows stochastic assignment of weights to each participating teacher models for each training example during training.",
            "score": 0.5210620285933685,
            "section_title": "II. BACKGROUND WORK",
            "char_start_offset": 5373,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 741,
                    "end": 745,
                    "matchedPaperCorpusId": "228376532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7890625
        },
        {
            "corpus_id": "251741056",
            "title": "Design Automation for Fast, Lightweight, and Effective Deep Learning Models: A Survey",
            "text": "Knowledge distillation (KD) is extended from knowledge transfer (KT) [143] by Ba and Caruana [144] to compress a cumbersome network (teacher) into a smaller and simpler network (student). This is done by making the student model mimic the function learned by the teacher model in order to achieve a competitive accuracy. It is later formally popularized by Hinton et al. [21] as a student-teacher paradigm, where the knowledge is transferred from the teacher to the student by minimizing the difference between the logits (features before the final softmax) of the teacher and student. In many situations, the performance of the teacher is almost perfect with a very high classification probability for the correct class and flat probabilities for the other classes. Therefore, the teacher is not able to provide much more information than the ground truth labels. Hinton et al. [21] introduce the concept of softmax temperature to transfer knowledge, which can better deliver the information of which classes the teacher find similar to the correct class. Formally, given the logits of the teacher model, the classification probability p i of the class i is: \n\nwhere \u03c4 is the temperature parameter. It controls how soft the labels from the teacher are. The soft labels together with the ground truth labels are used to supervise a compact student model. \n\nVanilla knowledge distillation mostly focuses on transferring knowledge to a student model with a fixed small architecture, which is manually designed in advance. However, different teachers and tasks favour different student architectures, and hand-crafted architectures are prone to be suboptimal. Considering these limitations, there is a growing trend to automate the architecture design of a student model [145], [146], [147], [148], [149], [150], [151]. The ground-truth labels are combined with the distillation labels to guide the automatic design process. AKDNet [145] proposes to search optimal student architectures for distilling a given teacher by RL-based NAS. It adopts the efficient search space of [48] and designs a KD-guided reward with a teacher network.",
            "score": 0.5209634795059445,
            "section_title": "B. Automated Knowledge Distillation",
            "char_start_offset": 84556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 74,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 93,
                    "end": 98,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 371,
                    "end": 375,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 879,
                    "end": 883,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1768,
                    "end": 1773,
                    "matchedPaperCorpusId": "208175624"
                },
                {
                    "start": 1782,
                    "end": 1787,
                    "matchedPaperCorpusId": "235428017"
                },
                {
                    "start": 1810,
                    "end": 1815,
                    "matchedPaperCorpusId": "233877900"
                },
                {
                    "start": 1929,
                    "end": 1934,
                    "matchedPaperCorpusId": "208175624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.802734375
        },
        {
            "corpus_id": "270703180",
            "title": "The Privileged Students: On the Value of Initialization in Multilingual Knowledge Distillation",
            "text": "Knowledge distillation (KD) is a technique used to transfer knowledge from a large, trained teacher model to a smaller student model.This process aims to retain the large model's performance while reducing the computational cost during inference.KD involves training the student model to mimic the outputs of the teacher model, often using a combination of the teacher's soft target outputs and the ground truth labels.\n\nVarious extensions of KD have been proposed to enhance its effectiveness.For instance, Tiny-BERT (Jiao et al., 2020;Ansell et al., 2023) introduces a two-step distillation process.First, the student model is pre-trained on a large corpus to acquire good initialization suitable for the next step.Afterward, the model is fine-tuned for the desired tasks.This approach requires substantial data and computational resources, making it challenging to implement with limited resources.\n\nIn this work, we aim to explore the impact of these components of knowledge distillation in multilingual settings, focusing on efficiency.Instead of the extensive pre-training step used in Jiao et al., 2020;Ansell et al., 2023, we employ a simpler and more efficient initialization approach by copying the weights from the teacher model to the student model, inspired by DistilBERT (Sanh et al., 2020).While this method has proven effective, it has not been thoroughly explored in multilingual settings, which presents an intriguing area to observe.\n\nThe methodology of these components used in this work is elaborated in \u00a72.1 (Distillation Architecture) and \u00a72.2 (Model Initialization).",
            "score": 0.520943051717274,
            "section_title": "Background",
            "char_start_offset": 2268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 133,
                    "end": 246
                },
                {
                    "start": 246,
                    "end": 419
                },
                {
                    "start": 421,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 601
                },
                {
                    "start": 601,
                    "end": 717
                },
                {
                    "start": 717,
                    "end": 774
                },
                {
                    "start": 774,
                    "end": 901
                },
                {
                    "start": 903,
                    "end": 1041
                },
                {
                    "start": 1041,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1452
                },
                {
                    "start": 1454,
                    "end": 1590
                }
            ],
            "ref_mentions": [
                {
                    "start": 518,
                    "end": 537,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 537,
                    "end": 557,
                    "matchedPaperCorpusId": "259064248"
                },
                {
                    "start": 1092,
                    "end": 1110,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1110,
                    "end": 1129,
                    "matchedPaperCorpusId": "259064248"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "258309753",
            "title": "Bayesian Optimization Meets Self-Distillation",
            "text": "Our framework is different from these methods in that these utilize prior knowledge from external sources to better estimate the surrogate model, while we utilize the prior knowledge from the given task to enhance the training of the target model. It means that these methods can be applied orthogonally to our framework. \n\nSelf-Distillation. Knowledge distillation (KD) is a model compression method that involves transferring the knowledge of a large teacher model to a small student model while maintaining performance. The original work by Hinton et al. [22] proposed distilling knowledge by matching the softmax distribution of the teacher and student models. Since then, various methods have been introduced to improve the knowledge transfer process. Self-Distillation (SD) is a special form of KD where the teacher and student networks have identical architecture. Born-Again Networks (BAN) [16] demonstrated that when training the student to match the output distribution of the teacher with the identical architecture, it could outperform the teacher. Furthermore, they showed that performing multiple rounds of BAN could further improve the performance where the trained student is set to be a new teacher in the following round. The ef-fectiveness of SD has been theoretically explained by the \"multi-view\" hypothesis introduced by Allen-Zhu and Li, who showed that self-distillation performs an implicit ensemble of various models [2]. Empirical evidence from Pham et al. [36] suggests that SD encourages the student to find flatter minima, leading to better generalization. In this work, we identify that SD can be an effective method for propagating the task knowledge learned in early stages of BO, to late stages of BO. This combination of the SD and BO processes is key to yielding a high-performing model, which we validate experimentally.",
            "score": 0.5207694591245493,
            "section_title": "Related Work",
            "char_start_offset": 5089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 558,
                    "end": 562,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94921875
        },
        {
            "corpus_id": "248683566",
            "title": "Teacher-student collaborative knowledge distillation for image classification",
            "text": "With the rapid development of deep learning, convolutional neural networks have exhibited excellent performance in various computer vision tasks [1,2]. In visual datasets, a category usually contains multi-view features that are easy to categorize, and simple models can only learn some of the features; however, deep neural networks can effectively handle this problem. As the number of network layers and parameters increase, models become prone to overfitting, which affects their performance. \n\nKnowledge distillation is an important method of knowledge transfer; in this process, a lightweight model learns valid information from a heavy model to enhance performance. This model structure is often considered as a teacher-student structure. With an experienced teacher network in place, the inferior student network learns from the valuable information in the teacher network through knowledge distillation and achieves a performance improvement. Similarly, self-distillation allows a model to learn another pretrained network with the same structure. Due to the stochastic nature of feature learning and differences among model initialization methods, models obtain knowledge in different ways. The performance of a network can also be effectively improved by knowledge transfer between models. A model can learn knowledge from other models or itself to improve performance. However, it is still unclear whether a model can achieve performance improvements under the guidance of both a teacher model and itself. \n\nIn this paper, our soft label information comes from the teacher network and the output of student network, therefore the student network can be regarded as its own second teacher. Similar to multi-teacher distillation, in our approach, we let a single model learn as many view features as possible from multiple networks. In the process of human learning, students guided by teachers can further improve their abilities with self-reflection. Inspired by this approach, a combination of teacher-student knowledge distillation and student self-distillation is used to enhance the performance of neural networks, and a method called teacher-student collaborative knowledge distillation (TSKD) is proposed. This method not only utilizes the category information from the teacher network but also absorbs student's knowledge. To construct the student self-distillation model, the student network builds multiple exit classifiers from shallow to deep.",
            "score": 0.5206498757187628,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1517
                },
                {
                    "start": 1520,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2223
                },
                {
                    "start": 2224,
                    "end": 2341
                },
                {
                    "start": 2342,
                    "end": 2466
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 148,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 148,
                    "end": 150,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8173828125
        },
        {
            "corpus_id": "249431538",
            "title": "Confidence-aware Self-Semantic Distillation on Knowledge Graph Embedding",
            "text": "To get both performance and inference speed, knowledge distillation technology has been proposed. The initial knowledge distillation approach focused on utilizing the teacher model's prediction probabilities as soft a targets for training the student model [17,41]. FitNet [33] advanced the process by using the intermediate features of the teacher model as training hints for the student model. Building on FitNet, SemCKD [9] introduced semantic calibration, which employed an attention mechanism to obtain adaptive intermediate features for student training. Additionally, SimKD [8] achieved competitive performance for the student model simply by reusing the discriminant classifier from the teacher model. \n\nThe pioneer in knowledge graph embedding, KD-MKB [34], first utilized knowledge distillation for knowledge graph embedding by establishing a cooperative learning framework between a teacher model and a student model. Following this, MulDE [42] and DualDE [55] are proposed. All these papers utilized highdimensional, powerful teacher models to facilitate the training of student models for improved performance. However, a highcapacity teacher model is not available in many cases. To address this limitation, we shifting our focus towards self-knowledge distillation strategies [14,23,49,52]. However, existing self-distillation methods predominantly cater to the field of computer vision and yet perform sub-optimally within the field of knowledge graph embedding, where they manage input data that generally assume independence from each other. However, in the Knowledge Graph Embedding (KGE) domain, the data that we input are interconnected. The current self-distillation algorithms fail to leverage this aspect of information. Consequently, to harness this component of data, we propose the Improved Self-Distillation (CSD) method.",
            "score": 0.5206150717423369,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7484,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1849
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 264,
                    "matchedPaperCorpusId": "248598127"
                },
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 423,
                    "end": 426,
                    "matchedPaperCorpusId": "227335337"
                },
                {
                    "start": 581,
                    "end": 584,
                    "matchedPaperCorpusId": "247762862"
                },
                {
                    "start": 761,
                    "end": 765,
                    "matchedPaperCorpusId": "227231666"
                },
                {
                    "start": 951,
                    "end": 955,
                    "matchedPaperCorpusId": "222341253"
                },
                {
                    "start": 967,
                    "end": 971,
                    "matchedPaperCorpusId": "245124050"
                },
                {
                    "start": 1291,
                    "end": 1295,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1295,
                    "end": 1298,
                    "matchedPaperCorpusId": "233714221"
                },
                {
                    "start": 1298,
                    "end": 1301,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87353515625
        },
        {
            "corpus_id": "247922354",
            "title": "Structured Pruning Learns Compact and Accurate Models",
            "text": "Knowledge distillation (Hinton et al., 2015) is a model compression approach that transfers knowledge from a larger teacher model to a smaller student model. General distillation (Sanh et al., 2019;Sun et al., 2020;Wang et al., 2020a) and task-specific distillation (Sun et al., 2019) exploit unlabeled data and task-specific data respectively for knowledge transfer. A combination of the two leads to increased performance (Jiao et al., 2020). General distillation or pre-training the student network on unlabeled corpus is essential for retaining performance while being computationally expensive (Turc et al., 2019;Jiao et al., 2020).  Figure 1: Comparison of (a) TinyBERT (Jiao et al., 2020) and (b) our pruning approach CoFi. TinyBERT trains a randomly-initialized network through two-step distillation: (1) general distillation on a large unlabeled corpus, which takes 3.5 days to finish on 4 GPUs, and (2) task-specific distillation on the task dataset. CoFi directly prunes the fine-tuned BERT model and jointly learns five types of mask variables (i.e., z FFN , z int , z MHA , z head , z hidn ) to prune different types of units ( \u00a73.1). CoFi takes at most 20 hours to finish on 1 GPU on all the GLUE datasets (smaller datasets need < 3 hour). 6 Different distillation objectives have been also explored. Besides standard distillation from the prediction layer (Hinton et al., 2015), transferring knowledge layer-by-layer from representations (Jiao et al., 2020;Sun et al., 2020) and multi-head attention matrices (Wang et al., 2020a;Jiao et al., 2020;Sun et al., 2020) lead to significant improvements. Most distillation approaches assume a fixed student structure prior to training. Hou et al. (2020) attempt to distill to a dynamic structure with specified widths and heights.  adopt a one-shot Neural Architecture Search solution to search architectures of",
            "score": 0.5205394290931125,
            "section_title": "Distillation",
            "char_start_offset": 5102,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 215,
                    "matchedPaperCorpusId": "215238853"
                },
                {
                    "start": 215,
                    "end": 234,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 266,
                    "end": 283,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 424,
                    "end": 443,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 618,
                    "end": 636,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 676,
                    "end": 695,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1453,
                    "end": 1472,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1472,
                    "end": 1489,
                    "matchedPaperCorpusId": "215238853"
                },
                {
                    "start": 1524,
                    "end": 1544,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 1544,
                    "end": 1562,
                    "matchedPaperCorpusId": "202719327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8154296875
        },
        {
            "corpus_id": "258832433",
            "title": "Revisiting Data Augmentation in Model Compression: An Empirical and Comprehensive Study",
            "text": "Recently, extensive following-up methods have been proposed to distill teacher in not only teacher logits, but also teacher knowledge in backbone features and the invariants, such as attention [92], relation [65], [77], positive value [30], task-oriented information [96] and so on. Besides classification, it has also achieved excellent performance in object detection [1], [5], [48], [50], [81], [95], semantic segmentation [56], image-to-image translation [47], [49], [52], [68], [94], machine translation [54], pretrained language models [71], [86], multi-exit models [97], [98], model robustness [64], [99] and so on. \n\nData Augmentation in Knowledge Distillation Recently, a few works have been proposed to study the usage of data augmentation in knowledge distillation. Fu et al. propose to apply different data augmentation policies to students and teachers to facilitate knowledge distillation [18]. Das et al. \n\nhave given an empirical study on the effectiveness of data augmentation in knowledge distillation, which finds that the mix-based data augmentation methods harms the performance of students [13]. Wang et al. find that knowledge distillation loss can tap into the extra information from different input views brought by data augmentation [79]. Wei et al. propose to circumvent the outliers of Autoaugment [12] with an additional knowledge distillation loss [83].",
            "score": 0.5204478908187511,
            "section_title": "B. Neural Network Pruning",
            "char_start_offset": 8724,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1383
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 212,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 214,
                    "end": 218,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 370,
                    "end": 373,
                    "matchedPaperCorpusId": "67876989"
                },
                {
                    "start": 375,
                    "end": 378,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "267213"
                },
                {
                    "start": 386,
                    "end": 390,
                    "matchedPaperCorpusId": "222178995"
                },
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "182952755"
                },
                {
                    "start": 426,
                    "end": 430,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 459,
                    "end": 463,
                    "matchedPaperCorpusId": "213175568"
                },
                {
                    "start": 471,
                    "end": 475,
                    "matchedPaperCorpusId": "226842089"
                },
                {
                    "start": 483,
                    "end": 487,
                    "matchedPaperCorpusId": "247447127"
                },
                {
                    "start": 601,
                    "end": 605,
                    "matchedPaperCorpusId": "2672720"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "219964413"
                },
                {
                    "start": 1378,
                    "end": 1382,
                    "matchedPaperCorpusId": "214641356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "paperId": "786701789d26604b7007d6b410853fe275ef6267",
            "corpusId": 265486383,
            "title": "Performance Analysis and Comparison of Knowledge Distillation Among Diverse Teacher and Student Models",
            "venue": "2023 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)",
            "year": 2023,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCE-Asia59966.2023.10326343?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCE-Asia59966.2023.10326343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268630173",
                    "name": "Jaeung Lee"
                },
                {
                    "authorId": "2123471573",
                    "name": "T. Vuong"
                },
                {
                    "authorId": "2268635281",
                    "name": "Jiamu Wang"
                },
                {
                    "authorId": "2268633939",
                    "name": "Jiang Kan"
                },
                {
                    "authorId": "2189380650",
                    "name": "Keunho Byeon"
                },
                {
                    "authorId": "2256462988",
                    "name": "Ju Cheon Lee"
                },
                {
                    "authorId": "2268556136",
                    "name": "Sumin Jung"
                },
                {
                    "authorId": "2146735404",
                    "name": "D. C. Bui"
                },
                {
                    "authorId": "2268724842",
                    "name": "A. Nguyen"
                },
                {
                    "authorId": "2268628029",
                    "name": "Hyun Yang"
                },
                {
                    "authorId": "2260351554",
                    "name": "Jin Tae Kwak"
                }
            ],
            "abstract": "With the advancements in deep learning, the development of large-scale models has become essential for many machine learning tasks in various fields such as computer vision and natural language. Such large-scale models demand extensive computational power and memory, posing challenges for practical deployment on mobile and edge devices. To address these issues, knowledge distillation, a technique for reproducing the performance of large-scale models with a smaller model, has been proposed. This paper investigates the effectiveness of knowledge distillation across various combinations of teach-student model pairs for image classification to deepen the understanding of knowledge distillation and the relationship between large- and small-scale models.",
            "corpus_id": "265486383",
            "text": "With the advancements in deep learning, the development of large-scale models has become essential for many machine learning tasks in various fields such as computer vision and natural language. Such large-scale models demand extensive computational power and memory, posing challenges for practical deployment on mobile and edge devices. To address these issues, knowledge distillation, a technique for reproducing the performance of large-scale models with a smaller model, has been proposed. This paper investigates the effectiveness of knowledge distillation across various combinations of teach-student model pairs for image classification to deepen the understanding of knowledge distillation and the relationship between large- and small-scale models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9208984375
        },
        {
            "paperId": "1ee7d1cf0303d84c4dd286f12eebcffc3187ec07",
            "corpusId": 273185561,
            "title": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based Knowledge Distillation",
            "venue": "",
            "year": 2024,
            "referenceCount": 29,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.05168, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2197478045",
                    "name": "Yuelyu Ji"
                },
                {
                    "authorId": "2267023159",
                    "name": "Zhuochun Li"
                },
                {
                    "authorId": "2302770443",
                    "name": "Rui Meng"
                },
                {
                    "authorId": "2281450231",
                    "name": "Daqing He"
                }
            ],
            "abstract": "Reranking documents based on their relevance to a given query is a critical task in information retrieval. Traditional reranking methods often lack transparency and rely on proprietary models, hindering reproducibility and interpretability. We propose Reason-to-Rank (R2R), a novel open-source reranking approach that enhances transparency by generating two types of reasoning: direct relevance reasoning, which explains how a document addresses the query, and comparison reasoning, which justifies the relevance of one document over another. We leverage large language models (LLMs) as teacher models to generate these explanations and distill this knowledge into smaller, openly available student models. Our student models are trained to generate meaningful reasoning and rerank documents, achieving competitive performance across multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate that R2R not only improves reranking accuracy but also provides valuable insights into the decision-making process. By offering a structured and interpretable solution with openly accessible resources, R2R aims to bridge the gap between effectiveness and transparency in information retrieval, fostering reproducibility and further research in the field.",
            "corpus_id": "273185561",
            "text": "Reranking documents based on their relevance to a given query is a critical task in information retrieval. Traditional reranking methods often lack transparency and rely on proprietary models, hindering reproducibility and interpretability. We propose Reason-to-Rank (R2R), a novel open-source reranking approach that enhances transparency by generating two types of reasoning: direct relevance reasoning, which explains how a document addresses the query, and comparison reasoning, which justifies the relevance of one document over another. We leverage large language models (LLMs) as teacher models to generate these explanations and distill this knowledge into smaller, openly available student models. Our student models are trained to generate meaningful reasoning and rerank documents, achieving competitive performance across multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate that R2R not only improves reranking accuracy but also provides valuable insights into the decision-making process. By offering a structured and interpretable solution with openly accessible resources, R2R aims to bridge the gap between effectiveness and transparency in information retrieval, fostering reproducibility and further research in the field.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8291015625
        },
        {
            "paperId": "daaf57ae76a8e58f2a6a1a286a5be99938eda7bd",
            "corpusId": 277779273,
            "title": "Comparison of Transfer Learning Performance in Lung and Colon Classification with Knowledge Distillation",
            "venue": "Journal of Advanced Health Informatics Research",
            "year": 2024,
            "referenceCount": 34,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.59247/jahir.v2i2.289?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.59247/jahir.v2i2.289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2355339844",
                    "name": "Annastasya Nabila Elsa Wulandari"
                },
                {
                    "authorId": "2355340858",
                    "name": "Aimar Yudhistira"
                },
                {
                    "authorId": "2355335792",
                    "name": "Purwono"
                },
                {
                    "authorId": "51453929",
                    "name": "Abdel-Nasser Sharkawy"
                }
            ],
            "abstract": "This research aims to apply the knowledge distillation method to medical image classification, specifically in the case of lung and colon image classification using various transfer learning models. Knowledge distillation allows the transfer of knowledge from a larger model (teacher) to a smaller model (student), which enables more efficient model building without sacrificing accuracy. In this research, the DenseNet169 model is used as the teacher model. The student model uses several alternative transfer learning architectures such as DenseNet121, MobileNet, ResNet50, InceptionV3, and Xception. The data used consists of 25,000 histopathology images that have been processed and divided into training, validation, and test data. Data augmentation was performed to enlarge the dataset from 750 to 25,000 images, which helped improve the performance of the model. Model performance evaluation was performed by measuring the accuracy and loss value of each student model compared to the teacher model. The results showed that the student models generated through the knowledge distillation process performed close to or even exceeded the teacher model in some cases, with the Xception model showing the highest accuracy of 96.95%. In conclusion, knowledge distillation is effective in reducing model complexity without compromising performance, which is particularly beneficial for implementation on resource-constrained devices.",
            "corpus_id": "277779273",
            "text": "This research aims to apply the knowledge distillation method to medical image classification, specifically in the case of lung and colon image classification using various transfer learning models. Knowledge distillation allows the transfer of knowledge from a larger model (teacher) to a smaller model (student), which enables more efficient model building without sacrificing accuracy. In this research, the DenseNet169 model is used as the teacher model. The student model uses several alternative transfer learning architectures such as DenseNet121, MobileNet, ResNet50, InceptionV3, and Xception. The data used consists of 25,000 histopathology images that have been processed and divided into training, validation, and test data. Data augmentation was performed to enlarge the dataset from 750 to 25,000 images, which helped improve the performance of the model. Model performance evaluation was performed by measuring the accuracy and loss value of each student model compared to the teacher model. The results showed that the student models generated through the knowledge distillation process performed close to or even exceeded the teacher model in some cases, with the Xception model showing the highest accuracy of 96.95%. In conclusion, knowledge distillation is effective in reducing model complexity without compromising performance, which is particularly beneficial for implementation on resource-constrained devices.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.95556640625
        },
        {
            "paperId": "4385314ade0ee492d99bc03c1e8ea98ec577c44c",
            "corpusId": 273350649,
            "title": "Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 40,
            "citationCount": 11,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.11325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145738382",
                    "name": "Wenda Xu"
                },
                {
                    "authorId": "2325953181",
                    "name": "Rujun Han"
                },
                {
                    "authorId": "2278799290",
                    "name": "Zifeng Wang"
                },
                {
                    "authorId": "2253355959",
                    "name": "Long T. Le"
                },
                {
                    "authorId": "10723295",
                    "name": "Dhruv Madeka"
                },
                {
                    "authorId": "2307183930",
                    "name": "Lei Li"
                },
                {
                    "authorId": "2312264839",
                    "name": "William Yang Wang"
                },
                {
                    "authorId": "2253488622",
                    "name": "Rishabh Agarwal"
                },
                {
                    "authorId": "2278969944",
                    "name": "Chen-Yu Lee"
                },
                {
                    "authorId": "2305619900",
                    "name": "Tomas Pfister"
                }
            ],
            "abstract": "Recent advances in knowledge distillation (KD) have enabled smaller student models to approach the performance of larger teacher models. However, popular methods such as supervised KD and on-policy KD, are adversely impacted by the knowledge gaps between teacher-student in practical scenarios. Supervised KD suffers from a distribution mismatch between training with a static dataset and inference over final student-generated outputs. Conversely, on-policy KD, which uses student-generated samples for training, can suffer from low-quality training examples with which teacher models are not familiar, resulting in inaccurate teacher feedback. To address these limitations, we introduce Speculative Knowledge Distillation (SKD), a novel approach that leverages cooperation between student and teacher models to generate high-quality training data on-the-fly while aligning with the student's inference-time distribution. In SKD, the student proposes tokens, and the teacher replaces poorly ranked ones based on its own distribution, transferring high-quality knowledge adaptively. We evaluate SKD on various text generation tasks, including translation, summarization, math, and instruction following, and show that SKD consistently outperforms existing KD methods across different domains, data sizes, and model initialization strategies.",
            "corpus_id": "273350649",
            "text": "Recent advances in knowledge distillation (KD) have enabled smaller student models to approach the performance of larger teacher models. However, popular methods such as supervised KD and on-policy KD, are adversely impacted by the knowledge gaps between teacher-student in practical scenarios. Supervised KD suffers from a distribution mismatch between training with a static dataset and inference over final student-generated outputs. Conversely, on-policy KD, which uses student-generated samples for training, can suffer from low-quality training examples with which teacher models are not familiar, resulting in inaccurate teacher feedback. To address these limitations, we introduce Speculative Knowledge Distillation (SKD), a novel approach that leverages cooperation between student and teacher models to generate high-quality training data on-the-fly while aligning with the student's inference-time distribution. In SKD, the student proposes tokens, and the teacher replaces poorly ranked ones based on its own distribution, transferring high-quality knowledge adaptively. We evaluate SKD on various text generation tasks, including translation, summarization, math, and instruction following, and show that SKD consistently outperforms existing KD methods across different domains, data sizes, and model initialization strategies.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.890625
        },
        {
            "paperId": "7224d3710607401c7d805ddc2c79b651f8fbd032",
            "corpusId": 269009774,
            "title": "Robust feature knowledge distillation for enhanced performance of lightweight crack segmentation models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 59,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.06258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294012206",
                    "name": "Zhaohui Chen"
                },
                {
                    "authorId": "102528331",
                    "name": "Elyas Asadi Shamsabadi"
                },
                {
                    "authorId": "2294306326",
                    "name": "Sheng Jiang"
                },
                {
                    "authorId": "2268080369",
                    "name": "Luming Shen"
                },
                {
                    "authorId": "2267987893",
                    "name": "Daniel Dias-da-Costa"
                }
            ],
            "abstract": "Vision-based crack detection faces deployment challenges due to the size of robust models and edge device limitations. These can be addressed with lightweight models trained with knowledge distillation (KD). However, state-of-the-art (SOTA) KD methods compromise anti-noise robustness. This paper develops Robust Feature Knowledge Distillation (RFKD), a framework to improve robustness while retaining the precision of light models for crack segmentation. RFKD distils knowledge from a teacher model's logit layers and intermediate feature maps while leveraging mixed clean and noisy images to transfer robust patterns to the student model, improving its precision, generalisation, and anti-noise performance. To validate the proposed RFKD, a lightweight crack segmentation model, PoolingCrack Tiny (PCT), with only 0.5 M parameters, is also designed and used as the student to run the framework. The results show a significant enhancement in noisy images, with RFKD reaching a 62% enhanced mean Dice score (mDS) compared to SOTA KD methods.",
            "corpus_id": "269009774",
            "text": "Vision-based crack detection faces deployment challenges due to the size of robust models and edge device limitations. These can be addressed with lightweight models trained with knowledge distillation (KD). However, state-of-the-art (SOTA) KD methods compromise anti-noise robustness. This paper develops Robust Feature Knowledge Distillation (RFKD), a framework to improve robustness while retaining the precision of light models for crack segmentation. RFKD distils knowledge from a teacher model's logit layers and intermediate feature maps while leveraging mixed clean and noisy images to transfer robust patterns to the student model, improving its precision, generalisation, and anti-noise performance. To validate the proposed RFKD, a lightweight crack segmentation model, PoolingCrack Tiny (PCT), with only 0.5 M parameters, is also designed and used as the student to run the framework. The results show a significant enhancement in noisy images, with RFKD reaching a 62% enhanced mean Dice score (mDS) compared to SOTA KD methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8544921875
        },
        {
            "paperId": "139be9f98735c72aa9932b7a9c2b6de33ca2245c",
            "corpusId": 265622441,
            "title": "Breast cancer diagnosis through knowledge distillation of Swin transformer-based teacher\u2013student models",
            "venue": "Machine Learning: Science and Technology",
            "year": 2023,
            "referenceCount": 11,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://iopscience.iop.org/article/10.1088/2632-2153/ad10cc/pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/2632-2153/ad10cc?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/2632-2153/ad10cc, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2244523635",
                    "name": "Bhavannarayanna Kolla"
                },
                {
                    "authorId": "2262699276",
                    "name": "V. P"
                }
            ],
            "abstract": "Breast cancer is a significant global health concern, emphasizing the crucial need for a timely and accurate diagnosis to enhance survival rates. Traditional diagnostic methods rely on pathologists analyzing whole-slide images (WSIs) to identify and diagnose malignancies. However, this task is complex, demanding specialized expertise and imposing a substantial workload on pathologists. Additionally, existing deep learning models, commonly employed for classifying histopathology images, often need enhancements to ensure their suitability for real-time deployment on WSI, especially when trained for small regions of interest (ROIs). This article introduces two Swin transformer-based architectures: the teacher model, characterized by its moderate size, and the lightweight student model. Both models are trained using a publicly available dataset of breast cancer histopathology images, focusing on ROIs with varying magnification factors. Transfer learning is applied to train the teacher model, and knowledge distillation (KD) transfers its capabilities to the student model. To enhance validation accuracy and minimize the total loss in KD, we employ the state\u2013action\u2013reward\u2013state\u2013action (SARSA) reinforcement learning algorithm. The algorithm dynamically computes temperature and a weighting factor throughout the KD process to achieve high accuracy within a considerably shorter training timeframe. Additionally, the student model is deployed to analyze malignancies in WSI. Despite the student model being only one-third the size and flops of the teacher model, it achieves an impressive accuracy of 98.71%, slightly below the teacher\u2019s accuracy of 98.91%. Experimental results demonstrate that the student model can process WSIs at a throughput of 1.67\u2009samples\u2009s\u22121 with an accuracy of 82%. The proposed student model, trained using KD and the SARSA algorithm, exhibits promising breast cancer classification and WSI analysis performance. These findings indicate its potential for assisting pathologists in diagnosing breast cancer accurately and effectively.",
            "corpus_id": "265622441",
            "text": "Breast cancer is a significant global health concern, emphasizing the crucial need for a timely and accurate diagnosis to enhance survival rates. Traditional diagnostic methods rely on pathologists analyzing whole-slide images (WSIs) to identify and diagnose malignancies. However, this task is complex, demanding specialized expertise and imposing a substantial workload on pathologists. Additionally, existing deep learning models, commonly employed for classifying histopathology images, often need enhancements to ensure their suitability for real-time deployment on WSI, especially when trained for small regions of interest (ROIs). This article introduces two Swin transformer-based architectures: the teacher model, characterized by its moderate size, and the lightweight student model. Both models are trained using a publicly available dataset of breast cancer histopathology images, focusing on ROIs with varying magnification factors. Transfer learning is applied to train the teacher model, and knowledge distillation (KD) transfers its capabilities to the student model. To enhance validation accuracy and minimize the total loss in KD, we employ the state\u2013action\u2013reward\u2013state\u2013action (SARSA) reinforcement learning algorithm. The algorithm dynamically computes temperature and a weighting factor throughout the KD process to achieve high accuracy within a considerably shorter training timeframe. Additionally, the student model is deployed to analyze malignancies in WSI. Despite the student model being only one-third the size and flops of the teacher model, it achieves an impressive accuracy of 98.71%, slightly below the teacher\u2019s accuracy of 98.91%. Experimental results demonstrate that the student model can process WSIs at a throughput of 1.67\u2009samples\u2009s\u22121 with an accuracy of 82%. The proposed student model, trained using KD and the SARSA algorithm, exhibits promising breast cancer classification and WSI analysis performance. These findings indicate its potential for assisting pathologists in diagnosing breast cancer accurately and effectively.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.71142578125
        },
        {
            "paperId": "2581abb5fb95b873f245092d0133912c8c1ff05a",
            "corpusId": 277781833,
            "title": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 39,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2410.05168?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2410.05168, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2197478045",
                    "name": "Yuelyu Ji"
                },
                {
                    "authorId": "2267023159",
                    "name": "Zhuochun Li"
                },
                {
                    "authorId": "2302770443",
                    "name": "Rui Meng"
                },
                {
                    "authorId": "2281450231",
                    "name": "Daqing He"
                }
            ],
            "abstract": "Reranking documents based on their relevance to a given query is a critical task in information retrieval. Traditional reranking methods often focus on improving the initial rankings but tend to lack transparency, failing to explain why one document is ranked higher than another. In this paper, we introduce Reason-ingRank, a novel reranking approach that enhances clarity by generating two types of reasoning: explicit reasoning, which explains how a document addresses the query, and comparison reasoning, which justifies the relevance of one document over another. We leverage large language models (LLMs) as teacher models to generate these explanations and distill this knowledge into smaller, more resource-efficient student models . While the student models may not outperform LLMs in speed, they significantly reduce the computational burden by requiring fewer resources, making them more suitable for large-scale or resource-constrained settings. These student models are trained to both generate meaningful reasoning and rerank documents, achieving competitive performance across multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate that ReasoningRank improves reranking accuracy and provides valuable insights into the decision-making process, offering a structured and interpretable solution for reranking tasks. 1",
            "corpus_id": "277781833",
            "text": "Reranking documents based on their relevance to a given query is a critical task in information retrieval. Traditional reranking methods often focus on improving the initial rankings but tend to lack transparency, failing to explain why one document is ranked higher than another. In this paper, we introduce Reason-ingRank, a novel reranking approach that enhances clarity by generating two types of reasoning: explicit reasoning, which explains how a document addresses the query, and comparison reasoning, which justifies the relevance of one document over another. We leverage large language models (LLMs) as teacher models to generate these explanations and distill this knowledge into smaller, more resource-efficient student models . While the student models may not outperform LLMs in speed, they significantly reduce the computational burden by requiring fewer resources, making them more suitable for large-scale or resource-constrained settings. These student models are trained to both generate meaningful reasoning and rerank documents, achieving competitive performance across multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate that ReasoningRank improves reranking accuracy and provides valuable insights into the decision-making process, offering a structured and interpretable solution for reranking tasks. 1",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.84326171875
        },
        {
            "paperId": "60c0a46aa00f200103209b8e94f1b9420d7f07e2",
            "corpusId": 276346900,
            "title": "Knowledge Distillation Framework of Pre-Trained Language Models Combined with Parameter Efficient Fine-Tuning",
            "venue": "ACM Cloud and Autonomic Computing Conference",
            "year": 2024,
            "referenceCount": 17,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CAC63892.2024.10864933?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CAC63892.2024.10864933, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345638785",
                    "name": "Rui Chen"
                },
                {
                    "authorId": "2237960325",
                    "name": "Yixin Su"
                },
                {
                    "authorId": "2345377502",
                    "name": "Hongchang Jing"
                }
            ],
            "abstract": "The development of pre-trained language models(PLMs) has increased the size of models, increasing the demand for computing resources and data when fine-tuning downstream tasks, and challenging deployment in resource-limited environments. In order to alleviate this demand, this paper proposes a knowledge distillation(KD) framework of PLMs combined with parameter efficient fine-tuning(PEFT). By introducing PEFT into both teacher fine-tuning and specific task distillation, the framework reduces computational costs without significantly compromising model performance. The experimental results show that in the GLUE benchmark test, the performance of the student model trained by the distillation framework on multiple NLU tasks is comparable to or even better than that of the teacher, and it shows higher efficiency and robustness in resource utilization.",
            "corpus_id": "276346900",
            "text": "The development of pre-trained language models(PLMs) has increased the size of models, increasing the demand for computing resources and data when fine-tuning downstream tasks, and challenging deployment in resource-limited environments. In order to alleviate this demand, this paper proposes a knowledge distillation(KD) framework of PLMs combined with parameter efficient fine-tuning(PEFT). By introducing PEFT into both teacher fine-tuning and specific task distillation, the framework reduces computational costs without significantly compromising model performance. The experimental results show that in the GLUE benchmark test, the performance of the student model trained by the distillation framework on multiple NLU tasks is comparable to or even better than that of the teacher, and it shows higher efficiency and robustness in resource utilization.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.890625
        },
        {
            "paperId": "b8a335c6ab5afe0b62e242c77be7fbb84158b45b",
            "corpusId": 260846373,
            "title": "IMF: Integrating Matched Features Using Attentive Logit in Knowledge Distillation",
            "venue": "International Joint Conference on Artificial Intelligence",
            "year": 2023,
            "referenceCount": 27,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2023/0108.pdf",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2023/108?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2023/108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2164327036",
                    "name": "Jeongho Kim"
                },
                {
                    "authorId": "2143329283",
                    "name": "Hanbeen Lee"
                },
                {
                    "authorId": "35154708",
                    "name": "Simon S. Woo"
                }
            ],
            "abstract": "Knowledge distillation (KD) is an effective method for transferring the knowledge of a teacher model to a student model, that aims to improve the latter's performance efficiently. Although generic knowledge distillation methods such as softmax representation distillation and intermediate feature matching have demonstrated improvements with various tasks, only marginal improvements are shown in student networks due to their limited model capacity. In this work, to address the student model's limitation, we propose a novel flexible KD framework, Integrating Matched Features using Attentive Logit in Knowledge Distillation (IMF). Our approach introduces an intermediate feature distiller (IFD) to improve the overall performance of the student model by directly distilling the teacher's knowledge into branches of student models. The generated output of IFD, which is trained by the teacher model, is effectively combined by attentive logit. We use only a few blocks of the student and the trained IFD during inference, requiring an equal or less number of parameters. Through extensive experiments, we demonstrate that IMF consistently outperforms other state-of-the-art methods with a large margin over the various datasets in different tasks without extra computation.",
            "corpus_id": "260846373",
            "text": "Knowledge distillation (KD) is an effective method for transferring the knowledge of a teacher model to a student model, that aims to improve the latter's performance efficiently. Although generic knowledge distillation methods such as softmax representation distillation and intermediate feature matching have demonstrated improvements with various tasks, only marginal improvements are shown in student networks due to their limited model capacity. In this work, to address the student model's limitation, we propose a novel flexible KD framework, Integrating Matched Features using Attentive Logit in Knowledge Distillation (IMF). Our approach introduces an intermediate feature distiller (IFD) to improve the overall performance of the student model by directly distilling the teacher's knowledge into branches of student models. The generated output of IFD, which is trained by the teacher model, is effectively combined by attentive logit. We use only a few blocks of the student and the trained IFD during inference, requiring an equal or less number of parameters. Through extensive experiments, we demonstrate that IMF consistently outperforms other state-of-the-art methods with a large margin over the various datasets in different tasks without extra computation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.92138671875
        },
        {
            "paperId": "0bb176b5dc0653ea8e4660301265f2dbd2e3a46b",
            "corpusId": 271193899,
            "title": "Efficient knowledge distillation for hybrid models: A vision transformer\u2010convolutional neural network to convolutional neural network approach for classifying remote sensing images",
            "venue": "IET Cyber-Systems and Robotics",
            "year": 2024,
            "referenceCount": 27,
            "citationCount": 19,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1049/csy2.12120",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1049/csy2.12120?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1049/csy2.12120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2238157424",
                    "name": "Huaxiang Song"
                },
                {
                    "authorId": "2298450196",
                    "name": "Yuxuan Yuan"
                },
                {
                    "authorId": "2298379228",
                    "name": "Zhiwei Ouyang"
                },
                {
                    "authorId": "2298509545",
                    "name": "Yu Yang"
                },
                {
                    "authorId": "2298438223",
                    "name": "Hui Xiang"
                }
            ],
            "abstract": "In various fields, knowledge distillation (KD) techniques that combine vision transformers (ViTs) and convolutional neural networks (CNNs) as a hybrid teacher have shown remarkable results in classification. However, in the realm of remote sensing images (RSIs), existing KD research studies are not only scarce but also lack competitiveness. This issue significantly impedes the deployment of the notable advantages of ViTs and CNNs. To tackle this, the authors introduce a novel hybrid\u2010model KD approach named HMKD\u2010Net, which comprises a CNN\u2010ViT ensemble teacher and a CNN student. Contrary to popular opinion, the authors posit that the sparsity in RSI data distribution limits the effectiveness and efficiency of hybrid\u2010model knowledge transfer. As a solution, a simple yet innovative method to handle variances during the KD phase is suggested, leading to substantial enhancements in the effectiveness and efficiency of hybrid knowledge transfer. The authors assessed the performance of HMKD\u2010Net on three RSI datasets. The findings indicate that HMKD\u2010Net significantly outperforms other cutting\u2010edge methods while maintaining a significantly smaller size. Specifically, HMKD\u2010Net exceeds other KD\u2010based methods with a maximum accuracy improvement of 22.8% across various datasets. As ablation experiments indicated, HMKD\u2010Net has cut down on time expenses by about 80% in the KD process. This research study validates that the hybrid\u2010model KD technique can be more effective and efficient if the data distribution sparsity in RSIs is well handled.",
            "corpus_id": "271193899",
            "text": "In various fields, knowledge distillation (KD) techniques that combine vision transformers (ViTs) and convolutional neural networks (CNNs) as a hybrid teacher have shown remarkable results in classification. However, in the realm of remote sensing images (RSIs), existing KD research studies are not only scarce but also lack competitiveness. This issue significantly impedes the deployment of the notable advantages of ViTs and CNNs. To tackle this, the authors introduce a novel hybrid\u2010model KD approach named HMKD\u2010Net, which comprises a CNN\u2010ViT ensemble teacher and a CNN student. Contrary to popular opinion, the authors posit that the sparsity in RSI data distribution limits the effectiveness and efficiency of hybrid\u2010model knowledge transfer. As a solution, a simple yet innovative method to handle variances during the KD phase is suggested, leading to substantial enhancements in the effectiveness and efficiency of hybrid knowledge transfer. The authors assessed the performance of HMKD\u2010Net on three RSI datasets. The findings indicate that HMKD\u2010Net significantly outperforms other cutting\u2010edge methods while maintaining a significantly smaller size. Specifically, HMKD\u2010Net exceeds other KD\u2010based methods with a maximum accuracy improvement of 22.8% across various datasets. As ablation experiments indicated, HMKD\u2010Net has cut down on time expenses by about 80% in the KD process. This research study validates that the hybrid\u2010model KD technique can be more effective and efficient if the data distribution sparsity in RSIs is well handled.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.787109375
        },
        {
            "paperId": "602f041b7ba9f1bc0bd30d9f26c8227026e812be",
            "corpusId": 267750520,
            "title": "Revisiting Knowledge Distillation for Autoregressive Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 52,
            "citationCount": 19,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2114810150",
                    "name": "Qihuang Zhong"
                },
                {
                    "authorId": "46573238",
                    "name": "Liang Ding"
                },
                {
                    "authorId": "2248152216",
                    "name": "Li Shen"
                },
                {
                    "authorId": "46701032",
                    "name": "Juhua Liu"
                },
                {
                    "authorId": "2212029373",
                    "name": "Bo Du"
                },
                {
                    "authorId": "2255502438",
                    "name": "D. Tao"
                }
            ],
            "abstract": "Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD. The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes. More encouragingly, ATKD can improve the student model generalization effectively.",
            "corpus_id": "267750520",
            "text": "Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD. The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes. More encouragingly, ATKD can improve the student model generalization effectively.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.91650390625
        },
        {
            "paperId": "9f1622d33fac64c06fc8bb1e518ca71ebe2730c6",
            "corpusId": 273970257,
            "title": "Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 30,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.08028, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2162405317",
                    "name": "Juanhui Li"
                },
                {
                    "authorId": "7529854",
                    "name": "Sreyashi Nag"
                },
                {
                    "authorId": "2326538488",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "2313288320",
                    "name": "Xianfeng Tang"
                },
                {
                    "authorId": "2330247676",
                    "name": "S. Sarwar"
                },
                {
                    "authorId": "2290923513",
                    "name": "Limeng Cui"
                },
                {
                    "authorId": "2330381905",
                    "name": "Hansu Gu"
                },
                {
                    "authorId": "2257362558",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "2315065019",
                    "name": "Qi He"
                },
                {
                    "authorId": "2314887353",
                    "name": "Jiliang Tang"
                }
            ],
            "abstract": "In real-world NLP applications, Large Language Models (LLMs) offer promising solutions due to their extensive training on vast datasets. However, the large size and high computation demands of LLMs limit their practicality in many applications, especially when further fine-tuning is required. To address these limitations, smaller models are typically preferred for deployment. However, their training is hindered by the scarcity of labeled data. In contrast, unlabeled data is often readily which can be leveraged by using LLMs to generate pseudo-labels for training smaller models. This enables the smaller models (student) to acquire knowledge from LLMs(teacher) while reducing computational costs. This process introduces challenges, such as potential noisy pseudo-labels. Selecting high-quality and informative data is therefore critical to enhance model performance while improving the efficiency of data utilization. To address this, we propose LLKD that enables Learning with Less computational resources and less data for Knowledge Distillation from LLMs. LLKD is an adaptive sample selection method that incorporates signals from both the teacher and student. Specifically, it prioritizes samples where the teacher demonstrates high confidence in its labeling, indicating reliable labels, and where the student exhibits a high information need, identifying challenging samples that require further learning. Our comprehensive experiments show that LLKD achieves superior performance across various datasets with higher data efficiency.",
            "corpus_id": "273970257",
            "text": "In real-world NLP applications, Large Language Models (LLMs) offer promising solutions due to their extensive training on vast datasets. However, the large size and high computation demands of LLMs limit their practicality in many applications, especially when further fine-tuning is required. To address these limitations, smaller models are typically preferred for deployment. However, their training is hindered by the scarcity of labeled data. In contrast, unlabeled data is often readily which can be leveraged by using LLMs to generate pseudo-labels for training smaller models. This enables the smaller models (student) to acquire knowledge from LLMs(teacher) while reducing computational costs. This process introduces challenges, such as potential noisy pseudo-labels. Selecting high-quality and informative data is therefore critical to enhance model performance while improving the efficiency of data utilization. To address this, we propose LLKD that enables Learning with Less computational resources and less data for Knowledge Distillation from LLMs. LLKD is an adaptive sample selection method that incorporates signals from both the teacher and student. Specifically, it prioritizes samples where the teacher demonstrates high confidence in its labeling, indicating reliable labels, and where the student exhibits a high information need, identifying challenging samples that require further learning. Our comprehensive experiments show that LLKD achieves superior performance across various datasets with higher data efficiency.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.84619140625
        },
        {
            "paperId": "4d447862c9316c671f3d5dac7a9cd9e046e773be",
            "corpusId": 268230792,
            "title": "Differentially Private Knowledge Distillation via Synthetic Text Generation",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 35,
            "citationCount": 14,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.00932, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2289844412",
                    "name": "James Flemings"
                },
                {
                    "authorId": "2285971718",
                    "name": "Murali Annavaram"
                }
            ],
            "abstract": "Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy puts pressure on practitioners to train LLMs with Differential Privacy (DP) on private data. Concurrently, the exponential growth in parameter size of LLMs necessitates model compression before deployment of LLMs on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, simultaneously applying both schemes can compound the utility degradation. To this end, we propose DistilDP: a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private teacher LLM. The knowledge of a teacher LLM is transferred onto the student in two ways: one way from the synthetic data itself -- the hard labels, and the other way by the output distribution of the teacher evaluated on the synthetic data -- the soft labels. Furthermore, if the teacher and student share a similar architectural structure, we can further distill knowledge by aligning the hidden representations between both. Our experimental results demonstrate that DistilDP can substantially improve the utility over existing baselines, at least $9.0$ PPL on the Big Patent dataset, with strong privacy parameters, $\\epsilon=2$. These promising results progress privacy-preserving compression of autoregressive LLMs. Our code can be accessed here: https://github.com/james-flemings/dp_compress.",
            "corpus_id": "268230792",
            "text": "Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy puts pressure on practitioners to train LLMs with Differential Privacy (DP) on private data. Concurrently, the exponential growth in parameter size of LLMs necessitates model compression before deployment of LLMs on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, simultaneously applying both schemes can compound the utility degradation. To this end, we propose DistilDP: a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private teacher LLM. The knowledge of a teacher LLM is transferred onto the student in two ways: one way from the synthetic data itself -- the hard labels, and the other way by the output distribution of the teacher evaluated on the synthetic data -- the soft labels. Furthermore, if the teacher and student share a similar architectural structure, we can further distill knowledge by aligning the hidden representations between both. Our experimental results demonstrate that DistilDP can substantially improve the utility over existing baselines, at least $9.0$ PPL on the Big Patent dataset, with strong privacy parameters, $\\epsilon=2$. These promising results progress privacy-preserving compression of autoregressive LLMs. Our code can be accessed here: https://github.com/james-flemings/dp_compress.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.634765625
        },
        {
            "paperId": "52f05bf38c7d90d03f62ab6ecf7e9fceaf97fd9c",
            "corpusId": 258480106,
            "title": "Avatar Knowledge Distillation: Self-ensemble Teacher Paradigm with Uncertainty",
            "venue": "ACM Multimedia",
            "year": 2023,
            "referenceCount": 63,
            "citationCount": 8,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.02722",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.02722, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1703465",
                    "name": "Yuan Zhang"
                },
                {
                    "authorId": "2128193841",
                    "name": "Weihua Chen"
                },
                {
                    "authorId": "2156141201",
                    "name": "Yichen Lu"
                },
                {
                    "authorId": "2110813546",
                    "name": "Tao Huang"
                },
                {
                    "authorId": "2109166067",
                    "name": "Xiuyu Sun"
                },
                {
                    "authorId": "2125080516",
                    "name": "Jian Cao"
                }
            ],
            "abstract": "Knowledge distillation is an effective paradigm for boosting the performance of pocket-size model, especially when multiple teacher models are available, the student would break the upper limit again. However, it is not economical to train diverse teacher models for the disposable distillation. In this paper, we introduce a new concept dubbed Avatars for distillation, which are the inference ensemble models derived from the teacher. Concretely, (1) For each iteration of distillation training, various Avatars are generated by a perturbation transformation. We validate that Avatars own higher upper limit of working capacity and teaching ability, aiding the student model in learning diverse and receptive knowledge perspectives from the teacher model. (2) During the distillation, we propose an uncertainty-aware factor from the variance of statistical differences between the vanilla teacher and Avatars, to adjust Avatars' contribution on knowledge transfer adaptively. Avatar Knowledge Distillation (AKD) is fundamentally different from existing methods and refines with the innovative view of unequal training. Comprehensive experiments demonstrate the effectiveness of our Avatars mechanism, which polishes up the state-of-the-art distillation methods for dense prediction without more extra computational cost. The AKD brings at most 0.7 AP gains on COCO 2017 for Object Detection and 1.83 mIoU gains on Cityscapes for Semantic Segmentation, respectively.",
            "corpus_id": "258480106",
            "text": "Knowledge distillation is an effective paradigm for boosting the performance of pocket-size model, especially when multiple teacher models are available, the student would break the upper limit again. However, it is not economical to train diverse teacher models for the disposable distillation. In this paper, we introduce a new concept dubbed Avatars for distillation, which are the inference ensemble models derived from the teacher. Concretely, (1) For each iteration of distillation training, various Avatars are generated by a perturbation transformation. We validate that Avatars own higher upper limit of working capacity and teaching ability, aiding the student model in learning diverse and receptive knowledge perspectives from the teacher model. (2) During the distillation, we propose an uncertainty-aware factor from the variance of statistical differences between the vanilla teacher and Avatars, to adjust Avatars' contribution on knowledge transfer adaptively. Avatar Knowledge Distillation (AKD) is fundamentally different from existing methods and refines with the innovative view of unequal training. Comprehensive experiments demonstrate the effectiveness of our Avatars mechanism, which polishes up the state-of-the-art distillation methods for dense prediction without more extra computational cost. The AKD brings at most 0.7 AP gains on COCO 2017 for Object Detection and 1.83 mIoU gains on Cityscapes for Semantic Segmentation, respectively.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.69921875
        },
        {
            "paperId": "a083cfbca81967e20ce32f7991d98e4545e77328",
            "corpusId": 268543692,
            "title": "Guiding the Student\u2019s Learning Curve: Augmenting Knowledge Distillation with Insights from GradCAM",
            "venue": "2023 International Conference on Modeling, Simulation & Intelligent Computing (MoSICom)",
            "year": 2023,
            "referenceCount": 20,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/MoSICom59118.2023.10458758?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/MoSICom59118.2023.10458758, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293143380",
                    "name": "Suvaditya Mukherjee"
                },
                {
                    "authorId": "2292319438",
                    "name": "Dev Vinod Chandan"
                },
                {
                    "authorId": "2268069324",
                    "name": "Shreyas Dongre"
                }
            ],
            "abstract": "In the pursuit of model compression techniques, Knowledge Distillation emerges as an effective strategy for developing compact neural network models. This approach primarily revolves around the minimization of a loss function that quantifies the disparity between the Prediction Logits generated by a larger Teacher network and those produced by a smaller Student network. In our research, we propose an enhancement to this method by incorporating the GradCAM (Gradient-weighted Class Activation Mapping) technique from the field of Model Explainability. This augmentation involves leveraging GradCAM as an additional input to the Student network for improved learning. Notably, our findings reveal that this approach facilitates expedited convergence, particularly when the Teacher network exhibits strong performance and a substantial size advantage over the Student network.",
            "corpus_id": "268543692",
            "text": "In the pursuit of model compression techniques, Knowledge Distillation emerges as an effective strategy for developing compact neural network models. This approach primarily revolves around the minimization of a loss function that quantifies the disparity between the Prediction Logits generated by a larger Teacher network and those produced by a smaller Student network. In our research, we propose an enhancement to this method by incorporating the GradCAM (Gradient-weighted Class Activation Mapping) technique from the field of Model Explainability. This augmentation involves leveraging GradCAM as an additional input to the Student network for improved learning. Notably, our findings reveal that this approach facilitates expedited convergence, particularly when the Teacher network exhibits strong performance and a substantial size advantage over the Student network.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9306640625
        },
        {
            "paperId": "dfa0e785ae04e0b5851ded616f0021fc033b5337",
            "corpusId": 264131526,
            "title": "TSOSVNet: Teacher-student collaborative knowledge distillation for Online Signature Verification",
            "venue": "2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
            "year": 2023,
            "referenceCount": 46,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCVW60793.2023.00082?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCVW60793.2023.00082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276468147",
                    "name": "Chandra Sekhar V"
                },
                {
                    "authorId": "2156593228",
                    "name": "Avinash Gautam"
                },
                {
                    "authorId": "2276464658",
                    "name": "Viswanath P"
                },
                {
                    "authorId": "2276464951",
                    "name": "Sreeja Sr"
                },
                {
                    "authorId": "2276447367",
                    "name": "Rama Krishna Sai G"
                }
            ],
            "abstract": "Online signature verification (OSV) is a standardized personal authentication scheme with wide social acceptance in critical real-time applications include access control, m-commerce, etc. Even though the current advances in Deep learning (DL) technologies catalysed state-of-the-art frameworks for challenging domains like computer vision, speech recognition, etc., the DL-based frameworks are voluminous with huge trainable parameters and are hard to deploy in real-time systems demanding faster inference. To adopt DL into OSV for improved performance, we propose an OSV framework made up of teacher-student collaborative knowledge distillation (TSKD) technique. A heavy Transformer based teacher is trained first and the teacher knowledge is distilled into a very lightweight Convolutional Neural Network (CNN) based student. A well trained teacher network results in an efficient deep representative feature learning by the student and results in a performance improvement. In a thorough set of experiments with three popular and standard datasets, i.e., the MCYT-100, SUSIG, and SVC, TSOSVNet framework, with a CNN based student model requiring only 3266 trainable parameters results in an EER of 12.42% compared to the recent SOTA 13.38% by a model with 206277 parameters in skilled_01 category of MCYT-100 dataset. In comparison to cutting-edge CNN-based OSV models, the proposed TSOSVNet produced a state-of-the-art EER in the most of the test categories with an average of 90% lesser trainable parameters.",
            "corpus_id": "264131526",
            "text": "Online signature verification (OSV) is a standardized personal authentication scheme with wide social acceptance in critical real-time applications include access control, m-commerce, etc. Even though the current advances in Deep learning (DL) technologies catalysed state-of-the-art frameworks for challenging domains like computer vision, speech recognition, etc., the DL-based frameworks are voluminous with huge trainable parameters and are hard to deploy in real-time systems demanding faster inference. To adopt DL into OSV for improved performance, we propose an OSV framework made up of teacher-student collaborative knowledge distillation (TSKD) technique. A heavy Transformer based teacher is trained first and the teacher knowledge is distilled into a very lightweight Convolutional Neural Network (CNN) based student. A well trained teacher network results in an efficient deep representative feature learning by the student and results in a performance improvement. In a thorough set of experiments with three popular and standard datasets, i.e., the MCYT-100, SUSIG, and SVC, TSOSVNet framework, with a CNN based student model requiring only 3266 trainable parameters results in an EER of 12.42% compared to the recent SOTA 13.38% by a model with 206277 parameters in skilled_01 category of MCYT-100 dataset. In comparison to cutting-edge CNN-based OSV models, the proposed TSOSVNet produced a state-of-the-art EER in the most of the test categories with an average of 90% lesser trainable parameters.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.6279296875
        },
        {
            "paperId": "3b743866f26ea1ce9ab300842b1e69d1f62c9a4e",
            "corpusId": 277954831,
            "title": "Learning from Stochastic Teacher Representations Using Student-Guided Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 38,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.14307, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2057319769",
                    "name": "Muhammad Haseeb Aslam"
                },
                {
                    "authorId": "2356582101",
                    "name": "Clara Martinez"
                },
                {
                    "authorId": "3048367",
                    "name": "Marco Pedersoli"
                },
                {
                    "authorId": "2263129961",
                    "name": "A. Koerich"
                },
                {
                    "authorId": "2269469250",
                    "name": "Ali Etemad"
                },
                {
                    "authorId": "2256991724",
                    "name": "Eric Granger"
                }
            ],
            "abstract": "Advances in self-distillation have shown that when knowledge is distilled from a teacher to a student using the same deep learning (DL) architecture, the student performance can surpass the teacher particularly when the network is overparameterized and the teacher is trained with early stopping. Alternatively, ensemble learning also improves performance, although training, storing, and deploying multiple models becomes impractical as the number of models grows. Even distilling an ensemble to a single student model or weight averaging methods first requires training of multiple teacher models and does not fully leverage the inherent stochasticity for generating and distilling diversity in DL models. These constraints are particularly prohibitive in resource-constrained or latency-sensitive applications such as wearable devices. This paper proposes to train only one model and generate multiple diverse teacher representations using distillation-time dropout. However, generating these representations stochastically leads to noisy representations that are misaligned with the learned task. To overcome this problem, a novel stochastic self-distillation (SSD) training strategy is introduced for filtering and weighting teacher representation to distill from task-relevant representations only, using student-guided knowledge distillation (SGKD). The student representation at each distillation step is used as authority to guide the distillation process. Experimental results on real-world affective computing, wearable/biosignal datasets from the UCR Archive, the HAR dataset, and image classification datasets show that the proposed SSD method can outperform state-of-the-art methods without increasing the model size at both training and testing time, and incurs negligible computational complexity compared to state-of-the-art ensemble learning and weight averaging methods.",
            "corpus_id": "277954831",
            "text": "Advances in self-distillation have shown that when knowledge is distilled from a teacher to a student using the same deep learning (DL) architecture, the student performance can surpass the teacher particularly when the network is overparameterized and the teacher is trained with early stopping. Alternatively, ensemble learning also improves performance, although training, storing, and deploying multiple models becomes impractical as the number of models grows. Even distilling an ensemble to a single student model or weight averaging methods first requires training of multiple teacher models and does not fully leverage the inherent stochasticity for generating and distilling diversity in DL models. These constraints are particularly prohibitive in resource-constrained or latency-sensitive applications such as wearable devices. This paper proposes to train only one model and generate multiple diverse teacher representations using distillation-time dropout. However, generating these representations stochastically leads to noisy representations that are misaligned with the learned task. To overcome this problem, a novel stochastic self-distillation (SSD) training strategy is introduced for filtering and weighting teacher representation to distill from task-relevant representations only, using student-guided knowledge distillation (SGKD). The student representation at each distillation step is used as authority to guide the distillation process. Experimental results on real-world affective computing, wearable/biosignal datasets from the UCR Archive, the HAR dataset, and image classification datasets show that the proposed SSD method can outperform state-of-the-art methods without increasing the model size at both training and testing time, and incurs negligible computational complexity compared to state-of-the-art ensemble learning and weight averaging methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.88623046875
        },
        {
            "paperId": "655a2cbec97a455f00040b0b8d4b9682c1e304bc",
            "corpusId": 267751335,
            "title": "Knowledge Distillation Based on Transformed Teacher Matching",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "referenceCount": 47,
            "citationCount": 21,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11148, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2240165697",
                    "name": "Kaixiang Zheng"
                },
                {
                    "authorId": "2240092055",
                    "name": "En-Hui Yang"
                }
            ],
            "abstract": "As a technique to bridge logit matching and probability distribution matching, temperature scaling plays a pivotal role in knowledge distillation (KD). Conventionally, temperature scaling is applied to both teacher's logits and student's logits in KD. Motivated by some recent works, in this paper, we drop instead temperature scaling on the student side, and systematically study the resulting variant of KD, dubbed transformed teacher matching (TTM). By reinterpreting temperature scaling as a power transform of probability distribution, we show that in comparison with the original KD, TTM has an inherent R\\'enyi entropy term in its objective function, which serves as an extra regularization term. Extensive experiment results demonstrate that thanks to this inherent regularization, TTM leads to trained students with better generalization than the original KD. To further enhance student's capability to match teacher's power transformed probability distribution, we introduce a sample-adaptive weighting coefficient into TTM, yielding a novel distillation approach dubbed weighted TTM (WTTM). It is shown, by comprehensive experiments, that although WTTM is simple, it is effective, improves upon TTM, and achieves state-of-the-art accuracy performance. Our source code is available at https://github.com/zkxufo/TTM.",
            "corpus_id": "267751335",
            "text": "As a technique to bridge logit matching and probability distribution matching, temperature scaling plays a pivotal role in knowledge distillation (KD). Conventionally, temperature scaling is applied to both teacher's logits and student's logits in KD. Motivated by some recent works, in this paper, we drop instead temperature scaling on the student side, and systematically study the resulting variant of KD, dubbed transformed teacher matching (TTM). By reinterpreting temperature scaling as a power transform of probability distribution, we show that in comparison with the original KD, TTM has an inherent R\\'enyi entropy term in its objective function, which serves as an extra regularization term. Extensive experiment results demonstrate that thanks to this inherent regularization, TTM leads to trained students with better generalization than the original KD. To further enhance student's capability to match teacher's power transformed probability distribution, we introduce a sample-adaptive weighting coefficient into TTM, yielding a novel distillation approach dubbed weighted TTM (WTTM). It is shown, by comprehensive experiments, that although WTTM is simple, it is effective, improves upon TTM, and achieves state-of-the-art accuracy performance. Our source code is available at https://github.com/zkxufo/TTM.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.86572265625
        },
        {
            "paperId": "b75f6b23da4e0b92d861647f5e519f208029c059",
            "corpusId": 258019735,
            "title": "Knowledge Distillation With Feature Self Attention",
            "venue": "IEEE Access",
            "year": 2023,
            "referenceCount": 23,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10093872.pdf",
                "status": "CLOSED",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2023.3265382?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2023.3265382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2213834407",
                    "name": "Sin-Gu Park"
                },
                {
                    "authorId": "39732859",
                    "name": "D. Kang"
                }
            ],
            "abstract": "With the rapid development of deep learning technology, the size and performance of the network continuously grow, making network compression essential for commercial applications. In this paper, we propose a Feature Self Attention (FSA) module that extracts correlation information between the hidden features of a network and a new method for distilling the correlation features to compress the model. FSA does not require a special module or network to match features between the teacher model and the student model. By removing the multi-head structure and the repeated self-attention blocks in the existing self-attention mechanism, it minimizes the addition of parameters. Based on ResNet-18, 34, the added parameters are only 2.00M and the training speed is also the fastest in comparison to benchmark models. It was demonstrated through experiments that the use of interrelationship loss between features can be beneficial for training student models, indicating the importance of considering correlation information in deep neural network compression. And it was verified through training from scratch on the vanilla without the pre-trained weight of the student model.",
            "corpus_id": "258019735",
            "text": "With the rapid development of deep learning technology, the size and performance of the network continuously grow, making network compression essential for commercial applications. In this paper, we propose a Feature Self Attention (FSA) module that extracts correlation information between the hidden features of a network and a new method for distilling the correlation features to compress the model. FSA does not require a special module or network to match features between the teacher model and the student model. By removing the multi-head structure and the repeated self-attention blocks in the existing self-attention mechanism, it minimizes the addition of parameters. Based on ResNet-18, 34, the added parameters are only 2.00M and the training speed is also the fastest in comparison to benchmark models. It was demonstrated through experiments that the use of interrelationship loss between features can be beneficial for training student models, indicating the importance of considering correlation information in deep neural network compression. And it was verified through training from scratch on the vanilla without the pre-trained weight of the student model.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7470703125
        },
        {
            "paperId": "dbe4c41dd51e778aedeb3e734bf969931603f46c",
            "corpusId": 277043070,
            "title": "Creating a Good Teacher for Knowledge Distillation in Acoustic Scene Classification",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 25,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.11363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2103245223",
                    "name": "Tobias Morocutti"
                },
                {
                    "authorId": "2261390466",
                    "name": "Florian Schmid"
                },
                {
                    "authorId": "28921847",
                    "name": "Khaled Koutini"
                },
                {
                    "authorId": "2261390402",
                    "name": "Gerhard Widmer"
                }
            ],
            "abstract": "Knowledge Distillation (KD) is a widespread technique for compressing the knowledge of large models into more compact and efficient models. KD has proved to be highly effective in building well-performing low-complexity Acoustic Scene Classification (ASC) systems and was used in all the top-ranked submissions to this task of the annual DCASE challenge in the past three years. There is extensive research available on establishing the KD process, designing efficient student models, and forming well-performing teacher ensembles. However, less research has been conducted on investigating which teacher model attributes are beneficial for low-complexity students. In this work, we try to close this gap by studying the effects on the student's performance when using different teacher network architectures, varying the teacher model size, training them with different device generalization methods, and applying different ensembling strategies. The results show that teacher model sizes, device generalization methods, the ensembling strategy and the ensemble size are key factors for a well-performing student network.",
            "corpus_id": "277043070",
            "text": "Knowledge Distillation (KD) is a widespread technique for compressing the knowledge of large models into more compact and efficient models. KD has proved to be highly effective in building well-performing low-complexity Acoustic Scene Classification (ASC) systems and was used in all the top-ranked submissions to this task of the annual DCASE challenge in the past three years. There is extensive research available on establishing the KD process, designing efficient student models, and forming well-performing teacher ensembles. However, less research has been conducted on investigating which teacher model attributes are beneficial for low-complexity students. In this work, we try to close this gap by studying the effects on the student's performance when using different teacher network architectures, varying the teacher model size, training them with different device generalization methods, and applying different ensembling strategies. The results show that teacher model sizes, device generalization methods, the ensembling strategy and the ensemble size are key factors for a well-performing student network.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.92138671875
        }
    ],
    "quotes": {
        "cost": 0.19221,
        "quotes": [
            {
                "idx": 0,
                "key": "[245650327 | Boschini et al. | 2022 | Citations: 141]",
                "snippets": "Furlanello et al. (Furlanello et al., 2018) show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[4110009 | Furlanello et al. | 2018 | Citations: 1034]": "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction."
                },
                "metadata": [
                    {
                        "section_title": "Self-Distillation",
                        "pdf_hash": "",
                        "start": 357,
                        "end": 547,
                        "sentence_offsets": [
                            {
                                "start": 342,
                                "end": 547
                            }
                        ],
                        "ref_mentions": [
                            "4110009"
                        ],
                        "quote": "Furlanello et al. (Furlanello et al., 2018) show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[247446679 | Gong et al. | 2022 | Citations: 29]",
                "snippets": "2. For both directions, the student model matches or outperforms its teacher. Usually, in knowledge distillation, the student model gets closer to, but is still weaker than, its teacher model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Which model is a good teacher?",
                        "pdf_hash": "",
                        "start": 1986,
                        "end": 2178,
                        "sentence_offsets": [
                            {
                                "start": 1986,
                                "end": 2063
                            },
                            {
                                "start": 2064,
                                "end": 2178
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "2. For both directions, the student model matches or outperforms its teacher. Usually, in knowledge distillation, the student model gets closer to, but is still weaker than, its teacher model."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[249209742 | Ojha et al. | 2022 | Citations: 19]",
                "snippets": "(Cho et al., 2019) showed that performing knowledge distillation with a bigger teacher does not necessarily improve the student's performance over that with a smaller teacher, and thus raised questions about the effectiveness of the distillation procedure in such cases.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[203642130 | Cho et al. | 2019 | Citations: 619]": "In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don\u2019t make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher\u2019s training early. Our results generalize across datasets and models."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 162,
                        "end": 436,
                        "sentence_offsets": [
                            {
                                "start": 162,
                                "end": 436
                            }
                        ],
                        "ref_mentions": [
                            "203642130"
                        ],
                        "quote": "(Cho et al., 2019) showed that performing knowledge distillation with a bigger teacher does not necessarily improve the student's performance over that with a smaller teacher, and thus raised questions about the effectiveness of the distillation procedure in such cases."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[254564348 | Wu et al. | 2022 | Citations: 2]",
                "snippets": "Knowledge distillation transfers the knowledge learned by large models (i.e., teacher) to a compact one (i.e., student) via a weighted combination of original loss using ground-truth labels and distillation loss calculated with the soft output of a teacher model (Hinton et al., 2015)...SPEQ (Boo et al., 2020) introduces a novel self-training paradigm where the high-precision teacher and the low-bit-width student from the same full precision source model are trained collaboratively. In our training process, where different bit-width networks are enclosed, we design a self-knowledge transfer that utilizes a relatively higher precision network's output as soft labels for the cooperative training of the lower precision ones.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[222066738 | Boo et al. | 2020 | Citations: 30]": "The quantization of deep neural networks (QDNNs) has been actively studied for deployment in edge devices. Recent studies employ the knowledge distillation (KD) method to improve the performance of quantized networks. In this study, we propose stochastic precision ensemble training for QDNNs (SPEQ). SPEQ is a knowledge distillation training scheme; however, the teacher is formed by sharing the model parameters of the student network. We obtain the soft labels of the teacher by randomly changing the bit precision of the activation stochastically at each layer of the forward-pass computation. The student model is trained with these soft labels to reduce the activation quantization noise. The cosine similarity loss is employed, instead of the KL-divergence, for KD training. As the teacher model changes continuously by random bit-precision assignment, it exploits the effect of stochastic ensemble KD. SPEQ outperforms the existing quantization training methods in various tasks, such as image classification, question-answering, and transfer learning without the need for cumbersome teacher networks.",
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 267,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 268
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Knowledge distillation transfers the knowledge learned by large models (i.e., teacher) to a compact one (i.e., student) via a weighted combination of original loss using ground-truth labels and distillation loss calculated with the soft output of a teacher model (Hinton et al., 2015)"
                    },
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 623,
                        "end": 1053,
                        "sentence_offsets": [
                            {
                                "start": 623,
                                "end": 808
                            },
                            {
                                "start": 809,
                                "end": 1052
                            }
                        ],
                        "ref_mentions": [
                            "222066738"
                        ],
                        "quote": "SPEQ (Boo et al., 2020) introduces a novel self-training paradigm where the high-precision teacher and the low-bit-width student from the same full precision source model are trained collaboratively. In our training process, where different bit-width networks are enclosed, we design a self-knowledge transfer that utilizes a relatively higher precision network's output as soft labels for the cooperative training of the lower precision ones."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[255998274 | Team et al. | 2023 | Citations: 119]",
                "snippets": "Additionally, we find that even when the model size is the same for both student and teacher, we observe large gains from distillation, for a constant total frame budget (Figure 16). We speculate that this is due to bad representations learned early on by the student agent (Cetin et al., 2022;Nikishin et al., 2022), which can be avoided by using distillation. This is also consistent with findings in offline RL, where additional data is often required to effectively scale the model (Reid et al., 2022). The effect is largest for the first round of distillation, with diminishing returns in subsequent rounds of distillation (Figure E.5).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Distillation improves performance and enables scaling agents",
                        "pdf_hash": "",
                        "start": 590,
                        "end": 1231,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Additionally, we find that even when the model size is the same for both student and teacher, we observe large gains from distillation, for a constant total frame budget (Figure 16). We speculate that this is due to bad representations learned early on by the student agent (Cetin et al., 2022;Nikishin et al., 2022), which can be avoided by using distillation. This is also consistent with findings in offline RL, where additional data is often required to effectively scale the model (Reid et al., 2022). The effect is largest for the first round of distillation, with diminishing returns in subsequent rounds of distillation (Figure E.5)."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[258048760 | Gao | 2023 | Citations: 3]",
                "snippets": "When knowledge distillation is performed, increasing the model size of the teacher network instead makes the knowledge distillation worse. The results are as follows: the student network is a 2-layer CNN, while the teacher network is a 4-10-layer CNN, respectively, and it can be seen from the results that continuing to increase the depth of the teacher network does not necessarily improve the performance of knowledge distillation.\n\nTo further prove that the gap does affect knowledge distillation, the authors conducted a small comparison experiment by fixing the teacher network as a 10-layer CNN and changing the number of layers of the student network, and the results are as follows.\n\nBased on the above observations, it is not difficult to think that a medium-sized network (teacher assistant [16], TA) should be added between the large network and the small network, and the teacher network should be used to distill the TA first, and then the student network should be distilled by the TA. When TA was added, the distillation effect of the student network was significantly better than the direct distillation of the teacher network.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[212908749 | Mirzadeh et al. | 2019 | Citations: 1081]": "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1042,
                        "end": 2186,
                        "sentence_offsets": [
                            {
                                "start": 915,
                                "end": 1127
                            },
                            {
                                "start": 1130,
                                "end": 1404
                            },
                            {
                                "start": 1405,
                                "end": 1700
                            },
                            {
                                "start": 1703,
                                "end": 1958
                            },
                            {
                                "start": 1961,
                                "end": 2268
                            }
                        ],
                        "ref_mentions": [
                            "212908749"
                        ],
                        "quote": "When knowledge distillation is performed, increasing the model size of the teacher network instead makes the knowledge distillation worse. The results are as follows: the student network is a 2-layer CNN, while the teacher network is a 4-10-layer CNN, respectively, and it can be seen from the results that continuing to increase the depth of the teacher network does not necessarily improve the performance of knowledge distillation.\n\nTo further prove that the gap does affect knowledge distillation, the authors conducted a small comparison experiment by fixing the teacher network as a 10-layer CNN and changing the number of layers of the student network, and the results are as follows.\n\nBased on the above observations, it is not difficult to think that a medium-sized network (teacher assistant [16], TA) should be added between the large network and the small network, and the teacher network should be used to distill the TA first, and then the student network should be distilled by the TA. When TA was added, the distillation effect of the student network was significantly better than the direct distillation of the teacher network."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[258309753 | Lee et al. | 2023 | Citations: 2]",
                "snippets": "Self-Distillation (SD) is a special form of KD where the teacher and student networks have identical architecture. Born-Again Networks (BAN) [16] demonstrated that when training the student to match the output distribution of the teacher with the identical architecture, it could outperform the teacher. Furthermore, they showed that performing multiple rounds of BAN could further improve the performance where the trained student is set to be a new teacher in the following round. The ef-fectiveness of SD has been theoretically explained by the \"multi-view\" hypothesis introduced by Allen-Zhu and Li, who showed that self-distillation performs an implicit ensemble of various models [2]. Empirical evidence from Pham et al. [36] suggests that SD encourages the student to find flatter minima, leading to better generalization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 757,
                        "end": 1586,
                        "sentence_offsets": [
                            {
                                "start": 757,
                                "end": 871
                            },
                            {
                                "start": 872,
                                "end": 1060
                            },
                            {
                                "start": 1061,
                                "end": 1239
                            },
                            {
                                "start": 1240,
                                "end": 1447
                            },
                            {
                                "start": 1448,
                                "end": 1586
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Self-Distillation (SD) is a special form of KD where the teacher and student networks have identical architecture. Born-Again Networks (BAN) [16] demonstrated that when training the student to match the output distribution of the teacher with the identical architecture, it could outperform the teacher. Furthermore, they showed that performing multiple rounds of BAN could further improve the performance where the trained student is set to be a new teacher in the following round. The ef-fectiveness of SD has been theoretically explained by the \"multi-view\" hypothesis introduced by Allen-Zhu and Li, who showed that self-distillation performs an implicit ensemble of various models [2]. Empirical evidence from Pham et al. [36] suggests that SD encourages the student to find flatter minima, leading to better generalization."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[260378128 | Zhang et al. | 2023 | Citations: 27]",
                "snippets": "Traditional knowldistillation follows a two-stage training pipeline, where large teacher model is initially trained and then distilled to a lightweight student model...For two-stage knowledge distillation experiments, we explore the following two schemes. \n\nr Strong Teacher Scheme: distilling knowledge from a pre- trained Cascade Mask RCNN teacher with ResNetX101-DCN backbone, which achieves 47.3 AP. \n\nr Weak Teacher Scheme: distilling knowledge from a pre- trained teacher which has the identical architecture with the student (i.e., Faster RCNN with ResNet50 backbone) and achieves 38.4 AP. Experimental results are shown in Table X. It is observed that one-stage knowledge distillation leads to 2.3 AP improvements over the baseline, which is 0.2 higher than two-stage knowledge distillation with a weak teacher, but still 0.8 AP lower than the two-stage knowledge distillation with a strong teacher. This observation indicates that one-stage knowledge distillation can achieve comparable and even better performance than two-stage knowledge distillation when no powerful teacher is available. Moreover, two-stage knowledge distillation can make more use of the knowledge from a powerful teacher and achieve better performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "5) Two-Stage KD versus One-Stage KD:",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 165,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 166
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Traditional knowldistillation follows a two-stage training pipeline, where large teacher model is initially trained and then distilled to a lightweight student model"
                    },
                    {
                        "section_title": "5) Two-Stage KD versus One-Stage KD:",
                        "pdf_hash": "",
                        "start": 795,
                        "end": 1862,
                        "sentence_offsets": [
                            {
                                "start": 795,
                                "end": 882
                            },
                            {
                                "start": 885,
                                "end": 1030
                            },
                            {
                                "start": 1033,
                                "end": 1223
                            },
                            {
                                "start": 1224,
                                "end": 1266
                            },
                            {
                                "start": 1267,
                                "end": 1534
                            },
                            {
                                "start": 1535,
                                "end": 1727
                            },
                            {
                                "start": 1728,
                                "end": 1861
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For two-stage knowledge distillation experiments, we explore the following two schemes. \n\nr Strong Teacher Scheme: distilling knowledge from a pre- trained Cascade Mask RCNN teacher with ResNetX101-DCN backbone, which achieves 47.3 AP. \n\nr Weak Teacher Scheme: distilling knowledge from a pre- trained teacher which has the identical architecture with the student (i.e., Faster RCNN with ResNet50 backbone) and achieves 38.4 AP. Experimental results are shown in Table X. It is observed that one-stage knowledge distillation leads to 2.3 AP improvements over the baseline, which is 0.2 higher than two-stage knowledge distillation with a weak teacher, but still 0.8 AP lower than the two-stage knowledge distillation with a strong teacher. This observation indicates that one-stage knowledge distillation can achieve comparable and even better performance than two-stage knowledge distillation when no powerful teacher is available. Moreover, two-stage knowledge distillation can make more use of the knowledge from a powerful teacher and achieve better performance."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[262084420 | Capogrosso et al. | 2023 | Citations: 43]",
                "snippets": "Online distillation is an end-to-end approach where both the teacher and student models are updated simultaneously, making it suitable when the teacher model is not significantly larger or higher performing (Zhang et al., 2021). Finally, self-distillation is a special case of online distillation where the teacher and student networks have the same architecture (Yun et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[214727822 | Yun et al. | 2020 | Citations: 281]": "Deep neural networks with millions of parameters may suffer from poor generalization due to overfitting. To mitigate the issue, we propose a new regularization method that penalizes the predictive distribution between similar samples. In particular, we distill the predictive distribution between different samples of the same label during training. This results in regularizing the dark knowledge (i.e., the knowledge on wrong predictions) of a single network (i.e., a self-knowledge distillation) by forcing it to produce more meaningful and consistent predictions in a class-wise manner. Consequently, it mitigates overconfident predictions and reduces intra-class variations. Our experimental results on various image classification tasks demonstrate that the simple yet powerful method can significantly improve not only the generalization ability but also the calibration performance of modern convolutional neural networks."
                },
                "metadata": [
                    {
                        "section_title": "3) Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1705,
                        "end": 2057,
                        "sentence_offsets": [
                            {
                                "start": 1686,
                                "end": 1917
                            },
                            {
                                "start": 1918,
                                "end": 2057
                            }
                        ],
                        "ref_mentions": [
                            "224914013",
                            "214727822"
                        ],
                        "quote": "Online distillation is an end-to-end approach where both the teacher and student models are updated simultaneously, making it suitable when the teacher model is not significantly larger or higher performing (Zhang et al., 2021). Finally, self-distillation is a special case of online distillation where the teacher and student networks have the same architecture (Yun et al., 2020)."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[268543692 | Mukherjee et al. | 2023 | Citations: 0]",
                "snippets": "Notably, our findings reveal that this approach facilitates expedited convergence, particularly when the Teacher network exhibits strong performance and a substantial size advantage over the Student network.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Notably, our findings reveal that this approach facilitates expedited convergence, particularly when the Teacher network exhibits strong performance and a substantial size advantage over the Student network.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[270755154 | Li et al. | 2024 | Citations: 2]",
                "snippets": "Online distillation is an end-to-end training framework where both teacher and student models are updated simultaneously, particularly suitable when largecapacity, high-performance teacher models are unavailable (Chen et al., 2019). Self-distillation is a special form of online distillation, involving the transfer of knowledge within the same network from deeper to shallower layers or from early to later training stages, thus achieving knowledge transfer and training optimization (Zhang et al., 2019).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[159041406 | Zhang et al. | 2019 | Citations: 865]": "Convolutional neural networks have been widely deployed in various application scenarios. In order to extend the applications' boundaries to some accuracy-crucial domains, researchers have been investigating approaches to boost accuracy through either deeper or wider network structures, which brings with them the exponential increment of the computational and storage cost, delaying the responding time. In this paper, we propose a general training framework named self distillation, which notably enhances the performance (accuracy) of convolutional neural networks through shrinking the size of the network rather than aggrandizing it. Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. The networks are firstly divided into several sections. Then the knowledge in the deeper portion of the networks is squeezed into the shallow ones. Experiments further prove the generalization of the proposed self distillation framework: enhancement of accuracy at average level is 2.65%, varying from 0.61% in ResNeXt as minimum to 4.07% in VGG19 as maximum. In addition, it can also provide flexibility of depth-wise scalable inference on resource-limited edge devices. Our codes have been released on github.",
                    "[208526905 | Chen et al. | 2019 | Citations: 302]": "Distillation is an effective knowledge-transfer technique that uses predicted distributions of a powerful teacher model as soft targets to train a less-parameterized student model. A pre-trained high capacity teacher, however, is not always available. Recently proposed online variants use the aggregated intermediate predictions of multiple student models as targets to train each student model. Although group-derived targets give a good recipe for teacher-free distillation, group members are homogenized quickly with simple aggregation functions, leading to early saturated solutions. In this work, we propose Online Knowledge Distillation with Diverse peers (OKDDip), which performs two-level distillation during training with multiple auxiliary peers and one group leader. In the first-level distillation, each auxiliary peer holds an individual set of aggregation weights generated with an attention-based mechanism to derive its own targets from predictions of other auxiliary peers. Learning from distinct target distributions helps to boost peer diversity for effectiveness of group-based distillation. The second-level distillation is performed to transfer the knowledge in the ensemble of auxiliary peers further to the group leader, i.e., the model used for inference. Experimental results show that the proposed framework consistently gives better performance than state-of-the-art approaches without sacrificing training or inference complexity, demonstrating the effectiveness of the proposed two-level distillation framework."
                },
                "metadata": [
                    {
                        "section_title": "C. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1229,
                        "end": 1704,
                        "sentence_offsets": [
                            {
                                "start": 1229,
                                "end": 1446
                            },
                            {
                                "start": 1447,
                                "end": 1704
                            }
                        ],
                        "ref_mentions": [
                            "208526905",
                            "159041406"
                        ],
                        "quote": "Online distillation is an end-to-end training framework where both teacher and student models are updated simultaneously, particularly suitable when largecapacity, high-performance teacher models are unavailable (Chen et al., 2019). Self-distillation is a special form of online distillation, involving the transfer of knowledge within the same network from deeper to shallower layers or from early to later training stages, thus achieving knowledge transfer and training optimization (Zhang et al., 2019)."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[270870796 | Mai et al. | 2024 | Citations: 5]",
                "snippets": "Homomorphic KD means the student and teacher models have similar or identical structures.In this approach, the student model learns by mimicking the teacher model's outputs (e.g., logits, feature layer outputs).Common homomorphic KD methods include logit-level distillation, feature-level distillation, and module-level distillation.For instance, TinyViT (Wu et al., 2022) applies distillation during pre-training, storing logits from a large teacher model on hardware to achieve memory and computational efficiency when transferring knowledge to a smaller student Transformer.DeiT-Tiny (Wang et al., 2022) adopts patchlevel distillation, training a small student model to match the pre-trained teacher model's patch structure, then optimizing with decomposed manifold matching loss to reduce computational costs.Module-level methods like m2mKD [48] separate the teacher module from a pre-trained unified model, combining student modules with modular models, and using a shared meta-model for composition, enabling student modules to mimic teacher module behavior.Feature-level distillation methods like MiniViT (Zhang et al., 2022) combine weights from consecutive Transformer blocks for cross-layer weight sharing, introducing transformations to enhance learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247230104 | Wang et al. | 2022 | Citations: 44]": "Convolution neural networks (CNNs) have long been dominating the model choice in on-device intelligent mobile applications. Recently, we are witnessing the fast development of vision transformers, which are notable for the use of the self-attention mechanism, have demonstrated the superiority in accuracy over CNNs. However, vision transformers are with expensive computation costs, and their inference efficiency on resource-constrained mobile devices are still unclear by now. This brings a lot of uncertainty for on-device intelligence to benefit from the vision transformers. In this work, we carry out the first empirical study to investigate the possibility of efficiently deploying vision transformers on mobile devices. Our twofold study (i) profiles the representative vision transformers to understand the inference performance on commercial mobile devices and the behind reasons; and (ii) study multi-dimensional DNN acceleration approaches to achieve minimal latency. Results show that it is too expensive for vision transformer inference on mobile devices. Its inference is 1.58x-41x slower than CNNs. By removing the redundant Attention heads and FFN layers, DeiT-Tiny saves 23.2\\% latency with negligible 0.75\\% accuracy loss. Our study provides 7 insightful findings for future efficient vision transformer optimization and design.",
                    "[248177918 | Zhang et al. | 2022 | Citations: 124]": "Vision Transformer (ViT) models have recently drawn much attention in computer vision due to their high model capability. However, ViT models suffer from huge number of parameters, restricting their applicability on devices with limited memory. To alleviate this problem, we propose MiniViT, a new compression framework, which achieves parameter reduction in vision transformers while retaining the same performance. The central idea of MiniViT is to multiplex the weights of consecutive transformer blocks. More specifically, we make the weights shared across layers, while imposing a transformation on the weights to increase diversity. Weight distillation over self-attention is also applied to transfer knowledge from large-scale ViT models to weight-multiplexed compact models. Comprehensive experiments demonstrate the efficacy of MiniViT, showing that it can reduce the size of the pre-trained Swin-B transformer by 48%, while achieving an increase of 1.0% in Top-1 accuracy on ImageNet. Moreover, using a single-layer of parameters, MiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters, without seriously compromising the performance. Finally, we verify the transferability of MiniViT by reporting its performance on downstream benchmarks. Code and models are available at here.",
                    "[250920355 | Wu et al. | 2022 | Citations: 265]": "Vision transformer (ViT) recently has drawn great attention in computer vision due to its remarkable model capability. However, most prevailing ViT models suffer from huge number of parameters, restricting their applicability on devices with limited resources. To alleviate this issue, we propose TinyViT, a new family of tiny and efficient small vision transformers pretrained on large-scale datasets with our proposed fast distillation framework. The central idea is to transfer knowledge from large pretrained models to small ones, while enabling small models to get the dividends of massive pretraining data. More specifically, we apply distillation during pretraining for knowledge transfer. The logits of large teacher models are sparsified and stored in disk in advance to save the memory cost and computation overheads. The tiny student transformers are automatically scaled down from a large pretrained model with computation and parameter constraints. Comprehensive experiments demonstrate the efficacy of TinyViT. It achieves a top-1 accuracy of 84.8% on ImageNet-1k with only 21M parameters, being comparable to Swin-B pretrained on ImageNet-21k while using 4.2 times fewer parameters. Moreover, increasing image resolutions, TinyViT can reach 86.5% accuracy, being slightly better than Swin-L while using only 11% parameters. Last but not the least, we demonstrate a good transfer ability of TinyViT on various downstream tasks. Code and models are available at https://github.com/microsoft/Cream/tree/main/TinyViT."
                },
                "metadata": [
                    {
                        "section_title": "C. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 537,
                        "end": 1758,
                        "sentence_offsets": [
                            {
                                "start": 537,
                                "end": 626
                            },
                            {
                                "start": 626,
                                "end": 748
                            },
                            {
                                "start": 748,
                                "end": 870
                            },
                            {
                                "start": 870,
                                "end": 1101
                            },
                            {
                                "start": 1101,
                                "end": 1322
                            },
                            {
                                "start": 1322,
                                "end": 1573
                            },
                            {
                                "start": 1573,
                                "end": 1758
                            }
                        ],
                        "ref_mentions": [
                            "250920355",
                            "247230104",
                            "248177918"
                        ],
                        "quote": "Homomorphic KD means the student and teacher models have similar or identical structures.In this approach, the student model learns by mimicking the teacher model's outputs (e.g., logits, feature layer outputs).Common homomorphic KD methods include logit-level distillation, feature-level distillation, and module-level distillation.For instance, TinyViT (Wu et al., 2022) applies distillation during pre-training, storing logits from a large teacher model on hardware to achieve memory and computational efficiency when transferring knowledge to a smaller student Transformer.DeiT-Tiny (Wang et al., 2022) adopts patchlevel distillation, training a small student model to match the pre-trained teacher model's patch structure, then optimizing with decomposed manifold matching loss to reduce computational costs.Module-level methods like m2mKD [48] separate the teacher module from a pre-trained unified model, combining student modules with modular models, and using a shared meta-model for composition, enabling student modules to mimic teacher module behavior.Feature-level distillation methods like MiniViT (Zhang et al., 2022) combine weights from consecutive Transformer blocks for cross-layer weight sharing, introducing transformations to enhance learning."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[270878760 | Yang et al. | 2024 | Citations: 33]",
                "snippets": "Among the various methods, knowledge distillation has emerged as an effective technique to enhance inference speed without greatly compromising performance.\n\nExperimental results show that the shallow BiLSTM-based model achieves performance comparable to the ELMo language model [104], but with approximately 100 times fewer parameters and a 15-fold increase in inference speed.\n\nCompared to BERT, DistilBERT maintains 97% of the performance while reducing the number of parameters by 40%.\n\nPKD method outperformed standard knowledge distillation methods. It achieved superior performance and better generalization, significantly enhancing training efficiency and reducing storage requirements while maintaining accuracy comparable to the original large-scale model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Among the various methods, knowledge distillation has emerged as an effective technique to enhance inference speed without greatly compromising performance.\n\nExperimental results show that the shallow BiLSTM-based model achieves performance comparable to the ELMo language model [104], but with approximately 100 times fewer parameters and a 15-fold increase in inference speed.\n\nCompared to BERT, DistilBERT maintains 97% of the performance while reducing the number of parameters by 40%.\n\nPKD method outperformed standard knowledge distillation methods. It achieved superior performance and better generalization, significantly enhancing training efficiency and reducing storage requirements while maintaining accuracy comparable to the original large-scale model.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[271039191 | Pareek et al. | 2024 | Citations: 6]",
                "snippets": "Challenging this interpretation, Li et al. [19] and Furlanello et al. [9] empirically observed performance gains through distillation even when the teacher and student are same-sized models. One can set T and S to have the same architecture, and S trained with the objective in Eq. (1) outperforms T. This is referred to as Born-Again Networks (BANs) or Self-Distillation (SD). Furthermore, repeatedly applying self-distillation on the same training data with a student model having the same architecture provides additional gains on benchmark datasets and architectures [9,35]43]. At each step, the student from the previous step acts as the teacher used to train a new student model under the self-distillation loss of Eq. (1).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[14659675 | Li et al. | 2017 | Citations: 550]": "The ability of learning from noisy labels is very useful in many visual recognition tasks, as a vast amount of data with noisy labels are relatively easy to obtain. Traditionally, label noise has been treated as statistical outliers, and techniques such as importance re-weighting and bootstrapping have been proposed to alleviate the problem. According to our observation, the real-world noisy labels exhibit multimode characteristics as the true labels, rather than behaving like independent random outliers. In this work, we propose a unified distillation framework to use \u201cside\u201d information, including a small clean dataset and label relations in knowledge graph, to \u201chedge the risk\u201d of learning from noisy labels. Unlike the traditional approaches evaluated based on simulated label noises, we propose a suite of new benchmark datasets, in Sports, Species and Artifacts domains, to evaluate the task of learning from noisy labels in the practical setting. The empirical study demonstrates the effectiveness of our proposed method in all the domains.",
                    "[4110009 | Furlanello et al. | 2018 | Citations: 1034]": "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 946,
                        "end": 1675,
                        "sentence_offsets": [
                            {
                                "start": 825,
                                "end": 1034
                            },
                            {
                                "start": 1034,
                                "end": 1252
                            },
                            {
                                "start": 1254,
                                "end": 1444
                            },
                            {
                                "start": 1444,
                                "end": 1554
                            },
                            {
                                "start": 1554,
                                "end": 1630
                            },
                            {
                                "start": 1630,
                                "end": 1833
                            }
                        ],
                        "ref_mentions": [
                            "14659675",
                            "4110009"
                        ],
                        "quote": "Challenging this interpretation, Li et al. [19] and Furlanello et al. [9] empirically observed performance gains through distillation even when the teacher and student are same-sized models. One can set T and S to have the same architecture, and S trained with the objective in Eq. (1) outperforms T. This is referred to as Born-Again Networks (BANs) or Self-Distillation (SD). Furthermore, repeatedly applying self-distillation on the same training data with a student model having the same architecture provides additional gains on benchmark datasets and architectures [9,35]43]. At each step, the student from the previous step acts as the teacher used to train a new student model under the self-distillation loss of Eq. (1)."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[271270180 | Violos et al. | 2024 | Citations: 4]",
                "snippets": "When the student model's capacity is excessively large, the expected improvements in distillation efficacy may not materialize. Larger student models tend to exhibit slower learning rates and are more susceptible to overfitting, resulting in diminished generalization performance. Furthermore, the computational and memory resources required for training and deploying larger student models pose significant challenges, particularly in resource-constrained environments such as edge devices.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Student Model Capacity",
                        "pdf_hash": "",
                        "start": 399,
                        "end": 890,
                        "sentence_offsets": [
                            {
                                "start": 350,
                                "end": 463
                            },
                            {
                                "start": 465,
                                "end": 604
                            },
                            {
                                "start": 604,
                                "end": 756
                            },
                            {
                                "start": 756,
                                "end": 966
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "When the student model's capacity is excessively large, the expected improvements in distillation efficacy may not materialize. Larger student models tend to exhibit slower learning rates and are more susceptible to overfitting, resulting in diminished generalization performance. Furthermore, the computational and memory resources required for training and deploying larger student models pose significant challenges, particularly in resource-constrained environments such as edge devices."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[271956980 | Yu | 2024 | Citations: 1]",
                "snippets": "In addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled (Mirzadeh et al., 2019). We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[212908749 | Mirzadeh et al. | 2019 | Citations: 1081]": "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."
                },
                "metadata": [
                    {
                        "section_title": "B. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1248,
                        "end": 1647,
                        "sentence_offsets": [
                            {
                                "start": 1248,
                                "end": 1451
                            },
                            {
                                "start": 1452,
                                "end": 1564
                            },
                            {
                                "start": 1565,
                                "end": 1647
                            }
                        ],
                        "ref_mentions": [
                            "212908749"
                        ],
                        "quote": "In addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled (Mirzadeh et al., 2019). We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[272753230 | Ballout et al. | 2024 | Citations: 3]",
                "snippets": "Our research demonstrates an effective and straightforward approach for generating rationales from a large model to enhance a smaller model. However, a significant performance gap between the teacher model (the large model) and the student model (the small model) still exists. As illustrated in table 6, the flan-t5-3b, which serves as the teacher model, significantly outperforms the smaller t5-small model across all four datasets we tested.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Limitation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 444,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 140
                            },
                            {
                                "start": 141,
                                "end": 277
                            },
                            {
                                "start": 278,
                                "end": 444
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our research demonstrates an effective and straightforward approach for generating rationales from a large model to enhance a smaller model. However, a significant performance gap between the teacher model (the large model) and the student model (the small model) still exists. As illustrated in table 6, the flan-t5-3b, which serves as the teacher model, significantly outperforms the smaller t5-small model across all four datasets we tested."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[273662110 | Iordache et al. | 2024 | Citations: 1]",
                "snippets": "Our method allows student architectures to be identical to those of the individual teachers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1881,
                        "end": 1973,
                        "sentence_offsets": [
                            {
                                "start": 1881,
                                "end": 1973
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our method allows student architectures to be identical to those of the individual teachers."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[273963558 | Fuente et al. | 2024 | Citations: 0]",
                "snippets": "Knowledge distillation transfers knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student) (Lin et al., 2024)...Distillation schemes include offline distillation, where the teacher is trained first and then guides the student; online distillation, where both models train simultaneously; and self-distillation, where the teacher and student share the same architecture (Ray, 2021), (Gou et al., 2020). This method creates lightweight models that can maintain or even surpass the performance of larger models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[219559263 | Gou et al. | 2020 | Citations: 2984]": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.",
                    "[264109973 | Lin et al. | 2024 | Citations: 55]": "Tiny machine learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of applications and enable ubiquitous intelligence. However, TinyML is challenging due to the hardware constraints: the tiny memory resource is difficult hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and introduce tiny on-device training techniques. Finally, we present future directions in this area. Today\u2019s \u201clarge\u201d model might be tomorrow\u2019s \u201ctiny\u201d model. The scope of TinyML should evolve and adapt over time."
                },
                "metadata": [
                    {
                        "section_title": "3) Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 131,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 132
                            }
                        ],
                        "ref_mentions": [
                            "264109973"
                        ],
                        "quote": "Knowledge distillation transfers knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student) (Lin et al., 2024)"
                    },
                    {
                        "section_title": "3) Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 511,
                        "end": 887,
                        "sentence_offsets": [
                            {
                                "start": 511,
                                "end": 779
                            },
                            {
                                "start": 780,
                                "end": 886
                            }
                        ],
                        "ref_mentions": [
                            "248507674",
                            "219559263"
                        ],
                        "quote": "Distillation schemes include offline distillation, where the teacher is trained first and then guides the student; online distillation, where both models train simultaneously; and self-distillation, where the teacher and student share the same architecture (Ray, 2021), (Gou et al., 2020). This method creates lightweight models that can maintain or even surpass the performance of larger models."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[276776153 | Medina et al. | 2025 | Citations: 0]",
                "snippets": "However, one limitation of traditional KD is the performance drop when there is a large capacity gap between the teacher and student models because the student cannot effectively represent key features of the teacher. This limitation has been addressed by various methods (Passban et al., 2020)16,(Yim et al., 2017)(Zhang et al., 2019) that modify distillation algorithms or employ multi-stage learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[159041406 | Zhang et al. | 2019 | Citations: 865]": "Convolutional neural networks have been widely deployed in various application scenarios. In order to extend the applications' boundaries to some accuracy-crucial domains, researchers have been investigating approaches to boost accuracy through either deeper or wider network structures, which brings with them the exponential increment of the computational and storage cost, delaying the responding time. In this paper, we propose a general training framework named self distillation, which notably enhances the performance (accuracy) of convolutional neural networks through shrinking the size of the network rather than aggrandizing it. Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. The networks are firstly divided into several sections. Then the knowledge in the deeper portion of the networks is squeezed into the shallow ones. Experiments further prove the generalization of the proposed self distillation framework: enhancement of accuracy at average level is 2.65%, varying from 0.61% in ResNeXt as minimum to 4.07% in VGG19 as maximum. In addition, it can also provide flexibility of depth-wise scalable inference on resource-limited edge devices. Our codes have been released on github.",
                    "[229679667 | Passban et al. | 2020 | Citations: 122]": "Knowledge distillation is considered as a training and compression strategy in which two neural networks, namely a teacher and a student, are coupled together during training. The teacher network is supposed to be a trustworthy predictor and the student tries to mimic its predictions. Usually, a student with a lighter architecture is selected so we can achieve compression and yet deliver high-quality results. In such a setting, distillation only happens for final predictions whereas the student could also benefit from teacher\u2019s supervision for internal components.\n\nMotivated by this, we studied the problem of distillation for intermediate layers. Since there might not be a one-to-one alignment between student and teacher layers, existing techniques skip some teacher layers and only distill from a subset of them. This shortcoming directly impacts quality, so we instead propose a combinatorial technique which relies on attention. Our model fuses teacher-side information and takes each layer\u2019s significance into consideration, then it performs distillation between combined teacher layers and those of the student. Using our technique, we distilled a 12-layer BERT (Devlin et al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE tasks (Wang et al. 2018). Experimental results show that our combinatorial approach is able to outperform other existing techniques."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 467,
                        "end": 820,
                        "sentence_offsets": [
                            {
                                "start": 467,
                                "end": 684
                            },
                            {
                                "start": 685,
                                "end": 820
                            }
                        ],
                        "ref_mentions": [
                            "229679667",
                            "206596723",
                            "159041406"
                        ],
                        "quote": "However, one limitation of traditional KD is the performance drop when there is a large capacity gap between the teacher and student models because the student cannot effectively represent key features of the teacher. This limitation has been addressed by various methods (Passban et al., 2020)16,(Yim et al., 2017)(Zhang et al., 2019) that modify distillation algorithms or employ multi-stage learning."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[277217930 | Casey et al. | 2025 | Citations: 1]",
                "snippets": "Recent advances have demonstrated the importance of adaptive weighting in knowledge transfer, where confidence-based or similarity-based methods improve the effectiveness of model distillation [14,(Cho et al., 2019). This aligns with importance sampling techniques in transfer learning and domain adaptation, where distillation weights reflect the reliability of each teacher node [16,(Pan et al., 2010). Prior works [2]18] emphasize that knowledge transfer effectiveness depends on teacher-student alignment, motivating our approach of using Energy Coefficients for adaptive distillation weighting.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[203642130 | Cho et al. | 2019 | Citations: 619]": "In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don\u2019t make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher\u2019s training early. Our results generalize across datasets and models.",
                    "[740063 | Pan et al. | 2010 | Citations: 21010]": "A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation and Representative Datasets",
                        "pdf_hash": "",
                        "start": 626,
                        "end": 1195,
                        "sentence_offsets": [
                            {
                                "start": 626,
                                "end": 827
                            },
                            {
                                "start": 828,
                                "end": 1000
                            },
                            {
                                "start": 1001,
                                "end": 1195
                            }
                        ],
                        "ref_mentions": [
                            "203642130",
                            "740063"
                        ],
                        "quote": "Recent advances have demonstrated the importance of adaptive weighting in knowledge transfer, where confidence-based or similarity-based methods improve the effectiveness of model distillation [14,(Cho et al., 2019). This aligns with importance sampling techniques in transfer learning and domain adaptation, where distillation weights reflect the reliability of each teacher node [16,(Pan et al., 2010). Prior works [2]18] emphasize that knowledge transfer effectiveness depends on teacher-student alignment, motivating our approach of using Energy Coefficients for adaptive distillation weighting."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.023133,
        "cot": "For this query about size-matched knowledge distillation, I need to organize the quotes into clear dimensions that will help structure a comprehensive answer.\n\nFirst, I'll start with an introductory section that explains the basic concept of knowledge distillation and provides context for understanding the traditional vs. size-matched approaches. This will set the foundation for the more specific discussions that follow.\n\nThen, I'll organize the remaining content into these dimensions:\n\n1. Effectiveness of Size-Matched Distillation: This section should synthesize the research findings about knowledge distillation when teacher and student are similar or identical in size. Many quotes directly address the performance outcomes of such setups, so a synthesis format makes sense to present these findings cohesively.\n\n2. Performance Comparison with Traditional Distillation: This section should address how size-matched distillation compares to traditional setups with larger teacher models. A synthesis format will allow me to compare the approaches based on the research findings.\n\n3. Self-Distillation and Born-Again Networks: Several quotes specifically discuss self-distillation (where teacher and student have identical architecture) and Born-Again Networks. This appears to be an important subcategory of size-matched distillation that deserves its own section with a synthesis of the findings.\n\n4. Factors Affecting Distillation Effectiveness: Various quotes mention factors that influence the effectiveness of knowledge distillation, particularly related to model size relationships. A synthesis format will allow me to explain these relationships cohesively.\n\n5. Practical Applications and Implementations: This section should cover the practical applications and specific implementation approaches for size-matched distillation mentioned in the research. A list format makes sense here to clearly enumerate the different methods and techniques.\n\nFor each dimension, I'll identify which quotes provide supporting evidence.",
        "plan": {
            "Introduction to Knowledge Distillation (synthesis)": [
                3,
                12,
                18
            ],
            "Effectiveness of Size-Matched Distillation (synthesis)": [
                0,
                1,
                4,
                6,
                13,
                17
            ],
            "Performance Comparison with Traditional Distillation (synthesis)": [
                2,
                5,
                7,
                9,
                14,
                15,
                16,
                19
            ],
            "Self-Distillation and Online Distillation Approaches (synthesis)": [
                6,
                8,
                10,
                13
            ],
            "Factors Affecting Distillation Effectiveness (synthesis)": [
                5,
                9,
                14,
                15,
                19,
                20
            ],
            "Practical Applications and Implementation Methods (list)": [
                3,
                7,
                11,
                12,
                18
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Knowledge Distillation",
                "tldr": "Knowledge distillation is a technique that transfers knowledge from larger teacher models to smaller student models, enabling more efficient deployment while maintaining comparable performance. This approach has evolved to include various distillation schemes such as offline, online, and self-distillation methods. (7 sources)",
                "text": "\nKnowledge distillation is a powerful model compression technique that transfers the knowledge learned by large, complex models (teachers) to smaller, simpler models (students). Originally proposed by Hinton et al., this approach creates more efficient models that can be deployed in resource-constrained environments while maintaining competitive performance <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. The process typically involves training the student model using a weighted combination of the original loss from ground-truth labels and a distillation loss calculated from the teacher model's soft outputs <Paper corpusId=\"254564348\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper>.\n\nThe effectiveness of knowledge distillation is evident across various applications. For instance, DistilBERT maintains 97% of BERT's performance while reducing the parameter count by 40% <Paper corpusId=\"270878760\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. Similarly, shallow BiLSTM-based models have achieved performance comparable to the ELMo language model with approximately 100 times fewer parameters and 15 times faster inference <Paper corpusId=\"270878760\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. This makes knowledge distillation particularly valuable for deploying models on devices with limited resources, such as mobile phones and embedded devices <Paper corpusId=\"219559263\" paperTitle=\"(Gou et al., 2020)\" isShortName></Paper>.\n\nSeveral distillation schemes have emerged over time. Offline distillation follows the traditional approach where the teacher is trained first and then guides the student. Online distillation involves simultaneous training of both models, while self-distillation uses the same architecture for both teacher and student <Paper corpusId=\"273963558\" paperTitle=\"(Fuente et al., 2024)\" isShortName></Paper>. The SPEQ (Stochastic Precision Ensemble for Quantized DNNs) method exemplifies an innovative self-training paradigm where high-precision teachers and low-bit-width students derived from the same source model are trained collaboratively <Paper corpusId=\"254564348\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"222066738\" paperTitle=\"(Boo et al., 2020)\" isShortName></Paper>.\n\nKnowledge distillation has become especially important in the growing field of Tiny Machine Learning (TinyML), which aims to deploy AI models on microcontrollers and IoT devices with severe memory constraints <Paper corpusId=\"264109973\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>. Through system-algorithm co-design approaches, researchers continue to expand the capabilities of these compressed models, making them suitable for an increasingly diverse range of applications.",
                "citations": [
                    {
                        "id": "(Hinton et al., 2015)",
                        "snippets": [
                            "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                        ],
                        "paper": {
                            "corpus_id": 7200347,
                            "title": "Distilling the Knowledge in a Neural Network",
                            "authors": [
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "49959210",
                                    "name": "J. Dean"
                                }
                            ],
                            "year": 2015,
                            "venue": "arXiv.org",
                            "n_citations": 19742
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2022)",
                        "snippets": [
                            "Knowledge distillation transfers the knowledge learned by large models (i.e., teacher) to a compact one (i.e., student) via a weighted combination of original loss using ground-truth labels and distillation loss calculated with the soft output of a teacher model (Hinton et al., 2015)",
                            "SPEQ (Boo et al., 2020) introduces a novel self-training paradigm where the high-precision teacher and the low-bit-width student from the same full precision source model are trained collaboratively. In our training process, where different bit-width networks are enclosed, we design a self-knowledge transfer that utilizes a relatively higher precision network's output as soft labels for the cooperative training of the lower precision ones."
                        ],
                        "paper": {
                            "corpus_id": 254564348,
                            "title": "Vertical Layering of Quantized Neural Networks for Heterogeneous Inference",
                            "authors": [
                                {
                                    "authorId": "2187214993",
                                    "name": "Hai Wu"
                                },
                                {
                                    "authorId": "2053865518",
                                    "name": "Ruifei He"
                                },
                                {
                                    "authorId": "3218666",
                                    "name": "Hao Hao Tan"
                                },
                                {
                                    "authorId": "50844674",
                                    "name": "Xiaojuan Qi"
                                },
                                {
                                    "authorId": "2149775992",
                                    "name": "Kaibin Huang"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 2
                        },
                        "score": 0.92041015625
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Among the various methods, knowledge distillation has emerged as an effective technique to enhance inference speed without greatly compromising performance.\n\nExperimental results show that the shallow BiLSTM-based model achieves performance comparable to the ELMo language model [104], but with approximately 100 times fewer parameters and a 15-fold increase in inference speed.\n\nCompared to BERT, DistilBERT maintains 97% of the performance while reducing the number of parameters by 40%.\n\nPKD method outperformed standard knowledge distillation methods. It achieved superior performance and better generalization, significantly enhancing training efficiency and reducing storage requirements while maintaining accuracy comparable to the original large-scale model."
                        ],
                        "paper": {
                            "corpus_id": 270878760,
                            "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
                            "authors": [
                                {
                                    "authorId": "2309482604",
                                    "name": "Chuanpeng Yang"
                                },
                                {
                                    "authorId": "2290230390",
                                    "name": "Yao Zhu"
                                },
                                {
                                    "authorId": "2309429610",
                                    "name": "Wang Lu"
                                },
                                {
                                    "authorId": "2293893759",
                                    "name": "Yidong Wang"
                                },
                                {
                                    "authorId": "2276257486",
                                    "name": "Qian Chen"
                                },
                                {
                                    "authorId": "26155233",
                                    "name": "Chenlong Gao"
                                },
                                {
                                    "authorId": "2276089941",
                                    "name": "Bingjie Yan"
                                },
                                {
                                    "authorId": "2276178270",
                                    "name": "Yiqiang Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM Transactions on Intelligent Systems and Technology",
                            "n_citations": 33
                        },
                        "score": 0.9228515625
                    },
                    {
                        "id": "(Gou et al., 2020)",
                        "snippets": [
                            "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."
                        ],
                        "paper": {
                            "corpus_id": 219559263,
                            "title": "Knowledge Distillation: A Survey",
                            "authors": [
                                {
                                    "authorId": "38978232",
                                    "name": "Jianping Gou"
                                },
                                {
                                    "authorId": "2425630",
                                    "name": "B. Yu"
                                },
                                {
                                    "authorId": "144555237",
                                    "name": "S. Maybank"
                                },
                                {
                                    "authorId": "143719920",
                                    "name": "D. Tao"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Journal of Computer Vision",
                            "n_citations": 2984
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fuente et al., 2024)",
                        "snippets": [
                            "Knowledge distillation transfers knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student) (Lin et al., 2024)",
                            "Distillation schemes include offline distillation, where the teacher is trained first and then guides the student; online distillation, where both models train simultaneously; and self-distillation, where the teacher and student share the same architecture (Ray, 2021), (Gou et al., 2020). This method creates lightweight models that can maintain or even surpass the performance of larger models."
                        ],
                        "paper": {
                            "corpus_id": 273963558,
                            "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network",
                            "authors": [
                                {
                                    "authorId": "2330192141",
                                    "name": "Ra'ul de la Fuente"
                                },
                                {
                                    "authorId": "2047941908",
                                    "name": "Luciano Radrig\u00e1n"
                                },
                                {
                                    "authorId": "2330183718",
                                    "name": "Anibal S Morales"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.92041015625
                    },
                    {
                        "id": "(Boo et al., 2020)",
                        "snippets": [
                            "The quantization of deep neural networks (QDNNs) has been actively studied for deployment in edge devices. Recent studies employ the knowledge distillation (KD) method to improve the performance of quantized networks. In this study, we propose stochastic precision ensemble training for QDNNs (SPEQ). SPEQ is a knowledge distillation training scheme; however, the teacher is formed by sharing the model parameters of the student network. We obtain the soft labels of the teacher by randomly changing the bit precision of the activation stochastically at each layer of the forward-pass computation. The student model is trained with these soft labels to reduce the activation quantization noise. The cosine similarity loss is employed, instead of the KL-divergence, for KD training. As the teacher model changes continuously by random bit-precision assignment, it exploits the effect of stochastic ensemble KD. SPEQ outperforms the existing quantization training methods in various tasks, such as image classification, question-answering, and transfer learning without the need for cumbersome teacher networks."
                        ],
                        "paper": {
                            "corpus_id": 222066738,
                            "title": "Stochastic Precision Ensemble: Self-Knowledge Distillation for Quantized Deep Neural Networks",
                            "authors": [
                                {
                                    "authorId": "3350090",
                                    "name": "Yoonho Boo"
                                },
                                {
                                    "authorId": "1859825",
                                    "name": "Sungho Shin"
                                },
                                {
                                    "authorId": "2506452",
                                    "name": "Jungwook Choi"
                                },
                                {
                                    "authorId": "66936521",
                                    "name": "Wonyong Sung"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 30
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lin et al., 2024)",
                        "snippets": [
                            "Tiny machine learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of applications and enable ubiquitous intelligence. However, TinyML is challenging due to the hardware constraints: the tiny memory resource is difficult hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and introduce tiny on-device training techniques. Finally, we present future directions in this area. Today\u2019s \"large\" model might be tomorrow\u2019s \"tiny\" model. The scope of TinyML should evolve and adapt over time."
                        ],
                        "paper": {
                            "corpus_id": 264109973,
                            "title": "Tiny Machine Learning: Progress and Futures [Feature]",
                            "authors": [
                                {
                                    "authorId": "2258439127",
                                    "name": "Ji Lin"
                                },
                                {
                                    "authorId": "20515689",
                                    "name": "Ligeng Zhu"
                                },
                                {
                                    "authorId": "2164114675",
                                    "name": "Wei-Ming Chen"
                                },
                                {
                                    "authorId": "2258439498",
                                    "name": "Wei-Chen Wang"
                                },
                                {
                                    "authorId": "2258448604",
                                    "name": "Song Han"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Circuits and Systems Magazine",
                            "n_citations": 55
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Effectiveness of Size-Matched Distillation",
                "tldr": "Size-matched distillation, where teacher and student models have identical architectures, has shown surprising effectiveness with students often outperforming their teachers. This phenomenon, known as self-distillation or Born-Again Networks, demonstrates performance improvements that can be further enhanced through multiple rounds of distillation. (8 sources)",
                "text": "\nContrary to traditional knowledge distillation where smaller students aim to approach the performance of larger teachers, research reveals that size-matched distillation can yield unexpected benefits. Furlanello et al. introduced Born-Again Networks (BANs), demonstrating that multiple rounds of distillation between models with identical architectures can surprisingly improve the student's performance beyond that of the teacher <Paper corpusId=\"245650327\" paperTitle=\"(Boschini et al., 2022)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>. Their experiments with BANs based on DenseNets achieved state-of-the-art performance on CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, showing significant advantages from transferring knowledge between identically parameterized models <Paper corpusId=\"271039191\" paperTitle=\"(Pareek et al., 2024)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>.\n\nThis counterintuitive finding has been consistently observed across different domains. Gong et al. found that in size-matched distillation, the student model matches or outperforms its teacher, which differs from conventional distillation where students typically remain weaker than their teachers <Paper corpusId=\"247446679\" paperTitle=\"(Gong et al., 2022)\" isShortName></Paper>. Similarly, Team et al. observed large gains from distillation even when model sizes are identical for both student and teacher, with the most significant improvements occurring in the first round of distillation and diminishing returns in subsequent rounds <Paper corpusId=\"255998274\" paperTitle=\"(Team et al., 2023)\" isShortName></Paper>.\n\nSeveral theoretical explanations have been proposed for this phenomenon. The \"multi-view\" hypothesis suggests that self-distillation performs an implicit ensemble of various models <Paper corpusId=\"258309753\" paperTitle=\"(Lee et al., 2023)\" isShortName></Paper>. Empirical evidence also indicates that self-distillation encourages students to find flatter minima, leading to better generalization <Paper corpusId=\"258309753\" paperTitle=\"(Lee et al., 2023)\" isShortName></Paper>. Additionally, distillation may help avoid bad representations learned early by the student model, which is consistent with findings in offline reinforcement learning where additional data is often required to effectively scale models <Paper corpusId=\"255998274\" paperTitle=\"(Team et al., 2023)\" isShortName></Paper>.\n\nThe effectiveness of size-matched distillation extends to ensemble methods as well. Iordache et al. developed a technique that allows student architectures to be identical to those of individual teachers, enabling efficient knowledge transfer between same-sized models <Paper corpusId=\"273662110\" paperTitle=\"(Iordache et al., 2024)\" isShortName></Paper>. Furthermore, Li et al. and Furlanello et al. observed that repeatedly applying self-distillation on the same training data with identical architecture models provides additional performance gains on benchmark datasets <Paper corpusId=\"271039191\" paperTitle=\"(Pareek et al., 2024)\" isShortName></Paper> <Paper corpusId=\"14659675\" paperTitle=\"(Li et al., 2017)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Boschini et al., 2022)",
                        "snippets": [
                            "Furlanello et al. (Furlanello et al., 2018) show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student."
                        ],
                        "paper": {
                            "corpus_id": 245650327,
                            "title": "Class-Incremental Continual Learning Into the eXtended DER-Verse",
                            "authors": [
                                {
                                    "authorId": "51096265",
                                    "name": "Matteo Boschini"
                                },
                                {
                                    "authorId": "2123319338",
                                    "name": "Lorenzo Bonicelli"
                                },
                                {
                                    "authorId": "1429191945",
                                    "name": "Pietro Buzzega"
                                },
                                {
                                    "authorId": "51119730",
                                    "name": "Angelo Porrello"
                                },
                                {
                                    "authorId": "2175529",
                                    "name": "S. Calderara"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 141
                        },
                        "score": 0.92919921875
                    },
                    {
                        "id": "(Furlanello et al., 2018)",
                        "snippets": [
                            "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction."
                        ],
                        "paper": {
                            "corpus_id": 4110009,
                            "title": "Born Again Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2067208583",
                                    "name": "Tommaso Furlanello"
                                },
                                {
                                    "authorId": "32219137",
                                    "name": "Zachary Chase Lipton"
                                },
                                {
                                    "authorId": "143902495",
                                    "name": "Michael Tschannen"
                                },
                                {
                                    "authorId": "7326223",
                                    "name": "L. Itti"
                                },
                                {
                                    "authorId": "2047844",
                                    "name": "Anima Anandkumar"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1034
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pareek et al., 2024)",
                        "snippets": [
                            "Challenging this interpretation, Li et al. [19] and Furlanello et al. [9] empirically observed performance gains through distillation even when the teacher and student are same-sized models. One can set T and S to have the same architecture, and S trained with the objective in Eq. (1) outperforms T. This is referred to as Born-Again Networks (BANs) or Self-Distillation (SD). Furthermore, repeatedly applying self-distillation on the same training data with a student model having the same architecture provides additional gains on benchmark datasets and architectures [9,35]43]. At each step, the student from the previous step acts as the teacher used to train a new student model under the self-distillation loss of Eq. (1)."
                        ],
                        "paper": {
                            "corpus_id": 271039191,
                            "title": "Understanding the Gains from Repeated Self-Distillation",
                            "authors": [
                                {
                                    "authorId": "51176912",
                                    "name": "Divyansh Pareek"
                                },
                                {
                                    "authorId": "2330243848",
                                    "name": "S. S. Du"
                                },
                                {
                                    "authorId": "2310392746",
                                    "name": "Sewoong Oh"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6
                        },
                        "score": 0.9443359375
                    },
                    {
                        "id": "(Gong et al., 2022)",
                        "snippets": [
                            "2. For both directions, the student model matches or outperforms its teacher. Usually, in knowledge distillation, the student model gets closer to, but is still weaker than, its teacher model."
                        ],
                        "paper": {
                            "corpus_id": 247446679,
                            "title": "CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification",
                            "authors": [
                                {
                                    "authorId": "145802952",
                                    "name": "Yuan Gong"
                                },
                                {
                                    "authorId": "40570741",
                                    "name": "Sameer Khurana"
                                },
                                {
                                    "authorId": "2110769738",
                                    "name": "Andrew Rouditchenko"
                                },
                                {
                                    "authorId": "145898106",
                                    "name": "James R. Glass"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 29
                        },
                        "score": 0.927734375
                    },
                    {
                        "id": "(Team et al., 2023)",
                        "snippets": [
                            "Additionally, we find that even when the model size is the same for both student and teacher, we observe large gains from distillation, for a constant total frame budget (Figure 16). We speculate that this is due to bad representations learned early on by the student agent (Cetin et al., 2022;Nikishin et al., 2022), which can be avoided by using distillation. This is also consistent with findings in offline RL, where additional data is often required to effectively scale the model (Reid et al., 2022). The effect is largest for the first round of distillation, with diminishing returns in subsequent rounds of distillation (Figure E.5)."
                        ],
                        "paper": {
                            "corpus_id": 255998274,
                            "title": "Human-Timescale Adaptation in an Open-Ended Task Space",
                            "authors": [
                                {
                                    "authorId": "2202022003",
                                    "name": "Adaptive Agent Team"
                                },
                                {
                                    "authorId": "2067935448",
                                    "name": "Jakob Bauer"
                                },
                                {
                                    "authorId": "1734809439",
                                    "name": "Kate Baumli"
                                },
                                {
                                    "authorId": "70336016",
                                    "name": "Satinder Baveja"
                                },
                                {
                                    "authorId": "145124447",
                                    "name": "Feryal M. P. Behbahani"
                                },
                                {
                                    "authorId": "7567594",
                                    "name": "Avishkar Bhoopchand"
                                },
                                {
                                    "authorId": "1440707913",
                                    "name": "N. Bradley-Schmieg"
                                },
                                {
                                    "authorId": "47235561",
                                    "name": "Michael Chang"
                                },
                                {
                                    "authorId": "2201776471",
                                    "name": "Natalie Clay"
                                },
                                {
                                    "authorId": "69041729",
                                    "name": "Adrian Collister"
                                },
                                {
                                    "authorId": "2871171",
                                    "name": "Vibhavari Dasagi"
                                },
                                {
                                    "authorId": "2202047544",
                                    "name": "Lucy Gonzalez"
                                },
                                {
                                    "authorId": "144717963",
                                    "name": "Karol Gregor"
                                },
                                {
                                    "authorId": "37591038",
                                    "name": "Edward Hughes"
                                },
                                {
                                    "authorId": "80976942",
                                    "name": "Sheleem Kashem"
                                },
                                {
                                    "authorId": "2202022196",
                                    "name": "Maria Loks-Thompson"
                                },
                                {
                                    "authorId": "2085293302",
                                    "name": "Hannah Openshaw"
                                },
                                {
                                    "authorId": "1410302742",
                                    "name": "Jack Parker-Holder"
                                },
                                {
                                    "authorId": "1987123565",
                                    "name": "Shreyaan Pathak"
                                },
                                {
                                    "authorId": "1999879303",
                                    "name": "Nicolas Perez Nieves"
                                },
                                {
                                    "authorId": "3287144",
                                    "name": "Nemanja Rakicevic"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "3403061",
                                    "name": "Yannick Schroecker"
                                },
                                {
                                    "authorId": "3407592",
                                    "name": "Jakub Sygnowski"
                                },
                                {
                                    "authorId": "2274623",
                                    "name": "K. Tuyls"
                                },
                                {
                                    "authorId": "143981350",
                                    "name": "Sarah York"
                                },
                                {
                                    "authorId": "2156928090",
                                    "name": "Alexander Zacherl"
                                },
                                {
                                    "authorId": "2152836492",
                                    "name": "Lei M. Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 119
                        },
                        "score": 0.92041015625
                    },
                    {
                        "id": "(Lee et al., 2023)",
                        "snippets": [
                            "Self-Distillation (SD) is a special form of KD where the teacher and student networks have identical architecture. Born-Again Networks (BAN) [16] demonstrated that when training the student to match the output distribution of the teacher with the identical architecture, it could outperform the teacher. Furthermore, they showed that performing multiple rounds of BAN could further improve the performance where the trained student is set to be a new teacher in the following round. The ef-fectiveness of SD has been theoretically explained by the \"multi-view\" hypothesis introduced by Allen-Zhu and Li, who showed that self-distillation performs an implicit ensemble of various models [2]. Empirical evidence from Pham et al. [36] suggests that SD encourages the student to find flatter minima, leading to better generalization."
                        ],
                        "paper": {
                            "corpus_id": 258309753,
                            "title": "Bayesian Optimization Meets Self-Distillation",
                            "authors": [
                                {
                                    "authorId": "2279782391",
                                    "name": "HyunJae Lee"
                                },
                                {
                                    "authorId": "2157781156",
                                    "name": "Heon Song"
                                },
                                {
                                    "authorId": "2279825857",
                                    "name": "Hyeonsoo Lee"
                                },
                                {
                                    "authorId": "2215364195",
                                    "name": "Gi-hyeon Lee"
                                },
                                {
                                    "authorId": "2199808369",
                                    "name": "Suyeong Park"
                                },
                                {
                                    "authorId": "2283756",
                                    "name": "Donggeun Yoo"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 2
                        },
                        "score": 0.94921875
                    },
                    {
                        "id": "(Iordache et al., 2024)",
                        "snippets": [
                            "Our method allows student architectures to be identical to those of the individual teachers."
                        ],
                        "paper": {
                            "corpus_id": 273662110,
                            "title": "Multi-Level Feature Distillation of Joint Teachers Trained on Distinct Image Datasets",
                            "authors": [
                                {
                                    "authorId": "2328085900",
                                    "name": "Adrian Iordache"
                                },
                                {
                                    "authorId": "2365442",
                                    "name": "B. Alexe"
                                },
                                {
                                    "authorId": "2249763264",
                                    "name": "R. Ionescu"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                            "n_citations": 1
                        },
                        "score": 0.919921875
                    },
                    {
                        "id": "(Li et al., 2017)",
                        "snippets": [
                            "The ability of learning from noisy labels is very useful in many visual recognition tasks, as a vast amount of data with noisy labels are relatively easy to obtain. Traditionally, label noise has been treated as statistical outliers, and techniques such as importance re-weighting and bootstrapping have been proposed to alleviate the problem. According to our observation, the real-world noisy labels exhibit multimode characteristics as the true labels, rather than behaving like independent random outliers. In this work, we propose a unified distillation framework to use \"side\" information, including a small clean dataset and label relations in knowledge graph, to \"hedge the risk\" of learning from noisy labels. Unlike the traditional approaches evaluated based on simulated label noises, we propose a suite of new benchmark datasets, in Sports, Species and Artifacts domains, to evaluate the task of learning from noisy labels in the practical setting. The empirical study demonstrates the effectiveness of our proposed method in all the domains."
                        ],
                        "paper": {
                            "corpus_id": 14659675,
                            "title": "Learning from Noisy Labels with Distillation",
                            "authors": [
                                {
                                    "authorId": "2110470764",
                                    "name": "Yuncheng Li"
                                },
                                {
                                    "authorId": "1706007",
                                    "name": "Jianchao Yang"
                                },
                                {
                                    "authorId": "2317183",
                                    "name": "Yale Song"
                                },
                                {
                                    "authorId": "48749954",
                                    "name": "Liangliang Cao"
                                },
                                {
                                    "authorId": "33642939",
                                    "name": "Jiebo Luo"
                                },
                                {
                                    "authorId": "2040091191",
                                    "name": "Li-Jia Li"
                                }
                            ],
                            "year": 2017,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 550
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Performance Comparison with Traditional Distillation",
                "tldr": "Traditional distillation with larger teachers doesn't always outperform size-matched distillation due to capacity gaps that hinder knowledge transfer. Studies show that excessively large teachers can actually degrade student performance, leading to the development of multi-step approaches using intermediate-sized \"teacher assistants\" to bridge capacity gaps. (12 sources)",
                "text": "\nWhen comparing size-matched distillation to traditional approaches with significantly larger teacher models, researchers have identified several important trade-offs. Contrary to intuition, increasing the teacher model size doesn't always improve knowledge distillation effectiveness. Cho et al. demonstrated that more accurate, larger teachers don't necessarily make better teachers, raising fundamental questions about the distillation process in such scenarios <Paper corpusId=\"249209742\" paperTitle=\"(Ojha et al., 2022)\" isShortName></Paper> <Paper corpusId=\"203642130\" paperTitle=\"(Cho et al., 2019)\" isShortName></Paper>. This finding has been corroborated by multiple studies showing that student performance can actually degrade when the capacity gap between teacher and student becomes too large <Paper corpusId=\"258048760\" paperTitle=\"(Gao, 2023)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>.\n\nThe capacity gap phenomenon manifests in practical applications as well. Experiments with CNN architectures of varying depths revealed that continuing to increase the teacher network's depth beyond a certain point failed to improve distillation performance and sometimes led to worse results <Paper corpusId=\"258048760\" paperTitle=\"(Gao, 2023)\" isShortName></Paper>. Similarly, in natural language processing tasks, researchers observed significant performance gaps when distilling from very large models to much smaller ones <Paper corpusId=\"272753230\" paperTitle=\"(Ballout et al., 2024)\" isShortName></Paper>.\n\nTo address these limitations, researchers have developed several approaches. One effective strategy is multi-step knowledge distillation, which employs an intermediate-sized network (termed a \"teacher assistant\") to bridge the gap between large teachers and small students <Paper corpusId=\"258048760\" paperTitle=\"(Gao, 2023)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>. This approach has shown promising results across various datasets and model architectures. Alternative methods involve modifying distillation algorithms or employing multi-stage learning to better handle capacity disparities <Paper corpusId=\"276776153\" paperTitle=\"(Medina et al., 2025)\" isShortName></Paper> <Paper corpusId=\"159041406\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper> <Paper corpusId=\"229679667\" paperTitle=\"(Passban et al., 2020)\" isShortName></Paper>.\n\nStudent model capacity also affects distillation outcomes. When student models are excessively large relative to the task complexity, they may learn more slowly and become more susceptible to overfitting, resulting in diminished generalization performance <Paper corpusId=\"271270180\" paperTitle=\"(Violos et al., 2024)\" isShortName></Paper>. Conversely, knowledge distillation shows optimal results when teacher models achieve strong performance while maintaining an appropriate size advantage over student networks <Paper corpusId=\"268543692\" paperTitle=\"(Mukherjee et al., 2023)\" isShortName></Paper>. Some researchers have proposed building more powerful teacher models without changing the capacity (width and depth) of the student model to optimize performance transfer <Paper corpusId=\"271956980\" paperTitle=\"(Yu, 2024)\" isShortName></Paper>.\n\nIn object detection tasks, comparative studies between one-stage and two-stage knowledge distillation revealed that when no powerful teacher is available, one-stage approaches (similar to size-matched distillation) can achieve comparable or even better performance than two-stage distillation with weaker teachers. However, two-stage distillation with strong teachers still yields the best overall results, suggesting that traditional distillation maintains advantages when appropriate teacher-student pairings are selected <Paper corpusId=\"260378128\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Ojha et al., 2022)",
                        "snippets": [
                            "(Cho et al., 2019) showed that performing knowledge distillation with a bigger teacher does not necessarily improve the student's performance over that with a smaller teacher, and thus raised questions about the effectiveness of the distillation procedure in such cases."
                        ],
                        "paper": {
                            "corpus_id": 249209742,
                            "title": "What Knowledge Gets Distilled in Knowledge Distillation?",
                            "authors": [
                                {
                                    "authorId": "47284770",
                                    "name": "Utkarsh Ojha"
                                },
                                {
                                    "authorId": "1527091339",
                                    "name": "Yuheng Li"
                                },
                                {
                                    "authorId": "144756076",
                                    "name": "Yong Jae Lee"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 19
                        },
                        "score": 0.923828125
                    },
                    {
                        "id": "(Cho et al., 2019)",
                        "snippets": [
                            "In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don\u2019t make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher\u2019s training early. Our results generalize across datasets and models."
                        ],
                        "paper": {
                            "corpus_id": 203642130,
                            "title": "On the Efficacy of Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "39878282",
                                    "name": "Jang Hyun Cho"
                                },
                                {
                                    "authorId": "73710317",
                                    "name": "B. Hariharan"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 619
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gao, 2023)",
                        "snippets": [
                            "When knowledge distillation is performed, increasing the model size of the teacher network instead makes the knowledge distillation worse. The results are as follows: the student network is a 2-layer CNN, while the teacher network is a 4-10-layer CNN, respectively, and it can be seen from the results that continuing to increase the depth of the teacher network does not necessarily improve the performance of knowledge distillation.\n\nTo further prove that the gap does affect knowledge distillation, the authors conducted a small comparison experiment by fixing the teacher network as a 10-layer CNN and changing the number of layers of the student network, and the results are as follows.\n\nBased on the above observations, it is not difficult to think that a medium-sized network (teacher assistant [16], TA) should be added between the large network and the small network, and the teacher network should be used to distill the TA first, and then the student network should be distilled by the TA. When TA was added, the distillation effect of the student network was significantly better than the direct distillation of the teacher network."
                        ],
                        "paper": {
                            "corpus_id": 258048760,
                            "title": "A Survey on Recent Teacher-student Learning Studies",
                            "authors": [
                                {
                                    "authorId": "2147417425",
                                    "name": "Min Gao"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.92431640625
                    },
                    {
                        "id": "(Mirzadeh et al., 2019)",
                        "snippets": [
                            "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."
                        ],
                        "paper": {
                            "corpus_id": 212908749,
                            "title": "Improved Knowledge Distillation via Teacher Assistant",
                            "authors": [
                                {
                                    "authorId": "145156788",
                                    "name": "Seyed Iman Mirzadeh"
                                },
                                {
                                    "authorId": "1682124",
                                    "name": "Mehrdad Farajtabar"
                                },
                                {
                                    "authorId": "2112839418",
                                    "name": "Ang Li"
                                },
                                {
                                    "authorId": "153898744",
                                    "name": "Nir Levine"
                                },
                                {
                                    "authorId": "2063980545",
                                    "name": "Akihiro Matsukawa"
                                },
                                {
                                    "authorId": "144600887",
                                    "name": "H. Ghasemzadeh"
                                }
                            ],
                            "year": 2019,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 1081
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ballout et al., 2024)",
                        "snippets": [
                            "Our research demonstrates an effective and straightforward approach for generating rationales from a large model to enhance a smaller model. However, a significant performance gap between the teacher model (the large model) and the student model (the small model) still exists. As illustrated in table 6, the flan-t5-3b, which serves as the teacher model, significantly outperforms the smaller t5-small model across all four datasets we tested."
                        ],
                        "paper": {
                            "corpus_id": 272753230,
                            "title": "Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights",
                            "authors": [
                                {
                                    "authorId": "1491169373",
                                    "name": "Mohamad Ballout"
                                },
                                {
                                    "authorId": "1751765",
                                    "name": "U. Krumnack"
                                },
                                {
                                    "authorId": "2238205582",
                                    "name": "Gunther Heidemann"
                                },
                                {
                                    "authorId": "1743582",
                                    "name": "Kai-Uwe K\u00fchnberger"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Applications of Natural Language to Data Bases",
                            "n_citations": 3
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Medina et al., 2025)",
                        "snippets": [
                            "However, one limitation of traditional KD is the performance drop when there is a large capacity gap between the teacher and student models because the student cannot effectively represent key features of the teacher. This limitation has been addressed by various methods (Passban et al., 2020)16,(Yim et al., 2017)(Zhang et al., 2019) that modify distillation algorithms or employ multi-stage learning."
                        ],
                        "paper": {
                            "corpus_id": 276776153,
                            "title": "Mamba base PKD for efficient knowledge compression",
                            "authors": [
                                {
                                    "authorId": "2348480395",
                                    "name": "Jos\u00e9 Medina"
                                },
                                {
                                    "authorId": "3221342",
                                    "name": "Amnir Hadachi"
                                },
                                {
                                    "authorId": "2256850325",
                                    "name": "Paul Honeine"
                                },
                                {
                                    "authorId": "2314690317",
                                    "name": "Abdelaziz Bensrhair"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.916015625
                    },
                    {
                        "id": "(Zhang et al., 2019)",
                        "snippets": [
                            "Convolutional neural networks have been widely deployed in various application scenarios. In order to extend the applications' boundaries to some accuracy-crucial domains, researchers have been investigating approaches to boost accuracy through either deeper or wider network structures, which brings with them the exponential increment of the computational and storage cost, delaying the responding time. In this paper, we propose a general training framework named self distillation, which notably enhances the performance (accuracy) of convolutional neural networks through shrinking the size of the network rather than aggrandizing it. Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. The networks are firstly divided into several sections. Then the knowledge in the deeper portion of the networks is squeezed into the shallow ones. Experiments further prove the generalization of the proposed self distillation framework: enhancement of accuracy at average level is 2.65%, varying from 0.61% in ResNeXt as minimum to 4.07% in VGG19 as maximum. In addition, it can also provide flexibility of depth-wise scalable inference on resource-limited edge devices. Our codes have been released on github."
                        ],
                        "paper": {
                            "corpus_id": 159041406,
                            "title": "Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation",
                            "authors": [
                                {
                                    "authorId": "50081570",
                                    "name": "Linfeng Zhang"
                                },
                                {
                                    "authorId": "122151709",
                                    "name": "Jiebo Song"
                                },
                                {
                                    "authorId": "94970361",
                                    "name": "Anni Gao"
                                },
                                {
                                    "authorId": "2144141924",
                                    "name": "Jingwei Chen"
                                },
                                {
                                    "authorId": "2060106235",
                                    "name": "Chenglong Bao"
                                },
                                {
                                    "authorId": "2075321204",
                                    "name": "Kaisheng Ma"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 865
                        },
                        "score": 0
                    },
                    {
                        "id": "(Passban et al., 2020)",
                        "snippets": [
                            "Knowledge distillation is considered as a training and compression strategy in which two neural networks, namely a teacher and a student, are coupled together during training. The teacher network is supposed to be a trustworthy predictor and the student tries to mimic its predictions. Usually, a student with a lighter architecture is selected so we can achieve compression and yet deliver high-quality results. In such a setting, distillation only happens for final predictions whereas the student could also benefit from teacher\u2019s supervision for internal components.\n\nMotivated by this, we studied the problem of distillation for intermediate layers. Since there might not be a one-to-one alignment between student and teacher layers, existing techniques skip some teacher layers and only distill from a subset of them. This shortcoming directly impacts quality, so we instead propose a combinatorial technique which relies on attention. Our model fuses teacher-side information and takes each layer\u2019s significance into consideration, then it performs distillation between combined teacher layers and those of the student. Using our technique, we distilled a 12-layer BERT (Devlin et al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE tasks (Wang et al. 2018). Experimental results show that our combinatorial approach is able to outperform other existing techniques."
                        ],
                        "paper": {
                            "corpus_id": 229679667,
                            "title": "ALP-KD: Attention-Based Layer Projection for Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "5062230",
                                    "name": "Peyman Passban"
                                },
                                {
                                    "authorId": "2000863903",
                                    "name": "Yimeng Wu"
                                },
                                {
                                    "authorId": "1924511",
                                    "name": "Mehdi Rezagholizadeh"
                                },
                                {
                                    "authorId": "2115900360",
                                    "name": "Qun Liu"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 122
                        },
                        "score": 0
                    },
                    {
                        "id": "(Violos et al., 2024)",
                        "snippets": [
                            "When the student model's capacity is excessively large, the expected improvements in distillation efficacy may not materialize. Larger student models tend to exhibit slower learning rates and are more susceptible to overfitting, resulting in diminished generalization performance. Furthermore, the computational and memory resources required for training and deploying larger student models pose significant challenges, particularly in resource-constrained environments such as edge devices."
                        ],
                        "paper": {
                            "corpus_id": 271270180,
                            "title": "Towards Optimal Trade-Offs in Knowledge Distillation for CNNs and Vision Transformers at the Edge",
                            "authors": [
                                {
                                    "authorId": "2573061",
                                    "name": "John Violos"
                                },
                                {
                                    "authorId": "2276779989",
                                    "name": "Symeon Papadopoulos"
                                },
                                {
                                    "authorId": "1715604",
                                    "name": "Y. Kompatsiaris"
                                }
                            ],
                            "year": 2024,
                            "venue": "European Signal Processing Conference",
                            "n_citations": 4
                        },
                        "score": 0.91455078125
                    },
                    {
                        "id": "(Mukherjee et al., 2023)",
                        "snippets": [
                            "Notably, our findings reveal that this approach facilitates expedited convergence, particularly when the Teacher network exhibits strong performance and a substantial size advantage over the Student network."
                        ],
                        "paper": {
                            "corpus_id": 268543692,
                            "title": "Guiding the Student\u2019s Learning Curve: Augmenting Knowledge Distillation with Insights from GradCAM",
                            "authors": [
                                {
                                    "authorId": "2293143380",
                                    "name": "Suvaditya Mukherjee"
                                },
                                {
                                    "authorId": "2292319438",
                                    "name": "Dev Vinod Chandan"
                                },
                                {
                                    "authorId": "2268069324",
                                    "name": "Shreyas Dongre"
                                }
                            ],
                            "year": 2023,
                            "venue": "2023 International Conference on Modeling, Simulation & Intelligent Computing (MoSICom)",
                            "n_citations": 0
                        },
                        "score": 0.9306640625
                    },
                    {
                        "id": "(Yu, 2024)",
                        "snippets": [
                            "In addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled (Mirzadeh et al., 2019). We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student."
                        ],
                        "paper": {
                            "corpus_id": 271956980,
                            "title": "Bring the Power of Diffusion Model to Defect Detection",
                            "authors": [
                                {
                                    "authorId": "2317142609",
                                    "name": "Xuyi Yu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.927734375
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Traditional knowldistillation follows a two-stage training pipeline, where large teacher model is initially trained and then distilled to a lightweight student model",
                            "For two-stage knowledge distillation experiments, we explore the following two schemes. \n\nr Strong Teacher Scheme: distilling knowledge from a pre- trained Cascade Mask RCNN teacher with ResNetX101-DCN backbone, which achieves 47.3 AP. \n\nr Weak Teacher Scheme: distilling knowledge from a pre- trained teacher which has the identical architecture with the student (i.e., Faster RCNN with ResNet50 backbone) and achieves 38.4 AP. Experimental results are shown in Table X. It is observed that one-stage knowledge distillation leads to 2.3 AP improvements over the baseline, which is 0.2 higher than two-stage knowledge distillation with a weak teacher, but still 0.8 AP lower than the two-stage knowledge distillation with a strong teacher. This observation indicates that one-stage knowledge distillation can achieve comparable and even better performance than two-stage knowledge distillation when no powerful teacher is available. Moreover, two-stage knowledge distillation can make more use of the knowledge from a powerful teacher and achieve better performance."
                        ],
                        "paper": {
                            "corpus_id": 260378128,
                            "title": "Structured Knowledge Distillation for Accurate and Efficient Object Detection",
                            "authors": [
                                {
                                    "authorId": "50081570",
                                    "name": "Linfeng Zhang"
                                },
                                {
                                    "authorId": "2075321204",
                                    "name": "Kaisheng Ma"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 27
                        },
                        "score": 0.91552734375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Self-Distillation and Online Distillation Approaches",
                "tldr": "Self-distillation and online distillation represent important innovations where teacher and student models have identical architectures or are trained simultaneously. These approaches have shown remarkable effectiveness, often enabling students to outperform their identical teachers through mechanisms like knowledge transfer between network layers and collaborative training. (9 sources)",
                "text": "\nSelf-distillation is a specialized form of knowledge distillation where the teacher and student networks share identical architectures. Born-Again Networks (BANs) demonstrated that when a student is trained to match the output distribution of a teacher with the same architecture, it can surprisingly outperform the teacher <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>. This counter-intuitive finding has been consistently observed across multiple studies, with researchers noting that repeatedly applying self-distillation with identical architecture models provides additional performance gains on benchmark datasets <Paper corpusId=\"271039191\" paperTitle=\"(Pareek et al., 2024)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>.\n\nSeveral theoretical explanations have been proposed for the effectiveness of self-distillation. The \"multi-view\" hypothesis suggests that self-distillation performs an implicit ensemble of various models <Paper corpusId=\"258309753\" paperTitle=\"(Lee et al., 2023)\" isShortName></Paper>. Empirical evidence indicates that self-distillation encourages students to find flatter minima, leading to better generalization <Paper corpusId=\"258309753\" paperTitle=\"(Lee et al., 2023)\" isShortName></Paper>. Self-distillation can also operate within the same network by transferring knowledge from deeper to shallower layers, thus achieving knowledge transfer and training optimization <Paper corpusId=\"270755154\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"159041406\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>. This approach not only enhances accuracy (by an average of 2.65% across various architectures) but also provides flexibility for depth-wise scalable inference on resource-constrained devices <Paper corpusId=\"159041406\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>.\n\nOnline distillation represents another important advancement, where both teacher and student models are updated simultaneously during training <Paper corpusId=\"262084420\" paperTitle=\"(Capogrosso et al., 2023)\" isShortName></Paper>. This approach is particularly suitable when large-capacity, high-performance teacher models are unavailable <Paper corpusId=\"270755154\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"208526905\" paperTitle=\"(Chen et al., 2019)\" isShortName></Paper>. In online distillation, the aggregated intermediate predictions of multiple student models can serve as targets to train each student model, creating a collaborative learning environment <Paper corpusId=\"270755154\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nOne particularly effective variant of self-distillation regularizes the \"dark knowledge\" (knowledge about wrong predictions) of a network by forcing it to produce more meaningful and consistent predictions in a class-wise manner <Paper corpusId=\"262084420\" paperTitle=\"(Capogrosso et al., 2023)\" isShortName></Paper> <Paper corpusId=\"214727822\" paperTitle=\"(Yun et al., 2020)\" isShortName></Paper>. This approach mitigates overconfident predictions and reduces intra-class variations, significantly improving both generalization ability and calibration performance of modern convolutional neural networks <Paper corpusId=\"214727822\" paperTitle=\"(Yun et al., 2020)\" isShortName></Paper>.\n\nThe empirical success of self-distillation and online distillation challenges traditional interpretations of knowledge distillation. Li et al. and Furlanello et al. observed significant performance gains through distillation even when teacher and student models have identical architectures <Paper corpusId=\"271039191\" paperTitle=\"(Pareek et al., 2024)\" isShortName></Paper> <Paper corpusId=\"14659675\" paperTitle=\"(Li et al., 2017)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>. These findings suggest that distillation benefits extend beyond merely transferring knowledge from larger to smaller models, opening new possibilities for model improvement without increasing computational demands.",
                "citations": [
                    {
                        "id": "(Furlanello et al., 2018)",
                        "snippets": [
                            "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction."
                        ],
                        "paper": {
                            "corpus_id": 4110009,
                            "title": "Born Again Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2067208583",
                                    "name": "Tommaso Furlanello"
                                },
                                {
                                    "authorId": "32219137",
                                    "name": "Zachary Chase Lipton"
                                },
                                {
                                    "authorId": "143902495",
                                    "name": "Michael Tschannen"
                                },
                                {
                                    "authorId": "7326223",
                                    "name": "L. Itti"
                                },
                                {
                                    "authorId": "2047844",
                                    "name": "Anima Anandkumar"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1034
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pareek et al., 2024)",
                        "snippets": [
                            "Challenging this interpretation, Li et al. [19] and Furlanello et al. [9] empirically observed performance gains through distillation even when the teacher and student are same-sized models. One can set T and S to have the same architecture, and S trained with the objective in Eq. (1) outperforms T. This is referred to as Born-Again Networks (BANs) or Self-Distillation (SD). Furthermore, repeatedly applying self-distillation on the same training data with a student model having the same architecture provides additional gains on benchmark datasets and architectures [9,35]43]. At each step, the student from the previous step acts as the teacher used to train a new student model under the self-distillation loss of Eq. (1)."
                        ],
                        "paper": {
                            "corpus_id": 271039191,
                            "title": "Understanding the Gains from Repeated Self-Distillation",
                            "authors": [
                                {
                                    "authorId": "51176912",
                                    "name": "Divyansh Pareek"
                                },
                                {
                                    "authorId": "2330243848",
                                    "name": "S. S. Du"
                                },
                                {
                                    "authorId": "2310392746",
                                    "name": "Sewoong Oh"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6
                        },
                        "score": 0.9443359375
                    },
                    {
                        "id": "(Lee et al., 2023)",
                        "snippets": [
                            "Self-Distillation (SD) is a special form of KD where the teacher and student networks have identical architecture. Born-Again Networks (BAN) [16] demonstrated that when training the student to match the output distribution of the teacher with the identical architecture, it could outperform the teacher. Furthermore, they showed that performing multiple rounds of BAN could further improve the performance where the trained student is set to be a new teacher in the following round. The ef-fectiveness of SD has been theoretically explained by the \"multi-view\" hypothesis introduced by Allen-Zhu and Li, who showed that self-distillation performs an implicit ensemble of various models [2]. Empirical evidence from Pham et al. [36] suggests that SD encourages the student to find flatter minima, leading to better generalization."
                        ],
                        "paper": {
                            "corpus_id": 258309753,
                            "title": "Bayesian Optimization Meets Self-Distillation",
                            "authors": [
                                {
                                    "authorId": "2279782391",
                                    "name": "HyunJae Lee"
                                },
                                {
                                    "authorId": "2157781156",
                                    "name": "Heon Song"
                                },
                                {
                                    "authorId": "2279825857",
                                    "name": "Hyeonsoo Lee"
                                },
                                {
                                    "authorId": "2215364195",
                                    "name": "Gi-hyeon Lee"
                                },
                                {
                                    "authorId": "2199808369",
                                    "name": "Suyeong Park"
                                },
                                {
                                    "authorId": "2283756",
                                    "name": "Donggeun Yoo"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 2
                        },
                        "score": 0.94921875
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Online distillation is an end-to-end training framework where both teacher and student models are updated simultaneously, particularly suitable when largecapacity, high-performance teacher models are unavailable (Chen et al., 2019). Self-distillation is a special form of online distillation, involving the transfer of knowledge within the same network from deeper to shallower layers or from early to later training stages, thus achieving knowledge transfer and training optimization (Zhang et al., 2019)."
                        ],
                        "paper": {
                            "corpus_id": 270755154,
                            "title": "RSMoDM: Multimodal Momentum Distillation Model for Remote Sensing Visual Question Answering",
                            "authors": [
                                {
                                    "authorId": "2278383802",
                                    "name": "Pengfei Li"
                                },
                                {
                                    "authorId": "2192649954",
                                    "name": "Gang Liu"
                                },
                                {
                                    "authorId": "2221231272",
                                    "name": "Jinlong He"
                                },
                                {
                                    "authorId": "2299725033",
                                    "name": "Xiangxu Meng"
                                },
                                {
                                    "authorId": "2053866414",
                                    "name": "Shenjun Zhong"
                                },
                                {
                                    "authorId": "2308411177",
                                    "name": "Xun Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
                            "n_citations": 2
                        },
                        "score": 0.92578125
                    },
                    {
                        "id": "(Zhang et al., 2019)",
                        "snippets": [
                            "Convolutional neural networks have been widely deployed in various application scenarios. In order to extend the applications' boundaries to some accuracy-crucial domains, researchers have been investigating approaches to boost accuracy through either deeper or wider network structures, which brings with them the exponential increment of the computational and storage cost, delaying the responding time. In this paper, we propose a general training framework named self distillation, which notably enhances the performance (accuracy) of convolutional neural networks through shrinking the size of the network rather than aggrandizing it. Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. The networks are firstly divided into several sections. Then the knowledge in the deeper portion of the networks is squeezed into the shallow ones. Experiments further prove the generalization of the proposed self distillation framework: enhancement of accuracy at average level is 2.65%, varying from 0.61% in ResNeXt as minimum to 4.07% in VGG19 as maximum. In addition, it can also provide flexibility of depth-wise scalable inference on resource-limited edge devices. Our codes have been released on github."
                        ],
                        "paper": {
                            "corpus_id": 159041406,
                            "title": "Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation",
                            "authors": [
                                {
                                    "authorId": "50081570",
                                    "name": "Linfeng Zhang"
                                },
                                {
                                    "authorId": "122151709",
                                    "name": "Jiebo Song"
                                },
                                {
                                    "authorId": "94970361",
                                    "name": "Anni Gao"
                                },
                                {
                                    "authorId": "2144141924",
                                    "name": "Jingwei Chen"
                                },
                                {
                                    "authorId": "2060106235",
                                    "name": "Chenglong Bao"
                                },
                                {
                                    "authorId": "2075321204",
                                    "name": "Kaisheng Ma"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 865
                        },
                        "score": 0
                    },
                    {
                        "id": "(Capogrosso et al., 2023)",
                        "snippets": [
                            "Online distillation is an end-to-end approach where both the teacher and student models are updated simultaneously, making it suitable when the teacher model is not significantly larger or higher performing (Zhang et al., 2021). Finally, self-distillation is a special case of online distillation where the teacher and student networks have the same architecture (Yun et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 262084420,
                            "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning",
                            "authors": [
                                {
                                    "authorId": "2135267479",
                                    "name": "Luigi Capogrosso"
                                },
                                {
                                    "authorId": "1396330675",
                                    "name": "Federico Cunico"
                                },
                                {
                                    "authorId": "1780197",
                                    "name": "D. Cheng"
                                },
                                {
                                    "authorId": "2243336023",
                                    "name": "Franco Fummi"
                                },
                                {
                                    "authorId": "2238815087",
                                    "name": "Marco Cristani"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 43
                        },
                        "score": 0.9169921875
                    },
                    {
                        "id": "(Chen et al., 2019)",
                        "snippets": [
                            "Distillation is an effective knowledge-transfer technique that uses predicted distributions of a powerful teacher model as soft targets to train a less-parameterized student model. A pre-trained high capacity teacher, however, is not always available. Recently proposed online variants use the aggregated intermediate predictions of multiple student models as targets to train each student model. Although group-derived targets give a good recipe for teacher-free distillation, group members are homogenized quickly with simple aggregation functions, leading to early saturated solutions. In this work, we propose Online Knowledge Distillation with Diverse peers (OKDDip), which performs two-level distillation during training with multiple auxiliary peers and one group leader. In the first-level distillation, each auxiliary peer holds an individual set of aggregation weights generated with an attention-based mechanism to derive its own targets from predictions of other auxiliary peers. Learning from distinct target distributions helps to boost peer diversity for effectiveness of group-based distillation. The second-level distillation is performed to transfer the knowledge in the ensemble of auxiliary peers further to the group leader, i.e., the model used for inference. Experimental results show that the proposed framework consistently gives better performance than state-of-the-art approaches without sacrificing training or inference complexity, demonstrating the effectiveness of the proposed two-level distillation framework."
                        ],
                        "paper": {
                            "corpus_id": 208526905,
                            "title": "Online Knowledge Distillation with Diverse Peers",
                            "authors": [
                                {
                                    "authorId": "1684692",
                                    "name": "Defang Chen"
                                },
                                {
                                    "authorId": "1798852",
                                    "name": "Jian-Ping Mei"
                                },
                                {
                                    "authorId": "98262982",
                                    "name": "Can Wang"
                                },
                                {
                                    "authorId": "1692947908",
                                    "name": "Yan Feng"
                                },
                                {
                                    "authorId": "2588203",
                                    "name": "Chun Chen"
                                }
                            ],
                            "year": 2019,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 302
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yun et al., 2020)",
                        "snippets": [
                            "Deep neural networks with millions of parameters may suffer from poor generalization due to overfitting. To mitigate the issue, we propose a new regularization method that penalizes the predictive distribution between similar samples. In particular, we distill the predictive distribution between different samples of the same label during training. This results in regularizing the dark knowledge (i.e., the knowledge on wrong predictions) of a single network (i.e., a self-knowledge distillation) by forcing it to produce more meaningful and consistent predictions in a class-wise manner. Consequently, it mitigates overconfident predictions and reduces intra-class variations. Our experimental results on various image classification tasks demonstrate that the simple yet powerful method can significantly improve not only the generalization ability but also the calibration performance of modern convolutional neural networks."
                        ],
                        "paper": {
                            "corpus_id": 214727822,
                            "title": "Regularizing Class-Wise Predictions via Self-Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "66863443",
                                    "name": "Sukmin Yun"
                                },
                                {
                                    "authorId": "2109073979",
                                    "name": "Jongjin Park"
                                },
                                {
                                    "authorId": "3436470",
                                    "name": "Kimin Lee"
                                },
                                {
                                    "authorId": "143720148",
                                    "name": "Jinwoo Shin"
                                }
                            ],
                            "year": 2020,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 281
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2017)",
                        "snippets": [
                            "The ability of learning from noisy labels is very useful in many visual recognition tasks, as a vast amount of data with noisy labels are relatively easy to obtain. Traditionally, label noise has been treated as statistical outliers, and techniques such as importance re-weighting and bootstrapping have been proposed to alleviate the problem. According to our observation, the real-world noisy labels exhibit multimode characteristics as the true labels, rather than behaving like independent random outliers. In this work, we propose a unified distillation framework to use \"side\" information, including a small clean dataset and label relations in knowledge graph, to \"hedge the risk\" of learning from noisy labels. Unlike the traditional approaches evaluated based on simulated label noises, we propose a suite of new benchmark datasets, in Sports, Species and Artifacts domains, to evaluate the task of learning from noisy labels in the practical setting. The empirical study demonstrates the effectiveness of our proposed method in all the domains."
                        ],
                        "paper": {
                            "corpus_id": 14659675,
                            "title": "Learning from Noisy Labels with Distillation",
                            "authors": [
                                {
                                    "authorId": "2110470764",
                                    "name": "Yuncheng Li"
                                },
                                {
                                    "authorId": "1706007",
                                    "name": "Jianchao Yang"
                                },
                                {
                                    "authorId": "2317183",
                                    "name": "Yale Song"
                                },
                                {
                                    "authorId": "48749954",
                                    "name": "Liangliang Cao"
                                },
                                {
                                    "authorId": "33642939",
                                    "name": "Jiebo Luo"
                                },
                                {
                                    "authorId": "2040091191",
                                    "name": "Li-Jia Li"
                                }
                            ],
                            "year": 2017,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 550
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Factors Affecting Distillation Effectiveness",
                "tldr": "Several key factors influence knowledge distillation effectiveness, including the capacity gap between teacher and student models, training dynamics, and alignment strategies. Research shows that excessive disparities in model size can impede knowledge transfer, while adaptive weighting methods and early stopping techniques can enhance distillation performance. (11 sources)",
                "text": "\nThe effectiveness of knowledge distillation is significantly influenced by the capacity gap between teacher and student models. Contrary to intuition, increasing the teacher model size doesn't always improve distillation outcomes. Gao demonstrated that continuing to increase the depth of teacher networks (from 4 to 10 layers) while maintaining a fixed student architecture (2-layer CNN) failed to improve distillation performance and sometimes made it worse <Paper corpusId=\"258048760\" paperTitle=\"(Gao, 2023)\" isShortName></Paper>. This phenomenon occurs because an excessively large gap between teacher and student capacities hinders effective knowledge transfer <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>.\n\nStudent model capacity also plays a crucial role in distillation outcomes. When student models are too large relative to the task complexity, they tend to learn more slowly and become more susceptible to overfitting, resulting in diminished generalization performance <Paper corpusId=\"271270180\" paperTitle=\"(Violos et al., 2024)\" isShortName></Paper>. This challenge is particularly pronounced in resource-constrained environments such as edge devices, where computational and memory limitations further constrain model design.\n\nResearch suggests that optimal distillation occurs when the teacher exhibits strong performance while maintaining an appropriate size advantage over the student network <Paper corpusId=\"268543692\" paperTitle=\"(Mukherjee et al., 2023)\" isShortName></Paper>. This balanced approach facilitates expedited convergence during training while avoiding the negative effects of excessive capacity gaps. Some researchers have proposed building more powerful teacher models without changing the capacity (width and depth) of the student model to optimize performance transfer <Paper corpusId=\"271956980\" paperTitle=\"(Yu, 2024)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>.\n\nVarious strategies have been developed to address capacity gap challenges. Multi-step knowledge distillation employs intermediate-sized networks (\"teacher assistants\") to bridge the gap between large teachers and small students <Paper corpusId=\"258048760\" paperTitle=\"(Gao, 2023)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>. Other approaches modify distillation algorithms or implement multi-stage learning to improve knowledge transfer across capacity disparities <Paper corpusId=\"276776153\" paperTitle=\"(Medina et al., 2025)\" isShortName></Paper> <Paper corpusId=\"229679667\" paperTitle=\"(Passban et al., 2020)\" isShortName></Paper> <Paper corpusId=\"159041406\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>.\n\nAdaptive weighting methods have also proven effective in enhancing knowledge distillation. Confidence-based or similarity-based approaches improve the effectiveness of model distillation by dynamically adjusting the importance of different knowledge components <Paper corpusId=\"277217930\" paperTitle=\"(Casey et al., 2025)\" isShortName></Paper> <Paper corpusId=\"203642130\" paperTitle=\"(Cho et al., 2019)\" isShortName></Paper>. These techniques align with importance sampling methods in transfer learning, where distillation weights reflect the reliability of each teacher component <Paper corpusId=\"277217930\" paperTitle=\"(Casey et al., 2025)\" isShortName></Paper> <Paper corpusId=\"740063\" paperTitle=\"(Pan et al., 2010)\" isShortName></Paper>.\n\nTraining dynamics significantly impact distillation outcomes as well. Cho et al. demonstrated that stopping the teacher's training early can mitigate negative effects when distilling from larger teachers to smaller students <Paper corpusId=\"203642130\" paperTitle=\"(Cho et al., 2019)\" isShortName></Paper>. This finding suggests that teacher model optimization should be tailored specifically for distillation rather than pursuing maximum standalone performance.",
                "citations": [
                    {
                        "id": "(Gao, 2023)",
                        "snippets": [
                            "When knowledge distillation is performed, increasing the model size of the teacher network instead makes the knowledge distillation worse. The results are as follows: the student network is a 2-layer CNN, while the teacher network is a 4-10-layer CNN, respectively, and it can be seen from the results that continuing to increase the depth of the teacher network does not necessarily improve the performance of knowledge distillation.\n\nTo further prove that the gap does affect knowledge distillation, the authors conducted a small comparison experiment by fixing the teacher network as a 10-layer CNN and changing the number of layers of the student network, and the results are as follows.\n\nBased on the above observations, it is not difficult to think that a medium-sized network (teacher assistant [16], TA) should be added between the large network and the small network, and the teacher network should be used to distill the TA first, and then the student network should be distilled by the TA. When TA was added, the distillation effect of the student network was significantly better than the direct distillation of the teacher network."
                        ],
                        "paper": {
                            "corpus_id": 258048760,
                            "title": "A Survey on Recent Teacher-student Learning Studies",
                            "authors": [
                                {
                                    "authorId": "2147417425",
                                    "name": "Min Gao"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.92431640625
                    },
                    {
                        "id": "(Mirzadeh et al., 2019)",
                        "snippets": [
                            "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."
                        ],
                        "paper": {
                            "corpus_id": 212908749,
                            "title": "Improved Knowledge Distillation via Teacher Assistant",
                            "authors": [
                                {
                                    "authorId": "145156788",
                                    "name": "Seyed Iman Mirzadeh"
                                },
                                {
                                    "authorId": "1682124",
                                    "name": "Mehrdad Farajtabar"
                                },
                                {
                                    "authorId": "2112839418",
                                    "name": "Ang Li"
                                },
                                {
                                    "authorId": "153898744",
                                    "name": "Nir Levine"
                                },
                                {
                                    "authorId": "2063980545",
                                    "name": "Akihiro Matsukawa"
                                },
                                {
                                    "authorId": "144600887",
                                    "name": "H. Ghasemzadeh"
                                }
                            ],
                            "year": 2019,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 1081
                        },
                        "score": 0
                    },
                    {
                        "id": "(Violos et al., 2024)",
                        "snippets": [
                            "When the student model's capacity is excessively large, the expected improvements in distillation efficacy may not materialize. Larger student models tend to exhibit slower learning rates and are more susceptible to overfitting, resulting in diminished generalization performance. Furthermore, the computational and memory resources required for training and deploying larger student models pose significant challenges, particularly in resource-constrained environments such as edge devices."
                        ],
                        "paper": {
                            "corpus_id": 271270180,
                            "title": "Towards Optimal Trade-Offs in Knowledge Distillation for CNNs and Vision Transformers at the Edge",
                            "authors": [
                                {
                                    "authorId": "2573061",
                                    "name": "John Violos"
                                },
                                {
                                    "authorId": "2276779989",
                                    "name": "Symeon Papadopoulos"
                                },
                                {
                                    "authorId": "1715604",
                                    "name": "Y. Kompatsiaris"
                                }
                            ],
                            "year": 2024,
                            "venue": "European Signal Processing Conference",
                            "n_citations": 4
                        },
                        "score": 0.91455078125
                    },
                    {
                        "id": "(Mukherjee et al., 2023)",
                        "snippets": [
                            "Notably, our findings reveal that this approach facilitates expedited convergence, particularly when the Teacher network exhibits strong performance and a substantial size advantage over the Student network."
                        ],
                        "paper": {
                            "corpus_id": 268543692,
                            "title": "Guiding the Student\u2019s Learning Curve: Augmenting Knowledge Distillation with Insights from GradCAM",
                            "authors": [
                                {
                                    "authorId": "2293143380",
                                    "name": "Suvaditya Mukherjee"
                                },
                                {
                                    "authorId": "2292319438",
                                    "name": "Dev Vinod Chandan"
                                },
                                {
                                    "authorId": "2268069324",
                                    "name": "Shreyas Dongre"
                                }
                            ],
                            "year": 2023,
                            "venue": "2023 International Conference on Modeling, Simulation & Intelligent Computing (MoSICom)",
                            "n_citations": 0
                        },
                        "score": 0.9306640625
                    },
                    {
                        "id": "(Yu, 2024)",
                        "snippets": [
                            "In addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled (Mirzadeh et al., 2019). We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student."
                        ],
                        "paper": {
                            "corpus_id": 271956980,
                            "title": "Bring the Power of Diffusion Model to Defect Detection",
                            "authors": [
                                {
                                    "authorId": "2317142609",
                                    "name": "Xuyi Yu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.927734375
                    },
                    {
                        "id": "(Medina et al., 2025)",
                        "snippets": [
                            "However, one limitation of traditional KD is the performance drop when there is a large capacity gap between the teacher and student models because the student cannot effectively represent key features of the teacher. This limitation has been addressed by various methods (Passban et al., 2020)16,(Yim et al., 2017)(Zhang et al., 2019) that modify distillation algorithms or employ multi-stage learning."
                        ],
                        "paper": {
                            "corpus_id": 276776153,
                            "title": "Mamba base PKD for efficient knowledge compression",
                            "authors": [
                                {
                                    "authorId": "2348480395",
                                    "name": "Jos\u00e9 Medina"
                                },
                                {
                                    "authorId": "3221342",
                                    "name": "Amnir Hadachi"
                                },
                                {
                                    "authorId": "2256850325",
                                    "name": "Paul Honeine"
                                },
                                {
                                    "authorId": "2314690317",
                                    "name": "Abdelaziz Bensrhair"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.916015625
                    },
                    {
                        "id": "(Passban et al., 2020)",
                        "snippets": [
                            "Knowledge distillation is considered as a training and compression strategy in which two neural networks, namely a teacher and a student, are coupled together during training. The teacher network is supposed to be a trustworthy predictor and the student tries to mimic its predictions. Usually, a student with a lighter architecture is selected so we can achieve compression and yet deliver high-quality results. In such a setting, distillation only happens for final predictions whereas the student could also benefit from teacher\u2019s supervision for internal components.\n\nMotivated by this, we studied the problem of distillation for intermediate layers. Since there might not be a one-to-one alignment between student and teacher layers, existing techniques skip some teacher layers and only distill from a subset of them. This shortcoming directly impacts quality, so we instead propose a combinatorial technique which relies on attention. Our model fuses teacher-side information and takes each layer\u2019s significance into consideration, then it performs distillation between combined teacher layers and those of the student. Using our technique, we distilled a 12-layer BERT (Devlin et al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE tasks (Wang et al. 2018). Experimental results show that our combinatorial approach is able to outperform other existing techniques."
                        ],
                        "paper": {
                            "corpus_id": 229679667,
                            "title": "ALP-KD: Attention-Based Layer Projection for Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "5062230",
                                    "name": "Peyman Passban"
                                },
                                {
                                    "authorId": "2000863903",
                                    "name": "Yimeng Wu"
                                },
                                {
                                    "authorId": "1924511",
                                    "name": "Mehdi Rezagholizadeh"
                                },
                                {
                                    "authorId": "2115900360",
                                    "name": "Qun Liu"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 122
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2019)",
                        "snippets": [
                            "Convolutional neural networks have been widely deployed in various application scenarios. In order to extend the applications' boundaries to some accuracy-crucial domains, researchers have been investigating approaches to boost accuracy through either deeper or wider network structures, which brings with them the exponential increment of the computational and storage cost, delaying the responding time. In this paper, we propose a general training framework named self distillation, which notably enhances the performance (accuracy) of convolutional neural networks through shrinking the size of the network rather than aggrandizing it. Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. The networks are firstly divided into several sections. Then the knowledge in the deeper portion of the networks is squeezed into the shallow ones. Experiments further prove the generalization of the proposed self distillation framework: enhancement of accuracy at average level is 2.65%, varying from 0.61% in ResNeXt as minimum to 4.07% in VGG19 as maximum. In addition, it can also provide flexibility of depth-wise scalable inference on resource-limited edge devices. Our codes have been released on github."
                        ],
                        "paper": {
                            "corpus_id": 159041406,
                            "title": "Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation",
                            "authors": [
                                {
                                    "authorId": "50081570",
                                    "name": "Linfeng Zhang"
                                },
                                {
                                    "authorId": "122151709",
                                    "name": "Jiebo Song"
                                },
                                {
                                    "authorId": "94970361",
                                    "name": "Anni Gao"
                                },
                                {
                                    "authorId": "2144141924",
                                    "name": "Jingwei Chen"
                                },
                                {
                                    "authorId": "2060106235",
                                    "name": "Chenglong Bao"
                                },
                                {
                                    "authorId": "2075321204",
                                    "name": "Kaisheng Ma"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 865
                        },
                        "score": 0
                    },
                    {
                        "id": "(Casey et al., 2025)",
                        "snippets": [
                            "Recent advances have demonstrated the importance of adaptive weighting in knowledge transfer, where confidence-based or similarity-based methods improve the effectiveness of model distillation [14,(Cho et al., 2019). This aligns with importance sampling techniques in transfer learning and domain adaptation, where distillation weights reflect the reliability of each teacher node [16,(Pan et al., 2010). Prior works [2]18] emphasize that knowledge transfer effectiveness depends on teacher-student alignment, motivating our approach of using Energy Coefficients for adaptive distillation weighting."
                        ],
                        "paper": {
                            "corpus_id": 277217930,
                            "title": "Distributed Collaborative Learning with Representative Knowledge Sharing",
                            "authors": [
                                {
                                    "authorId": "2342413409",
                                    "name": "Joseph Casey"
                                },
                                {
                                    "authorId": "2351733702",
                                    "name": "Qianjiao Chen"
                                },
                                {
                                    "authorId": "2300177011",
                                    "name": "Mengchen Fan"
                                },
                                {
                                    "authorId": "2285910606",
                                    "name": "Baocheng Geng"
                                },
                                {
                                    "authorId": "2342412176",
                                    "name": "Roman Shterenberg"
                                },
                                {
                                    "authorId": "2344089016",
                                    "name": "Zhong Chen"
                                },
                                {
                                    "authorId": "2340963079",
                                    "name": "Keren Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "Mathematics",
                            "n_citations": 1
                        },
                        "score": 0.9248046875
                    },
                    {
                        "id": "(Cho et al., 2019)",
                        "snippets": [
                            "In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don\u2019t make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher\u2019s training early. Our results generalize across datasets and models."
                        ],
                        "paper": {
                            "corpus_id": 203642130,
                            "title": "On the Efficacy of Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "39878282",
                                    "name": "Jang Hyun Cho"
                                },
                                {
                                    "authorId": "73710317",
                                    "name": "B. Hariharan"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 619
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pan et al., 2010)",
                        "snippets": [
                            "A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research."
                        ],
                        "paper": {
                            "corpus_id": 740063,
                            "title": "A Survey on Transfer Learning",
                            "authors": [
                                {
                                    "authorId": "1746914",
                                    "name": "Sinno Jialin Pan"
                                },
                                {
                                    "authorId": "152290618",
                                    "name": "Qiang Yang"
                                }
                            ],
                            "year": 2010,
                            "venue": "IEEE Transactions on Knowledge and Data Engineering",
                            "n_citations": 21010
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Practical Applications and Implementation Methods",
                "tldr": "Knowledge distillation has been successfully implemented across various domains, from computer vision to natural language processing, using multiple approaches including homomorphic, feature-level, and module-level distillation techniques. These methods have enabled the deployment of efficient models on resource-constrained devices while maintaining competitive performance. (12 sources)",
                "text": "\n- **Logit-level distillation**: The most common implementation method transfers knowledge using soft outputs from teacher models. This approach combines original loss (using ground-truth labels) with distillation loss (calculated from teacher's soft outputs), enabling effective knowledge transfer between models <Paper corpusId=\"254564348\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\n- **Feature-level distillation**: Methods like MiniViT transfer knowledge at the feature level by combining weights from consecutive Transformer blocks for cross-layer weight sharing. This approach introduces transformations to enhance learning while reducing model size <Paper corpusId=\"270870796\" paperTitle=\"(Mai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"248177918\" paperTitle=\"(Zhang et al., 2022)\" isShortName></Paper>.\n\n- **Module-level distillation**: Techniques like m2mKD separate teacher modules from pre-trained unified models and combine student modules with modular models. This approach uses a shared meta-model for composition, enabling student modules to effectively mimic teacher module behavior <Paper corpusId=\"270870796\" paperTitle=\"(Mai et al., 2024)\" isShortName></Paper>.\n\n- **Patch-level distillation**: DeiT-Tiny employs patch-level distillation where a small student model is trained to match the pre-trained teacher model's patch structure. This method optimizes with decomposed manifold matching loss to reduce computational costs <Paper corpusId=\"270870796\" paperTitle=\"(Mai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247230104\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\n- **One-stage vs. two-stage distillation**: In object detection, one-stage knowledge distillation can achieve comparable or better performance than two-stage distillation when powerful teachers aren't available. However, two-stage distillation with strong teachers generally yields the best overall performance <Paper corpusId=\"260378128\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\n- **Self-training paradigms**: SPEQ introduces a novel approach where high-precision teachers and low-bit-width students derived from the same full precision source model are trained collaboratively. This method utilizes a relatively higher precision network's output as soft labels for training lower precision ones <Paper corpusId=\"254564348\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"222066738\" paperTitle=\"(Boo et al., 2020)\" isShortName></Paper>.\n\n- **Distillation for Transformers**: TinyViT applies distillation during pre-training, storing logits from large teacher models on hardware to achieve memory and computational efficiency when transferring knowledge to smaller student Transformers <Paper corpusId=\"270870796\" paperTitle=\"(Mai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"250920355\" paperTitle=\"(Wu et al._1, 2022)\" isShortName></Paper>.\n\n- **Natural language processing applications**: Shallow BiLSTM-based models have achieved performance comparable to the ELMo language model with approximately 100 times fewer parameters and 15 times faster inference. Similarly, DistilBERT maintains 97% of BERT's performance while reducing parameter count by 40% <Paper corpusId=\"270878760\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\n- **Tiny Machine Learning**: Knowledge distillation has become essential for deploying AI on microcontrollers and IoT devices with severe memory constraints. System-algorithm co-design approaches enable squeezing deep learning models into billions of resource-constrained devices <Paper corpusId=\"273963558\" paperTitle=\"(Fuente et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264109973\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>.\n\n- **Distillation schemes**: Implementation approaches include offline distillation (teacher trained first, then guides student), online distillation (both models train simultaneously), and self-distillation (teacher and student share architecture). These methods create lightweight models that can maintain or even surpass larger models' performance <Paper corpusId=\"273963558\" paperTitle=\"(Fuente et al., 2024)\" isShortName></Paper> <Paper corpusId=\"219559263\" paperTitle=\"(Gou et al., 2020)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Wu et al., 2022)",
                        "snippets": [
                            "Knowledge distillation transfers the knowledge learned by large models (i.e., teacher) to a compact one (i.e., student) via a weighted combination of original loss using ground-truth labels and distillation loss calculated with the soft output of a teacher model (Hinton et al., 2015)",
                            "SPEQ (Boo et al., 2020) introduces a novel self-training paradigm where the high-precision teacher and the low-bit-width student from the same full precision source model are trained collaboratively. In our training process, where different bit-width networks are enclosed, we design a self-knowledge transfer that utilizes a relatively higher precision network's output as soft labels for the cooperative training of the lower precision ones."
                        ],
                        "paper": {
                            "corpus_id": 254564348,
                            "title": "Vertical Layering of Quantized Neural Networks for Heterogeneous Inference",
                            "authors": [
                                {
                                    "authorId": "2187214993",
                                    "name": "Hai Wu"
                                },
                                {
                                    "authorId": "2053865518",
                                    "name": "Ruifei He"
                                },
                                {
                                    "authorId": "3218666",
                                    "name": "Hao Hao Tan"
                                },
                                {
                                    "authorId": "50844674",
                                    "name": "Xiaojuan Qi"
                                },
                                {
                                    "authorId": "2149775992",
                                    "name": "Kaibin Huang"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 2
                        },
                        "score": 0.92041015625
                    },
                    {
                        "id": "(Hinton et al., 2015)",
                        "snippets": [
                            "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                        ],
                        "paper": {
                            "corpus_id": 7200347,
                            "title": "Distilling the Knowledge in a Neural Network",
                            "authors": [
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "49959210",
                                    "name": "J. Dean"
                                }
                            ],
                            "year": 2015,
                            "venue": "arXiv.org",
                            "n_citations": 19742
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mai et al., 2024)",
                        "snippets": [
                            "Homomorphic KD means the student and teacher models have similar or identical structures.In this approach, the student model learns by mimicking the teacher model's outputs (e.g., logits, feature layer outputs).Common homomorphic KD methods include logit-level distillation, feature-level distillation, and module-level distillation.For instance, TinyViT (Wu et al., 2022) applies distillation during pre-training, storing logits from a large teacher model on hardware to achieve memory and computational efficiency when transferring knowledge to a smaller student Transformer.DeiT-Tiny (Wang et al., 2022) adopts patchlevel distillation, training a small student model to match the pre-trained teacher model's patch structure, then optimizing with decomposed manifold matching loss to reduce computational costs.Module-level methods like m2mKD [48] separate the teacher module from a pre-trained unified model, combining student modules with modular models, and using a shared meta-model for composition, enabling student modules to mimic teacher module behavior.Feature-level distillation methods like MiniViT (Zhang et al., 2022) combine weights from consecutive Transformer blocks for cross-layer weight sharing, introducing transformations to enhance learning."
                        ],
                        "paper": {
                            "corpus_id": 270870796,
                            "title": "From Efficient Multimodal Models to World Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2276937443",
                                    "name": "Xinji Mai"
                                },
                                {
                                    "authorId": "2261831274",
                                    "name": "Zeng Tao"
                                },
                                {
                                    "authorId": "2261891655",
                                    "name": "Junxiong Lin"
                                },
                                {
                                    "authorId": "2276807843",
                                    "name": "Haoran Wang"
                                },
                                {
                                    "authorId": "2276969811",
                                    "name": "Yang Chang"
                                },
                                {
                                    "authorId": "2212014366",
                                    "name": "Yanlan Kang"
                                },
                                {
                                    "authorId": "2276879376",
                                    "name": "Yan Wang"
                                },
                                {
                                    "authorId": "2276819302",
                                    "name": "Wenqiang Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.92626953125
                    },
                    {
                        "id": "(Zhang et al., 2022)",
                        "snippets": [
                            "Vision Transformer (ViT) models have recently drawn much attention in computer vision due to their high model capability. However, ViT models suffer from huge number of parameters, restricting their applicability on devices with limited memory. To alleviate this problem, we propose MiniViT, a new compression framework, which achieves parameter reduction in vision transformers while retaining the same performance. The central idea of MiniViT is to multiplex the weights of consecutive transformer blocks. More specifically, we make the weights shared across layers, while imposing a transformation on the weights to increase diversity. Weight distillation over self-attention is also applied to transfer knowledge from large-scale ViT models to weight-multiplexed compact models. Comprehensive experiments demonstrate the efficacy of MiniViT, showing that it can reduce the size of the pre-trained Swin-B transformer by 48%, while achieving an increase of 1.0% in Top-1 accuracy on ImageNet. Moreover, using a single-layer of parameters, MiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters, without seriously compromising the performance. Finally, we verify the transferability of MiniViT by reporting its performance on downstream benchmarks. Code and models are available at here."
                        ],
                        "paper": {
                            "corpus_id": 248177918,
                            "title": "MiniViT: Compressing Vision Transformers with Weight Multiplexing",
                            "authors": [
                                {
                                    "authorId": "3431885",
                                    "name": "Jinnian Zhang"
                                },
                                {
                                    "authorId": "2484788",
                                    "name": "Houwen Peng"
                                },
                                {
                                    "authorId": "2115597973",
                                    "name": "Kan Wu"
                                },
                                {
                                    "authorId": "2152968847",
                                    "name": "Mengchen Liu"
                                },
                                {
                                    "authorId": "2054421528",
                                    "name": "Bin Xiao"
                                },
                                {
                                    "authorId": "3247966",
                                    "name": "Jianlong Fu"
                                },
                                {
                                    "authorId": "2150687325",
                                    "name": "Lu Yuan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 124
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Convolution neural networks (CNNs) have long been dominating the model choice in on-device intelligent mobile applications. Recently, we are witnessing the fast development of vision transformers, which are notable for the use of the self-attention mechanism, have demonstrated the superiority in accuracy over CNNs. However, vision transformers are with expensive computation costs, and their inference efficiency on resource-constrained mobile devices are still unclear by now. This brings a lot of uncertainty for on-device intelligence to benefit from the vision transformers. In this work, we carry out the first empirical study to investigate the possibility of efficiently deploying vision transformers on mobile devices. Our twofold study (i) profiles the representative vision transformers to understand the inference performance on commercial mobile devices and the behind reasons; and (ii) study multi-dimensional DNN acceleration approaches to achieve minimal latency. Results show that it is too expensive for vision transformer inference on mobile devices. Its inference is 1.58x-41x slower than CNNs. By removing the redundant Attention heads and FFN layers, DeiT-Tiny saves 23.2\\% latency with negligible 0.75\\% accuracy loss. Our study provides 7 insightful findings for future efficient vision transformer optimization and design."
                        ],
                        "paper": {
                            "corpus_id": 247230104,
                            "title": "Towards efficient vision transformer inference: a first study of transformers on mobile devices",
                            "authors": [
                                {
                                    "authorId": "2144671295",
                                    "name": "Xudong Wang"
                                },
                                {
                                    "authorId": "2152831498",
                                    "name": "Li Zhang"
                                },
                                {
                                    "authorId": "2157274867",
                                    "name": "Yang Wang"
                                },
                                {
                                    "authorId": "51169628",
                                    "name": "Mao Yang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Workshop on Mobile Computing Systems and Applications",
                            "n_citations": 44
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Traditional knowldistillation follows a two-stage training pipeline, where large teacher model is initially trained and then distilled to a lightweight student model",
                            "For two-stage knowledge distillation experiments, we explore the following two schemes. \n\nr Strong Teacher Scheme: distilling knowledge from a pre- trained Cascade Mask RCNN teacher with ResNetX101-DCN backbone, which achieves 47.3 AP. \n\nr Weak Teacher Scheme: distilling knowledge from a pre- trained teacher which has the identical architecture with the student (i.e., Faster RCNN with ResNet50 backbone) and achieves 38.4 AP. Experimental results are shown in Table X. It is observed that one-stage knowledge distillation leads to 2.3 AP improvements over the baseline, which is 0.2 higher than two-stage knowledge distillation with a weak teacher, but still 0.8 AP lower than the two-stage knowledge distillation with a strong teacher. This observation indicates that one-stage knowledge distillation can achieve comparable and even better performance than two-stage knowledge distillation when no powerful teacher is available. Moreover, two-stage knowledge distillation can make more use of the knowledge from a powerful teacher and achieve better performance."
                        ],
                        "paper": {
                            "corpus_id": 260378128,
                            "title": "Structured Knowledge Distillation for Accurate and Efficient Object Detection",
                            "authors": [
                                {
                                    "authorId": "50081570",
                                    "name": "Linfeng Zhang"
                                },
                                {
                                    "authorId": "2075321204",
                                    "name": "Kaisheng Ma"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 27
                        },
                        "score": 0.91552734375
                    },
                    {
                        "id": "(Boo et al., 2020)",
                        "snippets": [
                            "The quantization of deep neural networks (QDNNs) has been actively studied for deployment in edge devices. Recent studies employ the knowledge distillation (KD) method to improve the performance of quantized networks. In this study, we propose stochastic precision ensemble training for QDNNs (SPEQ). SPEQ is a knowledge distillation training scheme; however, the teacher is formed by sharing the model parameters of the student network. We obtain the soft labels of the teacher by randomly changing the bit precision of the activation stochastically at each layer of the forward-pass computation. The student model is trained with these soft labels to reduce the activation quantization noise. The cosine similarity loss is employed, instead of the KL-divergence, for KD training. As the teacher model changes continuously by random bit-precision assignment, it exploits the effect of stochastic ensemble KD. SPEQ outperforms the existing quantization training methods in various tasks, such as image classification, question-answering, and transfer learning without the need for cumbersome teacher networks."
                        ],
                        "paper": {
                            "corpus_id": 222066738,
                            "title": "Stochastic Precision Ensemble: Self-Knowledge Distillation for Quantized Deep Neural Networks",
                            "authors": [
                                {
                                    "authorId": "3350090",
                                    "name": "Yoonho Boo"
                                },
                                {
                                    "authorId": "1859825",
                                    "name": "Sungho Shin"
                                },
                                {
                                    "authorId": "2506452",
                                    "name": "Jungwook Choi"
                                },
                                {
                                    "authorId": "66936521",
                                    "name": "Wonyong Sung"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 30
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al._1, 2022)",
                        "snippets": [
                            "Vision transformer (ViT) recently has drawn great attention in computer vision due to its remarkable model capability. However, most prevailing ViT models suffer from huge number of parameters, restricting their applicability on devices with limited resources. To alleviate this issue, we propose TinyViT, a new family of tiny and efficient small vision transformers pretrained on large-scale datasets with our proposed fast distillation framework. The central idea is to transfer knowledge from large pretrained models to small ones, while enabling small models to get the dividends of massive pretraining data. More specifically, we apply distillation during pretraining for knowledge transfer. The logits of large teacher models are sparsified and stored in disk in advance to save the memory cost and computation overheads. The tiny student transformers are automatically scaled down from a large pretrained model with computation and parameter constraints. Comprehensive experiments demonstrate the efficacy of TinyViT. It achieves a top-1 accuracy of 84.8% on ImageNet-1k with only 21M parameters, being comparable to Swin-B pretrained on ImageNet-21k while using 4.2 times fewer parameters. Moreover, increasing image resolutions, TinyViT can reach 86.5% accuracy, being slightly better than Swin-L while using only 11% parameters. Last but not the least, we demonstrate a good transfer ability of TinyViT on various downstream tasks. Code and models are available at https://github.com/microsoft/Cream/tree/main/TinyViT."
                        ],
                        "paper": {
                            "corpus_id": 250920355,
                            "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
                            "authors": [
                                {
                                    "authorId": "2115597973",
                                    "name": "Kan Wu"
                                },
                                {
                                    "authorId": "2108041613",
                                    "name": "Jinnian Zhang"
                                },
                                {
                                    "authorId": "2484788",
                                    "name": "Houwen Peng"
                                },
                                {
                                    "authorId": "2152968847",
                                    "name": "Mengchen Liu"
                                },
                                {
                                    "authorId": "2054421528",
                                    "name": "Bin Xiao"
                                },
                                {
                                    "authorId": "3247966",
                                    "name": "Jianlong Fu"
                                },
                                {
                                    "authorId": "2150687325",
                                    "name": "Lu Yuan"
                                }
                            ],
                            "year": 2022,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 265
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Among the various methods, knowledge distillation has emerged as an effective technique to enhance inference speed without greatly compromising performance.\n\nExperimental results show that the shallow BiLSTM-based model achieves performance comparable to the ELMo language model [104], but with approximately 100 times fewer parameters and a 15-fold increase in inference speed.\n\nCompared to BERT, DistilBERT maintains 97% of the performance while reducing the number of parameters by 40%.\n\nPKD method outperformed standard knowledge distillation methods. It achieved superior performance and better generalization, significantly enhancing training efficiency and reducing storage requirements while maintaining accuracy comparable to the original large-scale model."
                        ],
                        "paper": {
                            "corpus_id": 270878760,
                            "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
                            "authors": [
                                {
                                    "authorId": "2309482604",
                                    "name": "Chuanpeng Yang"
                                },
                                {
                                    "authorId": "2290230390",
                                    "name": "Yao Zhu"
                                },
                                {
                                    "authorId": "2309429610",
                                    "name": "Wang Lu"
                                },
                                {
                                    "authorId": "2293893759",
                                    "name": "Yidong Wang"
                                },
                                {
                                    "authorId": "2276257486",
                                    "name": "Qian Chen"
                                },
                                {
                                    "authorId": "26155233",
                                    "name": "Chenlong Gao"
                                },
                                {
                                    "authorId": "2276089941",
                                    "name": "Bingjie Yan"
                                },
                                {
                                    "authorId": "2276178270",
                                    "name": "Yiqiang Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM Transactions on Intelligent Systems and Technology",
                            "n_citations": 33
                        },
                        "score": 0.9228515625
                    },
                    {
                        "id": "(Fuente et al., 2024)",
                        "snippets": [
                            "Knowledge distillation transfers knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student) (Lin et al., 2024)",
                            "Distillation schemes include offline distillation, where the teacher is trained first and then guides the student; online distillation, where both models train simultaneously; and self-distillation, where the teacher and student share the same architecture (Ray, 2021), (Gou et al., 2020). This method creates lightweight models that can maintain or even surpass the performance of larger models."
                        ],
                        "paper": {
                            "corpus_id": 273963558,
                            "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network",
                            "authors": [
                                {
                                    "authorId": "2330192141",
                                    "name": "Ra'ul de la Fuente"
                                },
                                {
                                    "authorId": "2047941908",
                                    "name": "Luciano Radrig\u00e1n"
                                },
                                {
                                    "authorId": "2330183718",
                                    "name": "Anibal S Morales"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.92041015625
                    },
                    {
                        "id": "(Lin et al., 2024)",
                        "snippets": [
                            "Tiny machine learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of applications and enable ubiquitous intelligence. However, TinyML is challenging due to the hardware constraints: the tiny memory resource is difficult hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and introduce tiny on-device training techniques. Finally, we present future directions in this area. Today\u2019s \"large\" model might be tomorrow\u2019s \"tiny\" model. The scope of TinyML should evolve and adapt over time."
                        ],
                        "paper": {
                            "corpus_id": 264109973,
                            "title": "Tiny Machine Learning: Progress and Futures [Feature]",
                            "authors": [
                                {
                                    "authorId": "2258439127",
                                    "name": "Ji Lin"
                                },
                                {
                                    "authorId": "20515689",
                                    "name": "Ligeng Zhu"
                                },
                                {
                                    "authorId": "2164114675",
                                    "name": "Wei-Ming Chen"
                                },
                                {
                                    "authorId": "2258439498",
                                    "name": "Wei-Chen Wang"
                                },
                                {
                                    "authorId": "2258448604",
                                    "name": "Song Han"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Circuits and Systems Magazine",
                            "n_citations": 55
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gou et al., 2020)",
                        "snippets": [
                            "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."
                        ],
                        "paper": {
                            "corpus_id": 219559263,
                            "title": "Knowledge Distillation: A Survey",
                            "authors": [
                                {
                                    "authorId": "38978232",
                                    "name": "Jianping Gou"
                                },
                                {
                                    "authorId": "2425630",
                                    "name": "B. Yu"
                                },
                                {
                                    "authorId": "144555237",
                                    "name": "S. Maybank"
                                },
                                {
                                    "authorId": "143719920",
                                    "name": "D. Tao"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Journal of Computer Vision",
                            "n_citations": 2984
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.165369
    }
}