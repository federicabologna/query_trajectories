{
    "query": "people a long context Transformer model K context length using sequence parallelism techniques these methods each other",
    "user_id": "lib_user",
    "task_id": "a879f376-440e-4f6f-9290-e88e413a096a",
    "timestamp": "2025-06-23T22:25:42.211894",
    "n_retrieval": 256,
    "n_retrieved": 270,
    "n_candidates": 17,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.270084,
    "decomposed_query": {
        "rewritten_query": "Long context Transformer model with K context length using sequence parallelism techniques and how these methods compare to each other.",
        "keyword_query": "long context Transformer model context length sequence parallelism techniques methods compare",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009771,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 255,
            "citation_count": 65,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.12351, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267505808",
                    "name": "Yunpeng Huang"
                },
                {
                    "authorId": "2158383105",
                    "name": "Jingwei Xu"
                },
                {
                    "authorId": "2267505325",
                    "name": "Zixu Jiang"
                },
                {
                    "authorId": "2267491104",
                    "name": "Junyu Lai"
                },
                {
                    "authorId": "15401196",
                    "name": "Zenan Li"
                },
                {
                    "authorId": "2254287436",
                    "name": "Yuan Yao"
                },
                {
                    "authorId": "2267520498",
                    "name": "Taolue Chen"
                },
                {
                    "authorId": "2267505839",
                    "name": "Lijuan Yang"
                },
                {
                    "authorId": "2267485767",
                    "name": "Zhou Xin"
                },
                {
                    "authorId": "2267813458",
                    "name": "Xiaoxing Ma"
                }
            ],
            "abstract": "Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as optimization toolkits such as libraries, frameworks, and compilers to boost the efficacy of LLMs across different stages in runtime. Finally, we discuss the challenges and potential avenues for future research. A curated repository of relevant literature, continuously updated, is available at https://github.com/Strivin0311/long-llms-learning.",
            "corpus_id": 265308945,
            "sentences": [
                {
                    "corpus_id": "265308945",
                    "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey",
                    "text": "Additionally, the lack of a robust and generalizable mechanism for positional embeddings (PEs) leads to performance degradation and fluctuation during inference, particularly with longer sequences or position shifting on relevant information [139]. \n\nWith LLMs deeply ingrained in various applications that require long-context comprehension [114,248] and generation [89,142], the demand for long-context LLMs capable of comprehending and generating extremely long sequences effectively and efficiently becomes increasingly indispensable and urgent. Consequently, researchers have devoted significant efforts to enhancing the Transformer architecture to address the long-context problem in LLMs, including optimization on the efficiency of attention (Section 3), context window extension with extra memory mechanisms (Section 4), effective length generalization with extrapolative PEs (Section 5), context pre/postprocessing (Section 6), and other miscellaneous methods (Section 7) such as specific pretraining objectives, mixture of experts (MoE), quantization, parallelism, etc. \n\nExisting surveys. The field of long-context LLMs has become one of the most rapidly developing research areas on LLMs recently, with some existing surveys [65,112,137,216,270]. [112] offers an overview of long document summarization, but does not delve into techniques of long text modeling. [216] and [137] primarily concentrate on improving the computational efficiency of Transformers in long-text scenarios. Although [270] underscores the challenges LLMs face when engaging with extensive sequences, its discussed methods predominantly align with efficient Transformers, similar to [216] and [137]. A more recent survey [65] bears the closest resemblance to our study, but is considerably less comprehensive than ours. In particular, we review the advancement in breaking the barriers of context length across all Manuscript submitted to ACM stages for more intricate and scalable Transformer-based LLMs by exploring the Transformer from both an algorithmic design and system architecture perspective. This survey aims to present a panorama of literature on architecture evolution for scaling the effective context window length of the state-of-the-art Transformer-based LLMs. The main contributions are as follows.",
                    "score": 0.5396556266551796,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1744,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 248
                        },
                        {
                            "start": 251,
                            "end": 549
                        },
                        {
                            "start": 550,
                            "end": 1080
                        },
                        {
                            "start": 1083,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1374
                        },
                        {
                            "start": 1375,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1685
                        },
                        {
                            "start": 1686,
                            "end": 1805
                        },
                        {
                            "start": 1806,
                            "end": 2088
                        },
                        {
                            "start": 2089,
                            "end": 2263
                        },
                        {
                            "start": 2264,
                            "end": 2302
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 242,
                            "end": 247,
                            "matchedPaperCorpusId": "259360665"
                        },
                        {
                            "start": 1242,
                            "end": 1246,
                            "matchedPaperCorpusId": "250118028"
                        },
                        {
                            "start": 1246,
                            "end": 1250,
                            "matchedPaperCorpusId": "235368340"
                        },
                        {
                            "start": 1254,
                            "end": 1258,
                            "matchedPaperCorpusId": "241606169"
                        },
                        {
                            "start": 1260,
                            "end": 1265,
                            "matchedPaperCorpusId": "250118028"
                        },
                        {
                            "start": 1385,
                            "end": 1390,
                            "matchedPaperCorpusId": "235368340"
                        },
                        {
                            "start": 1504,
                            "end": 1509,
                            "matchedPaperCorpusId": "241606169"
                        },
                        {
                            "start": 1679,
                            "end": 1684,
                            "matchedPaperCorpusId": "235368340"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7431640625
                }
            ],
            "relevance_judgement": 0.7431640625,
            "relevance_judgment_input_expanded": "# Title: Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey\n# Venue: arXiv.org\n# Authors: Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, Xiaoxing Ma\n## Abstract\nTransformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as optimization toolkits such as libraries, frameworks, and compilers to boost the efficacy of LLMs across different stages in runtime. Finally, we discuss the challenges and potential avenues for future research. A curated repository of relevant literature, continuously updated, is available at https://github.com/Strivin0311/long-llms-learning.\n## INTRODUCTION\nAdditionally, the lack of a robust and generalizable mechanism for positional embeddings (PEs) leads to performance degradation and fluctuation during inference, particularly with longer sequences or position shifting on relevant information [139]. \n\nWith LLMs deeply ingrained in various applications that require long-context comprehension [114,248] and generation [89,142], the demand for long-context LLMs capable of comprehending and generating extremely long sequences effectively and efficiently becomes increasingly indispensable and urgent. Consequently, researchers have devoted significant efforts to enhancing the Transformer architecture to address the long-context problem in LLMs, including optimization on the efficiency of attention (Section 3), context window extension with extra memory mechanisms (Section 4), effective length generalization with extrapolative PEs (Section 5), context pre/postprocessing (Section 6), and other miscellaneous methods (Section 7) such as specific pretraining objectives, mixture of experts (MoE), quantization, parallelism, etc. \n\nExisting surveys. The field of long-context LLMs has become one of the most rapidly developing research areas on LLMs recently, with some existing surveys [65,112,137,216,270]. [112] offers an overview of long document summarization, but does not delve into techniques of long text modeling. [216] and [137] primarily concentrate on improving the computational efficiency of Transformers in long-text scenarios. Although [270] underscores the challenges LLMs face when engaging with extensive sequences, its discussed methods predominantly align with efficient Transformers, similar to [216] and [137]. A more recent survey [65] bears the closest resemblance to our study, but is considerably less comprehensive than ours. In particular, we review the advancement in breaking the barriers of context length across all Manuscript submitted to ACM stages for more intricate and scalable Transformer-based LLMs by exploring the Transformer from both an algorithmic design and system architecture perspective. This survey aims to present a panorama of literature on architecture evolution for scaling the effective context window length of the state-of-the-art Transformer-based LLMs. The main contributions are as follows.",
            "reference_string": "[265308945 | Huang et al. | 2023 | Citations: 65]"
        },
        {
            "title": "Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.04881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2278970280",
                    "name": "Zi Yang"
                },
                {
                    "authorId": "2278831818",
                    "name": "Nan Hua"
                }
            ],
            "abstract": "As LLMs have become capable of processing more complex types of inputs, researchers have recently studied how to efficiently and affordably process possibly arbitrarily long sequences. One effective approach is to use a FIFO memory to store keys and values of an attention sublayer from past chunks to allow subsequent queries to attend. However, this approach requires a large memory and/or takes into the consideration the specific LM architecture. Moreover, due to the causal nature between the key-values in prior context and the queries at present, this approach cannot be extended to bidirectional attention such as in an encoder-decoder or PrefixLM decoder-only architecture. In this paper, we propose to use eviction policies, such as LRA and LFA, to reduce the memory size and adapt to various architectures, and we also propose the Attendre layer, a wait-to-attend mechanism by retrieving the key-value memory (K/V memory) with evicted queries in the query memory (Q memory). As a first step, we evaluate this method in the context length extension setup using the TriviaQA reading comprehension task, and show the effectiveness of the approach.",
            "corpus_id": 266903047,
            "sentences": [
                {
                    "corpus_id": "266903047",
                    "title": "Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing",
                    "text": "Long context modeling (or long range modeling, long sequence processing) has become a very broad research topic. In this section, we focus on memory-based Transformer models that support arbitrarily long inputs, in particular, what to memorize and how to update. Other sparsity and compression methods can be orthogonal to and thus combine with ours to achieve better efficiency. Interested readers should refer to other survey papers, e.g. Dong et al. (2023) and Huang et al. (2023), for a brief review of the state of this research area. Memory entry types. Transformer-XL (Dai et al., 2019) and MART (Lei et al., 2020) use a cache to memorize the contextualized embeddings computed during the previous step at each layer. Compressive Transformer (Rae et al., 2019) adds an additional compressed memory to collect the discarded entries from the Transformer-XL cache. Memorizing Transformer (Wu et al., 2022b) and StreamingLLM (Xiao et al., 2023) modify the transient Q/K/V structure of a dot-product attention, and allows K/Vs to persist. LongMem (Wang et al., 2023) is similar to Memorizing Transformer but further freezes the backbone LLM that generates K/Vs and introduces a separate trainable SideNet to overcome the staleness issue of Memorizing Transformer. Unlimiformer (Bertsch et al., 2023) stores the hidden states of encoder layers with no capacity limit, and retrieves top keys directly from the memory in cross attention after a reformulated attention equation. TRAMS (Yu et al., 2023) combines Transformer-XL with Unlimiformer, which retrieves top-m keys from a memory pool size of M , which itself uses the FIFO policy despite the memory selection method for top keys. All of the above methods insert existing intermediate activations into the memory. Memformer (Wu et al., 2022a) and TTM (Ryoo et al., 2023) implicitly selects and creates, using a \"Write\" operation, memory entries from a combination of layer or model inputs and outputs and existing memory entries.",
                    "score": 0.5892578434829847,
                    "section_title": "Related Work",
                    "char_start_offset": 3962,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 112
                        },
                        {
                            "start": 113,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 559
                        },
                        {
                            "start": 560,
                            "end": 868
                        },
                        {
                            "start": 869,
                            "end": 1040
                        },
                        {
                            "start": 1041,
                            "end": 1265
                        },
                        {
                            "start": 1266,
                            "end": 1476
                        },
                        {
                            "start": 1477,
                            "end": 1685
                        },
                        {
                            "start": 1686,
                            "end": 1768
                        },
                        {
                            "start": 1769,
                            "end": 1984
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 575,
                            "end": 593,
                            "matchedPaperCorpusId": "57759363"
                        },
                        {
                            "start": 603,
                            "end": 621,
                            "matchedPaperCorpusId": "218595710"
                        },
                        {
                            "start": 892,
                            "end": 910,
                            "matchedPaperCorpusId": "247519194"
                        },
                        {
                            "start": 1279,
                            "end": 1301,
                            "matchedPaperCorpusId": "258436892"
                        },
                        {
                            "start": 1483,
                            "end": 1500,
                            "matchedPaperCorpusId": "264439578"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7177734375
                }
            ],
            "relevance_judgement": 0.7177734375,
            "relevance_judgment_input_expanded": "# Title: Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing\n# Venue: arXiv.org\n# Authors: Zi Yang, Nan Hua\n## Abstract\nAs LLMs have become capable of processing more complex types of inputs, researchers have recently studied how to efficiently and affordably process possibly arbitrarily long sequences. One effective approach is to use a FIFO memory to store keys and values of an attention sublayer from past chunks to allow subsequent queries to attend. However, this approach requires a large memory and/or takes into the consideration the specific LM architecture. Moreover, due to the causal nature between the key-values in prior context and the queries at present, this approach cannot be extended to bidirectional attention such as in an encoder-decoder or PrefixLM decoder-only architecture. In this paper, we propose to use eviction policies, such as LRA and LFA, to reduce the memory size and adapt to various architectures, and we also propose the Attendre layer, a wait-to-attend mechanism by retrieving the key-value memory (K/V memory) with evicted queries in the query memory (Q memory). As a first step, we evaluate this method in the context length extension setup using the TriviaQA reading comprehension task, and show the effectiveness of the approach.\n## Related Work\nLong context modeling (or long range modeling, long sequence processing) has become a very broad research topic. In this section, we focus on memory-based Transformer models that support arbitrarily long inputs, in particular, what to memorize and how to update. Other sparsity and compression methods can be orthogonal to and thus combine with ours to achieve better efficiency. Interested readers should refer to other survey papers, e.g. Dong et al. (2023) and Huang et al. (2023), for a brief review of the state of this research area. Memory entry types. Transformer-XL (Dai et al., 2019) and MART (Lei et al., 2020) use a cache to memorize the contextualized embeddings computed during the previous step at each layer. Compressive Transformer (Rae et al., 2019) adds an additional compressed memory to collect the discarded entries from the Transformer-XL cache. Memorizing Transformer (Wu et al., 2022b) and StreamingLLM (Xiao et al., 2023) modify the transient Q/K/V structure of a dot-product attention, and allows K/Vs to persist. LongMem (Wang et al., 2023) is similar to Memorizing Transformer but further freezes the backbone LLM that generates K/Vs and introduces a separate trainable SideNet to overcome the staleness issue of Memorizing Transformer. Unlimiformer (Bertsch et al., 2023) stores the hidden states of encoder layers with no capacity limit, and retrieves top keys directly from the memory in cross attention after a reformulated attention equation. TRAMS (Yu et al., 2023) combines Transformer-XL with Unlimiformer, which retrieves top-m keys from a memory pool size of M , which itself uses the FIFO policy despite the memory selection method for top keys. All of the above methods insert existing intermediate activations into the memory. Memformer (Wu et al., 2022a) and TTM (Ryoo et al., 2023) implicitly selects and creates, using a \"Write\" operation, memory entries from a combination of layer or model inputs and outputs and existing memory entries.",
            "reference_string": "[266903047 | Yang et al. | 2024 | Citations: 4]"
        },
        {
            "title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 26,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.17588, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308098543",
                    "name": "Shawn Gavin"
                },
                {
                    "authorId": "2300091474",
                    "name": "Tuney Zheng"
                },
                {
                    "authorId": "2294523552",
                    "name": "Jiaheng Liu"
                },
                {
                    "authorId": "2303653097",
                    "name": "Quehry Que"
                },
                {
                    "authorId": "2303796897",
                    "name": "Noah Wang"
                },
                {
                    "authorId": "2308240515",
                    "name": "Jian Yang"
                },
                {
                    "authorId": "2214538677",
                    "name": "Chenchen Zhang"
                },
                {
                    "authorId": "2239245627",
                    "name": "Wenhao Huang"
                },
                {
                    "authorId": "2249847177",
                    "name": "Wenhu Chen"
                },
                {
                    "authorId": "2308233656",
                    "name": "Ge Zhang"
                }
            ],
            "abstract": "The long-context capabilities of large language models (LLMs) have been a hot topic in recent years. To evaluate the performance of LLMs in different scenarios, various assessment benchmarks have emerged. However, as most of these benchmarks focus on identifying key information to answer questions, which mainly requires the retrieval ability of LLMs, these benchmarks can partially represent the reasoning performance of LLMs from large amounts of information. Meanwhile, although LLMs often claim to have context windows of 32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual supported length of these LLMs. To address these issues, we propose the LongIns benchmark dataset, a challenging long-context instruction-based exam for LLMs, which is built based on the existing instruction datasets. Specifically, in our LongIns, we introduce three evaluation settings: Global Instruction&Single Task (GIST), Local Instruction&Single Task (LIST), and Local Instruction&Multiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations on existing LLMs and have the following important findings: (1). The top-performing GPT-4 with 128k context length performs poorly on the evaluation context window of 16k in our LongIns. (2). For the multi-hop reasoning ability of many existing LLMs, significant efforts are still needed under short context windows (less than 4k).",
            "corpus_id": 270710851,
            "sentences": [
                {
                    "corpus_id": "270710851",
                    "title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs",
                    "text": "Long-context LLM The computational cost of processing sequences in Transformer-based models increases quadratically with sequence length, resulting in higher resource consumption and performance issues when handling long context inputs.\n\nMany studies explore various strategies to address this challenge, including the use of new positional encodings to achieve position interpolation or extrapolation (Peng et al., 2023;Chen et al., 2023;Liu et al., 2024b).For example, Rope (Su et al., 2021) extends the positional knowledge learned during the pre-training phase to longer sequence lengths through extrapolation, including various variants such as NTK-RoPE (NormXU, 2021).Alibi (Press et al., 2021), on the other hand, maps long sequences to within recognizable lengths through interpolation.Some studies attempt to fine-tune LLMs to give the model a longer context window.\n\nLongLoRA (Chen et al., 2024)is an efficient finetuning approach that significantly extends the context sizes of pre-trained LLMs with limited computational cost by using sparse local attention and improved parameter-efficient fine-tuning techniques.RingAttention (Liu et al., 2023a) can reduce the memory requirements of the Transformer model, allowing it to train sequences over 500 times longer than previous memory-efficient methods, without needing to approximate the attention mechanism.Additionally, traditional engineering techniques such as sliding windows or RAG (Lewis et al., 2020) are also solutions for addressing long context scenarios in LLMs.These methods improve the performance of LLMs in handling long contexts in certain aspects.\n\nLong-context Benchmark To evaluate the performance of different LLMs on long texts, various benchmarks are often used to test different aspects of LLMs' capabilities.For instance, Zeroscrolls (Shaham et al., 2023) is a zero-shot benchmark for natural language understanding over long texts, which contains only test and small validation sets, without training data.LongBench (Bai et al., 2023b) is a bilingual, multi-task benchmark for long context understanding, which also provides a subset with uniformly distributed data called LongBench-E.",
                    "score": 0.5066865808993154,
                    "section_title": "Input:",
                    "char_start_offset": 4459,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 236
                        },
                        {
                            "start": 238,
                            "end": 458
                        },
                        {
                            "start": 458,
                            "end": 674
                        },
                        {
                            "start": 674,
                            "end": 794
                        },
                        {
                            "start": 794,
                            "end": 875
                        },
                        {
                            "start": 877,
                            "end": 1126
                        },
                        {
                            "start": 1126,
                            "end": 1369
                        },
                        {
                            "start": 1369,
                            "end": 1535
                        },
                        {
                            "start": 1535,
                            "end": 1626
                        },
                        {
                            "start": 1628,
                            "end": 1794
                        },
                        {
                            "start": 1794,
                            "end": 1993
                        },
                        {
                            "start": 1993,
                            "end": 2172
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1449,
                            "end": 1469,
                            "matchedPaperCorpusId": "218869575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.689453125
                },
                {
                    "corpus_id": "270710851",
                    "title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs",
                    "text": "My favorite team has won the championship.\n\nOutput: Joy] [3.Instruction:... Input:... Output:...] ... ... [N.Instruction:... Input:... Output:...] Please identify all the incorrectly answered questions above.\n\nAnswer: [1,4,7...] ( of key information from extended texts.We evaluate the performance of 20 different LLMs under LongIns, including GPT-4o.We observe that LLMs generally perform worse on tasks requiring understanding of complete long sequences compared to retrieval tasks of the same length.As the total length increases, the performance gap becomes more pronounced.Additionally, the models' performance is largely independent of their advertised context window length, leading us to believe that the advertised context window length for most models should be understood as the \"maximum acceptable sequence length\" rather than the \"maximum comprehensible sequence length.\"Moreover, by controlling the distribution of incorrect answers and response situations, we analyze the distribution of attention changes at different text positions.Further analysis on the density of key information within the same total length shows that, except for GPT-4 and GPT-4o, the accuracy of most models rapidly declines as the density of key information increases.In summary, the primary contributions can be summarized as: \u2022 We evaluate a series of long-context LLMs using this benchmark and find that most models fail to achieve high scores when the critical information length is only 8k.Even GPT-4 and GPT-4o score poorly at 16k length.This result is significantly different from the commonly recognized long context lengths (128k or longer), indicating that current LLMs still have considerable shortcomings in performing such tasks.We hope these results can provide a reference for research in the field of long-context LLMs.\n\nLong-context LLM The computational cost of processing sequences in Transformer-based models increases quadratically with sequence length, resulting in higher resource consumption and performance issues when handling long context inputs.",
                    "score": 0.5285653860207242,
                    "section_title": "Input:",
                    "char_start_offset": 2631,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 42
                        },
                        {
                            "start": 44,
                            "end": 60
                        },
                        {
                            "start": 60,
                            "end": 109
                        },
                        {
                            "start": 109,
                            "end": 208
                        },
                        {
                            "start": 210,
                            "end": 270
                        },
                        {
                            "start": 270,
                            "end": 351
                        },
                        {
                            "start": 351,
                            "end": 503
                        },
                        {
                            "start": 503,
                            "end": 578
                        },
                        {
                            "start": 578,
                            "end": 884
                        },
                        {
                            "start": 884,
                            "end": 1049
                        },
                        {
                            "start": 1049,
                            "end": 1259
                        },
                        {
                            "start": 1259,
                            "end": 1486
                        },
                        {
                            "start": 1486,
                            "end": 1535
                        },
                        {
                            "start": 1535,
                            "end": 1733
                        },
                        {
                            "start": 1733,
                            "end": 1826
                        },
                        {
                            "start": 1828,
                            "end": 2064
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.46044921875
                }
            ],
            "relevance_judgement": 0.689453125,
            "relevance_judgment_input_expanded": "# Title: LongIns: A Challenging Long-context Instruction-based Exam for LLMs\n# Venue: arXiv.org\n# Authors: Shawn Gavin, Tuney Zheng, Jiaheng Liu, Quehry Que, Noah Wang, Jian Yang, Chenchen Zhang, Wenhao Huang, Wenhu Chen, Ge Zhang\n## Abstract\nThe long-context capabilities of large language models (LLMs) have been a hot topic in recent years. To evaluate the performance of LLMs in different scenarios, various assessment benchmarks have emerged. However, as most of these benchmarks focus on identifying key information to answer questions, which mainly requires the retrieval ability of LLMs, these benchmarks can partially represent the reasoning performance of LLMs from large amounts of information. Meanwhile, although LLMs often claim to have context windows of 32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual supported length of these LLMs. To address these issues, we propose the LongIns benchmark dataset, a challenging long-context instruction-based exam for LLMs, which is built based on the existing instruction datasets. Specifically, in our LongIns, we introduce three evaluation settings: Global Instruction&Single Task (GIST), Local Instruction&Single Task (LIST), and Local Instruction&Multiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations on existing LLMs and have the following important findings: (1). The top-performing GPT-4 with 128k context length performs poorly on the evaluation context window of 16k in our LongIns. (2). For the multi-hop reasoning ability of many existing LLMs, significant efforts are still needed under short context windows (less than 4k).\n## Input:\nMy favorite team has won the championship.\n\nOutput: Joy] [3.Instruction:... Input:... Output:...] ... ... [N.Instruction:... Input:... Output:...] Please identify all the incorrectly answered questions above.\n\nAnswer: [1,4,7...] ( of key information from extended texts.We evaluate the performance of 20 different LLMs under LongIns, including GPT-4o.We observe that LLMs generally perform worse on tasks requiring understanding of complete long sequences compared to retrieval tasks of the same length.As the total length increases, the performance gap becomes more pronounced.Additionally, the models' performance is largely independent of their advertised context window length, leading us to believe that the advertised context window length for most models should be understood as the \"maximum acceptable sequence length\" rather than the \"maximum comprehensible sequence length.\"Moreover, by controlling the distribution of incorrect answers and response situations, we analyze the distribution of attention changes at different text positions.Further analysis on the density of key information within the same total length shows that, except for GPT-4 and GPT-4o, the accuracy of most models rapidly declines as the density of key information increases.In summary, the primary contributions can be summarized as: \u2022 We evaluate a series of long-context LLMs using this benchmark and find that most models fail to achieve high scores when the critical information length is only 8k.Even GPT-4 and GPT-4o score poorly at 16k length.This result is significantly different from the commonly recognized long context lengths (128k or longer), indicating that current LLMs still have considerable shortcomings in performing such tasks.We hope these results can provide a reference for research in the field of long-context LLMs.\n\nLong-context LLM The computational cost of processing sequences in Transformer-based models increases quadratically with sequence length, resulting in higher resource consumption and performance issues when handling long context inputs.\n...\nLong-context LLM The computational cost of processing sequences in Transformer-based models increases quadratically with sequence length, resulting in higher resource consumption and performance issues when handling long context inputs.\n\nMany studies explore various strategies to address this challenge, including the use of new positional encodings to achieve position interpolation or extrapolation (Peng et al., 2023;Chen et al., 2023;Liu et al., 2024b).For example, Rope (Su et al., 2021) extends the positional knowledge learned during the pre-training phase to longer sequence lengths through extrapolation, including various variants such as NTK-RoPE (NormXU, 2021).Alibi (Press et al., 2021), on the other hand, maps long sequences to within recognizable lengths through interpolation.Some studies attempt to fine-tune LLMs to give the model a longer context window.\n\nLongLoRA (Chen et al., 2024)is an efficient finetuning approach that significantly extends the context sizes of pre-trained LLMs with limited computational cost by using sparse local attention and improved parameter-efficient fine-tuning techniques.RingAttention (Liu et al., 2023a) can reduce the memory requirements of the Transformer model, allowing it to train sequences over 500 times longer than previous memory-efficient methods, without needing to approximate the attention mechanism.Additionally, traditional engineering techniques such as sliding windows or RAG (Lewis et al., 2020) are also solutions for addressing long context scenarios in LLMs.These methods improve the performance of LLMs in handling long contexts in certain aspects.\n\nLong-context Benchmark To evaluate the performance of different LLMs on long texts, various benchmarks are often used to test different aspects of LLMs' capabilities.For instance, Zeroscrolls (Shaham et al., 2023) is a zero-shot benchmark for natural language understanding over long texts, which contains only test and small validation sets, without training data.LongBench (Bai et al., 2023b) is a bilingual, multi-task benchmark for long context understanding, which also provides a subset with uniformly distributed data called LongBench-E.",
            "reference_string": "[270710851 | Gavin et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Long-context LLMs Struggle with Long In-context Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 192,
            "influential_citation_count": 11,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.02060, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2157466550",
                    "name": "Tianle Li"
                },
                {
                    "authorId": "2143853895",
                    "name": "Ge Zhang"
                },
                {
                    "authorId": "2294572942",
                    "name": "Quy Duc Do"
                },
                {
                    "authorId": "2284988933",
                    "name": "Xiang Yue"
                },
                {
                    "authorId": "2249847177",
                    "name": "Wenhu Chen"
                }
            ],
            "abstract": "Large Language Models (LLMs) have made significant strides in handling long sequences. Some models like Gemini could even to be capable of dealing with millions of tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their true abilities in more challenging, real-world scenarios. We introduce a benchmark (LongICLBench) for long in-context learning in extreme-label classification using six datasets with 28 to 174 classes and input lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct predictions. We evaluate on 15 long-context LLMs and find that they perform well on less challenging classification tasks with smaller label space and shorter demonstrations. However, they struggle with more challenging task like Discovery with 174 labels, suggesting a gap in their ability to process long, context-rich sequences. Further analysis reveals a bias towards labels presented later in the sequence and a need for improved reasoning over multiple pieces of information. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LongICLBench could serve as a more realistic evaluation for the future long-context LLMs.",
            "corpus_id": 268857023,
            "sentences": [
                {
                    "corpus_id": "268857023",
                    "title": "Long-context LLMs Struggle with Long In-context Learning",
                    "text": "Large language models have already entered the long context era.A myriad of LLMs has been released to support long context windows from 32K to 2M tokens.These methods (Hao et al., 2022;Chen et al., 2023a;Peng et al., 2023b;Ratner et al., 2023;Xiao et al., 2024;Jin et al., 2024) can unlock lots of complex real-world applications, such as long-document question-answering, multi-document summarization, long-horizon agent tasks, and repo-level code understanding.\n\nOne line of research is based on AliBi (Press et al., 2022) and RoPE (Su et al., 2024) embeddings, which allows us to train Transformers with short sequences and subsequently apply them to longer sequences during inference.Recently, different approaches (Xiong et al., 2023;Fu et al., 2024;Liu et al., 2024) help the model to extrapolate to 128K window size with continued pre-training.Later on, LongRoPE (Ding et al., 2024) was proposed to further extend the context window to 2M tokens.Another line of research also utilizes methodologies like context window sliding and segmentation to overcome the issue of the limited context window in original Transformers (Hao et al., 2022;Ratner et al., 2023).Furthermore, architectural innovations, transitioning from traditional Transformer-based designs to recurrent models or state space models, have shown promise in facilitating long-range computations naturally (Orvieto et al., 2023;Gu & Dao, 2023;Peng et al., 2023a).These techniques have been incorporated into several current open-source LLMs to enhance long sequence understanding capability (Chen et al., 2023b;Tworkowski et al., 2023).",
                    "score": 0.5268066030927275,
                    "section_title": "Introduction",
                    "char_start_offset": 295,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 64
                        },
                        {
                            "start": 64,
                            "end": 153
                        },
                        {
                            "start": 153,
                            "end": 463
                        },
                        {
                            "start": 465,
                            "end": 688
                        },
                        {
                            "start": 688,
                            "end": 851
                        },
                        {
                            "start": 851,
                            "end": 953
                        },
                        {
                            "start": 953,
                            "end": 1167
                        },
                        {
                            "start": 1167,
                            "end": 1433
                        },
                        {
                            "start": 1433,
                            "end": 1606
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 223,
                            "end": 243,
                            "matchedPaperCorpusId": "258686160"
                        },
                        {
                            "start": 243,
                            "end": 261,
                            "matchedPaperCorpusId": "263310483"
                        },
                        {
                            "start": 504,
                            "end": 524,
                            "matchedPaperCorpusId": "237347130"
                        },
                        {
                            "start": 534,
                            "end": 551,
                            "matchedPaperCorpusId": "233307138"
                        },
                        {
                            "start": 1146,
                            "end": 1166,
                            "matchedPaperCorpusId": "258686160"
                        },
                        {
                            "start": 1413,
                            "end": 1432,
                            "matchedPaperCorpusId": "258832459"
                        },
                        {
                            "start": 1561,
                            "end": 1581,
                            "matchedPaperCorpusId": "262084134"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64794921875
                },
                {
                    "corpus_id": "268857023",
                    "title": "Long-context LLMs Struggle with Long In-context Learning",
                    "text": "The effectiveness of Transformer-based models is hindered by the quadratic increase in computational cost relative to sequence length, particularly in handling long context inputs.Recent efforts have explored various strategies to address this challenge.Some studies have pursued continued fine-tuning of the LLM with longer context inputs (Rozi\u00e8re et al., 2024;Tworkowski et al., 2023).Others have leveraged position extrapolation or interpolation, building upon relative rotary positional embedding (Su et al., 2021), to extend input length beyond the training phase (Press et al., 2022;Chen et al., 2023a).Additionally, more approaches have been proposed to mitigate computational issues, including sliding memory window and chunk segmentation (Hao et al., 2022;Ratner et al., 2023;Zhu et al., 2024).Furthermore, alternative architectures beyond Transformer have been explored to handle long inputs more naturally, such as selective-state-spaces models (Peng et al., 2023a;Gu & Dao, 2023).These diverse approaches claim that they can enhance the capabilities of LLMs in processing long context inputs more efficiently.\n\nLong Context Evaluation Due to the imperious demands for the support of long-range LLMs, there is a series of benchmarks focusing on long context evaluation.Long-Range Arena (Tay et al., 2021) includes tasks consisting of sequences ranging from 1K to 16K tokens to evaluate variations of fast Transformers.LongBench (Bai et al., 2023b) comprises 21 bilingual datasets with an average length of around 6k words, which have been processed in a unified format to enable effortless evaluation.L-Eval Benchmark (An et al., 2023) supports 20 sub-tasks with input lengths of 3K to 200K tokens.LooGLE (Li et al., 2023b) focuses on summarization and long dependency QA tasks with test instances exceeding 100k words.Most recently, \u221eBench (Zhang et al., 2024) encompasses 12 tasks with an average length of 200K tokens.",
                    "score": 0.6105487887069162,
                    "section_title": "Long Context Techniques over LLMs",
                    "char_start_offset": 6695,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 180,
                            "end": 254
                        },
                        {
                            "start": 254,
                            "end": 387
                        },
                        {
                            "start": 387,
                            "end": 609
                        },
                        {
                            "start": 609,
                            "end": 803
                        },
                        {
                            "start": 803,
                            "end": 992
                        },
                        {
                            "start": 992,
                            "end": 1121
                        },
                        {
                            "start": 1123,
                            "end": 1280
                        },
                        {
                            "start": 1280,
                            "end": 1429
                        },
                        {
                            "start": 1429,
                            "end": 1612
                        },
                        {
                            "start": 1612,
                            "end": 1709
                        },
                        {
                            "start": 1709,
                            "end": 1830
                        },
                        {
                            "start": 1830,
                            "end": 1932
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 569,
                            "end": 589,
                            "matchedPaperCorpusId": "237347130"
                        },
                        {
                            "start": 765,
                            "end": 785,
                            "matchedPaperCorpusId": "258686160"
                        },
                        {
                            "start": 785,
                            "end": 802,
                            "matchedPaperCorpusId": "262053659"
                        },
                        {
                            "start": 956,
                            "end": 976,
                            "matchedPaperCorpusId": "258832459"
                        },
                        {
                            "start": 1297,
                            "end": 1315,
                            "matchedPaperCorpusId": "260440449"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.58154296875
                }
            ],
            "relevance_judgement": 0.64794921875,
            "relevance_judgment_input_expanded": "# Title: Long-context LLMs Struggle with Long In-context Learning\n# Venue: arXiv.org\n# Authors: Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, Wenhu Chen\n## Abstract\nLarge Language Models (LLMs) have made significant strides in handling long sequences. Some models like Gemini could even to be capable of dealing with millions of tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their true abilities in more challenging, real-world scenarios. We introduce a benchmark (LongICLBench) for long in-context learning in extreme-label classification using six datasets with 28 to 174 classes and input lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct predictions. We evaluate on 15 long-context LLMs and find that they perform well on less challenging classification tasks with smaller label space and shorter demonstrations. However, they struggle with more challenging task like Discovery with 174 labels, suggesting a gap in their ability to process long, context-rich sequences. Further analysis reveals a bias towards labels presented later in the sequence and a need for improved reasoning over multiple pieces of information. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LongICLBench could serve as a more realistic evaluation for the future long-context LLMs.\n## Introduction\nLarge language models have already entered the long context era.A myriad of LLMs has been released to support long context windows from 32K to 2M tokens.These methods (Hao et al., 2022;Chen et al., 2023a;Peng et al., 2023b;Ratner et al., 2023;Xiao et al., 2024;Jin et al., 2024) can unlock lots of complex real-world applications, such as long-document question-answering, multi-document summarization, long-horizon agent tasks, and repo-level code understanding.\n\nOne line of research is based on AliBi (Press et al., 2022) and RoPE (Su et al., 2024) embeddings, which allows us to train Transformers with short sequences and subsequently apply them to longer sequences during inference.Recently, different approaches (Xiong et al., 2023;Fu et al., 2024;Liu et al., 2024) help the model to extrapolate to 128K window size with continued pre-training.Later on, LongRoPE (Ding et al., 2024) was proposed to further extend the context window to 2M tokens.Another line of research also utilizes methodologies like context window sliding and segmentation to overcome the issue of the limited context window in original Transformers (Hao et al., 2022;Ratner et al., 2023).Furthermore, architectural innovations, transitioning from traditional Transformer-based designs to recurrent models or state space models, have shown promise in facilitating long-range computations naturally (Orvieto et al., 2023;Gu & Dao, 2023;Peng et al., 2023a).These techniques have been incorporated into several current open-source LLMs to enhance long sequence understanding capability (Chen et al., 2023b;Tworkowski et al., 2023).\n\n## Long Context Techniques over LLMs\nThe effectiveness of Transformer-based models is hindered by the quadratic increase in computational cost relative to sequence length, particularly in handling long context inputs.Recent efforts have explored various strategies to address this challenge.Some studies have pursued continued fine-tuning of the LLM with longer context inputs (Rozi\u00e8re et al., 2024;Tworkowski et al., 2023).Others have leveraged position extrapolation or interpolation, building upon relative rotary positional embedding (Su et al., 2021), to extend input length beyond the training phase (Press et al., 2022;Chen et al., 2023a).Additionally, more approaches have been proposed to mitigate computational issues, including sliding memory window and chunk segmentation (Hao et al., 2022;Ratner et al., 2023;Zhu et al., 2024).Furthermore, alternative architectures beyond Transformer have been explored to handle long inputs more naturally, such as selective-state-spaces models (Peng et al., 2023a;Gu & Dao, 2023).These diverse approaches claim that they can enhance the capabilities of LLMs in processing long context inputs more efficiently.\n\nLong Context Evaluation Due to the imperious demands for the support of long-range LLMs, there is a series of benchmarks focusing on long context evaluation.Long-Range Arena (Tay et al., 2021) includes tasks consisting of sequences ranging from 1K to 16K tokens to evaluate variations of fast Transformers.LongBench (Bai et al., 2023b) comprises 21 bilingual datasets with an average length of around 6k words, which have been processed in a unified format to enable effortless evaluation.L-Eval Benchmark (An et al., 2023) supports 20 sub-tasks with input lengths of 3K to 200K tokens.LooGLE (Li et al., 2023b) focuses on summarization and long dependency QA tasks with test instances exceeding 100k words.Most recently, \u221eBench (Zhang et al., 2024) encompasses 12 tasks with an average length of 200K tokens.",
            "reference_string": "[268857023 | Li et al. | 2024 | Citations: 192]"
        },
        {
            "title": "StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.08312, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291078805",
                    "name": "Jia-Nan Li"
                },
                {
                    "authorId": "2071635049",
                    "name": "Quan Tu"
                },
                {
                    "authorId": "34754580",
                    "name": "Cunli Mao"
                },
                {
                    "authorId": "2285331933",
                    "name": "Zhengtao Yu"
                },
                {
                    "authorId": "2263887786",
                    "name": "Ji-Rong Wen"
                },
                {
                    "authorId": "2277448117",
                    "name": "Rui Yan"
                }
            ],
            "abstract": "Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of \\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200K or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200K of utterances, resulting in a prolonged dialogue learning. In order to minimize information losses from reconstruction after compression, we design two learning strategies of short-memory reconstruction (SMR) and long-memory reactivation (LMR). Our method outperforms strong baselines in dialogue tasks and achieves a 4 $\\times$ speedup while reducing memory usage by 18 $\\times$ compared to dense attention recomputation.",
            "corpus_id": 268378933,
            "sentences": [
                {
                    "corpus_id": "268378933",
                    "title": "StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses",
                    "text": "StreamingDialogue efficiently handles long context, improving the model's long-term memory for conversation history. Existing methods for processing long context in transformer-based models broadly fall into three categories: efficient transformer design, long-term memory enhancement, and length extrapolation techniques.",
                    "score": 0.5029520418081233,
                    "section_title": "Related work",
                    "char_start_offset": 409,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 322
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63525390625
                }
            ],
            "relevance_judgement": 0.63525390625,
            "relevance_judgment_input_expanded": "# Title: StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses\n# Venue: Neural Information Processing Systems\n# Authors: Jia-Nan Li, Quan Tu, Cunli Mao, Zhengtao Yu, Ji-Rong Wen, Rui Yan\n## Abstract\nStandard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of \\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200K or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200K of utterances, resulting in a prolonged dialogue learning. In order to minimize information losses from reconstruction after compression, we design two learning strategies of short-memory reconstruction (SMR) and long-memory reactivation (LMR). Our method outperforms strong baselines in dialogue tasks and achieves a 4 $\\times$ speedup while reducing memory usage by 18 $\\times$ compared to dense attention recomputation.\n## Related work\nStreamingDialogue efficiently handles long context, improving the model's long-term memory for conversation history. Existing methods for processing long context in transformer-based models broadly fall into three categories: efficient transformer design, long-term memory enhancement, and length extrapolation techniques.",
            "reference_string": "[268378933 | Li et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context NLP Models",
            "venue": "NLPPOWER",
            "year": 2022,
            "reference_count": 24,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2204.07288",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.07288, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2089185954",
                    "name": "Phyllis Ang"
                },
                {
                    "authorId": "2060730422",
                    "name": "Bhuwan Dhingra"
                },
                {
                    "authorId": "1820939409",
                    "name": "L. Wills"
                }
            ],
            "abstract": "With many real-world applications of Natural Language Processing (NLP) comprising of long texts, there has been a rise in NLP benchmarks that measure the accuracy of models that can handle longer input sequences. However, these benchmarks do not consider the trade-offs between accuracy, speed, and power consumption as input sizes or model sizes are varied. In this work, we perform a systematic study of this accuracy vs. efficiency trade-off on two widely used long-sequence models - Longformer-Encoder-Decoder (LED) and Big Bird - during fine-tuning and inference on four datasets from the SCROLLS benchmark. To study how this trade-off differs across hyperparameter settings, we compare the models across four sequence lengths (1024, 2048, 3072, 4096) and two model sizes (base and large) under a fixed resource budget. We find that LED consistently achieves better accuracy at lower energy costs than Big Bird. For summarization, we find that increasing model size is more energy efficient than increasing sequence length for higher accuracy. However, this comes at the cost of a large drop in inference speed. For question answering, we find that smaller models are both more efficient and more accurate due to the larger training batch sizes possible under a fixed resource budget.",
            "corpus_id": 248218548,
            "sentences": [
                {
                    "corpus_id": "248218548",
                    "title": "Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context NLP Models",
                    "text": "Our main contribution is an analysis of how different sequence lengths affect the trade-off between accuracy, power, and speed in long-context Transformer models during fine-tuning and inference. Since our focus is on long-context NLP tasks, we investigated the following four input sequence lengths: 1024, 2048, 3072, and 4096.",
                    "score": 0.5101122198941723,
                    "section_title": "Methodology",
                    "char_start_offset": 5425,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 328
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.609375
                }
            ],
            "relevance_judgement": 0.609375,
            "relevance_judgment_input_expanded": "# Title: Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context NLP Models\n# Venue: NLPPOWER\n# Authors: Phyllis Ang, Bhuwan Dhingra, L. Wills\n## Abstract\nWith many real-world applications of Natural Language Processing (NLP) comprising of long texts, there has been a rise in NLP benchmarks that measure the accuracy of models that can handle longer input sequences. However, these benchmarks do not consider the trade-offs between accuracy, speed, and power consumption as input sizes or model sizes are varied. In this work, we perform a systematic study of this accuracy vs. efficiency trade-off on two widely used long-sequence models - Longformer-Encoder-Decoder (LED) and Big Bird - during fine-tuning and inference on four datasets from the SCROLLS benchmark. To study how this trade-off differs across hyperparameter settings, we compare the models across four sequence lengths (1024, 2048, 3072, 4096) and two model sizes (base and large) under a fixed resource budget. We find that LED consistently achieves better accuracy at lower energy costs than Big Bird. For summarization, we find that increasing model size is more energy efficient than increasing sequence length for higher accuracy. However, this comes at the cost of a large drop in inference speed. For question answering, we find that smaller models are both more efficient and more accurate due to the larger training batch sizes possible under a fixed resource budget.\n## Methodology\nOur main contribution is an analysis of how different sequence lengths affect the trade-off between accuracy, power, and speed in long-context Transformer models during fine-tuning and inference. Since our focus is on long-context NLP tasks, we investigated the following four input sequence lengths: 1024, 2048, 3072, and 4096.",
            "reference_string": "[248218548 | Ang et al. | 2022 | Citations: 6]"
        },
        {
            "title": "Equipping Transformer with Random-Access Reading for Long-Context Understanding",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.13216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2302927288",
                    "name": "Chenghao Yang"
                },
                {
                    "authorId": "2278970280",
                    "name": "Zi Yang"
                },
                {
                    "authorId": "2278831818",
                    "name": "Nan Hua"
                }
            ],
            "abstract": "Long-context modeling presents a significant challenge for transformer-based large language models (LLMs) due to the quadratic complexity of the self-attention mechanism and issues with length extrapolation caused by pretraining exclusively on short inputs. Existing methods address computational complexity through techniques such as text chunking, the kernel approach, and structured attention, and tackle length extrapolation problems through positional encoding, continued pretraining, and data engineering. These approaches typically require $\\textbf{sequential access}$ to the document, necessitating reading from the first to the last token. We contend that for goal-oriented reading of long documents, such sequential access is not necessary, and a proficiently trained model can learn to omit hundreds of less pertinent tokens. Inspired by human reading behaviors and existing empirical observations, we propose $\\textbf{random access}$, a novel reading strategy that enables transformers to efficiently process long documents without examining every token. Experimental results from pretraining, fine-tuning, and inference phases validate the efficacy of our method.",
            "corpus_id": 269982244,
            "sentences": [],
            "relevance_judgement": 0.59033203125,
            "relevance_judgment_input_expanded": "# Title: Equipping Transformer with Random-Access Reading for Long-Context Understanding\n# Venue: arXiv.org\n# Authors: Chenghao Yang, Zi Yang, Nan Hua\n## Abstract\nLong-context modeling presents a significant challenge for transformer-based large language models (LLMs) due to the quadratic complexity of the self-attention mechanism and issues with length extrapolation caused by pretraining exclusively on short inputs. Existing methods address computational complexity through techniques such as text chunking, the kernel approach, and structured attention, and tackle length extrapolation problems through positional encoding, continued pretraining, and data engineering. These approaches typically require $\\textbf{sequential access}$ to the document, necessitating reading from the first to the last token. We contend that for goal-oriented reading of long documents, such sequential access is not necessary, and a proficiently trained model can learn to omit hundreds of less pertinent tokens. Inspired by human reading behaviors and existing empirical observations, we propose $\\textbf{random access}$, a novel reading strategy that enables transformers to efficiently process long documents without examining every token. Experimental results from pretraining, fine-tuning, and inference phases validate the efficacy of our method.\n",
            "reference_string": "[269982244 | Yang et al. | 2024 | Citations: 1]"
        },
        {
            "title": "\\infty-former: Infinite Memory Transformer",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 54,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.00301, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144869806",
                    "name": "Pedro Henrique Martins"
                },
                {
                    "authorId": "2566656",
                    "name": "Zita Marinho"
                },
                {
                    "authorId": "145644643",
                    "name": "Andr\u00e9 F. T. Martins"
                }
            ],
            "abstract": "Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the \\infty-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \\infty-former\u2019s attention complexity becomes independent of the context length, trading off memory length with precision.In order to control where precision is more important, \\infty-former maintains \u201csticky memories,\u201d being able to model arbitrarily long contexts while keeping the computation budget fixed.Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the \\infty-former\u2019s ability to retain information from long sequences.",
            "corpus_id": 237412971,
            "sentences": [
                {
                    "corpus_id": "237412971",
                    "title": "\\infty-former: Infinite Memory Transformer",
                    "text": "When reading or writing a document, it is important to keep in memory the information previously read or written. Humans have a remarkable ability to remember long-term context, keeping in memory the relevant details (Carroll, 2007;Kuhbandner, 2020). Recently, transformer-based language models have achieved impressive results by increasing the context size (Radford et al., 2018(Radford et al., , 2019;;Dai et al., 2019;Rae et al., 2019;Brown et al., 2020). However, while humans process information sequentially, updating their memories continuously, and recurrent neural networks (RNNs) update a single memory vector during time, transformers do not -they exhaustively query every representation associated to the past events. Thus, the amount of computation they need to perform grows with the length of the context, and, consequently, transformers have computational limitations about how much information can fit into memory. For example, a vanilla transformer requires quadratic time to process an input sequence and linear time to attend to the context when generating every new word. \n\nSeveral variations have been proposed to address this problem (Tay et al., 2020b). Some propose using sparse attention mechanisms, either with data-dependent patterns (Kitaev et al., 2020;Vyas et al., 2020;Tay et al., 2020a;Roy et al., 2021;Wang et al., 2021) or data-independent patterns (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020), reducing the self-attention complexity (Katharopoulos et al., 2020;Choromanski et al., 2021;Peng et al., 2021;Jaegle et al., 2021), and caching past representations in a memory (Dai et al., 2019;Rae et al., 2019). These models are able to reduce the attention complexity, and, consequently, to scale up to longer contexts. However, as their complexity still depends on the context length, they cannot deal with unbounded context.",
                    "score": 0.5087693247219466,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 250
                        },
                        {
                            "start": 251,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 730
                        },
                        {
                            "start": 731,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1093
                        },
                        {
                            "start": 1096,
                            "end": 1178
                        },
                        {
                            "start": 1179,
                            "end": 1661
                        },
                        {
                            "start": 1662,
                            "end": 1770
                        },
                        {
                            "start": 1771,
                            "end": 1877
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 232,
                            "end": 249,
                            "matchedPaperCorpusId": "221320806"
                        },
                        {
                            "start": 405,
                            "end": 422,
                            "matchedPaperCorpusId": "57759363"
                        },
                        {
                            "start": 422,
                            "end": 439,
                            "matchedPaperCorpusId": "207930593"
                        },
                        {
                            "start": 439,
                            "end": 458,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1263,
                            "end": 1284,
                            "matchedPaperCorpusId": "209315300"
                        },
                        {
                            "start": 1284,
                            "end": 1302,
                            "matchedPaperCorpusId": "220424511"
                        },
                        {
                            "start": 1302,
                            "end": 1320,
                            "matchedPaperCorpusId": "211505992"
                        },
                        {
                            "start": 1320,
                            "end": 1337,
                            "matchedPaperCorpusId": "212718077"
                        },
                        {
                            "start": 1337,
                            "end": 1355,
                            "matchedPaperCorpusId": "221655697"
                        },
                        {
                            "start": 1487,
                            "end": 1515,
                            "matchedPaperCorpusId": "220250819"
                        },
                        {
                            "start": 1515,
                            "end": 1540,
                            "matchedPaperCorpusId": "222067132"
                        },
                        {
                            "start": 1540,
                            "end": 1558,
                            "matchedPaperCorpusId": "232105052"
                        },
                        {
                            "start": 1625,
                            "end": 1643,
                            "matchedPaperCorpusId": "57759363"
                        },
                        {
                            "start": 1643,
                            "end": 1660,
                            "matchedPaperCorpusId": "207930593"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.58251953125
                }
            ],
            "relevance_judgement": 0.58251953125,
            "relevance_judgment_input_expanded": "# Title: \\infty-former: Infinite Memory Transformer\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Pedro Henrique Martins, Zita Marinho, Andr\u00e9 F. T. Martins\n## Abstract\nTransformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the \\infty-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \\infty-former\u2019s attention complexity becomes independent of the context length, trading off memory length with precision.In order to control where precision is more important, \\infty-former maintains \u201csticky memories,\u201d being able to model arbitrarily long contexts while keeping the computation budget fixed.Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the \\infty-former\u2019s ability to retain information from long sequences.\n## Introduction\nWhen reading or writing a document, it is important to keep in memory the information previously read or written. Humans have a remarkable ability to remember long-term context, keeping in memory the relevant details (Carroll, 2007;Kuhbandner, 2020). Recently, transformer-based language models have achieved impressive results by increasing the context size (Radford et al., 2018(Radford et al., , 2019;;Dai et al., 2019;Rae et al., 2019;Brown et al., 2020). However, while humans process information sequentially, updating their memories continuously, and recurrent neural networks (RNNs) update a single memory vector during time, transformers do not -they exhaustively query every representation associated to the past events. Thus, the amount of computation they need to perform grows with the length of the context, and, consequently, transformers have computational limitations about how much information can fit into memory. For example, a vanilla transformer requires quadratic time to process an input sequence and linear time to attend to the context when generating every new word. \n\nSeveral variations have been proposed to address this problem (Tay et al., 2020b). Some propose using sparse attention mechanisms, either with data-dependent patterns (Kitaev et al., 2020;Vyas et al., 2020;Tay et al., 2020a;Roy et al., 2021;Wang et al., 2021) or data-independent patterns (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020), reducing the self-attention complexity (Katharopoulos et al., 2020;Choromanski et al., 2021;Peng et al., 2021;Jaegle et al., 2021), and caching past representations in a memory (Dai et al., 2019;Rae et al., 2019). These models are able to reduce the attention complexity, and, consequently, to scale up to longer contexts. However, as their complexity still depends on the context length, they cannot deal with unbounded context.",
            "reference_string": "[237412971 | Martins et al. | 2021 | Citations: 11]"
        },
        {
            "title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models",
            "venue": "International Conference on Computational Linguistics",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.20262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2955690",
                    "name": "Thibaut Thonet"
                },
                {
                    "authorId": "120419790",
                    "name": "Jos Rozen"
                },
                {
                    "authorId": "2259359878",
                    "name": "Laurent Besacier"
                }
            ],
            "abstract": "Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending the models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, we propose a new benchmark for long-context LLMs focused on a practical meeting assistant scenario in which the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data. Our benchmark, ELITR-Bench, augments the existing ELITR corpus by adding 271 manually crafted questions with their ground-truth answers, as well as noisy versions of meeting transcripts altered to target different Word Error Rate levels. Our experiments with 12 long-context LLMs on ELITR-Bench confirm the progress made across successive generations of both proprietary and open models, and point out their discrepancies in terms of robustness to transcript noise. We also provide a thorough analysis of our GPT-4-based evaluation, including insights from a crowdsourcing study. Our findings indicate that while GPT-4's scores align with human judges, its ability to distinguish beyond three score levels may be limited.",
            "corpus_id": 268793522,
            "sentences": [
                {
                    "corpus_id": "268793522",
                    "title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models",
                    "text": "The remainder of the paper is structured as follows. We provide a review of related literature in Section 2, before introducing the proposed ELITR-Bench in Section 3. \n\nWe then describe our experimental setup and results in Sections 4 and 5, respectively. Section 6 provides an in-depth assessment of our LLM-based evaluation methodology. Finally, Section 7 concludes the paper and provides some perspectives for future work. \n\ncontext modeling. 3 While an exhaustive survey of these methods is beyond the scope of this paper, they can generally be categorized into three main groups (excluding other distinct approaches such as retrieval-augmented generation [46] and context compression [10]): (a) the development of efficient transformer architectures to address the quadratic attention challenge, including sparse transformers [6,12,32,47], linear transformers [13,19,42], and hierarchical transformers [20,30,44]; (b) approaches like recurrent attention networks [7,14,36] and state-space models [16,41]; (c) length extrapolation or position embedding interpolation, where LLMs are fine-tuned or adapted at inference time to adjust tokens' positions to match the new context length [4,8,9,28,35,37,45]. These techniques also contributed to the context length expansion in proprietary models like GPT-4 (32K-128K), Claude-3 (200k), and Gemini-1.5 (128K-1M). \n\nLong-context benchmarks. Several benchmarks have recently emerged with the growing interest in evaluating techniques that extend the context length of LLMs. Long Range Arena [40] was proposed to assess the quality of efficient transformer models in longcontext scenarios, covering 1K-16K tokens sequences through different data types and modalities. L-Eval [1] offers a comprehensive evaluation suite with 20 subtasks and over 2,000 human-labeled query-response pairs, aggregating pre-existing datasets like Narra-tiveQA [22]. LongEval [25] proposes synthetic tasks of varying difficulty, while LongBench [3] and LongBench-Chat [4] aggregate several datasets in English and Chinese.",
                    "score": 0.517488836494415,
                    "section_title": "Introduction",
                    "char_start_offset": 2184,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 52
                        },
                        {
                            "start": 53,
                            "end": 166
                        },
                        {
                            "start": 169,
                            "end": 255
                        },
                        {
                            "start": 256,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 425
                        },
                        {
                            "start": 428,
                            "end": 447
                        },
                        {
                            "start": 448,
                            "end": 1207
                        },
                        {
                            "start": 1208,
                            "end": 1361
                        },
                        {
                            "start": 1364,
                            "end": 1388
                        },
                        {
                            "start": 1389,
                            "end": 1520
                        },
                        {
                            "start": 1521,
                            "end": 1713
                        },
                        {
                            "start": 1714,
                            "end": 1890
                        },
                        {
                            "start": 1891,
                            "end": 2046
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 660,
                            "end": 664,
                            "matchedPaperCorpusId": "263620134"
                        },
                        {
                            "start": 689,
                            "end": 693,
                            "matchedPaperCorpusId": "258865249"
                        },
                        {
                            "start": 837,
                            "end": 840,
                            "matchedPaperCorpusId": "219636190"
                        },
                        {
                            "start": 840,
                            "end": 843,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 865,
                            "end": 869,
                            "matchedPaperCorpusId": "222067132"
                        },
                        {
                            "start": 869,
                            "end": 872,
                            "matchedPaperCorpusId": "220250819"
                        },
                        {
                            "start": 914,
                            "end": 917,
                            "matchedPaperCorpusId": "235294151"
                        },
                        {
                            "start": 971,
                            "end": 974,
                            "matchedPaperCorpusId": "57759363"
                        },
                        {
                            "start": 974,
                            "end": 977,
                            "matchedPaperCorpusId": "258832459"
                        },
                        {
                            "start": 1200,
                            "end": 1203,
                            "matchedPaperCorpusId": "261493986"
                        },
                        {
                            "start": 1538,
                            "end": 1542,
                            "matchedPaperCorpusId": "260440449"
                        },
                        {
                            "start": 1885,
                            "end": 1889,
                            "matchedPaperCorpusId": "2593903"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56689453125
                }
            ],
            "relevance_judgement": 0.56689453125,
            "relevance_judgment_input_expanded": "# Title: ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models\n# Venue: International Conference on Computational Linguistics\n# Authors: Thibaut Thonet, Jos Rozen, Laurent Besacier\n## Abstract\nResearch on Large Language Models (LLMs) has recently witnessed an increasing interest in extending the models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, we propose a new benchmark for long-context LLMs focused on a practical meeting assistant scenario in which the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data. Our benchmark, ELITR-Bench, augments the existing ELITR corpus by adding 271 manually crafted questions with their ground-truth answers, as well as noisy versions of meeting transcripts altered to target different Word Error Rate levels. Our experiments with 12 long-context LLMs on ELITR-Bench confirm the progress made across successive generations of both proprietary and open models, and point out their discrepancies in terms of robustness to transcript noise. We also provide a thorough analysis of our GPT-4-based evaluation, including insights from a crowdsourcing study. Our findings indicate that while GPT-4's scores align with human judges, its ability to distinguish beyond three score levels may be limited.\n## Introduction\nThe remainder of the paper is structured as follows. We provide a review of related literature in Section 2, before introducing the proposed ELITR-Bench in Section 3. \n\nWe then describe our experimental setup and results in Sections 4 and 5, respectively. Section 6 provides an in-depth assessment of our LLM-based evaluation methodology. Finally, Section 7 concludes the paper and provides some perspectives for future work. \n\ncontext modeling. 3 While an exhaustive survey of these methods is beyond the scope of this paper, they can generally be categorized into three main groups (excluding other distinct approaches such as retrieval-augmented generation [46] and context compression [10]): (a) the development of efficient transformer architectures to address the quadratic attention challenge, including sparse transformers [6,12,32,47], linear transformers [13,19,42], and hierarchical transformers [20,30,44]; (b) approaches like recurrent attention networks [7,14,36] and state-space models [16,41]; (c) length extrapolation or position embedding interpolation, where LLMs are fine-tuned or adapted at inference time to adjust tokens' positions to match the new context length [4,8,9,28,35,37,45]. These techniques also contributed to the context length expansion in proprietary models like GPT-4 (32K-128K), Claude-3 (200k), and Gemini-1.5 (128K-1M). \n\nLong-context benchmarks. Several benchmarks have recently emerged with the growing interest in evaluating techniques that extend the context length of LLMs. Long Range Arena [40] was proposed to assess the quality of efficient transformer models in longcontext scenarios, covering 1K-16K tokens sequences through different data types and modalities. L-Eval [1] offers a comprehensive evaluation suite with 20 subtasks and over 2,000 human-labeled query-response pairs, aggregating pre-existing datasets like Narra-tiveQA [22]. LongEval [25] proposes synthetic tasks of varying difficulty, while LongBench [3] and LongBench-Chat [4] aggregate several datasets in English and Chinese.",
            "reference_string": "[268793522 | Thonet et al. | 2024 | Citations: 3]"
        },
        {
            "title": "CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13449, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116458151",
                    "name": "Zexue He"
                },
                {
                    "authorId": "2142741421",
                    "name": "Leonid Karlinsky"
                },
                {
                    "authorId": "2266361991",
                    "name": "Donghyun Kim"
                },
                {
                    "authorId": "2258962117",
                    "name": "Julian McAuley"
                },
                {
                    "authorId": "2284865239",
                    "name": "Dmitry Krotov"
                },
                {
                    "authorId": "2239199018",
                    "name": "Rog\u00e9rio Feris"
                }
            ],
            "abstract": "Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs. Memory-augmented models have emerged as a promising solution to this problem, but current methods are hindered by limited memory capacity and require costly re-training to integrate with a new LLM. In this work, we introduce an associative memory module which can be coupled to any pre-trained (frozen) attention-based LLM without re-training, enabling it to handle arbitrarily long input sequences. Unlike previous methods, our associative memory module consolidates representations of individual tokens into a non-parametric distribution model, dynamically managed by properly balancing the novelty and recency of the incoming data. By retrieving information from this consolidated associative memory, the base LLM can achieve significant (up to 29.7% on Arxiv) perplexity reduction in long-context modeling compared to other baselines evaluated on standard benchmarks. This architecture, which we call CAMELoT (Consolidated Associative Memory Enhanced Long Transformer), demonstrates superior performance even with a tiny context window of 128 tokens, and also enables improved in-context learning with a much larger set of demonstrations.",
            "corpus_id": 267770311,
            "sentences": [
                {
                    "corpus_id": "267770311",
                    "title": "CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory",
                    "text": "Long-range self-attention. Many efficient self-attention techniques have been proposed for long context modeling in transformer models, including low-rank factorization (Wang et al., 2020), local attention (Ramachandran et al., 2019), dilated attention (Ding et al., 2023), sparsity (Beltagy et al., 2020;Zaheer et al., 2020;Kitaev et al., 2020), and hardwareaware attention mechanisms such as FlashAttention (Dao et al., 2022;Dao, 2023). Despite notable progress, these methods are unable to handle unbounded context window sizes and struggle to retrieve information in the middle of the input (Liu et al., 2023). They can be used in tandem with our proposed approach for longer context modeling. \n\nMemory-augmented LLMs. Memory-augmented language models have emerged as a promising way to model extended context window sizes (Packer et al., 2023;Dai et al., 2019;Wu et al., 2022;Tworkowski et al., 2023;Weston et al., 2014). In particular, Wu et al. (2022) show that a kNN lookup into a memory cache bank containing (key, value) pairs of past inputs can improve language modeling. Tworkowski et al. (2023) further improved this approach using contrastive learning. In the same vein, Wang et al. (2023) addressed the memory staleness limitation of these works by training a side network model, while keeping the LLM frozen. Unlike these methods, our approach relies on consolidated representations of past tokens which are dynamically updated, enabling the context window to be arbitrarily large, without being limited by the number of memory slots. Moreover, different from these approaches, our method is training-free (memory updates occur solely at runtime), making it easier to integrate our memory module into any existing LLM architecture. (Ge et al., 2023;Mu et al., 2023;Chevalier et al., 2023) have been recently explored for extending the context length in transformer models.",
                    "score": 0.5375505390820658,
                    "section_title": "Related Work",
                    "char_start_offset": 5301,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 26
                        },
                        {
                            "start": 27,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 614
                        },
                        {
                            "start": 615,
                            "end": 697
                        },
                        {
                            "start": 700,
                            "end": 722
                        },
                        {
                            "start": 723,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1082
                        },
                        {
                            "start": 1083,
                            "end": 1166
                        },
                        {
                            "start": 1167,
                            "end": 1324
                        },
                        {
                            "start": 1325,
                            "end": 1550
                        },
                        {
                            "start": 1551,
                            "end": 1747
                        },
                        {
                            "start": 1748,
                            "end": 1888
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 305,
                            "end": 325,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 409,
                            "end": 427,
                            "matchedPaperCorpusId": "249151871"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55859375
                }
            ],
            "relevance_judgement": 0.55859375,
            "relevance_judgment_input_expanded": "# Title: CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory\n# Venue: arXiv.org\n# Authors: Zexue He, Leonid Karlinsky, Donghyun Kim, Julian McAuley, Dmitry Krotov, Rog\u00e9rio Feris\n## Abstract\nLarge Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs. Memory-augmented models have emerged as a promising solution to this problem, but current methods are hindered by limited memory capacity and require costly re-training to integrate with a new LLM. In this work, we introduce an associative memory module which can be coupled to any pre-trained (frozen) attention-based LLM without re-training, enabling it to handle arbitrarily long input sequences. Unlike previous methods, our associative memory module consolidates representations of individual tokens into a non-parametric distribution model, dynamically managed by properly balancing the novelty and recency of the incoming data. By retrieving information from this consolidated associative memory, the base LLM can achieve significant (up to 29.7% on Arxiv) perplexity reduction in long-context modeling compared to other baselines evaluated on standard benchmarks. This architecture, which we call CAMELoT (Consolidated Associative Memory Enhanced Long Transformer), demonstrates superior performance even with a tiny context window of 128 tokens, and also enables improved in-context learning with a much larger set of demonstrations.\n## Related Work\nLong-range self-attention. Many efficient self-attention techniques have been proposed for long context modeling in transformer models, including low-rank factorization (Wang et al., 2020), local attention (Ramachandran et al., 2019), dilated attention (Ding et al., 2023), sparsity (Beltagy et al., 2020;Zaheer et al., 2020;Kitaev et al., 2020), and hardwareaware attention mechanisms such as FlashAttention (Dao et al., 2022;Dao, 2023). Despite notable progress, these methods are unable to handle unbounded context window sizes and struggle to retrieve information in the middle of the input (Liu et al., 2023). They can be used in tandem with our proposed approach for longer context modeling. \n\nMemory-augmented LLMs. Memory-augmented language models have emerged as a promising way to model extended context window sizes (Packer et al., 2023;Dai et al., 2019;Wu et al., 2022;Tworkowski et al., 2023;Weston et al., 2014). In particular, Wu et al. (2022) show that a kNN lookup into a memory cache bank containing (key, value) pairs of past inputs can improve language modeling. Tworkowski et al. (2023) further improved this approach using contrastive learning. In the same vein, Wang et al. (2023) addressed the memory staleness limitation of these works by training a side network model, while keeping the LLM frozen. Unlike these methods, our approach relies on consolidated representations of past tokens which are dynamically updated, enabling the context window to be arbitrarily large, without being limited by the number of memory slots. Moreover, different from these approaches, our method is training-free (memory updates occur solely at runtime), making it easier to integrate our memory module into any existing LLM architecture. (Ge et al., 2023;Mu et al., 2023;Chevalier et al., 2023) have been recently explored for extending the context length in transformer models.",
            "reference_string": "[267770311 | He et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Long Context Compression with Activation Beacon",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 34,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.03462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153419738",
                    "name": "Peitian Zhang"
                },
                {
                    "authorId": "2240687341",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2051175765",
                    "name": "Shitao Xiao"
                },
                {
                    "authorId": "2278431511",
                    "name": "Ninglu Shao"
                },
                {
                    "authorId": "2278433956",
                    "name": "Qiwei Ye"
                },
                {
                    "authorId": "2257039188",
                    "name": "Zhicheng Dou"
                }
            ],
            "abstract": "Long context compression is a critical research problem due to its significance in reducing the high computational and memory costs associated with LLMs. In this paper, we propose Activation Beacon, a plug-in module for transformer-based LLMs that targets effective, efficient, and flexible compression of long contexts. To achieve this, our method introduces the following technical designs. 1) We directly compress the activations (i.e. keys and values at every layer), rather than leveraging soft prompts to relay information (which constitute a major bottleneck to encapsulate the complex information within long contexts). 2) We tailor the compression workflow, where each fine-grained input unit is progressively compressed, enabling high-quality compression and efficient computation during both training and inference. 3) We train the model through compression-based auto-regression, making full use of plain texts and instructional data to optimize the model's compression performance. 4) During training, we randomly sample a compression ratio at each step, teaching the model to support a wide range of compression configurations. Extensive evaluations are conducted on various long-context tasks whose lengths (e.g., 128K) may far exceed the maximum training length (20K), such as document understanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing methods struggle to handle these challenging tasks, Activation Beacon maintains a comparable performance to the uncompressed baseline across various scenarios, achieving a 2x acceleration in inference time and an 8x reduction of memory costs for KV cache. Our data, model, and code have been released at \\url{https://github.com/FlagOpen/FlagEmbedding/}.",
            "corpus_id": 266844488,
            "sentences": [
                {
                    "corpus_id": "266844488",
                    "title": "Long Context Compression with Activation Beacon",
                    "text": "Large language models (LLMs) need to process long contexts to accomplish many important tasks, such as long-document understanding [27], long-content creation [4], and long-term memorization/reasoning [41]. To address these needs, modern LLMs are built with extended context windows (e.g., 128K) that enable remarkable long-context processing capabilities [33; 39; 18]. Despite their effectiveness, LLMs encounter efficiency challenges in processing long contexts. On one hand, transformer-based LLMs incur substantial computational costs due to the quadratic complexity of self attention. On the other hand, they require tremendous GPU memory to hold the KV cache of the entire sequence for faster decoding. Both computation and memory costs increase as the context length grows. \n\nA wide array of studies are dedicated to alleviating efficiency issues, among which context compression is a promising direction [32; 11; 20; 24; 25]. This approach aims to compress raw input into more concise representations, allowing the generation process to be conditioned on a shorter context. Therefore, it helps to reduce both computation cost of inference and memory cost from KV cache, while also enabling the processing of longer inputs than the LLM's built-in context window. \n\nDespite the current progresses, it it remains a tough challenge to compress long contexts. Specifically, existing methods usually summarize the context into a few soft tokens [11; 20], which constitute the major bottleneck to summarize the complex information within long contexts. Besides, they try to compress the context \"all-at-once\", lacking a fine-grained handling of the detailed information. Moreover, these soft tokens must be re-encoded before generation, resulting in inferior efficiency in both training and inference. Lastly, these methods are learned to compress with a fixed number of soft tokens, thus, it's hard to customize the compression ratio for downstream tasks. While some alternamtive methods focus on deleting unimportant tokens [25; 31], they depend on the input question to estimate the token importance, limiting their efficiency in real-world multi-turn scenarios.",
                    "score": 0.5274210533800986,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 206
                        },
                        {
                            "start": 207,
                            "end": 369
                        },
                        {
                            "start": 370,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 589
                        },
                        {
                            "start": 590,
                            "end": 708
                        },
                        {
                            "start": 709,
                            "end": 780
                        },
                        {
                            "start": 783,
                            "end": 933
                        },
                        {
                            "start": 934,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1269
                        },
                        {
                            "start": 1272,
                            "end": 1362
                        },
                        {
                            "start": 1363,
                            "end": 1553
                        },
                        {
                            "start": 1554,
                            "end": 1671
                        },
                        {
                            "start": 1672,
                            "end": 1802
                        },
                        {
                            "start": 1803,
                            "end": 1957
                        },
                        {
                            "start": 1958,
                            "end": 2166
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5576171875
                }
            ],
            "relevance_judgement": 0.5576171875,
            "relevance_judgment_input_expanded": "# Title: Long Context Compression with Activation Beacon\n# Venue: International Conference on Learning Representations\n# Authors: Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, Zhicheng Dou\n## Abstract\nLong context compression is a critical research problem due to its significance in reducing the high computational and memory costs associated with LLMs. In this paper, we propose Activation Beacon, a plug-in module for transformer-based LLMs that targets effective, efficient, and flexible compression of long contexts. To achieve this, our method introduces the following technical designs. 1) We directly compress the activations (i.e. keys and values at every layer), rather than leveraging soft prompts to relay information (which constitute a major bottleneck to encapsulate the complex information within long contexts). 2) We tailor the compression workflow, where each fine-grained input unit is progressively compressed, enabling high-quality compression and efficient computation during both training and inference. 3) We train the model through compression-based auto-regression, making full use of plain texts and instructional data to optimize the model's compression performance. 4) During training, we randomly sample a compression ratio at each step, teaching the model to support a wide range of compression configurations. Extensive evaluations are conducted on various long-context tasks whose lengths (e.g., 128K) may far exceed the maximum training length (20K), such as document understanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing methods struggle to handle these challenging tasks, Activation Beacon maintains a comparable performance to the uncompressed baseline across various scenarios, achieving a 2x acceleration in inference time and an 8x reduction of memory costs for KV cache. Our data, model, and code have been released at \\url{https://github.com/FlagOpen/FlagEmbedding/}.\n## Introduction\nLarge language models (LLMs) need to process long contexts to accomplish many important tasks, such as long-document understanding [27], long-content creation [4], and long-term memorization/reasoning [41]. To address these needs, modern LLMs are built with extended context windows (e.g., 128K) that enable remarkable long-context processing capabilities [33; 39; 18]. Despite their effectiveness, LLMs encounter efficiency challenges in processing long contexts. On one hand, transformer-based LLMs incur substantial computational costs due to the quadratic complexity of self attention. On the other hand, they require tremendous GPU memory to hold the KV cache of the entire sequence for faster decoding. Both computation and memory costs increase as the context length grows. \n\nA wide array of studies are dedicated to alleviating efficiency issues, among which context compression is a promising direction [32; 11; 20; 24; 25]. This approach aims to compress raw input into more concise representations, allowing the generation process to be conditioned on a shorter context. Therefore, it helps to reduce both computation cost of inference and memory cost from KV cache, while also enabling the processing of longer inputs than the LLM's built-in context window. \n\nDespite the current progresses, it it remains a tough challenge to compress long contexts. Specifically, existing methods usually summarize the context into a few soft tokens [11; 20], which constitute the major bottleneck to summarize the complex information within long contexts. Besides, they try to compress the context \"all-at-once\", lacking a fine-grained handling of the detailed information. Moreover, these soft tokens must be re-encoded before generation, resulting in inferior efficiency in both training and inference. Lastly, these methods are learned to compress with a fixed number of soft tokens, thus, it's hard to customize the compression ratio for downstream tasks. While some alternamtive methods focus on deleting unimportant tokens [25; 31], they depend on the input question to estimate the token importance, limiting their efficiency in real-world multi-turn scenarios.",
            "reference_string": "[266844488 | Zhang et al. | 2024 | Citations: 34]"
        },
        {
            "title": "Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation",
            "venue": "International Conference on Learning Representations",
            "year": 2025,
            "reference_count": 24,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.12637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2356154321",
                    "name": "Linda He"
                },
                {
                    "authorId": "2252087284",
                    "name": "Jue Wang"
                },
                {
                    "authorId": "2110605429",
                    "name": "Maurice Weber"
                },
                {
                    "authorId": "2356004549",
                    "name": "Shang Zhu"
                },
                {
                    "authorId": "2304481349",
                    "name": "Ben Athiwaratkun"
                },
                {
                    "authorId": "2305565297",
                    "name": "Ce Zhang"
                }
            ],
            "abstract": "Large Language Models (LLMs) struggle with long-context reasoning, not only due to the quadratic scaling of computational complexity with sequence length but also because of the scarcity and expense of annotating long-context data. There has been barely any open-source work that systematically ablates long-context data, nor is there any openly available instruction tuning dataset with contexts surpassing 100K tokens. To bridge this gap, we introduce a novel post-training synthetic data generation strategy designed to efficiently extend the context window of LLMs while preserving their general task performance. Our approach scalably extends to arbitrarily long context lengths, unconstrained by the length of available real-world data, which effectively addresses the scarcity of raw long-context data. Through a step-by-step rotary position embedding (RoPE) scaling training strategy, we demonstrate that our model, with a context length of up to 1M tokens, performs well on the RULER benchmark and InfiniteBench and maintains robust performance on general language tasks.",
            "corpus_id": 277857470,
            "sentences": [
                {
                    "corpus_id": "277857470",
                    "title": "Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation",
                    "text": "Adapting transformers to enable longer context capabilities is a critical area of research in natural language processing. This effort primarily focuses on three key directions: (1) architectural modifications to the transformer model itself, (2) improvements in positional embedding techniques, and \n\n(3) the development and utilization of more extensive long-context datasets. \n\nEfficient Attention Mechanisms. To address the quadratic computational and memory demands of standard transformer self-attention, researchers have developed various architectural modifica-tions to improve efficiency and extend context lengths. Notable examples include Longformer (Beltagy et al., 2020), which combines sliding window and global attention, and BlockTransformer (Ho et al., 2024), which employs hierarchical global-to-local modeling. Linear Attention methods (Katharopoulos et al., 2020) reformulate self-attention for linear complexity, while InfiniteTransformer (Munkhdalai et al., 2024) incorporates unbounded long-term memory through continuousspace attention. State space models like Mamba (Gu & Dao, 2024) capture long-range dependencies efficiently without explicit attention mechanisms. Despite these advancements, bridging the gap with high-quality data remains a critical challenge and is the focus of this work. \n\nPosition Embedding Extension. Advances in positional encoding methods have enabled language models to handle longer sequences effectively. Techniques like RoPE (Su et al., 2023), ALiBi (Press et al., 2022), and xPos (Sun et al., 2022) have emerged as prominent solutions. RoPE has gained widespread adoption in LLaMA (Touvron et al., 2023), b) and PaLM (Anil et al., 2023), due to its ability to represent relative positions and its theoretical grounding in the complex plane.",
                    "score": 0.533057279961101,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 5526,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 122
                        },
                        {
                            "start": 123,
                            "end": 299
                        },
                        {
                            "start": 302,
                            "end": 378
                        },
                        {
                            "start": 381,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 624
                        },
                        {
                            "start": 625,
                            "end": 829
                        },
                        {
                            "start": 830,
                            "end": 1060
                        },
                        {
                            "start": 1061,
                            "end": 1190
                        },
                        {
                            "start": 1191,
                            "end": 1318
                        },
                        {
                            "start": 1321,
                            "end": 1350
                        },
                        {
                            "start": 1351,
                            "end": 1459
                        },
                        {
                            "start": 1460,
                            "end": 1592
                        },
                        {
                            "start": 1593,
                            "end": 1797
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5537109375
                }
            ],
            "relevance_judgement": 0.5537109375,
            "relevance_judgment_input_expanded": "# Title: Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation\n# Venue: International Conference on Learning Representations\n# Authors: Linda He, Jue Wang, Maurice Weber, Shang Zhu, Ben Athiwaratkun, Ce Zhang\n## Abstract\nLarge Language Models (LLMs) struggle with long-context reasoning, not only due to the quadratic scaling of computational complexity with sequence length but also because of the scarcity and expense of annotating long-context data. There has been barely any open-source work that systematically ablates long-context data, nor is there any openly available instruction tuning dataset with contexts surpassing 100K tokens. To bridge this gap, we introduce a novel post-training synthetic data generation strategy designed to efficiently extend the context window of LLMs while preserving their general task performance. Our approach scalably extends to arbitrarily long context lengths, unconstrained by the length of available real-world data, which effectively addresses the scarcity of raw long-context data. Through a step-by-step rotary position embedding (RoPE) scaling training strategy, we demonstrate that our model, with a context length of up to 1M tokens, performs well on the RULER benchmark and InfiniteBench and maintains robust performance on general language tasks.\n## RELATED WORK\nAdapting transformers to enable longer context capabilities is a critical area of research in natural language processing. This effort primarily focuses on three key directions: (1) architectural modifications to the transformer model itself, (2) improvements in positional embedding techniques, and \n\n(3) the development and utilization of more extensive long-context datasets. \n\nEfficient Attention Mechanisms. To address the quadratic computational and memory demands of standard transformer self-attention, researchers have developed various architectural modifica-tions to improve efficiency and extend context lengths. Notable examples include Longformer (Beltagy et al., 2020), which combines sliding window and global attention, and BlockTransformer (Ho et al., 2024), which employs hierarchical global-to-local modeling. Linear Attention methods (Katharopoulos et al., 2020) reformulate self-attention for linear complexity, while InfiniteTransformer (Munkhdalai et al., 2024) incorporates unbounded long-term memory through continuousspace attention. State space models like Mamba (Gu & Dao, 2024) capture long-range dependencies efficiently without explicit attention mechanisms. Despite these advancements, bridging the gap with high-quality data remains a critical challenge and is the focus of this work. \n\nPosition Embedding Extension. Advances in positional encoding methods have enabled language models to handle longer sequences effectively. Techniques like RoPE (Su et al., 2023), ALiBi (Press et al., 2022), and xPos (Sun et al., 2022) have emerged as prominent solutions. RoPE has gained widespread adoption in LLaMA (Touvron et al., 2023), b) and PaLM (Anil et al., 2023), due to its ability to represent relative positions and its theoretical grounding in the complex plane.",
            "reference_string": "[277857470 | He et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Blockwise Parallel Transformer for Large Context Models",
            "venue": "",
            "year": 2023,
            "reference_count": 61,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.19370, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2143856672",
                    "name": "Hao Liu"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                }
            ],
            "abstract": "Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.",
            "corpus_id": 258987968,
            "sentences": [
                {
                    "corpus_id": "258987968",
                    "title": "Blockwise Parallel Transformer for Large Context Models",
                    "text": "In conclusion, we propose a blockwise parallelization approach to reduce the memory requirements of Transformers, the backbone of state-of-the-art NLP models. Our approach enables processing longer input sequences while maintaining or improving performance. Through extensive experiments, we demonstrate its effectiveness, achieving up to 4x memory reduction than memory-efficient Transformers. Our contributions include a practical method for large context sizes in large Transformer models. With the increasing capability of hardware, larger models and longer context length are widely used in AI research. At the same time, as we are pushing up against physics and fabrication limits, it is more important to design scaling approaches as efficient as possible to scale up large models and large context size. Our approach holds promise for training and evaluating complex models with longer input sequences, potentially driving new breakthroughs in machine learning research. \n\nLimitations and Future Work. Although our method achieves state-of-the-art low memory usage for Transformer models, it does have some limitations that need to be addressed: \n\n\u2022 Optimal performance. While our implementation prioritizes simplicity with high-level Jax operations, optimizing low-level operations is crucial for achieving optimal performance. In future work, we suggest considering porting our method to CUDA and OpenAI Triton to achieve minimal memory cost and maximum speedup. \n\ntotal number of tokens) can be computationally slow. To reduce the sampling time, we limited the rollout to 16 trajectories.",
                    "score": 0.5152441020817581,
                    "section_title": "Conclusion",
                    "char_start_offset": 22929,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 394
                        },
                        {
                            "start": 395,
                            "end": 492
                        },
                        {
                            "start": 493,
                            "end": 608
                        },
                        {
                            "start": 609,
                            "end": 811
                        },
                        {
                            "start": 812,
                            "end": 978
                        },
                        {
                            "start": 981,
                            "end": 1009
                        },
                        {
                            "start": 1010,
                            "end": 1153
                        },
                        {
                            "start": 1156,
                            "end": 1178
                        },
                        {
                            "start": 1179,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1472
                        },
                        {
                            "start": 1475,
                            "end": 1527
                        },
                        {
                            "start": 1528,
                            "end": 1599
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53271484375
                },
                {
                    "corpus_id": "258987968",
                    "title": "Blockwise Parallel Transformer for Large Context Models",
                    "text": "Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.",
                    "score": 0.5204356573180369,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.424072265625
                },
                {
                    "corpus_id": "258987968",
                    "title": "Blockwise Parallel Transformer for Large Context Models",
                    "text": "Transformers [52] have become the backbone of many state-of-the-art natural language processing models [15,43,5,35]. They have demonstrated impressive performance across a wide range of AI problems, including language modeling, machine translation, image captioning, and protein folding [39,47,32,43,5,45,9]. Transformers achieve this success through their architecture design that uses self-attention and position-wise feedforward mechanisms. These components facilitate the efficient capture of long-range dependencies between input tokens, enabling scalability in terms of context length and model size through highly parallel computations. \n\nHowever, the memory requirements of Transformers limit their ability to handle long sequences, which is necessary for many AI problems, such as high-resolution images, podcasts, code, or books and especially those that involve multiple long sequences or long-term dependencies [10,7,39,7,34,29,47,32,1]. The quadratic self-attention and the large feed forward network of Transformers require a large amount of memory, which makes it challenging to scale to longer input sequences. This limitation has led to various techniques proposed to reduce the memory requirements of Transformers, including sparse-approximation, low-rank approximation, and low precision approximation [see e.g. 51,24,22,11,25,36,54]. \n\nOne distinct line of research does not rely on approximation but instead focuses on computing exact self-attention with linear memory complexity. This approach leverages the observation that the softmax matrix in self-attention can be computed without materializing the full matrix [37]. This technique has led to the development of FlashAttention [14] and Memory Efficient Attention [42]. Both methods propose a blockwise computation of the self-attention softmax, demonstrating reduced memory requirements. (A), (B), and (C) show evaluation using one, eight A100, and 64 TPUv4, respectively, with a single sequence. Our method enables training sequences 32 times longer than vanilla attention-based Transformer [52], and 2 to 4 times longer than FlashAttention [14] and Memory Efficient Attention [42]. Section 3.1 provides a memory cost breakdown.",
                    "score": 0.5265801252180842,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 643
                        },
                        {
                            "start": 646,
                            "end": 1126
                        },
                        {
                            "start": 1127,
                            "end": 1330
                        },
                        {
                            "start": 1331,
                            "end": 1353
                        },
                        {
                            "start": 1356,
                            "end": 1501
                        },
                        {
                            "start": 1502,
                            "end": 1643
                        },
                        {
                            "start": 1644,
                            "end": 1745
                        },
                        {
                            "start": 1746,
                            "end": 1864
                        },
                        {
                            "start": 1865,
                            "end": 1973
                        },
                        {
                            "start": 1974,
                            "end": 2160
                        },
                        {
                            "start": 2161,
                            "end": 2206
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 107,
                            "end": 110,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 110,
                            "end": 112,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 291,
                            "end": 294,
                            "matchedPaperCorpusId": "237260635"
                        },
                        {
                            "start": 294,
                            "end": 297,
                            "matchedPaperCorpusId": "246527904"
                        },
                        {
                            "start": 297,
                            "end": 300,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 300,
                            "end": 302,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 302,
                            "end": 305,
                            "matchedPaperCorpusId": "231939146"
                        },
                        {
                            "start": 940,
                            "end": 943,
                            "matchedPaperCorpusId": "237260635"
                        },
                        {
                            "start": 943,
                            "end": 946,
                            "matchedPaperCorpusId": "246527904"
                        },
                        {
                            "start": 946,
                            "end": 948,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 1331,
                            "end": 1334,
                            "matchedPaperCorpusId": "221702858"
                        },
                        {
                            "start": 1334,
                            "end": 1337,
                            "matchedPaperCorpusId": "232110866"
                        },
                        {
                            "start": 1704,
                            "end": 1708,
                            "matchedPaperCorpusId": "249151871"
                        },
                        {
                            "start": 2119,
                            "end": 2123,
                            "matchedPaperCorpusId": "249151871"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.380859375
                },
                {
                    "corpus_id": "258987968",
                    "title": "Blockwise Parallel Transformer for Large Context Models",
                    "text": "Section 3.1 provides a memory cost breakdown. \n\nDespite the resulting reduced memory requirements of the self-attention block in Transformer models, a significant challenge still arises from the feedforward network. This network contains a large number of parameters and produces high-dimensional intermediate vectors, resulting in substantial memory requirements. This issue is becomes the key memory challenge once employing memoryefficient attention mechanisms. Consequently, training Transformers on longer context lengths and scaling up Transformer models become significantly hindered due to the overwhelming memory demands imposed by the feedforward network. \n\nTo address this challenge, we make an important observation: when self-attention is computed in a blockwise manner to reduce memory requirements, it becomes feasible to merge the computation of the feedforward network. This eliminates the need to wait for the self-attention computation to finish before performing the feedforward step on the entire sequence. By computing the feedforward network on a block-by-block basis, we effectively reduce the memory cost associated with the feedforward network. This process involves the utilization of two nested loops over the input sequence blocks. In the outer loop, we iterate over each block and compute the query. In the inner loop, we iterate over each block to calculate the key and value. These key-value pairs, along with the query, are then used to compute the blockwise attention specific to the corresponding input block. This blockwise attention is subsequently used to calculate the output of the feedforward network, followed by a residual connection. This approach enables us to process longer input sequences while maintaining lower memory budget. Since our approach performs blockwise parallel computation and fuses the feedforward and self-attention computations, we name our method the Blockwise Parallel Transformer (BPT). \n\nWe evaluate the effectiveness of our approach on several benchmarks, including language modeling and reinforcement learning. Our experiments show that BPT can reduce the memory requirements of Transformers, enabling us to train 32 times longer sequence than vanilla attention [52] based GPT models and up to 4 times longer sequence than prior state-of-the-arts FlashAttention [14] and Memory Efficient Attention [42]. Furthermore, we demonstrate the application of BPT on the task of traning Transformer based RL agent.",
                    "score": 0.5244824160102592,
                    "section_title": "Introduction",
                    "char_start_offset": 2176,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 45
                        },
                        {
                            "start": 48,
                            "end": 215
                        },
                        {
                            "start": 216,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 665
                        },
                        {
                            "start": 668,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1027
                        },
                        {
                            "start": 1028,
                            "end": 1170
                        },
                        {
                            "start": 1171,
                            "end": 1260
                        },
                        {
                            "start": 1261,
                            "end": 1329
                        },
                        {
                            "start": 1330,
                            "end": 1407
                        },
                        {
                            "start": 1408,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1677
                        },
                        {
                            "start": 1678,
                            "end": 1775
                        },
                        {
                            "start": 1776,
                            "end": 1954
                        },
                        {
                            "start": 1957,
                            "end": 2081
                        },
                        {
                            "start": 2082,
                            "end": 2374
                        },
                        {
                            "start": 2375,
                            "end": 2476
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 2333,
                            "end": 2337,
                            "matchedPaperCorpusId": "249151871"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.34521484375
                }
            ],
            "relevance_judgement": 0.53271484375,
            "relevance_judgment_input_expanded": "# Title: Blockwise Parallel Transformer for Large Context Models\n# Venue: \n# Authors: Hao Liu, P. Abbeel\n## Abstract\nTransformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n## Introduction\nTransformers [52] have become the backbone of many state-of-the-art natural language processing models [15,43,5,35]. They have demonstrated impressive performance across a wide range of AI problems, including language modeling, machine translation, image captioning, and protein folding [39,47,32,43,5,45,9]. Transformers achieve this success through their architecture design that uses self-attention and position-wise feedforward mechanisms. These components facilitate the efficient capture of long-range dependencies between input tokens, enabling scalability in terms of context length and model size through highly parallel computations. \n\nHowever, the memory requirements of Transformers limit their ability to handle long sequences, which is necessary for many AI problems, such as high-resolution images, podcasts, code, or books and especially those that involve multiple long sequences or long-term dependencies [10,7,39,7,34,29,47,32,1]. The quadratic self-attention and the large feed forward network of Transformers require a large amount of memory, which makes it challenging to scale to longer input sequences. This limitation has led to various techniques proposed to reduce the memory requirements of Transformers, including sparse-approximation, low-rank approximation, and low precision approximation [see e.g. 51,24,22,11,25,36,54]. \n\nOne distinct line of research does not rely on approximation but instead focuses on computing exact self-attention with linear memory complexity. This approach leverages the observation that the softmax matrix in self-attention can be computed without materializing the full matrix [37]. This technique has led to the development of FlashAttention [14] and Memory Efficient Attention [42]. Both methods propose a blockwise computation of the self-attention softmax, demonstrating reduced memory requirements. (A), (B), and (C) show evaluation using one, eight A100, and 64 TPUv4, respectively, with a single sequence. Our method enables training sequences 32 times longer than vanilla attention-based Transformer [52], and 2 to 4 times longer than FlashAttention [14] and Memory Efficient Attention [42]. Section 3.1 provides a memory cost breakdown.\n...\nSection 3.1 provides a memory cost breakdown. \n\nDespite the resulting reduced memory requirements of the self-attention block in Transformer models, a significant challenge still arises from the feedforward network. This network contains a large number of parameters and produces high-dimensional intermediate vectors, resulting in substantial memory requirements. This issue is becomes the key memory challenge once employing memoryefficient attention mechanisms. Consequently, training Transformers on longer context lengths and scaling up Transformer models become significantly hindered due to the overwhelming memory demands imposed by the feedforward network. \n\nTo address this challenge, we make an important observation: when self-attention is computed in a blockwise manner to reduce memory requirements, it becomes feasible to merge the computation of the feedforward network. This eliminates the need to wait for the self-attention computation to finish before performing the feedforward step on the entire sequence. By computing the feedforward network on a block-by-block basis, we effectively reduce the memory cost associated with the feedforward network. This process involves the utilization of two nested loops over the input sequence blocks. In the outer loop, we iterate over each block and compute the query. In the inner loop, we iterate over each block to calculate the key and value. These key-value pairs, along with the query, are then used to compute the blockwise attention specific to the corresponding input block. This blockwise attention is subsequently used to calculate the output of the feedforward network, followed by a residual connection. This approach enables us to process longer input sequences while maintaining lower memory budget. Since our approach performs blockwise parallel computation and fuses the feedforward and self-attention computations, we name our method the Blockwise Parallel Transformer (BPT). \n\nWe evaluate the effectiveness of our approach on several benchmarks, including language modeling and reinforcement learning. Our experiments show that BPT can reduce the memory requirements of Transformers, enabling us to train 32 times longer sequence than vanilla attention [52] based GPT models and up to 4 times longer sequence than prior state-of-the-arts FlashAttention [14] and Memory Efficient Attention [42]. Furthermore, we demonstrate the application of BPT on the task of traning Transformer based RL agent.\n\n## Conclusion\nIn conclusion, we propose a blockwise parallelization approach to reduce the memory requirements of Transformers, the backbone of state-of-the-art NLP models. Our approach enables processing longer input sequences while maintaining or improving performance. Through extensive experiments, we demonstrate its effectiveness, achieving up to 4x memory reduction than memory-efficient Transformers. Our contributions include a practical method for large context sizes in large Transformer models. With the increasing capability of hardware, larger models and longer context length are widely used in AI research. At the same time, as we are pushing up against physics and fabrication limits, it is more important to design scaling approaches as efficient as possible to scale up large models and large context size. Our approach holds promise for training and evaluating complex models with longer input sequences, potentially driving new breakthroughs in machine learning research. \n\nLimitations and Future Work. Although our method achieves state-of-the-art low memory usage for Transformer models, it does have some limitations that need to be addressed: \n\n\u2022 Optimal performance. While our implementation prioritizes simplicity with high-level Jax operations, optimizing low-level operations is crucial for achieving optimal performance. In future work, we suggest considering porting our method to CUDA and OpenAI Triton to achieve minimal memory cost and maximum speedup. \n\ntotal number of tokens) can be computationally slow. To reduce the sampling time, we limited the rollout to 16 trajectories.",
            "reference_string": "[258987968 | Liu et al. | 2023 | Citations: 11]"
        },
        {
            "title": "MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training",
            "venue": "Proc. ACM Manag. Data",
            "year": 2024,
            "reference_count": 104,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312019649",
                    "name": "Pinxue Zhao"
                },
                {
                    "authorId": "2288557803",
                    "name": "Hailin Zhang"
                },
                {
                    "authorId": "46182701",
                    "name": "Fangcheng Fu"
                },
                {
                    "authorId": "2113588952",
                    "name": "Xiaonan Nie"
                },
                {
                    "authorId": "2187417728",
                    "name": "Qibin Liu"
                },
                {
                    "authorId": "2311819555",
                    "name": "Fang Yang"
                },
                {
                    "authorId": "2311693690",
                    "name": "Yuanbo Peng"
                },
                {
                    "authorId": "2210796850",
                    "name": "Dian Jiao"
                },
                {
                    "authorId": "2303292656",
                    "name": "Shuaipeng Li"
                },
                {
                    "authorId": "2302814808",
                    "name": "Jinbao Xue"
                },
                {
                    "authorId": "2105710451",
                    "name": "Yangyu Tao"
                },
                {
                    "authorId": "2299156195",
                    "name": "Bin Cui"
                }
            ],
            "abstract": "Nowadays, Large Language Models (LLMs) have been trained using extended context lengths to foster more creative applications. However, long context training poses great challenges considering the constraint of GPU memory. It not only leads to substantial activation memory consumption during training, but also incurs considerable memory fragmentation. To facilitate long context training, existing frameworks have adopted strategies such as recomputation and various forms of parallelisms. Nevertheless, these techniques rely on redundant computation or extensive communication, resulting in low Model FLOPS Utilization (MFU). In this paper, we propose MEMO, a novel LLM training framework designed for fine-grained activation memory management. Given the quadratic scaling of computation and linear scaling of memory with sequence lengths when using FlashAttention, we offload memory-consuming activations to CPU memory after each layer's forward pass and fetch them during the backward pass. To maximize the swapping of activations without hindering computation, and to avoid exhausting limited CPU memory, we implement a token-wise activation recomputation and swapping mechanism. Furthermore, we tackle the memory fragmentation issue by employing a bi-level Mixed Integer Programming (MIP) approach, optimizing memory reuse across transformer layers. Empirical results demonstrate that MEMO achieves an average of 1.97x and 1.80x MFU compared to Megatron-LM and DeepSpeed, respectively. This improvement is attributed to MEMO's ability to minimize memory fragmentation, reduce recomputation and intensive communication, and circumvent the delays associated with the memory reorganization process due to fragmentation. By leveraging fine-grained activation memory management, MEMO facilitates efficient training of 7B LLM with 1 million sequence length on just 8 A800 GPUs, achieving an MFU of 52.30%.",
            "corpus_id": 271244689,
            "sentences": [
                {
                    "corpus_id": "271244689",
                    "title": "MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training",
                    "text": "Since the advent of ChatGPT [58], Large Language Models (LLMs) have demonstrated remarkable proficiency in comprehending and generating natural language texts. Besides revolutionizing the field of language processing, which encompasses translation [103], coding [23,72,94], etc., transformer-based LLMs have also found applications in multi-modal scenarios, such as image processing [15,61], video stream analysis [73], and AI for science [2,5]. To accommodate novel applications that require lengthy contexts [98], LLMs have developed to support long context input, from 2K-4K [79,81] to 32K [29,80], 128K [18,58], or even millions of tokens [1,9,41]. Considering the extrapolation problem [43,66], which refers to the decline in LLM performance when input sequences exceed the training length, it is necessary to conduct long context training [7,17,28] or fine-tuning [14,62] to facilitate long sequence inference. Beyond natural language processing, increasing the context length is also essential across diverse domains, including video processing [101], protein properties prediction [6], weather forecasting [54], and health care [40]. \n\nMaximizing system performance with limited memory is a common and significant challenge in the data management community. Within this context, training LLMs with long sequence lengths poses difficulties due to restricted GPU memory. During training, a large amount of activations 1 must be stored for gradient computation during the backward pass, resulting in substantial memory consumption. Typically, it is well known that the self-attention module in the transformer architecture has a quadratic computation and memory complexity w.r.t. the sequence length. FlashAttention [11,12], now a standard technique for attention computation in LLM training, accelerates computation and shrinks the memory complexity to be linear w.r.t. the sequence length by scheduling memory I/O and recomputing necessary components during the backward pass. Except for attention, the remaining activation memory also scales linearly with the sequence length, which can become quite large in long context scenarios.",
                    "score": 0.5990429143497105,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 160,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 652
                        },
                        {
                            "start": 653,
                            "end": 916
                        },
                        {
                            "start": 917,
                            "end": 1141
                        },
                        {
                            "start": 1144,
                            "end": 1265
                        },
                        {
                            "start": 1266,
                            "end": 1376
                        },
                        {
                            "start": 1377,
                            "end": 1536
                        },
                        {
                            "start": 1537,
                            "end": 1705
                        },
                        {
                            "start": 1706,
                            "end": 1983
                        },
                        {
                            "start": 1984,
                            "end": 2140
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 248,
                            "end": 253,
                            "matchedPaperCorpusId": "258048937"
                        },
                        {
                            "start": 269,
                            "end": 272,
                            "matchedPaperCorpusId": "272045337"
                        },
                        {
                            "start": 383,
                            "end": 387,
                            "matchedPaperCorpusId": "225039882"
                        },
                        {
                            "start": 387,
                            "end": 390,
                            "matchedPaperCorpusId": "254854389"
                        },
                        {
                            "start": 414,
                            "end": 418,
                            "matchedPaperCorpusId": "237581134"
                        },
                        {
                            "start": 442,
                            "end": 444,
                            "matchedPaperCorpusId": "271931441"
                        },
                        {
                            "start": 607,
                            "end": 611,
                            "matchedPaperCorpusId": "267682361"
                        },
                        {
                            "start": 691,
                            "end": 695,
                            "matchedPaperCorpusId": "263828829"
                        },
                        {
                            "start": 695,
                            "end": 698,
                            "matchedPaperCorpusId": "237347130"
                        },
                        {
                            "start": 870,
                            "end": 874,
                            "matchedPaperCorpusId": "267770308"
                        },
                        {
                            "start": 874,
                            "end": 877,
                            "matchedPaperCorpusId": "261493986"
                        },
                        {
                            "start": 1089,
                            "end": 1092,
                            "matchedPaperCorpusId": "255966856"
                        },
                        {
                            "start": 1114,
                            "end": 1118,
                            "matchedPaperCorpusId": "256231457"
                        },
                        {
                            "start": 1721,
                            "end": 1725,
                            "matchedPaperCorpusId": "259936734"
                        },
                        {
                            "start": 1725,
                            "end": 1728,
                            "matchedPaperCorpusId": "249151871"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.525390625
                }
            ],
            "relevance_judgement": 0.525390625,
            "relevance_judgment_input_expanded": "# Title: MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training\n# Venue: Proc. ACM Manag. Data\n# Authors: Pinxue Zhao, Hailin Zhang, Fangcheng Fu, Xiaonan Nie, Qibin Liu, Fang Yang, Yuanbo Peng, Dian Jiao, Shuaipeng Li, Jinbao Xue, Yangyu Tao, Bin Cui\n## Abstract\nNowadays, Large Language Models (LLMs) have been trained using extended context lengths to foster more creative applications. However, long context training poses great challenges considering the constraint of GPU memory. It not only leads to substantial activation memory consumption during training, but also incurs considerable memory fragmentation. To facilitate long context training, existing frameworks have adopted strategies such as recomputation and various forms of parallelisms. Nevertheless, these techniques rely on redundant computation or extensive communication, resulting in low Model FLOPS Utilization (MFU). In this paper, we propose MEMO, a novel LLM training framework designed for fine-grained activation memory management. Given the quadratic scaling of computation and linear scaling of memory with sequence lengths when using FlashAttention, we offload memory-consuming activations to CPU memory after each layer's forward pass and fetch them during the backward pass. To maximize the swapping of activations without hindering computation, and to avoid exhausting limited CPU memory, we implement a token-wise activation recomputation and swapping mechanism. Furthermore, we tackle the memory fragmentation issue by employing a bi-level Mixed Integer Programming (MIP) approach, optimizing memory reuse across transformer layers. Empirical results demonstrate that MEMO achieves an average of 1.97x and 1.80x MFU compared to Megatron-LM and DeepSpeed, respectively. This improvement is attributed to MEMO's ability to minimize memory fragmentation, reduce recomputation and intensive communication, and circumvent the delays associated with the memory reorganization process due to fragmentation. By leveraging fine-grained activation memory management, MEMO facilitates efficient training of 7B LLM with 1 million sequence length on just 8 A800 GPUs, achieving an MFU of 52.30%.\n## Introduction\nSince the advent of ChatGPT [58], Large Language Models (LLMs) have demonstrated remarkable proficiency in comprehending and generating natural language texts. Besides revolutionizing the field of language processing, which encompasses translation [103], coding [23,72,94], etc., transformer-based LLMs have also found applications in multi-modal scenarios, such as image processing [15,61], video stream analysis [73], and AI for science [2,5]. To accommodate novel applications that require lengthy contexts [98], LLMs have developed to support long context input, from 2K-4K [79,81] to 32K [29,80], 128K [18,58], or even millions of tokens [1,9,41]. Considering the extrapolation problem [43,66], which refers to the decline in LLM performance when input sequences exceed the training length, it is necessary to conduct long context training [7,17,28] or fine-tuning [14,62] to facilitate long sequence inference. Beyond natural language processing, increasing the context length is also essential across diverse domains, including video processing [101], protein properties prediction [6], weather forecasting [54], and health care [40]. \n\nMaximizing system performance with limited memory is a common and significant challenge in the data management community. Within this context, training LLMs with long sequence lengths poses difficulties due to restricted GPU memory. During training, a large amount of activations 1 must be stored for gradient computation during the backward pass, resulting in substantial memory consumption. Typically, it is well known that the self-attention module in the transformer architecture has a quadratic computation and memory complexity w.r.t. the sequence length. FlashAttention [11,12], now a standard technique for attention computation in LLM training, accelerates computation and shrinks the memory complexity to be linear w.r.t. the sequence length by scheduling memory I/O and recomputing necessary components during the backward pass. Except for attention, the remaining activation memory also scales linearly with the sequence length, which can become quite large in long context scenarios.",
            "reference_string": "[271244689 | Zhao et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs",
            "venue": "",
            "year": 2024,
            "reference_count": 73,
            "citation_count": 4,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.08845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2095707",
                    "name": "Ben Athiwaratkun"
                },
                {
                    "authorId": "1913939",
                    "name": "Sujan Kumar Gonugondla"
                },
                {
                    "authorId": "40892818",
                    "name": "Sanjay Krishna Gouda"
                },
                {
                    "authorId": "2287922898",
                    "name": "Haifeng Qian"
                },
                {
                    "authorId": "2113455281",
                    "name": "Hantian Ding"
                },
                {
                    "authorId": "2291224391",
                    "name": "Qing Sun"
                },
                {
                    "authorId": "2291153804",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "2291418208",
                    "name": "Jiacheng Guo"
                },
                {
                    "authorId": "2282351608",
                    "name": "Liangfu Chen"
                },
                {
                    "authorId": "50339091",
                    "name": "Parminder Bhatia"
                },
                {
                    "authorId": "1701451",
                    "name": "Ramesh Nallapati"
                },
                {
                    "authorId": "2072419570",
                    "name": "Sudipta Sengupta"
                },
                {
                    "authorId": "2258965075",
                    "name": "Bing Xiang"
                }
            ],
            "abstract": "This study introduces bifurcated attention, a method designed to enhance language model inference in shared-context batch decoding scenarios. Our approach addresses the challenge of redundant memory IO costs, a critical factor contributing to latency in high batch sizes and extended context lengths. Bifurcated attention achieves this by strategically dividing the attention mechanism during incremental decoding into two separate GEMM operations: one focusing on the KV cache from prefill, and another on the decoding process itself. While maintaining the computational load (FLOPs) of standard attention mechanisms, bifurcated attention ensures precise computation with significantly reduced memory IO. Our empirical results show over 2.1$\\times$ speedup when sampling 16 output sequences and more than 6.2$\\times$ speedup when sampling 32 sequences at context lengths exceeding 8k tokens on a 7B model that uses multi-head attention. The efficiency gains from bifurcated attention translate into lower latency, making it particularly suitable for real-time applications. For instance, it enables massively parallel answer generation without substantially increasing latency, thus enhancing performance when integrated with post-processing techniques such as re-ranking.",
            "corpus_id": 268385246,
            "sentences": [
                {
                    "corpus_id": "268385246",
                    "title": "Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs",
                    "text": "As language models are becoming general purpose and highly capable, the demand for language models to handle longer context sequences has grown significantly.Recently, there is an ongoing focus on models that can handle even longer context sequences (Bulatov et al., 2023;OpenAI, 2023;Team, 2023;?).As of today, GPT-4 (OpenAI, 2023) supports context length of 32k tokens, and MPT-7B (Team, 2023) extends it to 64k while Anthropic's Claude4 supports as long as 100k input length.Most recently, Bulatov et al proposed 1M token input context length for transformers.These models push the boundaries of context understanding and generation capabilities, enabling more comprehensive discourse understanding and contextually informed responses.\n\nThis trend is driven by the need for comprehensive discourse understanding in applications like Retrieval-Augmented Generation (RAG), as well as many complex prompting methods.Applications such as RAG (Guu et al., 2020;Izacard et al., 2022;Menick et al., 2022;Zhen et al., 2022) retrieve extensive passages or documents from external corpora, providing rich and grounded context for generating responses.Additionally, models like Toolformer (Schick et al., 2023) and WebGPT (Nakano et al., 2021) leverage external tools, such as APIs and search engines, to expand the context and enhance generation.\n\nLong context is disproportionately expensive for transformer family models because for vanilla self-attention both memory and time complexity are quadratic to the sequence length.To effectively handle longer context sequences, optimizing memory I/O and reducing computational overhead are critical.Currently, the dominant approaches to addressing this challenge have been to make the attention computation less expensive.Beltagy et al. (2020) proposed to sparsify self-attention using various attention patterns.Wang et al. (2020) explores low-rank approximation of self-attention.In addition to the compute bound improvements, advancements in memory-efficient attention mechanisms and techniques for reducing memory I/O will continue to propel the field forward, facilitating the handling of longer context sequences in language models.",
                    "score": 0.581525550391379,
                    "section_title": "B.2. Supporting Long Context Requires IO-Efficient Attention",
                    "char_start_offset": 37884,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 158,
                            "end": 299
                        },
                        {
                            "start": 299,
                            "end": 478
                        },
                        {
                            "start": 478,
                            "end": 563
                        },
                        {
                            "start": 563,
                            "end": 738
                        },
                        {
                            "start": 740,
                            "end": 916
                        },
                        {
                            "start": 916,
                            "end": 1144
                        },
                        {
                            "start": 1144,
                            "end": 1339
                        },
                        {
                            "start": 1341,
                            "end": 1520
                        },
                        {
                            "start": 1520,
                            "end": 1639
                        },
                        {
                            "start": 1639,
                            "end": 1762
                        },
                        {
                            "start": 1762,
                            "end": 1853
                        },
                        {
                            "start": 1853,
                            "end": 1922
                        },
                        {
                            "start": 1922,
                            "end": 2178
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 941,
                            "end": 959,
                            "matchedPaperCorpusId": "211204736"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5205078125
                }
            ],
            "relevance_judgement": 0.5205078125,
            "relevance_judgment_input_expanded": "# Title: Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs\n# Venue: \n# Authors: Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Jiacheng Guo, Liangfu Chen, Parminder Bhatia, Ramesh Nallapati, Sudipta Sengupta, Bing Xiang\n## Abstract\nThis study introduces bifurcated attention, a method designed to enhance language model inference in shared-context batch decoding scenarios. Our approach addresses the challenge of redundant memory IO costs, a critical factor contributing to latency in high batch sizes and extended context lengths. Bifurcated attention achieves this by strategically dividing the attention mechanism during incremental decoding into two separate GEMM operations: one focusing on the KV cache from prefill, and another on the decoding process itself. While maintaining the computational load (FLOPs) of standard attention mechanisms, bifurcated attention ensures precise computation with significantly reduced memory IO. Our empirical results show over 2.1$\\times$ speedup when sampling 16 output sequences and more than 6.2$\\times$ speedup when sampling 32 sequences at context lengths exceeding 8k tokens on a 7B model that uses multi-head attention. The efficiency gains from bifurcated attention translate into lower latency, making it particularly suitable for real-time applications. For instance, it enables massively parallel answer generation without substantially increasing latency, thus enhancing performance when integrated with post-processing techniques such as re-ranking.\n## B.2. Supporting Long Context Requires IO-Efficient Attention\nAs language models are becoming general purpose and highly capable, the demand for language models to handle longer context sequences has grown significantly.Recently, there is an ongoing focus on models that can handle even longer context sequences (Bulatov et al., 2023;OpenAI, 2023;Team, 2023;?).As of today, GPT-4 (OpenAI, 2023) supports context length of 32k tokens, and MPT-7B (Team, 2023) extends it to 64k while Anthropic's Claude4 supports as long as 100k input length.Most recently, Bulatov et al proposed 1M token input context length for transformers.These models push the boundaries of context understanding and generation capabilities, enabling more comprehensive discourse understanding and contextually informed responses.\n\nThis trend is driven by the need for comprehensive discourse understanding in applications like Retrieval-Augmented Generation (RAG), as well as many complex prompting methods.Applications such as RAG (Guu et al., 2020;Izacard et al., 2022;Menick et al., 2022;Zhen et al., 2022) retrieve extensive passages or documents from external corpora, providing rich and grounded context for generating responses.Additionally, models like Toolformer (Schick et al., 2023) and WebGPT (Nakano et al., 2021) leverage external tools, such as APIs and search engines, to expand the context and enhance generation.\n\nLong context is disproportionately expensive for transformer family models because for vanilla self-attention both memory and time complexity are quadratic to the sequence length.To effectively handle longer context sequences, optimizing memory I/O and reducing computational overhead are critical.Currently, the dominant approaches to addressing this challenge have been to make the attention computation less expensive.Beltagy et al. (2020) proposed to sparsify self-attention using various attention patterns.Wang et al. (2020) explores low-rank approximation of self-attention.In addition to the compute bound improvements, advancements in memory-efficient attention mechanisms and techniques for reducing memory I/O will continue to propel the field forward, facilitating the handling of longer context sequences in language models.",
            "reference_string": "[268385246 | Athiwaratkun et al. | 2024 | Citations: 4]"
        },
        {
            "title": "TransformerFAM: Feedback attention is working memory",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 86,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.09173, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2241835900",
                    "name": "Dongseong Hwang"
                },
                {
                    "authorId": "2243099428",
                    "name": "Weiran Wang"
                },
                {
                    "authorId": "2296717007",
                    "name": "Zhuoyuan Huo"
                },
                {
                    "authorId": "1693612",
                    "name": "K. Sim"
                },
                {
                    "authorId": "2103777583",
                    "name": "P. M. Mengibar"
                }
            ],
            "abstract": "While Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations. This design fosters the emergence of working memory within the Transformer, allowing it to process indefinitely long sequences. TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models. Our experiments show that TransformerFAM significantly improves Transformer performance on long-context tasks across various model sizes (1B, 8B, and 24B). These results showcase the potential to empower Large Language Models (LLMs) to process sequences of unlimited length.",
            "corpus_id": 269149105,
            "sentences": [
                {
                    "corpus_id": "269149105",
                    "title": "TransformerFAM: Feedback attention is working memory",
                    "text": "Gemini [69] evaluated long-context capabilities using the following tasks: NarrativeQA [19], Scrolls-Qasper, Scrolls-Quality [60], and XLSum [51].Additionally, PG-19 [21] and Isabelle [56] are another common evaluation tasks among long-context Transformer papers [21,56,67].Detailed information on the evaluation data is provided in Table 10 in Appendix D.2.\n\nWe evaluated the long-context capabilities of the 1B TransformerBSWA model trained in Section 3.1 using memory segment sizes ranging from 0 to 8. As shown in Fig. 3b, TransformerFAM outperformed TransformerBSWA on all the long context tasks (LCT), regardless of the number of memory segments in BSWA.It shows a significant performance improvement on ScrollsQasper and Narra-tiveQA, where it has to understand 5k to 500k tokens of context before answering a question.The LCT results demonstrate that TransformerFAM can effectively compress and retain important contextual information within extremely long contexts.\n\nAbove M1, the number of memory segments does not significantly impact LCT performance on TransformerBSWA, because the input sequences are much longer than the window size of all experiments.We observed the same phenomenon in TransformerFAM, and TransformerFAM uses 3 memory segments in Fig. 3b.The figure shows the normalized scores of all tasks to view the scores on the same scale.The raw results are in In addition, TransformerFAM marginally surpasses TransformerBSWA on GPT-3 tasks [33] (see Table 2).This result is unexpected since all tasks involve sequences shorter than 2k tokens.We hypothesize that this improvement arises from the efficient contextual representation by Transformer-FAM.By offloading contextual data to FAM, TransformerFAM reduces redundancy within input activations, optimizing latent space usage.Thus, BSWA memory segments (local representation) and FAM (global representation) complement each other.",
                    "score": 0.5934300344661031,
                    "section_title": "Long Context Tasks",
                    "char_start_offset": 23054,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 146
                        },
                        {
                            "start": 146,
                            "end": 274
                        },
                        {
                            "start": 274,
                            "end": 358
                        },
                        {
                            "start": 360,
                            "end": 660
                        },
                        {
                            "start": 660,
                            "end": 826
                        },
                        {
                            "start": 826,
                            "end": 974
                        },
                        {
                            "start": 976,
                            "end": 1166
                        },
                        {
                            "start": 1166,
                            "end": 1270
                        },
                        {
                            "start": 1270,
                            "end": 1359
                        },
                        {
                            "start": 1359,
                            "end": 1481
                        },
                        {
                            "start": 1481,
                            "end": 1564
                        },
                        {
                            "start": 1564,
                            "end": 1672
                        },
                        {
                            "start": 1672,
                            "end": 1800
                        },
                        {
                            "start": 1800,
                            "end": 1904
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5087890625
                }
            ],
            "relevance_judgement": 0.5087890625,
            "relevance_judgment_input_expanded": "# Title: TransformerFAM: Feedback attention is working memory\n# Venue: arXiv.org\n# Authors: Dongseong Hwang, Weiran Wang, Zhuoyuan Huo, K. Sim, P. M. Mengibar\n## Abstract\nWhile Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations. This design fosters the emergence of working memory within the Transformer, allowing it to process indefinitely long sequences. TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models. Our experiments show that TransformerFAM significantly improves Transformer performance on long-context tasks across various model sizes (1B, 8B, and 24B). These results showcase the potential to empower Large Language Models (LLMs) to process sequences of unlimited length.\n## Long Context Tasks\nGemini [69] evaluated long-context capabilities using the following tasks: NarrativeQA [19], Scrolls-Qasper, Scrolls-Quality [60], and XLSum [51].Additionally, PG-19 [21] and Isabelle [56] are another common evaluation tasks among long-context Transformer papers [21,56,67].Detailed information on the evaluation data is provided in Table 10 in Appendix D.2.\n\nWe evaluated the long-context capabilities of the 1B TransformerBSWA model trained in Section 3.1 using memory segment sizes ranging from 0 to 8. As shown in Fig. 3b, TransformerFAM outperformed TransformerBSWA on all the long context tasks (LCT), regardless of the number of memory segments in BSWA.It shows a significant performance improvement on ScrollsQasper and Narra-tiveQA, where it has to understand 5k to 500k tokens of context before answering a question.The LCT results demonstrate that TransformerFAM can effectively compress and retain important contextual information within extremely long contexts.\n\nAbove M1, the number of memory segments does not significantly impact LCT performance on TransformerBSWA, because the input sequences are much longer than the window size of all experiments.We observed the same phenomenon in TransformerFAM, and TransformerFAM uses 3 memory segments in Fig. 3b.The figure shows the normalized scores of all tasks to view the scores on the same scale.The raw results are in In addition, TransformerFAM marginally surpasses TransformerBSWA on GPT-3 tasks [33] (see Table 2).This result is unexpected since all tasks involve sequences shorter than 2k tokens.We hypothesize that this improvement arises from the efficient contextual representation by Transformer-FAM.By offloading contextual data to FAM, TransformerFAM reduces redundancy within input activations, optimizing latent space usage.Thus, BSWA memory segments (local representation) and FAM (global representation) complement each other.",
            "reference_string": "[269149105 | Hwang et al. | 2024 | Citations: 12]"
        },
        {
            "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 69,
            "citation_count": 603,
            "influential_citation_count": 171,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.14508",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.14508, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2141377570",
                    "name": "Yushi Bai"
                },
                {
                    "authorId": "48574888",
                    "name": "Xin Lv"
                },
                {
                    "authorId": "2107983722",
                    "name": "Jiajie Zhang"
                },
                {
                    "authorId": "2220304036",
                    "name": "Hong Lyu"
                },
                {
                    "authorId": "2204826271",
                    "name": "Jiankai Tang"
                },
                {
                    "authorId": "2234629967",
                    "name": "Zhidian Huang"
                },
                {
                    "authorId": "66395694",
                    "name": "Zhengxiao Du"
                },
                {
                    "authorId": "2111312892",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "2051712753",
                    "name": "Aohan Zeng"
                },
                {
                    "authorId": "2055765060",
                    "name": "Lei Hou"
                },
                {
                    "authorId": "2047998",
                    "name": "Yuxiao Dong"
                },
                {
                    "authorId": "2148911975",
                    "name": "Jie Tang"
                },
                {
                    "authorId": "2133353675",
                    "name": "Juanzi Li"
                }
            ],
            "abstract": "Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs' long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability. The code and datasets are available at https://github.com/THUDM/LongBench.",
            "corpus_id": 261245264,
            "sentences": [
                {
                    "corpus_id": "261245264",
                    "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",
                    "text": "Long Context Modeling Techniques. We first discuss some popular lines of methods that aim to tackle long context understanding. These studies are mainly aimed at solving two key challenges in long text modeling, including the high runtime overhead on longer context, and the catastrophic forgetting phenomenon when processing long sequence. A series of studies focus on how to make Transformers more efficient and unforgetful (Tay et al., 2022), with designs such as sparse and efficient computation (Child et al., 2019;Kitaev et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020;Wang et al., 2020;Fedus et al., 2022;Ding et al., 2023), recurrent and memory modules (Dai et al., 2019;Rae et al., 2020;Wu et al., 2022;Martins et al., 2022;Bulatov et al., 2022;Orvieto et al., 2023;Liang et al., 2023;Zhou et al., 2023). More recently, several methods (Press et al., 2022;Sun et al., 2022;Chen et al., 2023) have been proposed to enable length extrapolation of Transformers, and have been adopted in the training process of long context LLMs such as ChatGLM2-32k (Zeng et al., 2023) and LongChat-32k (Li et al., 2023). Evaluation for Long Context Understanding. Many previous works on long text modeling rely on the perplexity metric for evaluation (Beltagy et al., 2020;Roy et al., 2021;Press et al., 2022). However, as suggested in (Sun et al., 2021), the perplexity metric may not necessarily reflect the model's performance on sequence-level tasks in real applications Meanwhile, some works assess long text modeling through artificial tasks such as retrieval (Tay et al., 2021;Chen et al., 2023;Li et al., 2023), which may also fall short in mirroring real-world scenarios.",
                    "score": 0.5581563569907951,
                    "section_title": "Related Work",
                    "char_start_offset": 3596,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 33
                        },
                        {
                            "start": 34,
                            "end": 127
                        },
                        {
                            "start": 128,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1160
                        },
                        {
                            "start": 1161,
                            "end": 1307
                        },
                        {
                            "start": 1308,
                            "end": 1677
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 426,
                            "end": 444,
                            "matchedPaperCorpusId": "221702858"
                        },
                        {
                            "start": 520,
                            "end": 540,
                            "matchedPaperCorpusId": "209315300"
                        },
                        {
                            "start": 561,
                            "end": 581,
                            "matchedPaperCorpusId": "259165244"
                        },
                        {
                            "start": 599,
                            "end": 618,
                            "matchedPaperCorpusId": "231573431"
                        },
                        {
                            "start": 667,
                            "end": 685,
                            "matchedPaperCorpusId": "57759363"
                        },
                        {
                            "start": 685,
                            "end": 702,
                            "matchedPaperCorpusId": "207930593"
                        },
                        {
                            "start": 702,
                            "end": 718,
                            "matchedPaperCorpusId": "247519194"
                        },
                        {
                            "start": 739,
                            "end": 760,
                            "matchedPaperCorpusId": "250526424"
                        },
                        {
                            "start": 851,
                            "end": 871,
                            "matchedPaperCorpusId": "237347130"
                        },
                        {
                            "start": 1062,
                            "end": 1081,
                            "matchedPaperCorpusId": "252715691"
                        },
                        {
                            "start": 1270,
                            "end": 1287,
                            "matchedPaperCorpusId": "212718077"
                        },
                        {
                            "start": 1287,
                            "end": 1306,
                            "matchedPaperCorpusId": "237347130"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5009765625
                }
            ],
            "relevance_judgement": 0.5009765625,
            "relevance_judgment_input_expanded": "# Title: LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li\n## Abstract\nAlthough large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs' long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability. The code and datasets are available at https://github.com/THUDM/LongBench.\n## Related Work\nLong Context Modeling Techniques. We first discuss some popular lines of methods that aim to tackle long context understanding. These studies are mainly aimed at solving two key challenges in long text modeling, including the high runtime overhead on longer context, and the catastrophic forgetting phenomenon when processing long sequence. A series of studies focus on how to make Transformers more efficient and unforgetful (Tay et al., 2022), with designs such as sparse and efficient computation (Child et al., 2019;Kitaev et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020;Wang et al., 2020;Fedus et al., 2022;Ding et al., 2023), recurrent and memory modules (Dai et al., 2019;Rae et al., 2020;Wu et al., 2022;Martins et al., 2022;Bulatov et al., 2022;Orvieto et al., 2023;Liang et al., 2023;Zhou et al., 2023). More recently, several methods (Press et al., 2022;Sun et al., 2022;Chen et al., 2023) have been proposed to enable length extrapolation of Transformers, and have been adopted in the training process of long context LLMs such as ChatGLM2-32k (Zeng et al., 2023) and LongChat-32k (Li et al., 2023). Evaluation for Long Context Understanding. Many previous works on long text modeling rely on the perplexity metric for evaluation (Beltagy et al., 2020;Roy et al., 2021;Press et al., 2022). However, as suggested in (Sun et al., 2021), the perplexity metric may not necessarily reflect the model's performance on sequence-level tasks in real applications Meanwhile, some works assess long text modeling through artificial tasks such as retrieval (Tay et al., 2021;Chen et al., 2023;Li et al., 2023), which may also fall short in mirroring real-world scenarios.",
            "reference_string": "[261245264 | Bai et al. | 2023 | Citations: 603]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "235368340",
            "title": "A Survey of Transformers",
            "text": "The quadratic complexity of self-attention on sequences length can significantly limit the performance of some downstream tasks. For example, language modeling usually needs long-range context. Apart from the techniques introduced in Sec. 4, another effective way of dealing with long sequences is to use divide-and-conquer strategy, i.e., to decompose an input sequence into finer segments that can be efficiently processed by Transformer or Transformer modules. We identify two representative class of methods, recurrent and hierarchical Transformers, as illustrated in Fig. 13. These techniques can be understood as a wrapper for the Transformer model in which Transformer acts as an elementary component that is reused to process different input segments.\n\nTranformer Tranformer cache cache Tranformer \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (a) Recurrent Transformer Tranformer Tranformer \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Tranformer Tranformer (b) Hierarchical Transformer Fig. 13. Illustrations of recurrent and hierarchical Transformers.",
            "score": 0.7132948834216353,
            "section_title": "Transformers with Divide-and-Conquer Strategies",
            "char_start_offset": 73973,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1639404296875
        },
        {
            "corpus_id": "272310078",
            "title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer",
            "text": "Recent advancements in Transformer architectures have significantly enhanced their capability to process long sequences, which is crucial for tasks that require extensive contextual understanding. This section reviews pivotal contributions in this domain, each addressing the inherent memory limitations of standard Transformer models, while also pointing out some of their practical challenges. \n\nMegatron-SP (Korthikanti et al., 2023) adopts a sequence parallelism technique which is tightly integrated with its tensor parallelism. In this approach, sequences are partitioned along the sequence dimension, and all-gather and reduce-scatter collectives are employed to aggregate the QKV (query, key, value) projections for attention computation. The communication complexity analysis indicates that, in contrast to our approach, the communication volume in Megatron-SP's sequence parallelism increases linearly with the sequence length regardless of the number of compute devices. \n\nThe Blockwise Parallel Transformer (BPT) (Liu & Abbeel, 2024) employs a blockwise computation strategy for both self-attention and feedforward layers, optimizing memory usage and allowing the processing of sequences much longer than traditional Transformers. However, despite its efficiency, BPT requires careful tuning of block sizes and memory management to avoid diminishing returns on performance when scaling to extremely long sequences. \n\nRing Attention (Liu et al., 2023) enhances Transformer's scalability by distributing long sequences across multiple devices. This innovative approach overlaps the communication of key-value pairs with the computation of blockwise attention, effectively increasing the feasible sequence length proportionally to the number of available devices. However, reliance on device count for scaling and multi-step communications introduces potential issues in environments with sub-optimal hardware regards network interconnects, where performance can be unpredictably affected by network latency and bandwidth constraints. \n\nDeepSpeed Ulysses (Jacobs et al., 2023) tackles the challenges of sequence parallelism by partitioning input data along the sequence dimension and utilizing an efficient allto-all collective communication strategy for attention computations. Although this method maintains a constant communication volume regardless of the increase in sequence lengths and device counts, achieving significant speedups and scalability, it may still encounter practical hurdles in deployment related to large-scale clusters and the optimization of communication patterns across diverse computing environments.",
            "score": 0.7042485733123868,
            "section_title": "Long context training",
            "char_start_offset": 6723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 981
                },
                {
                    "start": 984,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1426
                },
                {
                    "start": 1429,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 2043
                },
                {
                    "start": 2046,
                    "end": 2287
                },
                {
                    "start": 2288,
                    "end": 2637
                }
            ],
            "ref_mentions": [
                {
                    "start": 410,
                    "end": 435,
                    "matchedPaperCorpusId": "248693351"
                },
                {
                    "start": 1025,
                    "end": 1045,
                    "matchedPaperCorpusId": "266351737"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1937255859375
        },
        {
            "corpus_id": "274437066",
            "title": "FlexSP: Accelerating Large Language Model Training via Flexible Sequence Parallelism",
            "text": "Large Transformer models [12,26,35,40,45], represented by Large Language Models (LLMs) [3,7,36,37,42,43,49,50,54], have made astonishing achievements in the field of artificial intelligence (AI). Recently, there is an increasing need to extend the context length of LLMs, which represents the maximum supported sequence length of the LLMs [11,15,47]. Therefore, long-context LLM training has garnered extensive attention from both the academia and industry [4,6,8,24,27,29,48]. \n\nIt is well known that training LLMs with longer context lengths demands increasingly more device (e.g., GPU) memory, and a promising paradigm is to parallelize the training inputs over multiple devices. Particularly, sequence parallelism (SP) [6,19,22,24,27,29] has emerged as a pivotal technique for long-context LLM training. In a nutshell, SP splits each training input in the sequence dimension and scatters different shards onto multiple devices to amortize the memory consumption. In order to carry out the training, it necessitates communication among the devices to exchange the intermediate results during the forward and backward propagation. By nature, if we wish to support longer training inputs, we shall increase the SP degree 1 (usually accompanied by a decrease in the data parallelism degree) to avoid encountering out-of-memory errors, while the communication overhead increases in the meantime, leading to efficiency degradation. \n\nAlthough SP delivers a method to support long-context LLM training, existing systems assume the training inputs are homogeneous in terms of their lengths and apply the same SP degree to all data parallel model replicas, yet such a fixed-length assumption does not hold in practice. Typically, the training samples of LLMs are usually organized as sequences of tokens, and a training corpus consists of varied-length sequences. In order to address the discrepancy between the fixed-length assumption and the varied-length characteristic, sequence packing is a widely used data preprocessing technique. Specifically, denote  as the maximum number of tokens supported for each model replica.",
            "score": 0.7006444279563462,
            "section_title": "Introduction",
            "char_start_offset": 952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1429
                },
                {
                    "start": 1432,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2120
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 29,
                    "end": 32,
                    "matchedPaperCorpusId": "257702195"
                },
                {
                    "start": 35,
                    "end": 38,
                    "matchedPaperCorpusId": "237491754"
                },
                {
                    "start": 90,
                    "end": 92,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 92,
                    "end": 95,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 95,
                    "end": 98,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 104,
                    "end": 107,
                    "matchedPaperCorpusId": "272045337"
                },
                {
                    "start": 110,
                    "end": 113,
                    "matchedPaperCorpusId": "265296903"
                },
                {
                    "start": 464,
                    "end": 467,
                    "matchedPaperCorpusId": "263671659"
                },
                {
                    "start": 467,
                    "end": 470,
                    "matchedPaperCorpusId": "246017095"
                },
                {
                    "start": 473,
                    "end": 476,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 732,
                    "end": 735,
                    "matchedPaperCorpusId": "263671659"
                },
                {
                    "start": 735,
                    "end": 738,
                    "matchedPaperCorpusId": "246017095"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.460205078125
        },
        {
            "corpus_id": "273812148",
            "title": "Context Parallelism for Scalable Million-Token Inference",
            "text": "In this work, we mainly address the challenges with extremely large (128K-1M) context lengths. \n\n\u2022 Compute: While an W -parameter Transformer model requires 2 \u2022 W matrix multiplication FLOPs for each token during inference or forward pass (Kaplan et al., 2020), the pairwise attention architecture found in mainstream transformers (Vaswani, 2017) incurs a quadratic cost in FLOPs w.r.t. context lengths, which would be dominating in long context cases. Several approximate and sparse methods were proposed, including focusing attention on a subset of tokens, and employing a combination of local and global attention strategies. Techniques such as window attention (Liu et al., 2021), local attention (Xiong et al., 2021), Linformer (Wang et al., 2020), and semi-local sparse attention (Jiang et al., 2024;Beltagy et al., 2020) are examples of such innovations that help manage the computational cost. \n\n\u2022 Memory: Memory usage for LLMs, particularly the KV cache (Pope et al., 2023), scales linearly with the context length. Model compression techniques such as KV cache quantization are crucial for bending the growth curve: lower precision formats like 3-bit, INT4/8 or FP8 can achieve a 2\u00d7 to 4\u00d7 reduction in memory requirements compared to using 16bit (Hooper et al., 2024;Lin et al., 2024). Grouped Query Attention (GQA) (Ainslie et al., 2023) and MQA (Shazeer, 2019) were widely adopted to reduce memory usage by reducing the number of KV heads by 8\u00d7 to 64\u00d7. Additionally, strategies like paged attention (Kwon et al., 2023) have been developed to provide efficient page-like memory management for large numbers of tokens.",
            "score": 0.6929630473951944,
            "section_title": "Challenges with Serving Long Context LLM",
            "char_start_offset": 4926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 97,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1628
                }
            ],
            "ref_mentions": [
                {
                    "start": 331,
                    "end": 346,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 665,
                    "end": 683,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 963,
                    "end": 982,
                    "matchedPaperCorpusId": "253420623"
                },
                {
                    "start": 1511,
                    "end": 1530,
                    "matchedPaperCorpusId": "261697361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.222900390625
        },
        {
            "corpus_id": "276961696",
            "title": "Radar: Fast Long-Context Decoding for Any Transformer",
            "text": "Transformer models demonstrate an extraordinary ability on different sequential processing tasks, including language modeling (Vaswani et al., 2017;Devlin et al., 2019;Raffel et al., 2020;Touvron et al., 2023b), image classification (Dosovitskiy et al., 2021;Liu et al., 2021;Hassani et al., 2022;Zhu et al., 2023), translation (Tang et al., 2020;Yao & Wan, 2020), and many more (Carion et al., 2020;Ding et al., 2022;Chang et al., 2023;Xu et al., 2022;Fang et al., 2023). In particular, Transformer models take each input as a sequence of tokens and compute the embedding of each token for downstream tasks. Among all components, the dot-product attention has been shown to be critical to the success of Transformer models (Choromanski et al., 2021). It not only enables parallel computation of sequences during training (Vyas et al., 2020), but also provides a high-quality method for sequence modeling (Sanford et al., 2023). \n\nDespite being at the core of Transformer models, the dot-product attention is not ideal for longcontext data: the time to process each token increases with context lengths, significantly slowing down the throughput on long-context data. Moreover, the maximum context length is limited during training, resulting in an inability to perform inference on long-context tasks. Yet, many real-world applications are naturally long-context (Tay et al., 2021;Beltagy et al., 2020;Wu et al., 2024). For example, a code file could have more than 10K tokens (Lozhkov et al., 2024;Kocetkov et al., 2022). \n\nWe conduct extensive experiments to verify the effectiveness of our approach. Specifically, we compared our method with StreamingLLM (Xiao et al., 2024), Landmark attention (Mohtashami & Jaggi, 2023), H 2 O (Zhang et al., 2023), and SnapKV (Li et al., 2024).",
            "score": 0.6888789643750979,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1784
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 148,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 148,
                    "end": 168,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 168,
                    "end": 188,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 233,
                    "end": 259,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 259,
                    "end": 276,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 276,
                    "end": 297,
                    "matchedPaperCorpusId": "235359074"
                },
                {
                    "start": 297,
                    "end": 314,
                    "matchedPaperCorpusId": "258832459"
                },
                {
                    "start": 347,
                    "end": 363,
                    "matchedPaperCorpusId": "220045418"
                },
                {
                    "start": 379,
                    "end": 400,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 400,
                    "end": 418,
                    "matchedPaperCorpusId": "248476190"
                },
                {
                    "start": 418,
                    "end": 437,
                    "matchedPaperCorpusId": "255372955"
                },
                {
                    "start": 453,
                    "end": 471,
                    "matchedPaperCorpusId": "263834729"
                },
                {
                    "start": 724,
                    "end": 750,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 822,
                    "end": 841,
                    "matchedPaperCorpusId": "220424511"
                },
                {
                    "start": 905,
                    "end": 927,
                    "matchedPaperCorpusId": "259075636"
                },
                {
                    "start": 1364,
                    "end": 1382,
                    "matchedPaperCorpusId": "260440449"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23095703125
        },
        {
            "corpus_id": "265034082",
            "title": "Ultra-Long Sequence Distributed Transformer",
            "text": "The transformer is a powerful neural network architecture widely used in natural language and image processing (Vaswani et al., 2017). Its versatility is evidenced by its wide range of applications, including machine translation (Wang et al., 2019), chatbots (Caldarini et al., 2022), speech recognition (Dong et al., 2018), image captioning (Yu et al., 2019), image segmentation (Valanarasu et al., 2021;Strudel et al., 2021), and classification (Chen et al., 2021b). The transformer achieves its impressive performance by recognizing that different input sequence tokens have varying levels of importance to the final output prediction. The transformer captures the relationship between each pair of input tokens using a process called \"self-attention\". This allows the transformer to generate highly accurate outputs by focusing on the most relevant tokens in an input sequence while also paying attention to the overall context. This approach has proven to be highly effective and makes transformer a leading technology in artificial intelligence. \n\nWith long sequence training, transformer attends to many more input tokens than a transformer trained with short sequences. Therefore, long sequence training often captures more contextual information and leads to markedly 1, Oak Ridge National Laboratory, US. 2, National Institute of Advanced Industrial Science and Technology, Japan. 3, Agency for Science, Technology and Research, Singapore. Corresponding email: wangx2@ornl.gov higher prediction accuracy for many tasks with long-range dependencies, such as DNA sequence analysis (Zaheer et al., 2020), long document summary (Beltagy et al., 2020) and image segmentation (Strudel et al., 2021). Unfortunately, transformer's memory footprint increases quadratically and computations increase cubically with longer sequence lengths (Beltagy et al., 2020;Dao et al., 2022). Therefore, the transformer's sequence length is typically truncated to no more than a couple thousand tokens due to runtime and memory constraints, despite longer sequences leading to higher accuracy. \n\nTo address this issue, there are three research approaches: hierarchical training, attention approximation, and distributed sequence parallelism.",
            "score": 0.6803928535795973,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1051
                },
                {
                    "start": 1054,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2080
                },
                {
                    "start": 2083,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 133,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 229,
                    "end": 248,
                    "matchedPaperCorpusId": "174799399"
                },
                {
                    "start": 304,
                    "end": 323,
                    "matchedPaperCorpusId": "52287921"
                },
                {
                    "start": 342,
                    "end": 359,
                    "matchedPaperCorpusId": "159041705"
                },
                {
                    "start": 380,
                    "end": 405,
                    "matchedPaperCorpusId": "231986084"
                },
                {
                    "start": 405,
                    "end": 426,
                    "matchedPaperCorpusId": "234470051"
                },
                {
                    "start": 447,
                    "end": 467,
                    "matchedPaperCorpusId": "232404237"
                },
                {
                    "start": 1589,
                    "end": 1610,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1680,
                    "end": 1702,
                    "matchedPaperCorpusId": "234470051"
                },
                {
                    "start": 1861,
                    "end": 1878,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.143798828125
        },
        {
            "corpus_id": "272753750",
            "title": "WallFacer: Harnessing Multi-dimensional Ring Parallelism for Efficient Long Sequence Model Training",
            "text": "Over the past decade, Transformer [41] models have made remarkable strides in diverse fields, including computer vision (CV) and natural language processing (NLP). As the technology has evolved, the ability to efficiently process long sequences with Transformer has emerged as a pivotal challenge. For instance, in text summarization, the ability to handle extensive sequences is vital, as the content to be summarized can range from lengthy chapters to entire books [19,3]. Similarly, chat-based applications, such as ChatGPT [1], require the capacity to process extensive dialogue histories to ensure conversational consistency. There are also applications in other fields like video generation [5,33] and protein structure prediction [16,7]. \n\nThe long context in the above scenarios has introduced several challenges for model training and inference: 1) Efficiency and Adaptability. The challenge of efficiency predominantly lies in handling long sequences that require quadratic computations during attention, and in addressing the large amount of communication during distributed processing. 2) Memory. Besides the major obstacle of storing the model weight and optimizer states, the activation has also exceeded the capacity of a single GPU and risen as a new memory challenge due to the extreme sequence length. 3) Scalability. Current Transformer models usually require thousands of GPUs for pre-training, even with datasets of regular lengths. For longer sequences, ensuring an acceptable scaling speedup rate with both the sequence length and the number of GPUs increasing is even more critical to reducing time and economic costs. \n\nTraditional parallelisms such as Data Parallelism [12,40,25,44], Tensor Parallelism [40,42,43], and Pipeline Parallelism [14,11,24,27] distribute the model, input batch, and the optimizer states, but can not directly address the large memory requirement of extremely long sequences as the sequence length dimension remains unchanged. To break through this obstacle, Sequence Parallelism has been introduced, splitting the input on the sequence length dimension. Mainstream Sequence Parallelism schemes can generally be classified into two categories: those based on all-to-all communication, and those based on ring peer-to-peer communication. Methods like DeepSpeed Ulysses [15], which are based on all-to-all communication, offer efficiency but require the splitting of attention heads.",
            "score": 0.6797868833743036,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1642
                },
                {
                    "start": 1645,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2288
                },
                {
                    "start": 2289,
                    "end": 2433
                }
            ],
            "ref_mentions": [
                {
                    "start": 467,
                    "end": 471,
                    "matchedPaperCorpusId": "250118028"
                },
                {
                    "start": 737,
                    "end": 741,
                    "matchedPaperCorpusId": "235959867"
                },
                {
                    "start": 741,
                    "end": 743,
                    "matchedPaperCorpusId": "267789157"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "2315965"
                },
                {
                    "start": 1702,
                    "end": 1705,
                    "matchedPaperCorpusId": "199543548"
                },
                {
                    "start": 1705,
                    "end": 1708,
                    "matchedPaperCorpusId": "20425665"
                },
                {
                    "start": 1733,
                    "end": 1736,
                    "matchedPaperCorpusId": "251979875"
                },
                {
                    "start": 1773,
                    "end": 1776,
                    "matchedPaperCorpusId": "235898937"
                },
                {
                    "start": 1776,
                    "end": 1779,
                    "matchedPaperCorpusId": "261339639"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1868896484375
        },
        {
            "corpus_id": "1572419",
            "title": "Deep Learning for Genomics: A Concise Overview",
            "text": "the Hyena architecture  and scales sub-quadratically in context length, while Zhou et al. (2023) replace k-mer tokenization used in Ji et al. (2021) with Byte Pair Encoding (BPE) to achieve a 3x efficiency improvement.\n\nIn light of dealing with extremely long-range interactions in DNA sequences, the Enformer model Avsec et al. (2021) employs transformer modules that scale a five times larger receptive field compared to previous CNN-based approaches Zhou et al. (2018); Kelley et al. (2018); Kelley (2020), and is capable of detecting sequence elements that are 100 kb away. Moreover, the recent success of ChatGPT Schulman et al. (2022) and GPT-4 OpenAI (2023) further illustrated the emergent capabilities of large language models (LLMs) to deal with such long DNA sequences. A typical transformer-based genomics foundational model can only take 512 to 4k tokens as input context, which is less than 0.001% of the human genome.  proposed an LLM-based genomic model that expands the input context length to 1 million tokens at the single nucleotide level, which is up to 500x increase over previous dense attention-based models.",
            "score": 0.6645443128202795,
            "section_title": "Transformer-based Large Language Models",
            "char_start_offset": 22479,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 453,
                    "end": 471,
                    "matchedPaperCorpusId": "49868839"
                },
                {
                    "start": 473,
                    "end": 493,
                    "matchedPaperCorpusId": "4716278"
                },
                {
                    "start": 495,
                    "end": 508,
                    "matchedPaperCorpusId": "195379917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.030731201171875
        },
        {
            "corpus_id": "276813725",
            "title": "L2M: Mutual Information Scaling Law for Long-Context Language Modeling",
            "text": "(d) We formulate the Long-context Language Modeling (L 2 M) condition, which states that a model's state size for storing past information must grow at least as the power-law scaling of the bipartite mutual information for effective long context length modeling. \n\nIn pushing these advances further, the ability to handle long contexts has become increasingly crucial. This ability is the key to document-level understanding, multi-turn dialogue, and complex reasoning. Models like GPT-o1 and DeepSeek-R1 often generate extensive chains of thought, sometimes spanning thousands of tokens to solve complex problems (Wei et al., 2022;Wang et al., 2024). However, processing long contexts remains a significant challenge. While transformer architectures have achieved remarkable success and recent advances like DeepSeek have dramatically improved the efficiency (DeepSeek-AI et al., 2024), they still suffer from the intrinsic computational cost quadratic in sequence length, resulting in challenges in long sequence length generation. \n\nAlthough various architectures have been proposed to address the quadratic scaling (Katharopoulos et al., 2020;Gu et al., 2022a;Zhu et al., 2021;Beltagy et al., 2020;Ding et al., 2023;Gu & Dao, 2024;Dao & Gu, 2024;Peng et al., 2023;Sun et al., 2024), a fundamental gap persists in our theoretical understanding of how their effectiveness for cap-turing long-range dependencies scales. In spite of existing efforts to characterize these dependencies through various statistical measures (Ebeling & P\u00f6schel, 1994a;Debowski, 2011;Bentz et al., 2017;Futrell et al., 2019) theories guiding practical efficient architecture design have not yet been formulated. \n\nIn this work, we address the challenges of understanding long-context language modeling through the following contributions (Fig. 1).",
            "score": 0.6574138061391451,
            "section_title": "Introduction",
            "char_start_offset": 1706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 265,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1690
                },
                {
                    "start": 1693,
                    "end": 1826
                }
            ],
            "ref_mentions": [
                {
                    "start": 614,
                    "end": 632,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1147,
                    "end": 1164,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 1250,
                    "end": 1268,
                    "matchedPaperCorpusId": "258832459"
                },
                {
                    "start": 1582,
                    "end": 1603,
                    "matchedPaperCorpusId": "204913961"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.412109375
        },
        {
            "corpus_id": "268852354",
            "title": "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding",
            "text": "Throughout this survey, we position length extrapolation as a promising avenue towards long-context transformers. However, as stated in \u00a71, it's the length limit and poor length extrapolation together that prevents transformers from processing long sequences, thus the more direct way to extend the context window is to simply relax the length limit. \n\nThe most intuitive way to achieve large context window is directly pre-training the model or fine-tuning (continual pre-training) a pre-trained model on long sequences. Xiong et al. (2023) empirically demonstrated that long context continual pre-training is more efficient and similarly effective compared to pre-training from scratch with long sequences. However, both pre-training and fine-tuning (continual pre-training) are costly and demand large-scale high-quality long data, which is scarce (Kazemnejad et al., 2023). To reduce memory and computational overhead during training, recurrent Transformer variances integrate recurrence with attention (Dai et al., 2019;Bulatov et al., 2022) while efficient Transformer variants (Tay et al., 2022;Fournier et al., 2023) mainly aim at improving the quadratic complexity of attention mechanism, but both usually compromise some of the modeling capability and still need large-scale long sequence data. Flash Attention (Dao et al., 2022;Dao, 2023) greatly improves both training and inference efficiency of Transformers with little to no overhead, leading to models with much larger context window (Jiang et al., 2023;Gunasekar et al., 2023;Li et al., 2023a). \n\nOn the other side, there are more radical research efforts that attempt to abandon attention and its quadratic complexity with regard to sequence length completely, such as S4 (Gu et al., 2022), RWKV (Peng et al., 2023a), and Hyena (Poli et al., 2023). Further, some recent studies have attempted to scale these novel architectures to billions of parameters, leading to the emergence of Mamba (Gu and Dao, 2023) and RWKV-5/6 (Peng et al., 2024).",
            "score": 0.6483359756660717,
            "section_title": "Length-Extrapolated and Long-Context Transformers",
            "char_start_offset": 30415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1561
                },
                {
                    "start": 1564,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 2009
                }
            ],
            "ref_mentions": [
                {
                    "start": 1007,
                    "end": 1025,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1025,
                    "end": 1046,
                    "matchedPaperCorpusId": "250526424"
                },
                {
                    "start": 1084,
                    "end": 1102,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1102,
                    "end": 1123,
                    "matchedPaperCorpusId": "232380042"
                },
                {
                    "start": 1321,
                    "end": 1339,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1543,
                    "end": 1560,
                    "matchedPaperCorpusId": "272768248"
                },
                {
                    "start": 1764,
                    "end": 1784,
                    "matchedPaperCorpusId": "258832459"
                },
                {
                    "start": 1796,
                    "end": 1815,
                    "matchedPaperCorpusId": "257050308"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.283935546875
        },
        {
            "corpus_id": "259936734",
            "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
            "text": "Scaling up the context length of Transformers [18] is a challenge, since the attention layer at their heart has runtime and memory requirements quadratic in the input sequence length. Ideally, we would like to go beyond the standard 2k sequence length limit to train models to understand books, high resolution images, and long-form videos. Just within the last year, there have been several language models with much longer context than before: GPT-4 [12] with context length 32k, MosaicML's MPT with context length 65k, and Anthropic's Claude with context length 100k. Emerging use cases such as long document querying and story writing have demonstrated a need for models with such long context. \n\nTo reduce the computational requirement of attention on such long context, there have been numerous methods proposed to approximate attention [2,3,4,8,9,14,19,20]. Though these methods have seen some use cases, as far as we know, most large-scale training runs still use standard attention. Motivated by this, Dao et al. [5] proposed to reorder the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. This yields 2-4\u00d7 wall-clock time speedup over optimized baselines, up to 10-20\u00d7 memory saving, with no approximation, and as a result FlashAttention has seen wide adoption in large-scale training and inference of Transformers. \n\nHowever, context length increases even more, FlashAttention is still not nearly as efficient as other primitives such as matrix-multiply (GEMM). In particular, while FlashAttention is already 2-4\u00d7 faster than a standard attention implementation, the forward pass only reaches 30-50% of the theoretical maximum FLOPs/s of the device (Fig. 5), while the backward pass is even more challenging, reaching only 25-35% of maximum throughput on A100 GPU (Fig. 6). In contrast, optimized GEMM can reach up to 80-90% of the theoretical maximum device throughput.",
            "score": 0.6480313274697059,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1455
                },
                {
                    "start": 1458,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 846,
                    "end": 848,
                    "matchedPaperCorpusId": "248498407"
                },
                {
                    "start": 848,
                    "end": 850,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 850,
                    "end": 852,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 852,
                    "end": 854,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 854,
                    "end": 857,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 860,
                    "end": 863,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1022,
                    "end": 1025,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.441162109375
        },
        {
            "corpus_id": "239885427",
            "title": "Hierarchical Transformers Are More Efficient Language Models",
            "text": "Transformer models (Vaswani et al., 2017) are capable of solving many sequence modeling tasks, including classical NLP tasks (Devlin et al., 2019), summarization (Zhang et al., 2020), language modeling (Radford et al., 2019;Brown et al., 2020), code generation (Chen et al., 2021), or even music generation (Huang et al., 2018;Dhariwal et al., 2020) and image generation (Parmar et al., 2018;Chen et al., 2020;Ramesh et al., 2021). One compelling feature of Transformers is their ability to handle long contexts given as part of the input. This is particularly visible in tasks where the output depends on parts of the context that may not be * Equal contribution. Order determined by coin toss. close-by in the generated sequence, like in summarization, where the summary may need to refer to information scattered across the context, or in largescale image generation, where pixels belonging to the same object may be far apart in the generation order. Transformers excel at such tasks thanks to self-attention, and they are used with longer and longer contexts. Transformer-XL Hourglass Figure 1: Bits-per-character vs. training cost for baseline (orange) and hierarchical Transformers (green). We observe significant perplexity improvements on enwik8 over the vanilla Transformer-XL baseline, see text for details.\n\nThe ability of Transformers to handle long contexts comes at a price: each self-attention layer, at least in its original form, has complexity quadratic in the length of the context. When a stack of n Transformer layers is used, both memory and time complexity is equal to O(L 2 n) where L is a sequence length and n number of decoder blocks. Due to this limitation, vanilla transformers are infeasible to train on tasks with very long input sequences, for instance, on high-resolution images. This issue has been studied extensively, and a number of techniques were introduced that modify attention mechanism without changing overall transformer architecture (Child et al., 2019;Roy et al., 2020;Ren et al., 2021). These sparse attention mechanisms reduce the complexity of self-attention but still force the model to operate on",
            "score": 0.6478556768404037,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.310791015625
        },
        {
            "corpus_id": "271083368",
            "title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
            "text": "Sequence parallelism [15], [89], is a novel approach proposed to efficiently train Transformers with longer sequences on GPUs. It addresses the quadratic memory requirements of self-attention in Transformer models. Unlike traditional methods, it does not require a single device to handle the entire sequence. By splitting sequences into chunks and distributing them across devices, it achieves effective training with infinitely long sequences. It introduces Ring Self-Attention to enhance the process, demonstrating superior performance in batch size and sequence length compared to tensor parallelism, handling sequences over 27\u00d7 longer than existing methods.",
            "score": 0.644474432368535,
            "section_title": "5) Sequence Parallelism",
            "char_start_offset": 92308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 662
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 25,
                    "matchedPaperCorpusId": "240070340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.033660888671875
        },
        {
            "corpus_id": "269757781",
            "title": "USP: A Unified Sequence Parallelism Approach for Long Context Generative AI",
            "text": "Sequence parallelism (SP), which divides the sequence dimension of input tensors across multiple computational devices, is becoming key to unlocking the long-context capabilities of generative AI models. This paper investigates the state-of-the-art SP approaches, i.e. DeepSpeed-Ulysses and Ring-Attention, and proposes a unified SP approach, which is more robust to transformer model architectures and network hardware topology. This paper compares the communication and memory cost of SP and existing parallelism, including data/tensor/zero/pipeline parallelism, and discusses the best practices for designing hybrid 4D parallelism involving SP. We achieved 47% MFU on two 8xA800 nodes using SP for the LLAMA3-8B model training using sequence length 208K. Our code is publicly available at https://github.com/feifeibear/long-context-attention.",
            "score": 0.6412234689475318,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06744384765625
        },
        {
            "corpus_id": "263134203",
            "title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization",
            "text": "Transformer (Vaswani et al., 2017) language models would ideally scale to long sequences, since their predictive abilities often improve as context length increases (Dai et al., 2019;Kaplan et al., 2020). Unfortunately, the standard transformer uses a self-attention mechanism with a quadratic time complexity with respect to sequence length. This limits the practicality of applying transformers to very long sequences, since increasing the sequence length by a factor of 10 n increases the attention computations by a factor of 100 n . Transformer variants that overcome this efficiency bottleneck have the potential to facilitate new long-context applications and enable new breakthroughs. \n\nUp to this point, a variety of efficient transformers (Tay et al., 2020b) have been proposed to scale to long sequences. Techniques include sparsity (Child et al., 2019;Ye et al., 2019;Beltagy et al., 2020;Kitaev et al., 2020;Qiu et al., 2020;Roy et al., 2021;Tay et al., 2020a;Sukhbaatar et al., 2021;Wu et al., 2022;Liu et al., 2023;Zhang et al., 2023), compression (Liu et al., 2018;Rae et al., 2020;Ainslie et al., 2020;Zhu et al., 2021;Ren et al., 2021;Nawrot et al., 2021;2023), low-rank approximations (Wang et al., 2020;Vyas et al., 2020;Katharopoulos et al., 2020;Xiong et al., 2021;Tay et al., 2021;Choromanski et al., 2021), and cross-attention operations (Dai et al., 2019;Ma et al., 2021;Hutchins et al., 2022;Hawthorne et al., 2022). Other efficient sequence models have also been proposed (Gu et al., 2022;Lee-Thorp et al., 2022;Mehta et al., 2022;Smith et al., 2022;Hasani et al., 2022;Poli et al., 2023;Peng et al., 2023).",
            "score": 0.6348055532530889,
            "section_title": "INTRODUCTION",
            "char_start_offset": 318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 692
                },
                {
                    "start": 695,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1634
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 34,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 165,
                    "end": 183,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 901,
                    "end": 921,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 921,
                    "end": 938,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 938,
                    "end": 955,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 973,
                    "end": 997,
                    "matchedPaperCorpusId": "234681615"
                },
                {
                    "start": 1063,
                    "end": 1081,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 1081,
                    "end": 1098,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 1098,
                    "end": 1119,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 1136,
                    "end": 1153,
                    "matchedPaperCorpusId": "235829099"
                },
                {
                    "start": 1223,
                    "end": 1241,
                    "matchedPaperCorpusId": "220424511"
                },
                {
                    "start": 1241,
                    "end": 1268,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1268,
                    "end": 1287,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 1287,
                    "end": 1304,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 1304,
                    "end": 1329,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1362,
                    "end": 1380,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1396,
                    "end": 1418,
                    "matchedPaperCorpusId": "247451135"
                },
                {
                    "start": 1418,
                    "end": 1441,
                    "matchedPaperCorpusId": "246867214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21484375
        },
        {
            "corpus_id": "258987968",
            "title": "Blockwise Parallel Transformer for Large Context Models",
            "text": "Transformers have garnered significant attention in the field of natural language processing (NLP) and have become the basis for numerous state-of-the-art models. Several works have explored memoryefficient techniques to address the memory limitations of Transformers and enable their application to longer input sequences. One line of research focuses on various approximation techniques or compressing along the sequence dimension [see e.g. 24,12,14,4,42,54,36,25]. Other works explored replacing attention [19,20,41,23,3,57,40,53]. Another line of work explores partitioning the large hidden dimension of the feedforward network into parts and retrieving only one part per token [30,48,17,26,58,60]. Additionally, extending the context by attending over states from previous sequences has been explored [13,44], as well as combining local and global contexts [21,11]. For a comprehensive review of these techniques, we recommend referring to the surveys by Tay et al. \n\n[51], Narang et al. [38], Tay et al. [50]. Several studies explored sharding large model on distributed devices tensor, data, or sequence parallelism [49,16,55,27,59,31,46]. Ours shares similarities with the sequence parallelism [27] where sequences are distributed across devices, in contrast, ours implements blockwise computation on sequences for each device. This creates an orthogonal relationship between our method and sequence parallelism, allowing for straightforward combination. In addition, our methodology is compatible with both tensor and data parallelism. Another direction involves computing exact self-attention in a blockwise manner using the tiling technique [37]. This approach has led to the development of memory efficient attention mechanisms [14,42]. In line with these advancements, our work falls into this category. We propose computing both the feedforward network and self-attention in a blockwise manner, resulting in a significant reduction in memory requirements.",
            "score": 0.6302010942686069,
            "section_title": "Related Work",
            "char_start_offset": 20945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1969
                }
            ],
            "ref_mentions": [
                {
                    "start": 443,
                    "end": 446,
                    "matchedPaperCorpusId": "232110866"
                },
                {
                    "start": 449,
                    "end": 452,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 509,
                    "end": 513,
                    "matchedPaperCorpusId": "221150566"
                },
                {
                    "start": 519,
                    "end": 522,
                    "matchedPaperCorpusId": "247011581"
                },
                {
                    "start": 689,
                    "end": 692,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1142,
                    "end": 1145,
                    "matchedPaperCorpusId": "221191193"
                },
                {
                    "start": 1740,
                    "end": 1744,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.276611328125
        },
        {
            "corpus_id": "256847015",
            "title": "A Unified View of Long-Sequence Models towards Modeling Million-Scale Dependencies",
            "text": "Ever since their conception, Transformers have taken over traditional sequence models in many tasks, such as NLP, image classification, and video/audio processing, for their fast training and superior performance. Much of the merit is attributable to positional encoding and multi-head attention. However, Transformers fall short in learning long-range dependencies mainly due to the quadratic complexity scaled with context length, in terms of both time and space. Consequently, over the past five years, a myriad of methods has been proposed to make Transformers more efficient. In this work, we first take a step back, study and compare existing solutions to long-sequence modeling in terms of their pure mathematical formulation. Specifically, we summarize them using a unified template, given their shared nature of token mixing. Through benchmarks, we then demonstrate that long context length does yield better performance, albeit application-dependent, and traditional Transformer models fall short in taking advantage of long-range dependencies. Next, inspired by emerging sparse models of huge capacity, we propose a machine learning system for handling million-scale dependencies. As a proof of concept, we evaluate the performance of one essential component of this system, namely, the distributed multi-head attention. We show that our algorithm can scale up attention computation by almost $40\\times$ using four GeForce RTX 4090 GPUs, compared to vanilla multi-head attention mechanism. We believe this study is an instrumental step towards modeling million-scale dependencies.",
            "score": 0.6200722623767371,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1702880859375
        },
        {
            "corpus_id": "248070506",
            "title": "Hardware-Software Co-Design of an In-Memory Transformer Network Accelerator",
            "text": "For example, in language modeling, the Penn Treebank (Marcus et al., 1993) and WikiText-103 (Merity et al., 2016) datasets, which are obtained from news and Wikipedia articles, have an average sequence length of 355 and 3.6 K words, respectively. On the other hand, PG-19 (Rae et al., 2019), a newer dataset for language modeling which is obtained from Project Gutenberg books, has an average sequence length of 69 K words. The use of transformer networks in image and video applications can also contribute to the sequence length explosion. As transformer networks improve and are used in more complex applications, the number of parameters also continues to increase (Sharir et al., 2020). The vanilla (original) transformer (Vaswani et al., 2017) began with millions of parameters. Later, transformer network models, e.g., Megatron (Shoeybi et al., 2019) and GPT-3 (Brown et al., 2020), contain billions of parameters. Recently, switch transformers (Fedus et al., 2021) used trillions of parameters to account for long-range dependencies in the language model of the PG-19 dataset. \n\nTransformer networks are commonly implemented using general-purpose graphical processing units (GPUs) to exploit the parallelism inherent in the attention mechanism. However, the complexity of implementing the attention mechanism in the GPU is limited to O (dn 2 /c), where n is the sequence length, d is the number of feature embedding dimensions, and c is the number of parallel cores. Increasing the sequence length and the number of parameters greatly increases the computation latency, memory bandwidth, and energy requirements of transformer networks (Vaswani et al., 2017) because of the quadratic time and space complexity with respect to the sequence length. Transformer networks with linear time complexity have been proposed (Beltagy et al., 2020;Zaheer et al., 2020), but incur the cost of additional space complexity, causing increased memory demand. Moreover, large transformer networks are severely limited by the memory bandwidth.",
            "score": 0.6125972186001885,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1617,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 727,
                    "end": 749,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 835,
                    "end": 857,
                    "matchedPaperCorpusId": "202660670"
                },
                {
                    "start": 1644,
                    "end": 1666,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1823,
                    "end": 1845,
                    "matchedPaperCorpusId": "204837321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.028594970703125
        },
        {
            "corpus_id": "262826014",
            "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
            "text": "Shown in Figure 1 is a sketch of building blocks of a typical multihead attention Transformer architecture [Vaswani et al., 2017]. It consists of input sequences which are projected into queries (Q),keys (K) and values (V) embeddings. \n\nQKV are typically a 3D tensor of size N, b, d where N is sequence length, b is micro batch size and d is hidden dimension. \n\nThe QKV tensors are fed to the attention block, a central component of Transformer model. Outputs of attentions are inputs to the multilayer perceptron (MLP) or position-wise feed-forward block of Transformer architecture. \n\nThe attention block followed by MLP block are replicated multiple times to form an encoder, a decoder or an encoderdecoder Transformer network. It is worth to note that our proposed approach is orthogonal to both data parallelism and ZeRO. Our proposed approach can be used with both methods. Also, by leveraging sequence parallelism to keep global batch size at reasonable size on large systems, we effectively ameliorate the impact of large batch size on model convergence. Sequence parallelism serves two purposes in this regard. First, sequence parallelism can accelerate time to solution for same (already explored) long sequence length; in other words, sequence parallelism reduces the iteration time proportional to additional compute resources. Second, sequence parallelism enables longer sequence training or continual pretraining where training context length gradually increase over time [Xiong et al., 2023]. Consider a real world scenario of large scale training on 1024 GPUs. The initial exploratory or pretraining set up of a (proxy) LLM has a sequence length of 8192 (8K), a micro batch size of 1 (thus, 8 million token global size) per GPU. A simple change to improve the quality of the pretrained model requires a change of sequence length from 8K to 32K, which would result in approximately 32 million global batch size. However, increasing the global batch size is not an option due to the negative impact on model quality. Therefore, sequence parallelism comes in handy as a system optimization technique with no requirement for laborious hyperparameter search.",
            "score": 0.6124607121337206,
            "section_title": "Transformer Architecture",
            "char_start_offset": 5182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2169
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04742431640625
        },
        {
            "corpus_id": "268857023",
            "title": "Long-context LLMs Struggle with Long In-context Learning",
            "text": "The effectiveness of Transformer-based models is hindered by the quadratic increase in computational cost relative to sequence length, particularly in handling long context inputs.Recent efforts have explored various strategies to address this challenge.Some studies have pursued continued fine-tuning of the LLM with longer context inputs (Rozi\u00e8re et al., 2024;Tworkowski et al., 2023).Others have leveraged position extrapolation or interpolation, building upon relative rotary positional embedding (Su et al., 2021), to extend input length beyond the training phase (Press et al., 2022;Chen et al., 2023a).Additionally, more approaches have been proposed to mitigate computational issues, including sliding memory window and chunk segmentation (Hao et al., 2022;Ratner et al., 2023;Zhu et al., 2024).Furthermore, alternative architectures beyond Transformer have been explored to handle long inputs more naturally, such as selective-state-spaces models (Peng et al., 2023a;Gu & Dao, 2023).These diverse approaches claim that they can enhance the capabilities of LLMs in processing long context inputs more efficiently.\n\nLong Context Evaluation Due to the imperious demands for the support of long-range LLMs, there is a series of benchmarks focusing on long context evaluation.Long-Range Arena (Tay et al., 2021) includes tasks consisting of sequences ranging from 1K to 16K tokens to evaluate variations of fast Transformers.LongBench (Bai et al., 2023b) comprises 21 bilingual datasets with an average length of around 6k words, which have been processed in a unified format to enable effortless evaluation.L-Eval Benchmark (An et al., 2023) supports 20 sub-tasks with input lengths of 3K to 200K tokens.LooGLE (Li et al., 2023b) focuses on summarization and long dependency QA tasks with test instances exceeding 100k words.Most recently, \u221eBench (Zhang et al., 2024) encompasses 12 tasks with an average length of 200K tokens.",
            "score": 0.6105487887069162,
            "section_title": "Long Context Techniques over LLMs",
            "char_start_offset": 6695,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 387
                },
                {
                    "start": 387,
                    "end": 609
                },
                {
                    "start": 609,
                    "end": 803
                },
                {
                    "start": 803,
                    "end": 992
                },
                {
                    "start": 992,
                    "end": 1121
                },
                {
                    "start": 1123,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1429
                },
                {
                    "start": 1429,
                    "end": 1612
                },
                {
                    "start": 1612,
                    "end": 1709
                },
                {
                    "start": 1709,
                    "end": 1830
                },
                {
                    "start": 1830,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 569,
                    "end": 589,
                    "matchedPaperCorpusId": "237347130"
                },
                {
                    "start": 765,
                    "end": 785,
                    "matchedPaperCorpusId": "258686160"
                },
                {
                    "start": 785,
                    "end": 802,
                    "matchedPaperCorpusId": "262053659"
                },
                {
                    "start": 956,
                    "end": 976,
                    "matchedPaperCorpusId": "258832459"
                },
                {
                    "start": 1297,
                    "end": 1315,
                    "matchedPaperCorpusId": "260440449"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58154296875
        },
        {
            "corpus_id": "237266377",
            "title": "Fastformer: Additive Attention Can Be All You Need",
            "text": "Transformer (Vaswani et al., 2017) and their variants have achieved great success in many fields. For example, Transformer is the backbone architecture of many state-of-the-art pre-trained language models in NLP, such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2019). Transformer also shows great promises in vision-related tasks (Dosovitskiy et al., 2021). The core of a Transformer model is self-attention mechanism, which allows the Transformer to model the contexts within an input sequence (Parikh et al., 2016). However, since self-attention computes the dot-product between the input representations at each pair of positions, its complexity is quadratic to the input sequence length (Vaswani et al., 2017). Thus, it is difficult for standard Transformer models to efficiently handle long input sequences (Tay et al., 2020). \n\nThere are many methods to accelerate the Transformer model (Beltagy et al., 2020;Zaheer et al., 2020;Wang et al., 2020b;Kitaev et al., 2020;Tay et al., 2021). For example, BigBird (Zaheer et al., 2020) computes sparse attention instead of a dense one. It uses a combination of local attention, global attention at certain positions and random attention between a certain number of tokens. However, sparse attention usually cannot fully model the global context (Wu et al., 2021b). Linformer (Wang et al., 2020b) exploits the low-rank characteristic of the self-attention matrix by computing approximated ones. It projects attention key and value into low-dimensional matrices that are independent of the sequence length. However, the approximation is in fact context-agnostic, which may weaken the context modeling ability of Transformer. In addition, these methods are not efficient enough when the input sequence length is very long. \n\nIn this paper we propose Fastformer 1 , which is an efficient Transformer variant based on additive attention that can achieve effective context modeling in linear complexity. In Fastformer, we first use additive attention mechanism to summarize the input attention query matrix into a global query vector.",
            "score": 0.6095811515087848,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2090
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 34,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 226,
                    "end": 247,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 256,
                    "end": 278,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 342,
                    "end": 368,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 507,
                    "end": 528,
                    "matchedPaperCorpusId": "8495258"
                },
                {
                    "start": 703,
                    "end": 725,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 986,
                    "end": 1003,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 1307,
                    "end": 1325,
                    "matchedPaperCorpusId": "235294151"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.156005859375
        },
        {
            "corpus_id": "265551773",
            "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
            "text": "Long context has become a popular subject, and several recent models have claimed to scale to longer and longer sequences. However, these are often from a computational standpoint and have not been extensively validated. These include: \n\n\u2022 Recurrent Memory Transformer (Bulatov, Kuratov, and Burtsev 2023), a lightweight wrapper around a Transformer backbone. It showed ability to generalize up to 1M sequences but only on synthetic memorization tasks; their main result is similar to our Induction Heads extrapolation experiment (Table 2). \n\n\u2022 LongNet (Ding et al. 2023), which claimed to scale to 1B length but only evaluated on length < 100 for actual tasks. \n\n\u2022 Hyena and HyenaDNA (Nguyen, Poli, et al. 2023;Poli et al. 2023), which claimed to leverage up to 1M context. However, their experiments trained on proportionally more data at longer contexts, making it hard to conclude if quality improvements at 1M context are due to context length or due to more data and computation. \n\n\u2022 Sparse Transformer (Child et al. 2019) showed a proof-of-concept of using a strided sparse attention Transformer to model audio waveforms of length 2 20 = 1048576, although did not discuss performance tradeoffs when controlling for computation and model size. \n\nIn contrast, we believe this work presents one of the first approaches to meaningfully demonstrate increasing performance with longer context. which is also called a leaky integrator.",
            "score": 0.6086366630012489,
            "section_title": "B.5 Long Context Models",
            "char_start_offset": 52218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1435
                }
            ],
            "ref_mentions": [
                {
                    "start": 712,
                    "end": 729,
                    "matchedPaperCorpusId": "257050308"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2354736328125
        },
        {
            "corpus_id": "268510227",
            "title": "DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers",
            "text": "In this work, we introduce Dynamic Sequence Parallelism (DSP), a novel sequence parallel abstraction for effectively scaling multi-dimensional transformers to long sequences. Unlike current embedded sequence parallel methods that only shard on single sequence dimension and are tailored to specific patterns, DSP offers a general and elegant solution by dynamically switching the parallel dimension during computation, decoupled from the computation module. \n\nThe key advantages of DSP are: 1) substantially reduced communication costs, 2) adaptability across modules without specialized modifications, and 3) remarkable ease of implementation enabled by a simple high-level API. Our experiments demonstrated DSP's superiority, achieving from 32.2% to 10\u00d7 higher end-to-end throughput and at least 75% lower communication volume compared to state-ofthe-art methods. Its elegance and ease of use make it a promising solution for efficient sequence parallelism across a wide range of applications. \n\nLimitations. One limitation of this work is that DSP is specifically designed for multi-dimensional transformers and may not adapt well to single-dimensional ones like language models. Additionally, while there are global operations that involve all sequence dimensions, which are rare in transformer, DSP may not be of optimal efficiency. \n\nFuture works. In the future, DSP could expand its scope beyond transformer architectures to architectures including convolution, recurrent, and graph neural networks to utilize its potential across various tasks. Furthermore, automated optimization techniques could enable DSP to dynamically and autonomously determine the most effective switching strategy based on network analysis, thereby optimizing overall system efficiency and efficacy.",
            "score": 0.6062451212552559,
            "section_title": "Conclusion",
            "char_start_offset": 21308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 995
                },
                {
                    "start": 998,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1782
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0054473876953125
        },
        {
            "corpus_id": "267406617",
            "title": "Repeat After Me: Transformers are Better than State Space Models at Copying",
            "text": "Transformers (Vaswani et al., 2017) are the workhorse of modern sequence modeling, achieving remarkable performance on a variety of tasks, but they have unavoidable inefficiencies.Specifically, they require \u2126(L) memory1 and Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024.Copyright 2024 by the author(s). 1In some naive implementations of transformers, it is common to allocate a L \u00d7 L matrix to compute the attention.However, compute to predict the next token of a sequence of length L.\n\nThis has spurred a boom in attempts to create architectures that can achieve similar performance as transformers, but with O(1) memory to predict each token.This class of models includes state space models like S4 (Gu et al., 2021) or Mamba (Gu & Dao, 2023), as well as traditional RNN models (Hochreiter & Schmidhuber, 1997) and models that can be trained in parallel like linear attention (Katharopoulos et al., 2020;Choromanski et al., 2020) and parallel RNNs (Bradbury et al., 2016;Peng et al., 2023;Sun et al., 2023).In this paper, we will refer to this entire class of models that use a fixed-size memory as \"generalized state space models\" or GSSMs (see a formal definition in Section 2).\n\nRecent work has demonstrated impressive performance of GSSMs, but it is not yet clear what these models sacrifice for their improved efficiency, if anything.In this paper, we find that one particular capability that is sacrificed is the ability to retrieve and repeat parts of the input context.As a result, transformers are better than GSSMs at a variety of tasks that require accessing arbitrary parts of the context.\n\nTo understand this gap in capabilities, we begin by presenting a theoretical analysis of the copying task2 .First, we show via construction that a simple transformer model can copy strings of length that is exponential in the number of heads of the transformer.",
            "score": 0.6034331530657968,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 327
                },
                {
                    "start": 327,
                    "end": 361
                },
                {
                    "start": 361,
                    "end": 473
                },
                {
                    "start": 473,
                    "end": 542
                },
                {
                    "start": 544,
                    "end": 701
                },
                {
                    "start": 701,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1239
                },
                {
                    "start": 1241,
                    "end": 1398
                },
                {
                    "start": 1398,
                    "end": 1536
                },
                {
                    "start": 1536,
                    "end": 1660
                },
                {
                    "start": 1662,
                    "end": 1770
                },
                {
                    "start": 1770,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 837,
                    "end": 869,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 935,
                    "end": 963,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.015716552734375
        },
        {
            "corpus_id": "269757781",
            "title": "USP: A Unified Sequence Parallelism Approach for Long Context Generative AI",
            "text": "The field of artificial intelligence is witnessing a trend as the context length in generative AI models grows ever longer.Claude has pioneered this trend in large language models (LLMs) by extending the sequence length to 100K tokens.Following closely in its footsteps, OpenAI's GPT-4 has expanded the context length to 128K tokens.The advent of multi-modality models is propelling this trend forward, with Gemini 1.5 Pro boasting a context length of a staggering 10 million tokens, and OpenAI's Sora, a Diffusion Model, accommodating at least 1 million visual tokens.These breakthroughs underscore the imperative for generative AI techniques to adeptly handle a larger context length.\n\nSequence Parallelism (SP), a technique that partitions input sequences, has emerged as a promising approach for the training or inference of longer sequences.Following an initial exploration period of two years, by the latter of 2023, two landmark works, DeepSpeed-Ulysses [1] and Ring-Attention [2], marked the maturation of the SP technique.DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally, while Ring-Attention hides P2P communication costs introduced by SP through overlapping computation and communication.However, challenges remain, such as the SP parallel degree of DeepSpeed-Ulysses is limited to less than the number of attention heads, and the computational efficiency of Ring-Attention degrading due to the subdivision of matrix multiplications.These limitations currently hinder the broader adoption of Sequence Parallelism in distributed Transformer computation.\n\nIn this paper, we delve deeper into the realm of SP.We begin by highlighting that Ulysses and Ring are not mutually exclusive approaches; they can be combined through a hybrid parallel strategy to mitigate their drawbacks.Then, we discussed the relationship between SP and data/tensor/zero/expert/pipeline parallelism.The most complex among these is the relationship between SP and tensor parallelism.Since tensor parallelism also has its specific sequence parallel optimizations to reduce activation memory cost [3].For each parallelism approach, whether SP should replace it or is there some issue to using SP with it together, remains an open question.",
            "score": 0.6025792504582856,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 123,
                    "end": 235
                },
                {
                    "start": 235,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 569
                },
                {
                    "start": 569,
                    "end": 686
                },
                {
                    "start": 688,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 1031
                },
                {
                    "start": 1031,
                    "end": 1278
                },
                {
                    "start": 1278,
                    "end": 1523
                },
                {
                    "start": 1523,
                    "end": 1642
                },
                {
                    "start": 1644,
                    "end": 1696
                },
                {
                    "start": 1696,
                    "end": 1866
                },
                {
                    "start": 1866,
                    "end": 1962
                },
                {
                    "start": 1962,
                    "end": 2045
                },
                {
                    "start": 2045,
                    "end": 2161
                },
                {
                    "start": 2161,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 2157,
                    "end": 2160,
                    "matchedPaperCorpusId": "248693351"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1580810546875
        },
        {
            "corpus_id": "248227781",
            "title": "FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks",
            "text": "Attention mechanisms, the key building block of transformer models, have enabled state-of-the-art results across a wide range of machine learning (ML) tasks-from natural language processing (NLP) [17,53,90], to object detection [6,82,111], image classification [28,100,108,110], image generation [7,21,66], and music synthesis [34,35].\n\nThis exponential growth of transformer models are expected to serve as the foundation of a new bread of machine learning models in the upcoming years. A key attribute of attention-based models is the sequence length ( ) defining the number of input elements for which a pairwise correlation scores is computed. Intuitively, increasing sequence length enables the attention-based models to better capture the context of input sentences or the relation between image segments. The demand for leveraging long-sequence (e.g. = 8 to = 69 ) attention-based models has already emerged in ML community [87], beyond natural language understanding [70] into protein folding [14] and text summarization [47] and audio generation [57]. Employing long sequences is pivotal in these algorithms because the property of input emerges from the global context. For example, two proteins may look identical if we examine identical sequence fragments, but when the entire sequence is considered, the differences in their function arise. We observe an analogous phenomenon in text summarization, where context can drastically alter the meaning of the selected text subset. In that instance, the subset represents a shorter sequence while the entire context refers to the full-length one.\n\nCompared to existing neural network accelerators [9,11,20,67,78,105], architecting accelerators for attention-based models poses different design challenges, attributed to their soaring demand for on-chip memory and compute complexities. Recent accelerators for attention-based models [30,31] have mainly relied on algorithmic optimizations, often with negative repercussion on model accuracy. Algorithmic techniques in practice include sparsification or compression [5, 12-14, 16, 17, 43, 47, 56, 66, 68, 70, 73, 80, 86, 94, 104] and/or leveraging lossy approximation [30,31,93].\n\nIn this work",
            "score": 0.6001670984212242,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 206,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 234,
                    "end": 238,
                    "matchedPaperCorpusId": "222208633"
                },
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "233004577"
                },
                {
                    "start": 265,
                    "end": 269,
                    "matchedPaperCorpusId": "232417787"
                },
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "235743105"
                },
                {
                    "start": 296,
                    "end": 299,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 331,
                    "end": 334,
                    "matchedPaperCorpusId": "54477714"
                },
                {
                    "start": 931,
                    "end": 935,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 1654,
                    "end": 1657,
                    "matchedPaperCorpusId": "207882941"
                },
                {
                    "start": 1660,
                    "end": 1663,
                    "matchedPaperCorpusId": "11504619"
                },
                {
                    "start": 1663,
                    "end": 1666,
                    "matchedPaperCorpusId": "211114941"
                },
                {
                    "start": 1666,
                    "end": 1669,
                    "matchedPaperCorpusId": "202547735"
                },
                {
                    "start": 1669,
                    "end": 1673,
                    "matchedPaperCorpusId": "21656909"
                },
                {
                    "start": 1894,
                    "end": 1897,
                    "matchedPaperCorpusId": "235414966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1551513671875
        },
        {
            "corpus_id": "271244689",
            "title": "MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training",
            "text": "Since the advent of ChatGPT [58], Large Language Models (LLMs) have demonstrated remarkable proficiency in comprehending and generating natural language texts. Besides revolutionizing the field of language processing, which encompasses translation [103], coding [23,72,94], etc., transformer-based LLMs have also found applications in multi-modal scenarios, such as image processing [15,61], video stream analysis [73], and AI for science [2,5]. To accommodate novel applications that require lengthy contexts [98], LLMs have developed to support long context input, from 2K-4K [79,81] to 32K [29,80], 128K [18,58], or even millions of tokens [1,9,41]. Considering the extrapolation problem [43,66], which refers to the decline in LLM performance when input sequences exceed the training length, it is necessary to conduct long context training [7,17,28] or fine-tuning [14,62] to facilitate long sequence inference. Beyond natural language processing, increasing the context length is also essential across diverse domains, including video processing [101], protein properties prediction [6], weather forecasting [54], and health care [40]. \n\nMaximizing system performance with limited memory is a common and significant challenge in the data management community. Within this context, training LLMs with long sequence lengths poses difficulties due to restricted GPU memory. During training, a large amount of activations 1 must be stored for gradient computation during the backward pass, resulting in substantial memory consumption. Typically, it is well known that the self-attention module in the transformer architecture has a quadratic computation and memory complexity w.r.t. the sequence length. FlashAttention [11,12], now a standard technique for attention computation in LLM training, accelerates computation and shrinks the memory complexity to be linear w.r.t. the sequence length by scheduling memory I/O and recomputing necessary components during the backward pass. Except for attention, the remaining activation memory also scales linearly with the sequence length, which can become quite large in long context scenarios.",
            "score": 0.5990429143497105,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 253,
                    "matchedPaperCorpusId": "258048937"
                },
                {
                    "start": 269,
                    "end": 272,
                    "matchedPaperCorpusId": "272045337"
                },
                {
                    "start": 383,
                    "end": 387,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 414,
                    "end": 418,
                    "matchedPaperCorpusId": "237581134"
                },
                {
                    "start": 442,
                    "end": 444,
                    "matchedPaperCorpusId": "271931441"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "267682361"
                },
                {
                    "start": 691,
                    "end": 695,
                    "matchedPaperCorpusId": "263828829"
                },
                {
                    "start": 695,
                    "end": 698,
                    "matchedPaperCorpusId": "237347130"
                },
                {
                    "start": 870,
                    "end": 874,
                    "matchedPaperCorpusId": "267770308"
                },
                {
                    "start": 874,
                    "end": 877,
                    "matchedPaperCorpusId": "261493986"
                },
                {
                    "start": 1089,
                    "end": 1092,
                    "matchedPaperCorpusId": "255966856"
                },
                {
                    "start": 1114,
                    "end": 1118,
                    "matchedPaperCorpusId": "256231457"
                },
                {
                    "start": 1721,
                    "end": 1725,
                    "matchedPaperCorpusId": "259936734"
                },
                {
                    "start": 1725,
                    "end": 1728,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.525390625
        },
        {
            "corpus_id": "264451707",
            "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
            "text": "Hierarchical Architecture / Sparse Attention. To overcome the quadratic complexity of attention, Dai et al. (2019) proposes the Transformer-XL that handles the long sequence at segment level by Transformer, with these segments interacting through a recurrence mechanism. The Recurrent Memory Transformer (Bulatov et al., 2022) refines this mechanism by incorporating special memory tokens into the recurrence, which is capable of scaling the context length to the millions (Bulatov et al., 2023). On the other hand, Child et al. (2019); Beltagy et al. (2020) proposed using the sparse attention to circumvent the full access to the long sequences, hence reducing the complexity. The sparse attention has been adopted by Ding et al. (2023a) to scale the context length of transformers into the billions. However, these methods sacrifice the utilisation of the entire sequence during attention, resulting in an inevitable loss of some contextual information. Additionally, modifications to the model architecture make these methods challenging to apply to existing pre-trained LLMs. Conversely, our CLEX serves as a drop-in component for LLMs, can efficiently extend the capacity of models to tack the entire long sequences without explicit drops of context information. \n\nLength Extrapolation. Building on the foundation laid by ALiBi (Press et al., 2022), a series of works (Sun et al., 2023;Chi et al., 2022;2023) seek to train the Transformer-based models on a short length, while directly testing on longer counterparts. These methods substitute the position embedding with bias introduced into attention scores, thereby incorporating positional information. Notably, the bias typically gives higher profits to closer tokens. This mechanism intuitively amplifies the local context for each token at the expense of distant information. Consequently, these length-extrapolation methods encounter challenges in effectively handling long contexts in practical applications (Pal et al., 2023). However, our CLEX demonstrates remarkable effectiveness in practical tasks such as summarization, indicating the de facto extrapolation ability for applications. \n\nPosition Embedding (PE) Scaling. Recent research has sought to extend the context length of Transformers through the scaling of the extensively employed RoPE.",
            "score": 0.5985137641264355,
            "section_title": "RELATED WORK",
            "char_start_offset": 21877,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2153
                },
                {
                    "start": 2156,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2314
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 114,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 304,
                    "end": 326,
                    "matchedPaperCorpusId": "250526424"
                },
                {
                    "start": 1334,
                    "end": 1354,
                    "matchedPaperCorpusId": "237347130"
                },
                {
                    "start": 1374,
                    "end": 1392,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 1392,
                    "end": 1409,
                    "matchedPaperCorpusId": "248965309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.407958984375
        },
        {
            "corpus_id": "269922123",
            "title": "The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving",
            "text": "We discuss works that explore the Sequence Parallelism (SP) dimension in a distributed fashion.\n\nHere, a long-context inference request is segmented into sub-sequences and distributed across nodes for parallel processing.While traditional distributed strategies like tensor parallelism (TP) or pipeline parallelism (PP) can also enhance inference performance, we omit them in this survey because, they are not specifically designed for long-context handling and generally serve as orthogonal or complementary to SP optimizations.\n\nOur analysis unfolds in two steps.First, we investigate methods to accelerate a single long-context request using SP.Second, we investigate methods to accelerate a cluster serving long-context requests.\n\nAccelerate a Single Request.\n\n\u2022 Figure 3 shows the relation among this line of research work.This line of research can be traced back to the online normalizer work [88], a mathematically equivalent method for block-wise softmax calculation that avoids materializing the full attention matrix\n\nOnline Normalizer [88] Memory Efficient Attention [89] Flash Attention [48] Blockwise Parallel Transformer [90] Ring Attention [91] Burst Attention [92] Striped Attention [93] Dist Attention [94] Figure 3: Works using sequence parallelism.Gray boxes are not tailored for long-context serving.\n\nsoftmax(QK T ).This method is a foundation for memory-efficient attention [89] and their CUDA implementations [48,96].\n\n\u2022 SP was first introduced by Li et al. [97] and has been widely used in distributed LLM training frameworks such as Megatron [98] and Deepspeed [99].In the context of LLM serving systems, new challenges emerge: (1) LLM serving is usually latency-sensitive and thus requires much smaller batch sizes than LLM training; (2) LLM serving has an autoregressive decode phase, where the sequence length is only one, but it requires large memory for KV cache storage; (3) LLM serving usually relies on large fused kernels for improving performance.While the feed-forward network (FFN) computations for each token in a sequence are linearly independent, the computations for attention are not.Consequently, substantial data exchange is involved when computing distributed attention using SP, thereby opening significant space for performance optimization.",
            "score": 0.5954378880647931,
            "section_title": "Distributed Acceleration",
            "char_start_offset": 30981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 97,
                    "end": 221
                },
                {
                    "start": 221,
                    "end": 529
                },
                {
                    "start": 531,
                    "end": 565
                },
                {
                    "start": 565,
                    "end": 648
                },
                {
                    "start": 648,
                    "end": 733
                },
                {
                    "start": 735,
                    "end": 763
                },
                {
                    "start": 765,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 1026
                },
                {
                    "start": 1028,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1320
                },
                {
                    "start": 1322,
                    "end": 1337
                },
                {
                    "start": 1337,
                    "end": 1440
                },
                {
                    "start": 1442,
                    "end": 1591
                },
                {
                    "start": 1591,
                    "end": 1982
                },
                {
                    "start": 1982,
                    "end": 2126
                },
                {
                    "start": 2126,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1135,
                    "end": 1139,
                    "matchedPaperCorpusId": "266351737"
                },
                {
                    "start": 1432,
                    "end": 1436,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1567,
                    "end": 1571,
                    "matchedPaperCorpusId": "248693351"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09930419921875
        },
        {
            "corpus_id": "269149105",
            "title": "TransformerFAM: Feedback attention is working memory",
            "text": "Gemini [69] evaluated long-context capabilities using the following tasks: NarrativeQA [19], Scrolls-Qasper, Scrolls-Quality [60], and XLSum [51].Additionally, PG-19 [21] and Isabelle [56] are another common evaluation tasks among long-context Transformer papers [21,56,67].Detailed information on the evaluation data is provided in Table 10 in Appendix D.2.\n\nWe evaluated the long-context capabilities of the 1B TransformerBSWA model trained in Section 3.1 using memory segment sizes ranging from 0 to 8. As shown in Fig. 3b, TransformerFAM outperformed TransformerBSWA on all the long context tasks (LCT), regardless of the number of memory segments in BSWA.It shows a significant performance improvement on ScrollsQasper and Narra-tiveQA, where it has to understand 5k to 500k tokens of context before answering a question.The LCT results demonstrate that TransformerFAM can effectively compress and retain important contextual information within extremely long contexts.\n\nAbove M1, the number of memory segments does not significantly impact LCT performance on TransformerBSWA, because the input sequences are much longer than the window size of all experiments.We observed the same phenomenon in TransformerFAM, and TransformerFAM uses 3 memory segments in Fig. 3b.The figure shows the normalized scores of all tasks to view the scores on the same scale.The raw results are in In addition, TransformerFAM marginally surpasses TransformerBSWA on GPT-3 tasks [33] (see Table 2).This result is unexpected since all tasks involve sequences shorter than 2k tokens.We hypothesize that this improvement arises from the efficient contextual representation by Transformer-FAM.By offloading contextual data to FAM, TransformerFAM reduces redundancy within input activations, optimizing latent space usage.Thus, BSWA memory segments (local representation) and FAM (global representation) complement each other.",
            "score": 0.5934300344661031,
            "section_title": "Long Context Tasks",
            "char_start_offset": 23054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 274
                },
                {
                    "start": 274,
                    "end": 358
                },
                {
                    "start": 360,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 826
                },
                {
                    "start": 826,
                    "end": 974
                },
                {
                    "start": 976,
                    "end": 1166
                },
                {
                    "start": 1166,
                    "end": 1270
                },
                {
                    "start": 1270,
                    "end": 1359
                },
                {
                    "start": 1359,
                    "end": 1481
                },
                {
                    "start": 1481,
                    "end": 1564
                },
                {
                    "start": 1564,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1800
                },
                {
                    "start": 1800,
                    "end": 1904
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5087890625
        },
        {
            "corpus_id": "246017095",
            "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
            "text": "Transformer achieves promising results on various tasks. However, self-attention suffers from quadratic memory requirements with respect to the sequence length. Existing work focuses on reducing time and space complexity from an algorithm perspective. In this work, we propose sequence parallelism, a memory-efficient parallelism to solve this issue from system perspective instead. Our approach is compatible with most existing parallelisms (e.g., data, pipeline, and tensor parallelism), which means our sequence parallelism makes 4D parallelism possible. More importantly, we no longer require a single device to hold the whole sequence. Besides, using efficient attention with linear complexity, our sequence parallelism enables us to train transformer with infinite long sequence. Specifically, we split the input sequence into multiple chunks and feed each chunk into its corresponding device (i.e., GPU). To compute the attention output, we integrated ring-style communication with self-attention calculation and proposed Ring Self-Attention (RSA). Experiments show that sequence parallelism performs well when scaling with batch size and sequence length. Compared with tensor parallelism, our approach achieved 13.7\\times and 3.0\\times maximum batch size and sequence length respectively when scaling up to 64 NVIDIA P100 GPUs. With efficient attention, sequence can handle sequence with over 114K tokens, which is over 27\\times longer than existing efficient attention works holding the whole sequence on a single device.",
            "score": 0.5913586630650359,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03271484375
        },
        {
            "corpus_id": "278207976",
            "title": "RWKV-X: A Linear Complexity Hybrid Language Model",
            "text": "The core of the Transformer (Vaswani et al., 2017) architecture is the self-attention mechanism, which enables the model to compare each token with every other token in the input sequence-crucial for modeling long-range dependencies. However, for each query q, attention must be computed against all keys K and values V , resulting in quadratic complexity with respect to sequence length. This makes full attention inefficient for very long sequences. The computation is defined as: \n\nHere, d k represents the dimensionality of the query and key vectors, which is used to scale the dot product to prevent extremely large values that could destabilize the softmax operation. \n\nTo overcome these limitations, recent work explores architectures that combine linear attention with dynamic state control. RWKV-7 offers an efficient alternative to Transformers for long-sequence tasks by blending the recurrence of RNNs with the parallelism of Transformers. It leverages a generalized Delta Rule (Schlag et al., 2021) with vectorvalued gating and context-dependent learning rates to enhance expressivity and efficiency. Inspired by DeltaNet, RWKV-7 further decouples state removal and addition, enabling channel-wise updates of state information. \n\nThe core mechanism of RWKV-7 introduces and optimizes the generalized Delta Rule as the foundation for state evolution. The state S t evolution and transition matrix M t are formulated as follows: \n\nwhere w t is the data-dependent decay vector, a t is the context-dependent learning rate, \u03bat is the normalized removal key, kt is the replacement key, and v t is the value vector. \n\nIn this paper, we explore the integration of Transformer and RWKV-7 architectures to build a hybrid model with linear complexity that combines the strengths of both. This model addresses the limitations of each architecture, offering a more efficient and scalable solution for sequence modeling.",
            "score": 0.5900980781365586,
            "section_title": "Preliminaries",
            "char_start_offset": 7452,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1439
                },
                {
                    "start": 1442,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1919
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 50,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 990,
                    "end": 1011,
                    "matchedPaperCorpusId": "235377069"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.036773681640625
        },
        {
            "corpus_id": "220831004",
            "title": "Big Bird: Transformers for Longer Sequences",
            "text": "\u2022 A block of local neighbors w so that each node attends on their local structure.\n\nOur design leads to empirically high performing attention mechanism scaling to much longer sequence lengths (8x) on standard hardware (\u223c16GB memory) for large size models. Use of gradient checkpointing [15] could potentially allow for handling even longer sequences.\n\nTo summarize, our main contributions are:\n\n1. BigBird satisfies all the known theoretical properties of full transformer (Sec. 3). In particular, we show that adding extra tokens allows one to express all continuous sequence to sequence functions with only O(n)-inner products. Furthermore, we show that under standard assumptions regarding precision, BigBird is Turing complete. 2. Empirically, we show that the extended context modelled by BigBird greatly benefits variety of NLP tasks. In particular, we achieve state of the art results for question answering and document summarization on a number of different datasets. Summary of these results are presented in Sec. 4.\n\n3. Lastly, we introduce a novel application of attention based models where long contexts are beneficial: extracting contextual representations of genomics sequences like DNA. With longer masked LM pretraining, BigBird improves performance on downstream tasks such as promoter-region and chromatin profile prediction (Sec. 5).",
            "score": 0.589667548001804,
            "section_title": "Introduction",
            "char_start_offset": 4062,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.383544921875
        },
        {
            "corpus_id": "266903047",
            "title": "Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing",
            "text": "Long context modeling (or long range modeling, long sequence processing) has become a very broad research topic. In this section, we focus on memory-based Transformer models that support arbitrarily long inputs, in particular, what to memorize and how to update. Other sparsity and compression methods can be orthogonal to and thus combine with ours to achieve better efficiency. Interested readers should refer to other survey papers, e.g. Dong et al. (2023) and Huang et al. (2023), for a brief review of the state of this research area. Memory entry types. Transformer-XL (Dai et al., 2019) and MART (Lei et al., 2020) use a cache to memorize the contextualized embeddings computed during the previous step at each layer. Compressive Transformer (Rae et al., 2019) adds an additional compressed memory to collect the discarded entries from the Transformer-XL cache. Memorizing Transformer (Wu et al., 2022b) and StreamingLLM (Xiao et al., 2023) modify the transient Q/K/V structure of a dot-product attention, and allows K/Vs to persist. LongMem (Wang et al., 2023) is similar to Memorizing Transformer but further freezes the backbone LLM that generates K/Vs and introduces a separate trainable SideNet to overcome the staleness issue of Memorizing Transformer. Unlimiformer (Bertsch et al., 2023) stores the hidden states of encoder layers with no capacity limit, and retrieves top keys directly from the memory in cross attention after a reformulated attention equation. TRAMS (Yu et al., 2023) combines Transformer-XL with Unlimiformer, which retrieves top-m keys from a memory pool size of M , which itself uses the FIFO policy despite the memory selection method for top keys. All of the above methods insert existing intermediate activations into the memory. Memformer (Wu et al., 2022a) and TTM (Ryoo et al., 2023) implicitly selects and creates, using a \"Write\" operation, memory entries from a combination of layer or model inputs and outputs and existing memory entries.",
            "score": 0.5892578434829847,
            "section_title": "Related Work",
            "char_start_offset": 3962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1984
                }
            ],
            "ref_mentions": [
                {
                    "start": 575,
                    "end": 593,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 603,
                    "end": 621,
                    "matchedPaperCorpusId": "218595710"
                },
                {
                    "start": 892,
                    "end": 910,
                    "matchedPaperCorpusId": "247519194"
                },
                {
                    "start": 1279,
                    "end": 1301,
                    "matchedPaperCorpusId": "258436892"
                },
                {
                    "start": 1483,
                    "end": 1500,
                    "matchedPaperCorpusId": "264439578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7177734375
        },
        {
            "corpus_id": "256847015",
            "title": "A Unified View of Long-Sequence Models towards Modeling Million-Scale Dependencies",
            "text": "Despite their great advantages, Transformers have critical disadvantages as well, and chief among them is their resource efficiency. Although Transformers are much faster compared to traditional sequence models, the attention matrix incurs a O(L 2 ) complexity, where L is the context length, in terms of both compute and storage. Such a complexity is especially prohibitive when dealing with context of thousands of tokens since the time and memory it takes scale quadratically, for example, summarizing books, processing audio/video, dealing with high-resolution images, etc. In these tasks, the context can easily scale to thousands or even millions of tokens as making a decision may need information from far earlier steps in the sequence (e.g., a name in the first chapter of a book). To counter this challenge, over the past five years a great deal of studies has focused on making Transformers more efficient. The proposed methods include (but not limited to) for example, approximating the attention matrix with sparsity [24,3,46], clustering before computing attention [35,29], making assumptions via conditional probability [34], low-rank estimation [42], better memory I/O [12], matrix orthogonality and associativity [7], etc. Apart from tackling the efficiency problem of Transformers directly, many other models have been proposed to cater the need for long-context learning. To name a few, MLP-Mixer [37], FNet [30] and SGConv [31] learn from and (solely) utilize the token-mixing paradigm from Transformers; Memorizing Transformers [44] take Neural Turing Machines [17] to extreme and make Transformers a huge LSTM-like structure; S4 [20,21] rejuvenates traditional State Space Models combined with orthogonal polynomial projection to reconstruct histories. \n\nAcknowledging the significance in both the novelty and volume of prior work, we take a step back and compile this work with the following objectives: \n\n1. Categorizing existing solutions to long-range dependency problems purely by their mathematical formulations. 2. Comparing various methods using a unified template, with which we study the tradeoffs in capturing global and local dependencies in input sequence.",
            "score": 0.5861290008389933,
            "section_title": "Introduction",
            "char_start_offset": 2921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1926
                },
                {
                    "start": 1929,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 1036,
                    "end": 1039,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1079,
                    "end": 1083,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1135,
                    "end": 1139,
                    "matchedPaperCorpusId": "235829099"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1416,
                    "end": 1420,
                    "matchedPaperCorpusId": "233714958"
                },
                {
                    "start": 1651,
                    "end": 1655,
                    "matchedPaperCorpusId": "240354066"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48583984375
        },
        {
            "corpus_id": "272828151",
            "title": "PecSched: Preemptive and Efficient Cluster Scheduling for LLM Inference",
            "text": "In recent years, the transformer-based generative large language models (LLMs) scaled up towards billions and even trillions of parameters to improve model capabilities, which have been well exemplified by the GPT model series [1][2][3], OPT [4], and Llama series [5,6]. As the models' contextual understanding and ability to handle longer and more complex text inputs have been increasingly enhanced, the allowable input sequence length has been substantially increased, ranging from 4K to 1M tokens [7][8][9][10][11]. For example, applications such as book summarization [12][13][14], document classification [15,16], and coding assistance [17] require a longer or unlimited sequence length to fully understand the extended context. Some long-sequence applications, such as coding assistance, require short response time (e.g., in seconds). However, through experimental measurements, we made Observation (O): O1. The existing serving system that handles long sequences, Sarathi-Serve [18], generates long Time-To-First-Token (TTFT) (in minutes) due to sequential chunk processing, high Time-Between-Token (TBT) (e.g., 6 seconds) from batching long-sequence prefills and decodes, and low throughput due to small batch size caused by constrained KV cache size and long sequences. \n\nTo address the problems, we propose sequence parallelism (SP) architectures that partition a long input sequence and use multiple GPUs to process the partitions in parallel. \n\n(1) SP-based Prefil. We propose two SP architectures: SP-NTP for Non-Tensor Parallelism and SP-TP for Tensor Parallelism. SP-NTP lets each GPU process one sequence partition (SPT), then distributes the Query-Key-Value (QKV) of different heads across GPUs via the all-to-all (A2A) communication for self-attention computations, and finally uses another A2A to collect the self-attention output of each partition to a separate GPU.",
            "score": 0.5857895040079112,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 230,
                    "end": 233,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 573,
                    "end": 577,
                    "matchedPaperCorpusId": "208246040"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "246823944"
                },
                {
                    "start": 987,
                    "end": 991,
                    "matchedPaperCorpusId": "268249103"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.118408203125
        },
        {
            "corpus_id": "219792650",
            "title": "I-BERT: Inductive Generalization of Transformer to Arbitrary Context Lengths",
            "text": "Neural network models based on self-attention, known as Transformers [1][2][3] have proven to be effective at natural language processing (NLP) tasks such as language modeling, translation, question-answering, and summarization. Recent state-of-the-art NLP models are based on pre-trained Transformer encoders such as BERT [2], GPT-2 [4], XLNet [5], RoBERTa [6], and Megatron-LM [7], which are trained by language modeling tasks. Compared with the recurrent neural network (RNN) based models, Transformer models are easy to parallelize and are scalable, because parallel self-attention with positional encodings replaces sequential recurrent connections [3]. This parallel nature of self-attention limits the observable context length of a Transformer to a fixed size. Prior works have shown that Transformer models cannot generalize to inputs of longer lengths in algorithmic tasks that require inductive bias [8][9][10]. Transformer [3] relies on the absolute positional encodings for locality, which cannot be generalized to arbitrarily long context lengths. Approaches such as Transformer-XL [11] and XLNet [5] enable longer context lengths using the relative positional encodings. Although they achieve state-of-the-art performance in NLP tasks, there is no proof that they can extend learned rules to unobserved longer inputs. \n\nWe define inductive generalization to verify a model's capability of extending the rules to unobserved context lengths. Inductive generalization requires a model to extend the rules learned during training to validation dataset of higher-dimensional samples. Of course, this requires a change in the method one uses to sample training and validation sets. Existing interpolation and extrapolation methods evaluate models on data that share the same sample space with the training data. But for the case of inductive generalization, we need to split the data samples into a lower-dimensional training set and a higher-dimensional validation set based on the sequence length threshold k. The two sets are mutually exclusive in that the lengths of the validation samples cannot be observed during training.",
            "score": 0.5856653152681409,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1332
                },
                {
                    "start": 1335,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 72,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 72,
                    "end": 75,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 75,
                    "end": 78,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 323,
                    "end": 326,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 345,
                    "end": 348,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 358,
                    "end": 361,
                    "matchedPaperCorpusId": "198953378"
                },
                {
                    "start": 654,
                    "end": 657,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 911,
                    "end": 914,
                    "matchedPaperCorpusId": "49667762"
                },
                {
                    "start": 914,
                    "end": 917,
                    "matchedPaperCorpusId": "189928186"
                },
                {
                    "start": 917,
                    "end": 921,
                    "matchedPaperCorpusId": "53277713"
                },
                {
                    "start": 935,
                    "end": 938,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 1096,
                    "end": 1100,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1111,
                    "end": 1114,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2342529296875
        },
        {
            "corpus_id": "273695526",
            "title": "TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters",
            "text": "Transformer can also achieve model reuse to a certain extent. Net2Net (Chen et al., 2015), a classical model growth method, proposes a technique to expand the width of neural networks by duplicating neurons. In this method, the pre-trained weight matrix of a transformer layer in the smaller model denoted W old s \u2208 R ds\u00d7ds , is used to create a larger weight matrix \n\nto fill the larger model. This expansion is formulated as follows, \n\nwhere ds) , and ds) are new parameters for expansion. The scaling procedures are the same as schemes introduced in Section 4.1. \n\nControllable cost of token-token interaction for long-context modeling. Recent advancements in Chain-of-Thought (CoT) modeling (Wei et al., 2022) have emphasized the critical importance of efficiently processing lengthy textual sequences (Tay et al., 2020) within Large Language Models (LLMs). As delineated in Section 1, the training costs of transformer architectures are primarily divided into two components: interactions involving model parameters and interactions among input sequences. Table 3 demonstrates that the computational complexity of transformer-based models exhibits a quadratic dependence on text length, scaling linearly with token-parameter interactions and quadratically with token-token interactions. Consequently, it is imperative to expand model parameters while controlling the computational burden of token-token interaction part. Conventionally, scaling transformer models involves increasing the channel dimension. For a fixed text length, this results in higher computational costs, mainly because dominant token-token interactions become more intensive, which hampers the model's performance with long texts. Our proposed model takes a different approach by decoupling the computation cost of token-token interactions from model scaling. We increase the parameter size without changing the token channel dimension, thereby maintaining the computational cost associated with token-token interactions. As shown in Figure 5, our model exhibits increasingly significant computational advantages over Transformers as the number of parameters grows, especially when processing longer sequences. \n\nScaling without losing the well-learned distribution. Our Tokenformer can maintain the existing output distribution when new key parameters are initialized to zero. This characteristic is beneficial for continuously scaling models to incorporate additional data, as it facilitates an increase in model capacity without disrupting the ongoing training process, thereby promoting rapid convergence.",
            "score": 0.58542752933449,
            "section_title": "COMPARISON WITH STANDARD TRANSFORMER",
            "char_start_offset": 21115,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2187
                },
                {
                    "start": 2190,
                    "end": 2243
                },
                {
                    "start": 2244,
                    "end": 2354
                },
                {
                    "start": 2355,
                    "end": 2586
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 89,
                    "matchedPaperCorpusId": "6702706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08868408203125
        },
        {
            "corpus_id": "235368027",
            "title": "Staircase Attention for Recurrent Processing of Sequences",
            "text": "Transformer-XL [15] employs a segment-level recurrence mechanism to effectively cache and speed up computations in long-context sequence tasks. We note that a number of recent architectures have also focused on allowing long-context in Transformers, although typically without employing recurrence [16,17,18]. Finally, the Feedback Transformer [19], perhaps the most similar work to ours, incorporates step-wise recurrence in the Transformer, with a step size of one and a fixed cached memory in the past. It achieves good results but has relatively high computational cost due to its architecture not fully exploiting parallelism. \n\nIn this work, we compare architectures with the number of model parameters fixed, and explore increasing recurrence and/or compute given that fixed budget. An orthogonal topic of study is to fix the compute budget instead, but do not fix the amount of parameters, e.g. research into large, sparse (typically non-recurrent) models that may require to be spread over a cluster [20,21]. We focus on the former here, but learnings from each direction should be complementary. Token chunks (Time) 3 Method",
            "score": 0.5852996100737078,
            "section_title": "Related Work",
            "char_start_offset": 5957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1134
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 19,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "209315300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0987548828125
        },
        {
            "corpus_id": "268385246",
            "title": "Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs",
            "text": "As language models are becoming general purpose and highly capable, the demand for language models to handle longer context sequences has grown significantly.Recently, there is an ongoing focus on models that can handle even longer context sequences (Bulatov et al., 2023;OpenAI, 2023;Team, 2023;?).As of today, GPT-4 (OpenAI, 2023) supports context length of 32k tokens, and MPT-7B (Team, 2023) extends it to 64k while Anthropic's Claude4 supports as long as 100k input length.Most recently, Bulatov et al proposed 1M token input context length for transformers.These models push the boundaries of context understanding and generation capabilities, enabling more comprehensive discourse understanding and contextually informed responses.\n\nThis trend is driven by the need for comprehensive discourse understanding in applications like Retrieval-Augmented Generation (RAG), as well as many complex prompting methods.Applications such as RAG (Guu et al., 2020;Izacard et al., 2022;Menick et al., 2022;Zhen et al., 2022) retrieve extensive passages or documents from external corpora, providing rich and grounded context for generating responses.Additionally, models like Toolformer (Schick et al., 2023) and WebGPT (Nakano et al., 2021) leverage external tools, such as APIs and search engines, to expand the context and enhance generation.\n\nLong context is disproportionately expensive for transformer family models because for vanilla self-attention both memory and time complexity are quadratic to the sequence length.To effectively handle longer context sequences, optimizing memory I/O and reducing computational overhead are critical.Currently, the dominant approaches to addressing this challenge have been to make the attention computation less expensive.Beltagy et al. (2020) proposed to sparsify self-attention using various attention patterns.Wang et al. (2020) explores low-rank approximation of self-attention.In addition to the compute bound improvements, advancements in memory-efficient attention mechanisms and techniques for reducing memory I/O will continue to propel the field forward, facilitating the handling of longer context sequences in language models.",
            "score": 0.581525550391379,
            "section_title": "B.2. Supporting Long Context Requires IO-Efficient Attention",
            "char_start_offset": 37884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 299
                },
                {
                    "start": 299,
                    "end": 478
                },
                {
                    "start": 478,
                    "end": 563
                },
                {
                    "start": 563,
                    "end": 738
                },
                {
                    "start": 740,
                    "end": 916
                },
                {
                    "start": 916,
                    "end": 1144
                },
                {
                    "start": 1144,
                    "end": 1339
                },
                {
                    "start": 1341,
                    "end": 1520
                },
                {
                    "start": 1520,
                    "end": 1639
                },
                {
                    "start": 1639,
                    "end": 1762
                },
                {
                    "start": 1762,
                    "end": 1853
                },
                {
                    "start": 1853,
                    "end": 1922
                },
                {
                    "start": 1922,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 941,
                    "end": 959,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5205078125
        },
        {
            "corpus_id": "273025630",
            "title": "Were RNNs All We Needed?",
            "text": "Due to this limitation, Transformers replaced LSTMs and GRUs as the defacto sequence modelling method for years by leveraging parallelization during training. However, Transformers have a quadratic complexity in the sequence length, limiting their ability to scale to long contexts. Recently, a resurgence of many new recurrent models have been proposed as replacements for Transformers that achieve comparable performance and are trainable in parallel, while avoiding the BPTT issue that traditional RNNs (e.g., LSTMs and GRUs) faced. Although many different architectures have been proposed, many of these models are efficiently trained using the parallel prefix scan algorithm (Blelloch, 1990). \n\nThe parallel scan algorithm is a parallel computation method for computing N prefix computations from N sequential data points via an associative operator \u2295 (e.g., + and \u00d7). The algorithm efficiently computes \n\nIn particular, we can apply the parallel scan method for efficiently computing a popular family of functions: v t = a t v t\u22121 + b t where v t , a t , b t \u2208 R and v 0 \u2190 b 0 (Heinsen, 2023). The method takes as input a 1 , . . . , a n and b 0 , b 1 , . . . , b n and computes via parallel scans v 1 , . . . , v n .",
            "score": 0.580465103191073,
            "section_title": "Parallel Scan",
            "char_start_offset": 6434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 908
                },
                {
                    "start": 911,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1223
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02886962890625
        },
        {
            "corpus_id": "273849947",
            "title": "LASER: Attention with Exponential Transformation",
            "text": "The attention mechanism was used in Bahdanau et al. (2015) to drastically improve machine translation performance compared to encoder-decoder recurrent neural networks (RNNs) (Cho, 2014). This was later adopted in Transformers (Vaswani et al., 2017), which introduced self-attention to improve the performance in machine translation even further. Efficient attention mechanisms have been an active area of research due to the quadratic computational complexity in sequence length of Attention, which prevents long-context language modeling. One notable contribution is Linear Attention (Katharopoulos et al., 2020), which reduces the quadratic complexity of self-attention to linear in sequence length by using kernel approximation of the softmax operation. Similarly, the Performer (Choromanski et al., 2021) develops an alternative kernel approximation using random feature maps to achieve linear complexity. \n\nThe Mamba architecture introduces state-space models (SSMs) as a replacement for traditional attention. Models like S6 (S4+selection+scan) (Gu & Dao, 2023) from Mamba and SSD from Mamba-2 (Dao & Gu, 2024) have linear computational complexity in sequence length without the use of attention. However, despite these innovations, attention-based models like Gemini 1.5 Flash (Gemini, 2024) and LLaMA 3 (Dubey et al., 2024) continue to dominate long context regime, particularly through advancements in context parallelism (Liu et al., 2023), which ensures scalability while maintaining the strengths of attention mechanisms in Transformer models. \n\nEfficient attention mechanisms have become critical in handling long sequences, especially in Transformer-based architectures. This mechanism is used for faster inference and training, particularly when scaling up to large sequence lengths. Sparse Transformers (Child et al., 2019) use fixed sparse attention patterns, enabling them to efficiently handle very long sequences by reducing the quadratic complexity of standard attention to linear or sub-quadratic in practice. Routing Transformers (Roy et al., 2021) take a different approach by introducing a mechanism that sparsifies attention through data-dependent sparsity patterns with subquadratic computational complexity in sequence length.",
            "score": 0.5789332351599092,
            "section_title": "RELATED WORK",
            "char_start_offset": 5651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2255
                }
            ],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 58,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 227,
                    "end": 249,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 586,
                    "end": 614,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 783,
                    "end": 809,
                    "matchedPaperCorpusId": "222067132"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1683349609375
        },
        {
            "corpus_id": "267637090",
            "title": "World Model on Million-Length Video And Language With Blockwise RingAttention",
            "text": "Learning long-range dependencies over sequences of millions of tokens requires (1) scalable training on such long documents, as well as a need to (2) stably extend the context of our base language. Scalable Training on Long Documents. Training on long documents becomes prohibitively expensive due to memory constraints imposed by the quadratic complexity of computing the attention weights. In order to address these computational constraints, we use the Blockwise RingAttention [LZA24,LA23] implementation that leverages block-wise transformer with sequence parallelism to theoretically extend to an infinite context, bounded only by the number of devices available. We further fuse Blockwise RingAttention with FlashAttention [DFE + 22, RS21] using Pallas [BFH + 18] to optimize performance compared with using XLA compiler. In general, given a large enough tokens per device, the communication cost during Blockwise Transformer and RingAttention fully overlap with computation, and does not add any extra overhead. \n\nFigure 4 LWM is a autoregressive transformer on sequences of millions-length tokens. Each frame in the video is tokenized with VQGAN into 256 tokens. These tokens are concatenated with text tokens and fed into transformers to predict the next token autoregressively. The input and output tokens' order reflect the varied training data formats, including image-text, text-image, video, text-video, and purely text formats. The model is essentially trained in an any-to-any manner using multiple modalities. To differentiate between image and text tokens, and for decoding, we surround video and image tokens with the special delimiters <vision> and </vision>. We also include <eof> and <eov> vision tokens to mark the end of intermediate and final frames in images and videos. For simplicity, these delimiters are not shown. \n\nProgressive Training on Increasing Context Length. Although our implementation allows us to train on long documents of millions of tokens, it still remains costly since the quadratic computational complexity of attention remains, where gradient step time scales roughly linearly with context size (given a fixed number of tokens per batch).",
            "score": 0.5779653150941357,
            "section_title": "Extending Context",
            "char_start_offset": 5558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1018
                },
                {
                    "start": 1021,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1844
                },
                {
                    "start": 1847,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2187
                }
            ],
            "ref_mentions": [
                {
                    "start": 480,
                    "end": 487,
                    "matchedPaperCorpusId": "263608461"
                },
                {
                    "start": 487,
                    "end": 492,
                    "matchedPaperCorpusId": "258987968"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.222900390625
        },
        {
            "corpus_id": "268248144",
            "title": "TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax",
            "text": "Ever since their introduction by Vaswani et al. [28], Transformers have revolutionized numerous domains of deep learning, from Natural Language Processing to Computer Vision, while also underpinning the emergence of novel applications such as Large Language Models. This success stems largely from their ability to capture intricate dependencies and token-to-token interactions. \n\nTo extend the utility of Transformers to more complex tasks, they need to be able to process long sequences. However, the computational complexity of the attention mechanism scales quadratically in the length of the input sequence O(N 2 ). Therefore, computing twice as many sequence elements requires four times the number of computations, which hinders scaling to very long context windows. This makes some practitioners turn to approaches like compressing portions of the input into single states [3,5], which reduces the amount of information available at each step. Despite this progress, exploiting long context windows to significantly improve performance and incorporate new information without retraining remains challenging. Current Transformers encounter limitations when processing long documents, high-resolution images, or a combination of data from multiple domains and modalities. Especially, considering the limited resources of smaller enterprises or individual consumers. \n\nWhile linearly scaling Transformers have been proposed, these often experience compromised accuracy [20], specialize in a particular domain, like language [34] or images [15], or only convey averaged global information across tokens, neglecting individual token-to-token interactions [1,9]. These models end up being ill-suited for handling longer sequences, leaving the standard Transformer as the preferred choice due to its large capacity and established performance [14]. \n\nIn this work, we approach this bottleneck of the Transformer by reformulating the softmax function in the attention mechanism after introducing the Taylor approximation of the exponential. While some methods alter the softmax, their goal is to split interactions of queries and keys, computing global average interactions only [1,4]. In contrast, our proposed approach, TaylorShift, preserves individual token-to-token interactions. Combining a tensor-product-based operator with the Taylor approximation of the exponential function allows us to compute full token-to-token interactions in linear time. Moreover, this approach has the added benefit of adhering to concrete error bounds when viewed as an approximation of vanilla attention [12].",
            "score": 0.5777831618062197,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1849
                },
                {
                    "start": 1852,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2185
                },
                {
                    "start": 2186,
                    "end": 2284
                },
                {
                    "start": 2285,
                    "end": 2454
                },
                {
                    "start": 2455,
                    "end": 2596
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37109375
        },
        {
            "corpus_id": "269293007",
            "title": "A Survey on Efficient Inference for Large Language Models",
            "text": "Several recent studies have applied long convolution in the context of modeling long sequences [59], [60], [61]. These investigations primarily concentrate on refining the parameterization of the convolution kernel. For instance, Hyena [61] employs an data-dependent parameterization method for long convolutions using a shallow feed-forward neural network (FFN). \n\nOther studies [62], [63] aim to design the operation that has a similar form as the attention operation but can be enrolled to the recurrent manner, enabling both efficient training and efficient inference. For instance, RWKV [62] builds upon AFT [109], which proposes to substitute the attention operation in the Transformer model with the following equation: \n\nwhere Q, K, and V are the query, key, and value matrices as in Transformer, w \u2208 R T \u00d7T denotes a learnable pairwise position bias and \u03c3 q (\u2022) denotes a non-linear function. Specifically, it further reparameterizes the position bias as On the other hand, during inference, most studies opt for recurrent architectures to maintain linear computational complexity in the prefilling stage and to remain context length-agnostic in the decoding stage. Furthermore, in the decoding phase, these novel architectures eliminate the need to cache and load features of previous tokens (similar to the key-value cache in Transformer-based language models), resulting in significant memory access cost savings.",
            "score": 0.5777045634706253,
            "section_title": "State Space Model. The State Space Model (SSM) has demonstrated competitive modeling capabilities in certain",
            "char_start_offset": 49061,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 363
                },
                {
                    "start": 366,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1425
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 111,
                    "matchedPaperCorpusId": "257050308"
                },
                {
                    "start": 236,
                    "end": 240,
                    "matchedPaperCorpusId": "257050308"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053192138671875
        },
        {
            "corpus_id": "271516597",
            "title": "MistralBSM: Leveraging Mistral-7B for Vehicular Networks Misbehavior Detection",
            "text": "The transformer is a groundbreaking architecture that revolutionized the task of sequenced data processing, introducing innovative mechanisms for capturing complex dependencies and patterns within sequences. This was achieved by leveraging the self-attention mechanism to weight the importance of each token in sequence with regard to the other tokens. The weights are obtained by applying three linear transformations, namely (keys (k), queries (Q) and values (V)) on the input sequence of size d k following the equation 1. Several variations of transformers have been proposed and customized to address a broad spectrum of tasks, encompassing language understanding, generation, translation, and more. Notable examples include the original Transformer architecture [30] which is presented in Figure 2, comprised of both encoder and decoder blocks, as well as BERT [12] and RoBERTa [31], both of which are encoder-only transformers. Additionally, LLAMA [11] and GPT exemplify the decoderonly type of transformers, primarily designed for generation tasks. Mistral-7B, is also a decoder-only transformers that employs specific design decisions, allowing it to outperform models such as the 13 billion-parameter variation of Llama-2 and the 34 billion-parameter Llama-1 with only 7 billion parameters. Those technical choices involve using Sliding Window Attention (SWA) that operates by defining a fixed-size window surrounding each token, thus limiting the number of tokens it has to tend for [32]. Compared to the vanilla self-attention used in [30], this technique enables longer sequence processing with lower computation cost. Moreover, Mistral-7B employs the Grouped-query attention (GQA) technique where a group of queries shares a single key and value heads [33], thus speeding up the inference time of the model. \n\nFig. 2: Transformer architecture",
            "score": 0.5753416692919637,
            "section_title": "B. Transformers and LLMs",
            "char_start_offset": 14597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1821
                },
                {
                    "start": 1824,
                    "end": 1856
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0180206298828125
        },
        {
            "corpus_id": "271097641",
            "title": "How Well Can a Long Sequence Model Model Long Sequences? Comparing Architechtural Inductive Biases on Long-Context Abilities",
            "text": "Efficient Long-Context Models. Due to the computational bottleneck of attention (Bahdanau et al., 2015) relative to sequence length, significant modifications have been made to overcome this limitation of the Transformer (Child et al., 2019;Katharopoulos et al., 2020;Su et al., 2023) yet they remain theoretically bounded in terms of its context length. Alternatively, sequence models (Rumelhart et al., 1986;Jordan, 1986;Hochreiter and Schmidhuber, 1997;Cho et al., 2014) originally faced significant issues that limited their application but recent modifications (Gu et al., 2020(Gu et al., , 2021) ) have led to the prominence of linear sequence models which are significantly more compute-effective than Transformer-based architechtures. \n\nOn the Limits of Long Sequence Models. Due to their more intuitive and interpretable architechture, long/linear sequence models remain easier to analyze when placed in comparision to Transformers. As such, their limitations also become easier to discover and analyze. Vardasbi et al. (2023) first show that SSMs struggle at sequenceto-sequence tasks due to to the use of a fixed-size hidden representation which compresses the entire prior context, making it difficult to extract information from the past, fact further substantiated by Jelassi et al. (2024). Park et al. (2024) additionally demonstrate that these models have difficulty with more complex in-context learning tasks, while Merrill et al. (2024) show them to possess similar limiations in terms of representational power as Transformers (Merrill and Sabharwal, 2023). Waleffe et al. (2024) finally make a comparision between Mamba, Transformers as well as a hybrid and observe hybrid models to perform better on long-context tasks, while Mamba2 often trails behind Transformers. These observations thus beg a question: can long sequence models really model long sequences? Given the hints that long sequence models may not always be as they seem, a more formal investigation is necessary. We distinguish ourselves by conducting a more controlled but intricate study which aims to uncover why some of the prior results might occur, which we discuss in the work that follows. \n\n3 Background",
            "score": 0.5747675687025943,
            "section_title": "Related Work",
            "char_start_offset": 3606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2183
                },
                {
                    "start": 2186,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 103,
                    "matchedPaperCorpusId": "259936734"
                },
                {
                    "start": 241,
                    "end": 268,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 423,
                    "end": 456,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 566,
                    "end": 582,
                    "matchedPaperCorpusId": "221150566"
                },
                {
                    "start": 582,
                    "end": 603,
                    "matchedPaperCorpusId": "239998472"
                },
                {
                    "start": 1013,
                    "end": 1035,
                    "matchedPaperCorpusId": "258309400"
                },
                {
                    "start": 1282,
                    "end": 1303,
                    "matchedPaperCorpusId": "267406617"
                },
                {
                    "start": 1305,
                    "end": 1323,
                    "matchedPaperCorpusId": "267499935"
                },
                {
                    "start": 1434,
                    "end": 1455,
                    "matchedPaperCorpusId": "269149086"
                },
                {
                    "start": 1547,
                    "end": 1576,
                    "matchedPaperCorpusId": "257050206"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.161865234375
        },
        {
            "corpus_id": "276724714",
            "title": "ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length on More Than 12,000 GPUs",
            "text": "In recent years, large language models (LLMs) have achieved remarkable success across various domains. The impressive performance of LLMs is attributed to increased model sizes, larger volumes of training data, and longer context windows, all in accordance with the scaling law [20]. The demand for long-context capabilities of LLMs has increased rapidly, as modern LLM applications like documents summarization [19], video understanding [41,42], agent interaction [1] and code completion [27], require the model to understand long-range dependencies. It has driven many organizations to extend their models' context lengths. For instance, Meta's LLaMA3 [11] and OpenAI's GPT-4o [33] support 128K contexts, Anthropic's Claude3 [3] supports 200K, and Google's Gemini-1.5 Pro [13] supports up to 2M contexts. \n\nA fundamental challenge in scaling to a long context is the quadratic scaling of memory and computation for selfattention. Flash Attention [7,8] has been proposed to reduce the memory complexity from  ( 2 ) to  (), where  is the sequence length. To further scale the context length, it's necessary to partition the sequences across multiple devices. There are broadly two categories: inter-data partitioning (a.k.a. Data Parallelism, DP [9,24,37]) distributes different sequences across the devices, while intra-data partitioning (a.k.a. Context Parallelism, CP [4,23,25,31]) scatter a single sequence. Both categories evenly reduce the memory consumption on each device, while inevitably incurring extra communication overhead. Existing LLM training frameworks, such as Megatron-LM [21,30,38], DeepSpeed [17,36] and MegaScale [18], treat the two categories as individual parallelism strategies, and establish DP\u00d7CP communication groups to organize the devices as a static mesh (e.g., a 2D mesh), where the size of each CP group is dependent on the maximum sequence length (i.e., context length).",
            "score": 0.5742772247170569,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1905
                }
            ],
            "ref_mentions": [
                {
                    "start": 438,
                    "end": 442,
                    "matchedPaperCorpusId": "268510077"
                },
                {
                    "start": 442,
                    "end": 445,
                    "matchedPaperCorpusId": "268889590"
                },
                {
                    "start": 489,
                    "end": 493,
                    "matchedPaperCorpusId": "259937834"
                },
                {
                    "start": 951,
                    "end": 953,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1246,
                    "end": 1249,
                    "matchedPaperCorpusId": "372467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.428955078125
        },
        {
            "corpus_id": "202577893",
            "title": "A Self-Attentional Neural Architecture for Code Completion with Multi-Task learning",
            "text": "In our training and test datasets, the programs are represented as node sequences. The completion happens at every point in the node sequence, and the nodes before the point form as the contextual partial AST. 1 We adopt Transformer-XL network [9] to encode the partial AST, which can capture long-range dependencies in the sequence. In the vanilla Transformer language model, the length of the context is fixed. To address the limitations of using a fixed-length context, Transformer-XL is proposed to introduce a recurrence mechanism to the Transformer architecture. In Transformer-XL architecture, as shown in Figure 4, the hidden states of each new segment are obtained by reusing that of the previous segments, instead of computed from the scratch. In this way, the recurrent connection is created and the reused hidden states can serve as memory for the current segment, which enables the information to propagate through the recurrent connections, thus the model can capture very long-term dependency. \n\nx \u03c4,1 x \u03c4,2 \n\nx \u03c4,3 x \u03c4,4 x \u03c4+1,1 x \u03c4+1,2 x \u03c4+1,3 x \u03c4+1,4 Formally, let s \u03c4 = [x \u03c4,1 , x \u03c4,2 , ..., x \u03c4,L ] and s \u03c4 +1 = [x \u03c4 +1,1 , x \u03c4 +1,2 , ..., x \u03c4 +1,L ] represent two consecutive segments of length L. For the \u03c4 -th segment s \u03c4 , the n-th layer hidden state sequence is denoted as h n \u03c4 \u2208 R L\u00d7d , where d is the dimension of the hidden units. The n-th layer hidden state for segment s \u03c4 is computed as: \n\nwhere SG(\u2022) stands for stop-gradient, the notation [h u \u2022 h v ] indicates the concatenation of two hidden sequences along the length dimension, and W. denotes model parameters. Compared to the standard Transformer, the critical difference lies in that the key k n \u03c4 +1 and value v n \u03c4 +1 are conditioned on the extended context h n\u22121 \u03c4 +1 and hence h n\u22121 \u03c4 +1 cached from the previous segment.",
            "score": 0.5735370507815828,
            "section_title": "C. Partial AST Encoder",
            "char_start_offset": 14790,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1419
                },
                {
                    "start": 1422,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1815
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 247,
                    "matchedPaperCorpusId": "57759363"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.076416015625
        },
        {
            "corpus_id": "262826014",
            "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
            "text": "Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input data along the sequence dimension and employs an efficient all-to-all collective communication for attention computation. Theoretical communication analysis shows that whereas other methods incur communication overhead as sequence length increases, DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally. Furthermore, experimental evaluations show that DeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the existing method SOTA baseline.",
            "score": 0.5733104629824279,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0210113525390625
        },
        {
            "corpus_id": "274234686",
            "title": "A Method for Building Large Language Models with Predefined KV Cache Capacity",
            "text": "The Bounded-Cache Transformer (BCT) introduces several key implementation aspects that enhance its efficiency and effectiveness in sequence modeling: \n\n\u2022 Fixed-Length Cache: BCT utilizes a fixed-length key-value (KV) cache, which contrasts with the expanding cache in traditional Transformers. This design is essential for managing memory efficiency and ensuring stable performance across varying context lengths. \n\n\u2022 Adaptive Update Rule: BCT features an adaptive update rule that refines the hidden state through a self-supervised learning approach during inference. This enables the model to dynamically adjust to new sequences, improving its capacity to capture long-range dependencies within the bounded cache. \n\n\u2022 Optimized for Parallelism: The BCT is designed to take full advantage of parallel processing capabilities. Since the update of KV content at each time step is independent, it allows for token-level parallel training. This is achieved through the computation of all KV elements via concurrent traversal and accumulation operations, significantly enhancing training efficiency. \n\n\u2022 Learning Rate Adaptation: BCT incorporates an adaptive learning rate mechanism, which adjusts the step size for each token. This is vital for stabilizing the learning process and can lead to faster convergence. \n\n\u2022 Hardware Efficiency: The BCT's design is mindful of modern hardware constraints, aligning its operations with the strengths of GPUs and TPUs. This includes the use of efficient matrix operations and strategic memory management to ensure high-performance computing. \n\nThese aspects highlight BCT's innovative approach to sequence modeling, focusing on efficiency, adaptability, and hardware utilization, which are critical for advancing the state of the art in language model design.",
            "score": 0.5722702981071014,
            "section_title": "Key Implementation Aspects",
            "char_start_offset": 7958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 152,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1310
                },
                {
                    "start": 1313,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1579
                },
                {
                    "start": 1582,
                    "end": 1797
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08343505859375
        },
        {
            "corpus_id": "263608461",
            "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
            "text": "Transformers have garnered significant attention in the field of AI and have become the backbone for numerous state-of-the-art models. Several works have explored memory-efficient techniques to address the memory limitations of Transformers and enable their application to a wider range of problems. Computing exact self-attention in a blockwise manner using the tiling technique [24] has led to the development of memory efficient attention mechanisms [30] and its efficient CUDA implementation [9], and blockwise parallel transformer [23] that proposes computing both feedforward and self-attention block-by-block, resulting in a significant reduction in memory requirements. \n\nIn line with these advancements, our work falls into the category of memory efficient computation for Transformers. Other works have investigated the approximation of attention mechanisms, yet these efforts have often yielded sub-optimal results or encountered challenges during scaling up. For an in-depth review of these techniques, we recommend referring to the surveys [26,35]. Another avenue of research explores various parallelism methods, including data parallelism [10], tensor parallelism [34], pipeline parallelism [27,15,28], sequence parallelism [21,18,17], and FSDP [11,31]. The activations of self-attention take a substantial amount of memory for large context models. \n\nTensor parallelism can only reduce parts of activations memory and sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation. Prior work has studied sharding along sequence and attention heads, and gathering sequences via an optimized all-to-all topology, achieving reduced communication [17]. However, this method is restricted by the number of attention heads and requires gathering the full sequence on each device. In comparison, our approach fully overlaps communication with blockwise computation, enhancing its scalability. Prior work extends sequence parallelism for computing self-attention using a ring topology [21], which reduces the communication cost compared to standard sequence parallelism. However, overlapping communication with computation remains challenging due to the constraints of arithmetic intensity. The communication overheads render this approach infeasible for training and inference in largecontext scenarios.",
            "score": 0.571196197803214,
            "section_title": "Related Work",
            "char_start_offset": 21820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1364
                },
                {
                    "start": 1367,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2368
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 499,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "258987968"
                },
                {
                    "start": 1154,
                    "end": 1158,
                    "matchedPaperCorpusId": "372467"
                },
                {
                    "start": 1206,
                    "end": 1210,
                    "matchedPaperCorpusId": "202488191"
                },
                {
                    "start": 1210,
                    "end": 1213,
                    "matchedPaperCorpusId": "53670168"
                },
                {
                    "start": 1213,
                    "end": 1216,
                    "matchedPaperCorpusId": "219720945"
                },
                {
                    "start": 1239,
                    "end": 1243,
                    "matchedPaperCorpusId": "246017095"
                },
                {
                    "start": 1264,
                    "end": 1267,
                    "matchedPaperCorpusId": "269617042"
                },
                {
                    "start": 2049,
                    "end": 2053,
                    "matchedPaperCorpusId": "246017095"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.025665283203125
        },
        {
            "corpus_id": "277128409",
            "title": "A review on the applications of Transformer-based language models for nucleotide sequence analysis",
            "text": "Transformer-based language models provide effective and significant results when applied to biological sequences, especially for their ability to define and handle a huge number of context-dependent features. Nevertheless, in order to build even more reliable and better performing Transformer models, some issues remain to be addressed. One of the most relevant one is the need of a huge amount of computational resources to build Transformer models. Thus, efforts are being made to reduce the computational load, both in terms of time and space requirements. Also, some features of the models can be customized and particular attention could be paid to overcome the common limitation of deep learning model in interpreting and reading the intrinsic meaning of the models. There is therefore still a large margin of improvement that could be achieved by developing models tailored for specific contexts, as well as focus can be given on generative or decoder-based models. Moreover, features different from DNA sequences may be considered in order to come up with better performance results. \n\nBelow, we provide a summary of the main issues that we believe worthy of attention to develop research and computational methods in the field addressed in this survey. \n\n\u2022 Computational cost: The main advantage of Transformers come from the self-attention module, which also leads to a very high computational expense, which increases quadratically with the input sequence length. Transformers may not be able to model long sequences. As compared to traditional large language models, one recent work [92] has claimed to have a very low development cost. Some other works have also tried to mitigate or improve this problem, as discussed below: \n\n-Improvement in self-attention module: Beltagy et al. [17] have introduced local windowed attention with task motivated global attention in place of standard self-attention. This enables processing of longer sequences. BIGBIRD, proposed in [93], uses a sparse attention mechanism to reduce the quadratic dependency on sequence length to linear. Performers introduced in [94] have used linear attention by replacing softmax with another approach, thereby using only linear space and time complexity. However, all these works have focused on customizing the self-attention module, thus retraining the whole Transformer model. -Dividing long sequences into chunks: Xie et al. [95] have proposed dividing the long input sequences into a batch of chunks of feasible lengths and then selecting the most relevant tokens for decoding.",
            "score": 0.5710430892262881,
            "section_title": "Challenges and future directions",
            "char_start_offset": 47191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1739
                },
                {
                    "start": 1742,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2240
                },
                {
                    "start": 2241,
                    "end": 2365
                },
                {
                    "start": 2366,
                    "end": 2568
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0204925537109375
        },
        {
            "corpus_id": "204735695",
            "title": "Mind The Facts: Knowledge-Boosted Coherent Abstractive Text Summarization",
            "text": "Recently, Transformer architectures have been immensely successful in various natural language processing applications including neural machine translation, question answering and neural summarization and pretrained language modeling. However, Transformers have fixed-length context, which results in worse performance while encoding long source text. In addition, these fixed-length context segments do not respect the sentence boundaries, resulting in context fragmentation which is a problem even for the short sequences. Recently, Transformer-XL has offered an effective solution for this long-range dependency problem in the context of language modeling. They have introduced the notion of recurrence into a self-attention-based model by reusing hidden states from the previous segments, and have introduced the idea of relative positional encoding to make the recurrence scheme possible. Transformer-XL has state-of-the-art perplexity performance, learns dependency 450% longer than vanilla Transformers, and is up to 1,800+ times faster than vanilla Transformers at inference time on language modeling tasks. \n\nInspired by the strong performance of the Transformer-XL language model on modeling long-range dependency, we extend Transformer-XL to an encoder-decoder architecture based on the Transformer architecture. In other words, we calculate the attention scores at every multi-head attention layer in our architecture shown in Figure 1 based on Transformer-XL attention decomposition. We compare the attention decompositions of vanilla Transformer and Transformer-XL. Below equations show the attention computation between query q i and key vector k j within the same segment. U matrix shows the absolute positional encoding, E matrix is the token embedding matrix, W q and W k represent the query and key matrices. In the Transformer-XL attention formulation, R i\u2212j is the relative positional encoding matrix without trainable parameters, and u, v, W k,R , W k,E are all trainable parameters. \n\nOverall, Transformer-XL's architecture is shown below for a segment \u03c4 in the n-th transformer layer. SG denotes stop-gradient, and \u2022 denotes concatenation. We refer the readers to the original Transformer-XL paper [8] for further discussion on the new parameterization for attention calculations and more details on the design decisions for the architecture.",
            "score": 0.5686052342882442,
            "section_title": "Transformer vs. Transformer-XL",
            "char_start_offset": 2509,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 2005
                },
                {
                    "start": 2008,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2163
                },
                {
                    "start": 2164,
                    "end": 2366
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.178955078125
        },
        {
            "corpus_id": "270391285",
            "title": "An Empirical Study of Mamba-based Language Models",
            "text": "As with the RULER evaluations above, these experiments highlight again the strong ability for hybrid models to perform in-context learning and to retrieve information from a long context.\n\nA 128K Mamba-2-Hybrid Model.\n\nWhile we focused in this section on evaluating 16K and 32K Mamba-2-Hybrid long-context extensions and comparing them to corresponding Transformer models, we now show that the hybrid architecture can extend to context lengths well beyond 32K.We extend the base 4K Mamba-2-Hybrid model to a sequence length of 128K through continued pretraining as described above, using full global attention for the four self-attention layers.This training required only tensor and pipeline parallelism in Megatron-LM to prevent out-of-memory issues.We report results for this model on the Phonebook task in Figure 8.As for the 4K, 16K, and 32K Mamba-2-Hybrid models, the 128K model is able to do this task perfectly up to and beyond the sequence length it was trained on.This experiment highlights the promising potential for extending hybrid models to long context lengths.Takeaway.\n\nWe have presented a detailed evaluation of long-context 8B-parameter Mamba-2-Hybrid models and compared them with their Transformer counterparts.Overall, the hybrid models match or exceed the long-context capabilities of the Transformers in most tasks.This is particularly true for tasks like Phonebook and the Needle In A Haystack (NIAH) present in the synthetic RULER benchmark.We have identified, however, a few tasks where the hybrid models failed to reach Transformer-level accuracy (e.g., Multi-Document Question Answering in the Long-Bench evaluation suite).We encourage further research into these settings and into long-context versions of hybrid SSM-Transformer architectures.",
            "score": 0.568133380131308,
            "section_title": "Results on Natural Long-Context Tasks.",
            "char_start_offset": 39097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 189,
                    "end": 217
                },
                {
                    "start": 219,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 645
                },
                {
                    "start": 645,
                    "end": 752
                },
                {
                    "start": 752,
                    "end": 819
                },
                {
                    "start": 819,
                    "end": 974
                },
                {
                    "start": 974,
                    "end": 1077
                },
                {
                    "start": 1077,
                    "end": 1086
                },
                {
                    "start": 1088,
                    "end": 1233
                },
                {
                    "start": 1233,
                    "end": 1340
                },
                {
                    "start": 1340,
                    "end": 1468
                },
                {
                    "start": 1468,
                    "end": 1653
                },
                {
                    "start": 1653,
                    "end": 1774
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3154296875
        },
        {
            "corpus_id": "208617454",
            "title": "Neural Academic Paper Generation",
            "text": "The Transformer model makes use of the constant number of context tokens since the model takes fixed-size sequences. The context length is generally selected as few hundreds in practice because of the computational limitations. In addition to these limitations, Transformer architecture also has no ability to carry information between context segments of the sequences. The problem that generally occurs in practice is to segment a given sequence to vectors consisting of fixedsized context tokens without respecting semantic boundaries, which is also a problem about long term dependencies. Similarly, L A T E X includes long term dependencies such as the dependency between \\begin and \\end statements since there could be a long text in-between. \n\nBesides, the study done by [21] showed that LSTM language models have the capacity of using 200 context tokens on average. Intuitively, a model without ability to learn interconnections on segments of the sequences would not be Figure 2: Transformer model, where h is the number of self attention heads in a multi-head attention layer, N is the number of hidden encoder and decoder layers, \u2295 is the concatenation layer, + is the vector addition layer, arrows(\u2191) refer to the flow of data, Q,K and V are abbreviations for X Q W Q i , X K W K i and X V W V i respectively, X Q , X K and X V are the corresponding inputs and i is the index of a self attention head in a multi-head attention layer. sufficient to generate L A T E X files that desires longer dependencies successfully. Therefore, a model that handles longer dependencies between sequences become more appropriate solution against Char-LSTM and Transformer on L A T E X generation task. \n\nRecently, [4] addressed the mentioned dependency problems and introduced Transformer-XL, an extended version of Transformer which stores and makes use of previous hidden states so that it increases the capacity of the model to capture long term dependencies. The main extension made by [4] to Transformer is to change the self attention layers as follows:",
            "score": 0.567041385867256,
            "section_title": "Transformer-XL",
            "char_start_offset": 12249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 748
                },
                {
                    "start": 751,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1698
                },
                {
                    "start": 1701,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2056
                }
            ],
            "ref_mentions": [
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "21700944"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0260009765625
        },
        {
            "corpus_id": "254877252",
            "title": "A Length-Extrapolatable Transformer",
            "text": "Long-sequence Transformers aim to solve two key problems. First, the computation or memory consumption is not efficient enough for long sequences. Second, there is a trade-off between performance and efficiency. One popular solution (Wang et al., 2020b;Katharopoulos et al., 2020;Choromanski et al., 2020) is linear attention, i.e., using a kernel-based or low-rank approximation to replace vanilla attention. The methods typically target efficiency while underperforming vanilla Transformers for regular length. Another strand is sparse attention Beltagy et al., 2020;Zaheer et al., 2020;Xiong et al., 2021), which usually leverages structured sparsity to reduce computation. For causal sequence modeling, the recurrent-style designs Hutchins et al., 2022;Ma et al., 2022b) are also competitive.\n\nIn comparison, we focus on length extrapolation (Press et al., 2021) for language modeling, i.e., training on short texts while evaluating long texts. The training process is kept the same as vanilla Transformers. The capability of long-sequence modeling is given for free during inference. So training efficiency (which is typically expensive for large-scale language models) is not affected compared with previous work. Moreover, the performance on regular length is perfectly retained, without trade-offs for long-sequence modeling.",
            "score": 0.5655056328660504,
            "section_title": "Long-Sequence Transformers",
            "char_start_offset": 20468,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 569,
                    "end": 589,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 735,
                    "end": 757,
                    "matchedPaperCorpusId": "247451135"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038970947265625
        },
        {
            "corpus_id": "268091246",
            "title": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models",
            "text": "We investigate the ability of Hawk and Griffin to improve their predictions with longer contexts. In particular, we evaluate our trained models by measuring the loss on a held-out books dataset across a range of sequence lengths. Using these long documents allows us to evaluate the ability of the models In Transformers, this ability to extrapolate is largely determined by the positional encoding used for the attention layers (Kazemnejad et al., 2024). For recurrent models, it is instead dictated by the capacity of the model to keep refining the representation stored in the recurrence state as the context becomes longer. From the left plot of Figure 5, we observe that, up to some maximal length, both Hawk and Griffin improve next token prediction given longer contexts, and they are overall able to extrapolate to significantly longer sequences (at least 4x longer) than they were trained on. In particular, Griffin extrapolates remarkably well even when using RoPE (Su et al., 2021) for the local attention layers. \n\nThe results so far evaluate models that have been trained on sequences of 2048 tokens. In order to assess whether our models can also effectively learn from longer contexts, we train 1B parameter models on sequences of 8192 (8k) tokens on MassiveText, and compare them to models trained on the same dataset but on sequences of length 2048 (2k) tokens. We keep the total number of training tokens the same across the models by reducing the batch size by a factor of 4 for the models trained on the sequence length of 8192 (while keeping the number of training steps fixed). As illustrated in the right plot of Figure 5, we find that Hawk-8k and Griffin-8k do achieve lower evaluation loss for sequences of length 8192 or larger, compared to Hawk-2k and Griffin-2k. This indicates that Hawk and Griffin are able to learn to use longer contexts during training. Interestingly, when evaluating at short sequence lengths, we find that Hawk-2k and Griffin-2k perform slightly better than Hawk-8k and Griffin-8k. This suggests that the training sequence length should be carefully chosen according to the intended downstream use of the model.",
            "score": 0.5652970775920859,
            "section_title": "Improving next token prediction with longer contexts",
            "char_start_offset": 30485,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 454,
                    "matchedPaperCorpusId": "258987259"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.244873046875
        },
        {
            "corpus_id": "268247927",
            "title": "xT: Nested Tokenization for Larger Context in Large Images",
            "text": "xT utilizes long-context models originally designed for text in order to mix information across large images. These methods extend the context length beyond the typical limit of transformers. Below we briefly review two techniques which we build upon as our context encoders: Transformer-XL [4] and Mamba [10]. \n\nTransformer-XL uses recurrence to pass prior information to future windows via prior hidden states. This effect propagates through depth, so an N -layer transformer capable of taking a length L sequence can be easily extended to handle a sequence of length N L. \n\nEach hidden state h n \u03c4 of layer n for sequence \u03c4 is computed from the previous layer hidden states h n\u22121 \u03c4 \u22121 and h n\u22121 \u03c4 as \n\nwhere SG stands for a stop gradient. This is the same as the original Transformer, except that the keys and values k n \u03c4 , v n \u03c4 are computed using the previous sequence's hidden state h n\u22121 \u03c4 \u22121 in addition to the current sequence's hidden state h n\u22121 \u03c4 using cross attention. This mechanism allows for the recurrence of the hidden states h n \u03c4 across layers. The application of a stop gradient between sequences lets information be propagated without suffering the memory costs incurred with full sequence backpropagation. \n\nState Space Models State space models [11,25] have been re-discovered recently as a potential replacement for transformers in long-sequence modeling. These models can be formulated as ordinary differential equations of the form \n\nwhere x(t) \u2208 R is the input signal and y(t) \u2208 R is the output signal. Practically, this is computed through a discretization of the ODE via the zero-order hold (ZOH) rule: \n\nMamba [10] is a new state space model that introduces a selective scan mechanism that allows time-varying parameterizations. Mamba theoretically carries context across very long sequences without any loss in accuracy and is implemented efficiently using custom CUDA kernels.",
            "score": 0.5650922270603568,
            "section_title": "Long-Context Models as Context Encoders",
            "char_start_offset": 8711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 310
                },
                {
                    "start": 313,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1633
                },
                {
                    "start": 1636,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1910
                }
            ],
            "ref_mentions": [
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1270,
                    "end": 1274,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 1274,
                    "end": 1277,
                    "matchedPaperCorpusId": "252873383"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.146728515625
        },
        {
            "corpus_id": "278327448",
            "title": "CodeSSM: Towards State Space Models for Code Understanding",
            "text": "Transformer models have been used in code intelligence tasks (Feng et al., 2020;Guo et al., 2022a;Wang et al., 2021Wang et al., , 2023b)). However, they struggle to scale with long sequence lengths due to the quadratic complexity of self-attention (Vaswani et al., 2017). Various approaches have been proposed to address quadratic complexity, including linear attention (Katharopoulos et al., 2020) and methods using sparse attention patterns (Guo et al., 2022b;Zaheer et al., 2020;Condevaux and Harispe, 2022). LongCoder (Guo et al., 2023) uses the sparse attention mechanism. However, efficient transformers have seen limited practical adoption due to a minimal gain in computational cost in practice (Yang et al., 2025) and a potential degradation in performance (Qin et al., 2022). Dao et al. (2022;2024) introduced hardware-level optimizations, but the benefits of these optimizations are hardwarespecific and do not necessarily apply to newer or different hardware architectures (Shah et al., 2024). \n\nMoreover, self-attention models are trained with a fixed context window and cannot be used beyond the pretraining context window. Techniques such as ALiBi (Press et al., 2022) and RoPE (Su et al., 2024) have been proposed to alleviate this problem. HiRoPE (Zhang et al., 2024) adapted RoPE to code models. However, these methods come with their own set of challenges (Chen et al., 2023b;Hua et al., 2025a;Lin et al., 2024). In contrast, SSMs, and hence CodeSSM, are efficient by design and have been shown to better capture long-range dependencies compared to self-attention (Gu et al., 2022b). Additionally, CodeSSM does not have positional embedding, alleviating the issues of position-specific biases and length generalization.",
            "score": 0.5648595910762568,
            "section_title": "Related Work",
            "char_start_offset": 23550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1005
                },
                {
                    "start": 1008,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1738
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 80,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 80,
                    "end": 98,
                    "matchedPaperCorpusId": "247315559"
                },
                {
                    "start": 98,
                    "end": 115,
                    "matchedPaperCorpusId": "237386541"
                },
                {
                    "start": 248,
                    "end": 270,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 370,
                    "end": 398,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 443,
                    "end": 462,
                    "matchedPaperCorpusId": "245144820"
                },
                {
                    "start": 462,
                    "end": 482,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 482,
                    "end": 510,
                    "matchedPaperCorpusId": "253157377"
                },
                {
                    "start": 522,
                    "end": 540,
                    "matchedPaperCorpusId": "259262301"
                },
                {
                    "start": 703,
                    "end": 722,
                    "matchedPaperCorpusId": "267770686"
                },
                {
                    "start": 766,
                    "end": 784,
                    "matchedPaperCorpusId": "252992749"
                },
                {
                    "start": 786,
                    "end": 803,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 803,
                    "end": 808,
                    "matchedPaperCorpusId": "259936734"
                },
                {
                    "start": 985,
                    "end": 1004,
                    "matchedPaperCorpusId": "271098045"
                },
                {
                    "start": 1163,
                    "end": 1183,
                    "matchedPaperCorpusId": "237347130"
                },
                {
                    "start": 1193,
                    "end": 1210,
                    "matchedPaperCorpusId": "233307138"
                },
                {
                    "start": 1375,
                    "end": 1395,
                    "matchedPaperCorpusId": "266053571"
                },
                {
                    "start": 1413,
                    "end": 1430,
                    "matchedPaperCorpusId": "270845965"
                },
                {
                    "start": 1583,
                    "end": 1601,
                    "matchedPaperCorpusId": "240354066"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0240020751953125
        },
        {
            "corpus_id": "278171038",
            "title": "DYNAMAX: Dynamic computing for Transformers and Mamba based architectures",
            "text": "The Transformer [3] and Mamba [11] model architectures represent two distinct approaches to handling longrange dependencies in sequential data. Transformers leverage self-attention to capture global context within sequences by computing pairwise attention across all tokens. Self-attention computes a weighted sum of values (V ), where the weights are determined by the compatibility between the query (Q) and key (K) vectors. Mathematically: \n\nwhere d k is the dimension of the key vectors. The multihead attention mechanism extends this by computing multiple attention heads in parallel: \n\nwhere each attention head is computed as: \n\nTransformer models offer powerful parallelization and expressive capabilities but are computationally and memoryintensive, with training costs scaling quadratically with sequence length. To enhance efficiency, transfer learning has become a key strategy in NLP, enabling models to be pretrained on large datasets and fine-tuned for specific tasks. While pretraining is resource-intensive, parameter-efficient fine-tuning techniques (PEFT) [28], such as LoRA (low-rank adaptation) [29] or adapters, reduce computational demands, allowing models to leverage open-source pre-trained weights for adaptation across domains. \n\nThe main focus of this work is the reduction of computational complexity; the number of operations of the Transformer block is re-examined. The main source of computational cost of the block is the self-attention mechanism and the feedforward network. Note that we will not take into account the layer norm or the residual connections' compute. \n\nT represents the sequence length and d model represents the internal dimension of the Transformer. We can see how attention contributes with quadratic dependence on the length. The complexity of inference implementing one token forwarding with KV caching would be very similar, but dependence on the sequence length is decreased in all terms by one order of magnitude. \n\nHowever, Mamba is an efficient architecture that utilizes state-space models (SSMs) for sequence processing. Unlike Transformers, which rely on self-attention, Mamba processes sequences using linear operations in combination with learned dynamics. SSMs describe the evolution of a hidden state x t over time based on input u t :",
            "score": 0.564813163967635,
            "section_title": "III. TECHNICAL BACKGROUND A. Transformer and Mamba Architectures",
            "char_start_offset": 6907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1601
                },
                {
                    "start": 1604,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1972
                },
                {
                    "start": 1975,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2222
                },
                {
                    "start": 2223,
                    "end": 2303
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 19,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07684326171875
        },
        {
            "corpus_id": "277150473",
            "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning",
            "text": "Since its introduction, the Transformer architecture (Vaswani et al., 2017) has revolutionized the field of language modeling, becoming the de facto standard for building foundation models. However, its self-attention mechanism has a quadratic time complexity with respect to its context length. In contrast, the recently proposed  Mamba architecture (Gu and Dao, 2023) introduces linear-time sequence modeling with selective state space models, making it significantly more efficient for handling long sequences. In practice, the selective state spaces of Mamba may not be sufficient to capture every detail within long sequences. To address this, a small portion of Transformer layers is incorporated for long-context modeling, giving rise to the hybrid Mamba-MLP-Transformer architecture (Waleffe et al., 2024). \n\nIn Cosmos-Reason1-8B and 56B, we use the pre-trained LLMs with hybrid Mamba-MLP-Transformer architecture (Nvidia et al., 2024;Waleffe et al., 2024) as our LLM backbone. An illustration of the 8B LLM and 56B LLM architectures can be found in Fig. 4. We train the Cosmos-Reason1-8B model with a Tensor Parallelism of 4 (TP=4) (Shoeybi et al., 2019), while the Cosmos-Reason1-56B model is trained with a Tensor Parallelism of 8 and a Pipeline Parallelism of 2 (TP=8, PP=2) to support longer video training.",
            "score": 0.5643875006921094,
            "section_title": "Hybrid Mamba-MLP-Transformer Backbone",
            "char_start_offset": 14049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1320
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.219482421875
        },
        {
            "corpus_id": "276422382",
            "title": "APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs",
            "text": "Large language models (LLMs) (OpenAI, 2024;Anthropic, 2024;DeepSeek-AI, 2024) have demonstrated unprecedented proficiencies, pushing the boundaries of artificial intelligence research and practical applications. Recent advancements are not only transforming usage paradigms but also empowering intelligent systems such as LLM-based agents (Li, 2025;Qin et al., 2024;Zhao et al Figure 1: The prefill speed of methods with and without sequence parallelism when processing different input lengths. \"SP\" indicates sequence parallelism. \"x\" represents that the setting triggers out-of-memory error. 2024), robotics (Zeng et al., 2023;Kim et al., 2024), and prompting methodologies (Chu et al., 2023;Sahoo et al., 2024). These systems often rely on extended context inference. To address the growing demand for longer inputs, contemporary foundation models have been increasingly designed to support extended context lengths. For instance, Llama-3.1 (Dubey et al., 2024) supports up to 128K tokens, Claude-3.5 (Anthropic, 2024) extends input capacity to 200K tokens, and MiniMax-01 (Li et al., 2025) can even process input sequences up to 4M tokens. \n\nAs context lengths grow, the quadratic computational cost of attention makes single-GPU inference both infeasible and inefficient for LLMs. To address this, various optimizations aim to enhance parallelism or reduce compute. Sequence parallelism (Li et al., 2023), which aims to enhance parallelism, partitions the sequence across devices (termed as hosts) and significantly improves the prefill speed, especially for extremely long inputs (Figure 1). However, the overall computations remain unchanged to ensure the accuracy of the attention results. On the other hand, approximate attention mechanisms (Zhang et al., 2024e;Li et al., 2024b;Jiang et al., 2024), which compute only those elements selected from the attention matrix, accelerate inference by reducing compute but face scalabil-ity challenges and performance degradation when processing longer inputs.",
            "score": 0.5627563122948261,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 366,
                    "matchedPaperCorpusId": "269617042"
                },
                {
                    "start": 1392,
                    "end": 1409,
                    "matchedPaperCorpusId": "246017095"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2017822265625
        },
        {
            "corpus_id": "265034082",
            "title": "Ultra-Long Sequence Distributed Transformer",
            "text": "Transformer models trained on long sequences often achieve higher accuracy than short sequences. Unfortunately, conventional transformers struggle with long sequence training due to the overwhelming computation and memory requirements. Existing methods for long sequence training offer limited speedup and memory reduction, and may compromise accuracy. This paper presents a novel and efficient distributed training method, the Long Short-Sequence Transformer (LSS Transformer), for training transformer with long sequences. It distributes a long sequence into segments among GPUs, with each GPU computing a partial self-attention for its segment. Then, it uses a fused communication and a novel double gradient averaging technique to avoid the need to aggregate partial self-attention and minimize communication overhead. We evaluated the performance between LSS Transformer and the state-of-the-art Nvidia sequence parallelism on a Wikipedia enwik8 dataset. Results show that our proposed method lead to 5.6x faster and 10.2x more memory-efficient implementation compared to state-of-the-art sequence parallelism on 144 Nvidia V100 GPUs. Moreover, our algorithm scales to an extreme sequence length of 50,112 at 3,456 GPUs, achieving 161% super-linear parallel efficiency and a throughput of 32 petaflops.",
            "score": 0.5625506096874309,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.029754638671875
        },
        {
            "corpus_id": "269149610",
            "title": "Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length",
            "text": "In many real-world applications, such as multi-turn conversation, long-document comprehension, and video generation, large language models (LLMs) must efficiently process long sequential data, understand internal long-range dynamics, and generate coherent output.The Transformer architecture (Vaswani et al., 2017), despite its remarkable capabilities, faces challenges with quadratic computational complexity and limited inductive bias for length generalization, making it inefficient for long sequence modeling (Wang et al., 2024;Zhou et al., 2024).Even with recently proposed distributed attention solutions (Li et al., 2023b;Liu et al., 2024), computing a single training step of a 7B parameter model over a 1M-token sequence is more than 100 times slower than performing the equivalent computation using 256 separate sequences of 4K tokens each.\n\nTechniques like efficient attention mechanisms (Tay et al., 2020;Ma et al., 2021) and structured state space models (Gu et al., 2022a;Poli et al., 2023;Gu and Dao, 2023) have been introduced to overcome these limitations, aiming to enhance scalability and performance.However, the practical application of these methods still falls short of Transformers (Tay et al., 2022;Gu and Dao, 2023).This work introduces an unlimited context model that outperforms the canonical Transformer architecture on real-world language modeling.We introduce MEGALODON, an improved MEGA architecture (Ma et al., 2023), which harnesses the gated attention mechanism with the classical exponential moving average (EMA) (Hunter, 1986) approach ( \u00a72).To further improve the capability and efficiency of MEGALODON on large-scale longcontext pretraining, we propose multiple novel technical components.First, MEGALODON introduces the complex exponential moving average (CEMA) component, which extends the multi-dimensional damped EMA in MEGA to the complex domain ( \u00a73.1).Then, MEGALODON proposes the timestep normalization layer, which generalizes the group normalization layer (Wu and He, 2018) to autoregressive sequence modeling tasks to allow normalization along the sequential dimension ( \u00a73.2).",
            "score": 0.5623755436534742,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 263,
                    "end": 551
                },
                {
                    "start": 551,
                    "end": 850
                },
                {
                    "start": 852,
                    "end": 1120
                },
                {
                    "start": 1120,
                    "end": 1242
                },
                {
                    "start": 1242,
                    "end": 1378
                },
                {
                    "start": 1378,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1728
                },
                {
                    "start": 1728,
                    "end": 1898
                },
                {
                    "start": 1898,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 314,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 611,
                    "end": 629,
                    "matchedPaperCorpusId": "270973564"
                },
                {
                    "start": 968,
                    "end": 986,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 986,
                    "end": 1004,
                    "matchedPaperCorpusId": "257050308"
                },
                {
                    "start": 1432,
                    "end": 1449,
                    "matchedPaperCorpusId": "252439127"
                },
                {
                    "start": 1549,
                    "end": 1563,
                    "matchedPaperCorpusId": "109597376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278076171875
        },
        {
            "corpus_id": "273025930",
            "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference with Partial Contexts",
            "text": "LLM context length extension. Due to the overhead of training transformers on long sequences, context extension is usually separated from standard pretraining as a dedicated post-training stage. \n\nState-of-the-art models, such as Gemini (Team et al., 2023), Llama series (Touvron et al., 2023;Dubey et al., 2024) and Qwen (Bai et al., 2023) are typically pretrained on large short-sequence corpora, then undergo length extension on relatively smaller amounts of longer sequences. For example, Llama-3 is pre-trained on 15T tokens within an 8K context length, then post-trained on an additional 800B tokens to extend to 128K length (Dubey et al., 2024;Xiong et al., 2023). A related data engineering work (Fu et al., 2024) significantly reduce the tokens needed for context extension to only 5B by carefully balancing the data source to be similar with that of the pretraining corpus. Typically, there is no architectural modifications during length extension. As a result, these long-context models are challenging to serve due to the memory overhead incurred by KV cache (Jiang et al., 2024a). Therefore, techniques are required to reduce the inference-time memory overhead of these models. Inference-time KV reduction fails to generalize to long contexts. To reduce the inference cost of large transformer models, one straightforward idea is to apply inference time KV reduction methods. By only saving the mostly attended KV Cache, they reduce inference memory consumption and forward computing FLOPs. For efficient decoding, the key and value vectors over the context are kept in the GPU memory, usually called KV cache. Its memory overhead has become the primary bottleneck of serving LLMs. Various techniques have been developed to reduce KV cache overhead by storing only a subset of the KV and evicting the rest (Xiao et al., 2024;Liu et al., 2023;Ge et al., 2024). However, as we show here, they often underperform in long-context scenarios, where KV cache reduction is needed the most.",
            "score": 0.562232938145869,
            "section_title": "BACKGROUND",
            "char_start_offset": 4477,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1995
                }
            ],
            "ref_mentions": [
                {
                    "start": 1820,
                    "end": 1839,
                    "matchedPaperCorpusId": "263310483"
                },
                {
                    "start": 1839,
                    "end": 1856,
                    "matchedPaperCorpusId": "258947558"
                },
                {
                    "start": 1856,
                    "end": 1872,
                    "matchedPaperCorpusId": "263609075"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1474609375
        },
        {
            "corpus_id": "237355013",
            "title": "Topic-Guided Abstractive Text Summarization: a Joint Learning Approach",
            "text": "Natural language generation tasks are best expressed as sequence-to-sequence (Seq2Seq) problems. They often assume that each word is encoded into a vector representation. Then a document d with n words can be represented as a sequence of n vectors: d = X 1:n = {x 1 , ..., x n }. Consequently, the language generation problem can be defined as finding a function f mapping an input sequence X to a sequence s of m target vectors s = Y 1:m = {y 1 , ..., y m }. A typical seq2seq model usually consists of three components: (1) An encoder, denoted as f encoder that accepts an input sequence X 1:n , and generates a corresponding sequence of contextualized representation h. (2) A context vector, c that is a function of h conveying the essence of the input document to the decoder. \n\n(3) A decoder, f decoder that uses c to generate an arbitrary length of sequence Y 1:m based on the task-specific requirement. \n\nTransformer has become the most effective neural network architecture for natural language modeling (Vaswani et al., 2017). Comparing to Recurrent Neural Network (RNN), Transformers apply self-attention to compute in parallel every word from the input text an attention weight that gauges the influence each word has on others, thus allowing for parallelization than RNNs for large-scale model training (He et al., 2020). Transformerbased encoder-decoder models are introduced with scaled dot-product attention (Vaswani et al., 2017). It consists of an encoder and a decoder, both are stacks of residual attention blocks, which can process an input sequence X 1:n of variable length n without exhibiting a recurrent structure. This benefits the transformer-based encoder-decoders to be highly parallelizable, which makes the model orders of magnitude more computationally efficient(Platen, 2020). \n\nThe transformer-based encoder encodes the input sequence X 1:n to a sequence of hidden states X1:n (a.k.a. h) by f encoder : X 1:n \u2192 X1:n .",
            "score": 0.5617191627126813,
            "section_title": "Transformer-based Seq2Seq Model",
            "char_start_offset": 7557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1808
                },
                {
                    "start": 1811,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 1012,
                    "end": 1034,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1423,
                    "end": 1445,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0284423828125
        },
        {
            "corpus_id": "267035076",
            "title": "A Survey on Hardware Accelerators for Large Language Models",
            "text": "In 2021, Laguna et al. presented a novel in-memory architecture for the acceleration of transformer networks for long sentences called iMCAT [16]. The proposed framework uses a combination of XBars and CAMs to accelerate transformer networks. The acceleration of transformer networks is achieved by combining several techniques such as computing in-memory, thus minimizing the memory transfer overhead, caching reusable parameters to reduce the number of operations, exploiting the available parallelism in the attention mechanism, and finally using locality sensitive hashing to filter the number of sequence elements by their importance. \n\nThe performance evaluation shows that this approach achieves a 200x speedup and 41x energy improvement for a sequence length of 4098.",
            "score": 0.5608218578035769,
            "section_title": "iMCAT",
            "char_start_offset": 35843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 775
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "236151453"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0301513671875
        },
        {
            "corpus_id": "236924765",
            "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention",
            "text": "The self-attention mechanism is used to learn long-range dependencies while enabling parallel processing of the input sequence. For a given input sequence X := [x 1 , x 2 , \u2022 \u2022 \u2022 , x N ] \u2208 R N \u00d7Dx of N feature vectors that have been encoded in a D x -dimensional vector space, self-attention transforms X into an output sequence V in the following two steps: \n\nStep 1. Project the input sequence X into three matrices via the following linear transformations \n\nwhere W Q , W K \u2208 R D\u00d7Dx , and W V \u2208 R Dv\u00d7Dx are the weight matrices. We denote \n\nwhere the vectors q i , k i , v i for i = 1, \u2022 \u2022 \u2022 , N are the query, key, and value vectors, respectively. \n\nStep 2. For each query vector q i for i = 1, \u2022 \u2022 \u2022 , N , we compute the output vector vi as follows \n\nwhere the softmax function is applied to each row of the matrix (QK )/ \u221a D. \n\nFor long sequences, the computational time and memory footprint of transformers are dominated by (1). It is evident that the memory cost is O(N 2 ) to store the attention matrix A. Also, the computational complexities of computing the matrix-matrix products QK and AV are both O(N 2 ). These limitations impede the application of transformers to many important settings that involve very long sequences [38,28,46]. When applying self-attention for long sequence modeling, we have to limit the context window to a reasonable size to make it computationally feasible, limiting the effectiveness of learning long-term dependencies. Efficient transformer models have been proposed, including leveraging sparse and low-rank attention. Many of the existing efficient transformers gain computational and memory efficiency at the cost of significant accuracy degradation.",
            "score": 0.5596705854920216,
            "section_title": "Self-attention",
            "char_start_offset": 824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 930,
                    "end": 933,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 1236,
                    "end": 1240,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 1240,
                    "end": 1243,
                    "matchedPaperCorpusId": "54477714"
                },
                {
                    "start": 1243,
                    "end": 1246,
                    "matchedPaperCorpusId": "3353110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0740966796875
        },
        {
            "corpus_id": "253452739",
            "title": "A Sequence-to-Sequence Framework Based on Transformer With Masked Language Model for Optical Music Recognition",
            "text": "Seq2Seq is an encoder-decoder framework whose input and output are all sequences [30]. The encoder encodes the input sequence of arbitrary length into a context vector. The decoder decodes the context vector into a prediction sequence with arbitrary length. Due to the arbitrary length of sequences, RNN is always employed as the encoder and the decoder [31]. However, the parallel efficiency of RNN is poor because of its iterative calculation. Therefore, Vaswani et al. [32] proposed Transformer, a powerful and parallel neural network module, to replace the RNN in Seq2Seq framework. The Transformer dispenses with the iterative calculation in encoder and based solely on attention mechanism. The Transformer encoder can achieve computational parallelism, but the Transformer decoder still needs to compute iteratively. After that, Dong et al. [33] used a designed mask matrix in Transformer to implement a parallel unidirectional language model, which makes it possible to decode parallelly in training process. Therefore, we refer to UNILM and design a new mask matrix to train model parallelly in OMR task.",
            "score": 0.5592577508452999,
            "section_title": "B. Seq2Seq FRAMEWORK",
            "char_start_offset": 7886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1112
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 85,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "1169492"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.027435302734375
        },
        {
            "corpus_id": "270370875",
            "title": "Recurrent Context Compression: Efficiently Expanding the Context Window of LLM",
            "text": "With the rapid advancement of natural language processing technologies, Transformer-based large language models (LLMs) have become a key driving force in this field.However, when handling long text inputs, LLMs often encounter limitations in context window length.These limitations stem from several inherent factors in the model architecture and training methods.Firstly, during the inference phase, models are constrained by the pretraining text length, leading to a significant decline in quality when the generated sequence exceeds the pretrained context window.Secondly, the design of the Transformer architecture requires storing information from the entire input sequence, which results in a substantial memory footprint due to the KV-Cache during inference.\n\nTo address these issues, related research works (Hochreiter and Schmidhuber, 1997;Child et al., 2019;Wu et al., 2022;Rae et al., 2019;Bulatov et al., 2022;Liu et al., 2023;Mohtashami and Jaggi, 2023;Beltagy et al., 2020) have optimized training methods, model structures, and KV-Cache optimization, thereby extending the context window of LLMs.Among these, context compression techniques (Rae et al., 2019;Snell et al., 2022;Chevalier et al., 2023;Wingate et al., 2022;Mu et al., 2023;Ge et al., 2023;Munkhdalai et al., 2024;Ren et al., 2023;Li et al., 2023) are considered promising because they can compress context or prompts into shorter forms while maintaining good performance, thus enabling the inference of longer context windows within limited resources.Figure 1 compares the memory resource consumption of our method with non-compression methods.Additionally, most text compression-based works can be integrated and combined with other context window extension techniques to enhance performance.\n\nHowever, existing context compression methods face three major challenges in long-text language modeling.Firstly, the efficiency of compression has certain limitations.For example, ICAE with 14B parameters (Ge et al., 2023) experiences a significant performance drop beyond an 8x compression rate.Secondly, most context compression research focuses on shorter sequences rather than long texts.",
            "score": 0.5591625911267083,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 264
                },
                {
                    "start": 264,
                    "end": 364
                },
                {
                    "start": 364,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 765
                },
                {
                    "start": 767,
                    "end": 1111
                },
                {
                    "start": 1111,
                    "end": 1530
                },
                {
                    "start": 1530,
                    "end": 1623
                },
                {
                    "start": 1623,
                    "end": 1772
                },
                {
                    "start": 1774,
                    "end": 1879
                },
                {
                    "start": 1879,
                    "end": 1942
                },
                {
                    "start": 1942,
                    "end": 2071
                },
                {
                    "start": 2071,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 815,
                    "end": 849,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 901,
                    "end": 922,
                    "matchedPaperCorpusId": "250526424"
                },
                {
                    "start": 939,
                    "end": 966,
                    "matchedPaperCorpusId": "258887482"
                },
                {
                    "start": 1192,
                    "end": 1215,
                    "matchedPaperCorpusId": "129945531"
                },
                {
                    "start": 1215,
                    "end": 1236,
                    "matchedPaperCorpusId": "252762169"
                },
                {
                    "start": 1236,
                    "end": 1252,
                    "matchedPaperCorpusId": "258179012"
                },
                {
                    "start": 1292,
                    "end": 1309,
                    "matchedPaperCorpusId": "263909553"
                },
                {
                    "start": 1309,
                    "end": 1325,
                    "matchedPaperCorpusId": "263830231"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.202392578125
        },
        {
            "corpus_id": "267740599",
            "title": "ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling",
            "text": "Maximum path length measures the longest distance information must traverse to establish relationships, reflecting the model's ability to learn long-range dependencies. \n\nAs noted in Table 1, vanilla Transformer is an efficient model for sequence learning with O(1) sequential operations and O(1) path length because of the parallel attention mechanism, whereas the recurrent nature of RNN and Neural ODE rely on autoregressive property and suffer from a large number of sequential operations. Utilizing Eq. ( 9) and leveraging the parallelism inherent in the Transformer architecture, our model, ContiFormer, achieves O(S) sequential operations and O(1) path length, while also enjoying the advantage of capturing complex continuous-time dynamic systems. We have S \u226a T and we generally set S < N in our experiments.",
            "score": 0.5585385263552101,
            "section_title": "Complexity Analysis",
            "char_start_offset": 17647,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 171,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 816
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.029541015625
        },
        {
            "corpus_id": "278208203",
            "title": "Galvatron: An Automatic Distributed System for Efficient Foundation Model Training",
            "text": "In recent years, large-scale Transformer-base Foundation Models, particularly Large Language Models (LLMs), have demonstrated exceptional performance in text understanding and generation (Vaswani et al., 2017;Zhao et al., 2023a;Brown et al., 2020;OpenAI, 2023;Anil et al., 2023;Dubey et al., 2024). Scaling laws suggest that increasing model parameters enhances performance (Kaplan et al., 2020), driving the need for large-scale distributed training. To optimize such training, various parallelization methods have been proposed, including data parallelism (Dean et al., 2012;Brown et al., 2020), tensor parallelism (Narayanan et al., 2021b), pipeline parallelism (Huang et al., 2019;Narayanan et al., 2019Narayanan et al., , 2021a)), sharded data parallelism (Rajbhandari et al., 2020;Zhao et al., 2023b), and sequence/context parallelism (Korthikanti et al., 2023;Jacobs et al., 2023;Liu et al., 2023). Each method presents distinct memory, computation, and communication characteristics. \n\nTransformer-based Foundation Models exhibit various architectural designs, with decoderonly models (e.g., GPT, Llama) being predominant for its general ability, while encoder-only and encoder-decoder models like BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) are widely used in specific downstream applications (Zhao et al., 2023a). Within the decoder-only category, notable examples include GPT (Brown et al., 2020), Llama (Dubey et al., 2024), and Qwen (Yang et al., 2024), each exhibiting unique design and workload characteristics. Additionally, models of the same type often have multiple configurations, influencing their computational and memory properties. \n\nBeyond architectural differences, training settings introduce additional complexity.",
            "score": 0.5583829491649661,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1667
                },
                {
                    "start": 1670,
                    "end": 1754
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 247,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 558,
                    "end": 577,
                    "matchedPaperCorpusId": "372467"
                },
                {
                    "start": 577,
                    "end": 596,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 617,
                    "end": 642,
                    "matchedPaperCorpusId": "236635565"
                },
                {
                    "start": 665,
                    "end": 685,
                    "matchedPaperCorpusId": "53670168"
                },
                {
                    "start": 685,
                    "end": 707,
                    "matchedPaperCorpusId": "202488191"
                },
                {
                    "start": 707,
                    "end": 734,
                    "matchedPaperCorpusId": "219720945"
                },
                {
                    "start": 761,
                    "end": 787,
                    "matchedPaperCorpusId": "269617042"
                },
                {
                    "start": 787,
                    "end": 806,
                    "matchedPaperCorpusId": "258297871"
                },
                {
                    "start": 841,
                    "end": 867,
                    "matchedPaperCorpusId": "248693351"
                },
                {
                    "start": 1211,
                    "end": 1232,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1240,
                    "end": 1261,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1399,
                    "end": 1419,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02508544921875
        },
        {
            "corpus_id": "267412232",
            "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models",
            "text": "The overarching goal of the survey is to provide a detailed insight into those methods, as well as to highlight possible directions for future research. The techniques include architectural modifications, such as positional encoding modification, modified attention mechanisms and model compression techniques, which aim to optimize the processing of longer sequences without exponentially increasing computational and memory demands. Additionally, we explore the methods that can be adopted in different phases (training, fine-tuning, and inference), and have been pivotal in enabling LLMs to handle longer sequences, efficiently. The taxonomy of our literature review is shown in Figure 1. While there are existing surveys addressing LLMs with Figure 1: Taxonomy of Long-context LLM literature, which includes five distinct sections: length extrapolation, attention approximation, attention-free transformers, model compression, and hardware-aware transformers. We also establish connections between the methodologies and their related applicability scenarios. Some entail training a new model from scratch, others involve fine-tuning pre-trained models, and some implement over inference without any updates of hyper-parameters. a more general scope [Zhao et al., 2023;Naveed et al., 2023;Wan et al., 2023], this survey is particularly focused on evaluating the articles dealing with long sequences in LLMs. Moreover, there are other reviews on efficient Transformers and their training methodologies [Zhuang et al., 2023;Huang et al., 2023], but this survey specifically focuses on models and strategies that aim at enhancing the management of longer input sequences.",
            "score": 0.5582458677251942,
            "section_title": "Introduction",
            "char_start_offset": 2247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1671
                }
            ],
            "ref_mentions": [
                {
                    "start": 1253,
                    "end": 1272,
                    "matchedPaperCorpusId": "256503897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.455078125
        },
        {
            "corpus_id": "261245264",
            "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",
            "text": "Long Context Modeling Techniques. We first discuss some popular lines of methods that aim to tackle long context understanding. These studies are mainly aimed at solving two key challenges in long text modeling, including the high runtime overhead on longer context, and the catastrophic forgetting phenomenon when processing long sequence. A series of studies focus on how to make Transformers more efficient and unforgetful (Tay et al., 2022), with designs such as sparse and efficient computation (Child et al., 2019;Kitaev et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020;Wang et al., 2020;Fedus et al., 2022;Ding et al., 2023), recurrent and memory modules (Dai et al., 2019;Rae et al., 2020;Wu et al., 2022;Martins et al., 2022;Bulatov et al., 2022;Orvieto et al., 2023;Liang et al., 2023;Zhou et al., 2023). More recently, several methods (Press et al., 2022;Sun et al., 2022;Chen et al., 2023) have been proposed to enable length extrapolation of Transformers, and have been adopted in the training process of long context LLMs such as ChatGLM2-32k (Zeng et al., 2023) and LongChat-32k (Li et al., 2023). Evaluation for Long Context Understanding. Many previous works on long text modeling rely on the perplexity metric for evaluation (Beltagy et al., 2020;Roy et al., 2021;Press et al., 2022). However, as suggested in (Sun et al., 2021), the perplexity metric may not necessarily reflect the model's performance on sequence-level tasks in real applications Meanwhile, some works assess long text modeling through artificial tasks such as retrieval (Tay et al., 2021;Chen et al., 2023;Li et al., 2023), which may also fall short in mirroring real-world scenarios.",
            "score": 0.5581563569907951,
            "section_title": "Related Work",
            "char_start_offset": 3596,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1677
                }
            ],
            "ref_mentions": [
                {
                    "start": 426,
                    "end": 444,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 520,
                    "end": 540,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 561,
                    "end": 581,
                    "matchedPaperCorpusId": "259165244"
                },
                {
                    "start": 599,
                    "end": 618,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 667,
                    "end": 685,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 685,
                    "end": 702,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 702,
                    "end": 718,
                    "matchedPaperCorpusId": "247519194"
                },
                {
                    "start": 739,
                    "end": 760,
                    "matchedPaperCorpusId": "250526424"
                },
                {
                    "start": 851,
                    "end": 871,
                    "matchedPaperCorpusId": "237347130"
                },
                {
                    "start": 1062,
                    "end": 1081,
                    "matchedPaperCorpusId": "252715691"
                },
                {
                    "start": 1270,
                    "end": 1287,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1287,
                    "end": 1306,
                    "matchedPaperCorpusId": "237347130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5009765625
        },
        {
            "corpus_id": "273549714",
            "title": "Taipan: Efficient and Expressive State Space Language Models with Selective Attention",
            "text": "Figure 1 illustrates Taipan's superior performance in handling extended sequences compared to Transformer, Jamba, and Mamba models. In perplexity evaluations across context lengths from 1K to 1M tokens (Figure 1a), Taipan yields the lowest perplexity, particularly excelling beyond the training context length. \n\nThis performance contrasts sharply with other models: Transformers struggle with longer contexts due to quadratic computational complexity and linear memory scaling with sequence length, often leading to out-of-memory errors. Jamba, despite its hybrid nature, faces similar challenges due to its use of full attention mechanisms. Both Transformer and Jamba models exhibit limited extrapolation ability beyond their training context lengths. Mamba, while more efficient than Transformers and Jamba, still shows performance degradation for very long sequences. \n\nLatency comparisons (Figure 1b) further highlight Taipan's exceptional efficiency. It demonstrates the lowest latency among all models, with linear scaling across sequence lengths. This contrasts with the quadratic scaling of Transformers and higher latency growth of Jamba. Notably, Taipan consistently outperforms Mamba-2, primarily due to its selective attention mechanism.",
            "score": 0.5577334422469632,
            "section_title": "LONG-CONTEXT EXTRAPOLATION",
            "char_start_offset": 19922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 310
                },
                {
                    "start": 313,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 871
                },
                {
                    "start": 874,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1250
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.175537109375
        },
        {
            "corpus_id": "276422382",
            "title": "APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs",
            "text": "In this section, we briefly introduce experimental settings, and more details are in Appendix B. \n\nBenchmarks. We evaluate APB against selected baselines on two long-context evaluation benchmarks: \u221eBench (Zhang et al., 2024b) and RULER (Hsieh et al., 2024). \u221eBench is a benchmark with an average context length exceeding 100K tokens, encompassing a mix of both synthetic and real-world tasks. Following MINFER-ENCE (Jiang et al., 2024), we utilize 10 tasks, excluding Math.Calc and Code.Run due to their difficulty, which leads to failures for all methods. RULER is a benchmark featuring a variety of synthetic tasks with a controllable context length. We evaluate all 13 tasks of the benchmark. We provide the mapping of task abbreviations to the original task name in Appendix D. Additional experiments and results are placed in Appendix E. \n\nModels. For the end-to-end benchmark (Q1), we test APB with the baselines on the instruct versions of Llama-3.1-8B, Qwen-2.5-14B, and Yi-34B-200K. When benchmarking across various input lengths (Q2), we utilize Llama-3-8B-1M1 , which supports up to 1M input tokens. \n\nMetrics. When evaluating performance on the benchmark, we directly adopt the original performance metrics for each task. For speed measurement, we define inference speed as the total number  of tokens in both the prefill and decoding stages, divided by the total inference time, which includes both the prefill and decoding stages. \n\nBaselines. Existing methods for accelerating long-context inference can be categorized into four types based on their use of sequence parallelism and approximate attention mechanisms. We select FLASHATTN, ULYSSES, RINGATTN, MINFERENCE, and STARATTN as our baselines. FLASHATTN does not use sequence parallelism and maintains accurate attention computation. ULYSSES and RINGATTN are two major approaches for performing accurate attention with sequence parallelism, each with a different communication design. MINFERENCE is an approximate attention mechanism that accelerates longcontext inference by applying sparse attention with-out sequence parallelism.",
            "score": 0.5576493178143825,
            "section_title": "Experimental Setup",
            "char_start_offset": 15764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 99,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1110
                },
                {
                    "start": 1113,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2102
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07977294921875
        },
        {
            "corpus_id": "266999544",
            "title": "The What, Why, and How of Context Length Extension Techniques in Large Language Models - A Detailed Survey",
            "text": "LONGMEM exhibits an average score increase of +8.0 over pre-trained GPT-2* and MemTRM in the 20-shot setting, emphasizing its proficiency in utilizing auxiliary contextual demonstrations for superior in-context learning. Additionally, the model shows promise in open-ended generation tasks, achieving a +4.5 EM score increase on SQuAD, showcasing its versatility in leveraging cached memory for improved in-context learning. The results affirm LONGMEM's effectiveness and superiority in long-context modeling, understanding, and many-shot in-context learning, establishing it as a potent approach in the landscape of language models. Ablation studies further explore the impact of hyperparameters, such as chunk size and memory size, providing insights into their effects on task performance. \n\nRelated work. Prominent language models like GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), OPT (Zhang et al., 2022), and BLOOM (Workshop et al., 2022) have drastically reshaped NLP research, elevating performance benchmarks in language understanding, generation (Wang et al., 2022c), and vision-language (Wang et al., 2022b) tasks. These models, collectively known as LLMs, exhibit groundbreaking abilities such as few-shot in-context learning and multi-step reasoning (Wei et al., 2022) by scaling their parameters. To address the challenge of processing longer contexts, a cat-egory of transformer models, termed \"x-formers\", has been proposed. Transformer-XL (Dai et al., 2019b) pioneers a caching mechanism for attention keys and values from past segments, while recent innovations like LinFormer (Wang et al., 2020), LongFormer (Beltagy et al., 2020), and Routing Transformer (Roy et al., 2021) leverage sparse attention mechanisms to mitigate the quadratic complexity issue. Despite their efficiency gains, these models face limitations when dealing with booklength sequences. BigBird (Zaheer et al., 2020) extends sequence length but remains constrained to 16k tokens.",
            "score": 0.5574703122874435,
            "section_title": "Memory/Retrieval Augmented approaches",
            "char_start_offset": 71170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 1982
                }
            ],
            "ref_mentions": [
                {
                    "start": 846,
                    "end": 868,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1069,
                    "end": 1089,
                    "matchedPaperCorpusId": "237363762"
                },
                {
                    "start": 1276,
                    "end": 1294,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1688,
                    "end": 1706,
                    "matchedPaperCorpusId": "212718077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275146484375
        },
        {
            "corpus_id": "204734354",
            "title": "Evolution of transfer learning in natural language processing",
            "text": "The Transformer-XL [20]was able to model very long-range dependencies. It did so by overcoming one limitation of the vanilla Transformer-fixed-length context. Vanilla Transformers were incapable of accommodating a very long sequence owing to this limitation. Hence they resorted to alternatives such as splitting the corpus into segments which could be managed by the Transformers. This led to loss of context among individual segments despite being part of the same corpus. However, the Transformer-XL was able to take the entire large corpus as input, thus preserving this contextual information. In essence a vanilla Transformer, it relied on two novel techniques-Recurrence mechanism and Positional Encoding-to provide the improvement. \n\n1) Recurrence mechanism: Instead of training the Transformer on individual segments of the corpus(without regards to previous context), the authors propose caching the hidden sequence state computed from the previous segments. Consequently, the model computes self attention(and other operations) on the current hidden/input state as well as these cached hidden states. The number of states cached during training is limited due to memory limitations of the GPU. However, during inference, the authors can increase the number of cached hidden states used to model long-term dependency. \n\n2) Relative Positional Encoding: An inherent problem with using the said Recurrence mechanism is preserving relative positional information while reusing cached states. The authors overcame this problem by incorporating Relative Positional Encoding in the Attention mechanism(instead of hidden states) of the Transformer. They do so by encoding information regarding the positional embedding dynamically in the Attention scores themselves. The distance of the key vectors for the query vector is the temporal information that is encoded in the Attention scores. In essence, computing attention is facilitated as temporal distance is still available to the model while still preserving previous states. Furthermore, information regarding the absolute position can be recovered recursively. \n\nThe Transformer-XL was able to surpass the state-of-the-art results for language modelling tasks on the enwiki8 and text8 datasets.",
            "score": 0.557196657067226,
            "section_title": "F. Transformer-XL",
            "char_start_offset": 37152,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2118
                },
                {
                    "start": 2121,
                    "end": 2252
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1630859375
        },
        {
            "corpus_id": "277150882",
            "title": "ATTENTION2D: Communication Efficient Distributed Self-Attention Mechanism",
            "text": "Memory-efficient attention. Self-attention is a fundamental component of Transformer models, but it tends to be slow and memory-intensive, particularly for long sequences, due to its quadratic memory cost relative to sequence length. This challenge has spurred the creation of memory-efficient attention mechanisms [22,28,9,8] that achieve linear memory costs. These methods primarily address memory cost improvements on a single GPU. Our proposed solution leverages these techniques to reduce memory costs on a single GPU while introducing an efficient parallelization strategy to scale self-attention computations across multiple GPUs. \n\nParallelism methods. Various self-attention parallelism methods [32,17,19,20,18,13,12] have been proposed in the past. However, these methods mainly focus on reducing memory costs, often neglecting communication costs. As shown in Table 1, all these techniques incur a communication cost that increases linearly with the sequence length, regardless of the number of devices used. In contrast, our proposed parallelism method achieves a sub-linear communication cost that decreases as the number of devices increases. Parallelization strategies like data parallelism [11], pipeline parallelism [23], and ZeRO [29,3] focus on parallelizing the entire LLM. Many of these methods complement our approach. Therefore, similar to the strategy employed by USP [12], a hybrid approach that combines these techniques with ours can be utilized to achieve further improvements. Approximation of attention. To reduce the computational cost of attention on long contexts, numerous methods have explored approximations of attention mechanisms [4,7,16,35,15,37]. Although these methods have been used in some cases, standard attention is still often preferred because these approximations can yield sub-optimal results. In contrast, our method does not rely on any approximations. Instead, we propose improvements to the standard self-attention operation for parallel settings. Grouped Query Attention (GQA) [1] and Multi-Query Attention (MQA) [31] are variants of standard self-attention where multiple query heads attend to the same key and value head, reducing the size of the KV cache during inference. Since our parallelism method operates at the level of single attention heads, it can be directly applied to these variants. \n\nLong context inference.",
            "score": 0.5566487903822144,
            "section_title": "Discussion and Related Work",
            "char_start_offset": 33159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2230
                },
                {
                    "start": 2231,
                    "end": 2354
                },
                {
                    "start": 2357,
                    "end": 2380
                }
            ],
            "ref_mentions": [
                {
                    "start": 322,
                    "end": 324,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 324,
                    "end": 326,
                    "matchedPaperCorpusId": "259936734"
                },
                {
                    "start": 711,
                    "end": 714,
                    "matchedPaperCorpusId": "246017095"
                },
                {
                    "start": 717,
                    "end": 720,
                    "matchedPaperCorpusId": "270973564"
                },
                {
                    "start": 1206,
                    "end": 1210,
                    "matchedPaperCorpusId": "372467"
                },
                {
                    "start": 1248,
                    "end": 1252,
                    "matchedPaperCorpusId": "269617042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0728759765625
        },
        {
            "corpus_id": "216562722",
            "title": "Fast and Memory-Efficient Neural Code Completion",
            "text": "(Transformer) An alternative to RNN-based and CNN-based sequence models are transformers [47] which have recently shown exceptional performance in natural language processing, such as in BERT [13] and GPT-2 [40]. Although transformers can be parallelized efficiently, they have quadratic runtime memory requirements with respect to the sequence length. \n\nHere, we employ the standard transformer encoder architecture and use as context representation the first vector of the output. \n\nCompletion Location-Annotated Encoders. We can provide additional information to any of the above context encoders C, e.g. information derived from analyzing the code. Here, we test adding some lightweight information that is useful for API completion. Specifically, we annotate the variable or namespace on which an API completion is performed. For example, in the code of Figure 1 we indicate to the context encoders all the tokens that refer to the receiver object array1. This may allow a context encoder, to recognize API patterns, e.g. if foo.open() was previously invoked then invoking read() on foo is likely. Of course, other annotations are also possible here, but we do not test them in this work. \n\nTo capture this long-range context, we simply wrap the token encoder of Equation 4 such that it appends a 0/1 component to each token encoding r t . This bit is set to one for the tokens that are bound to the variable or namespace whose API is about to be completed. This provides additional information to the context encoders at a minimal cost. We denote such encoders by appending to their name a diamond (\u22c4), e.g. Gru \u22c4 .",
            "score": 0.5558495093563041,
            "section_title": "Context Encoders C",
            "char_start_offset": 19208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1621
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 93,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037109375
        },
        {
            "corpus_id": "274130590",
            "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
            "text": "The Selective State-Space Model (SSM) (Gu et al., 2021b(Gu et al., , 2022) ) has recently emerged as a powerful alternative to Transformers (Vaswani et al., 2017) in language modeling, demonstrating comparable or superior performance at small to moderate scales. SSMs are characterized by linear scaling in sequence length during training and a constant state size during generation, which significantly reduces computational and memory overhead, making them more efficient in terms of speed and memory usage. The representative SSM model of Mamba (Gu and Dao, 2024;Dao and Gu, 2024) has a significant advantage when handling long context sequences due to its linear complexity. In contrast, conventional transformers suffer from quadratic complexity as the sequence length increases. This means that for tasks involving large input sequences or extended contexts, Mamba is much more efficient, as its memory and computational requirements scale linearly with the sequence length. In practice, this allows Mamba to process long sequences faster and with lower resource consumption, making it ideal for applications such as long document processing, conversational agents, and any scenario where managing large amounts of contextual information is crucial. On the other hand, transformers require exponentially more resources as the context length increases, which can quickly become a bottleneck in long-context tasks. \n\nPrior works on Transformers have been extensively studied for several years (Devlin et al., 2019;Radford et al., 2019;Touvron et al., 2023;Achiam et al., 2023), with numerous methods proposed to alleviate their high computational and storage costs, such as pruning (Ma et al., 2023), model quantization (Frantar et al., 2023), and KV Cache (Pope et al., 2023;Kwon et al., 2023). Among these methods, quantization has proven to be highly effective (Dettmers et al., 2022a). Researchers have successfully quantized transformer models from 16-bit to 8-bit, 4-bit, and even 1-bit representations, often with minimal performance degradation (Frantar et al., 2023;Ma et al., 2024b,a).",
            "score": 0.5557986328354505,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1418
                },
                {
                    "start": 1421,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2099
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 55,
                    "matchedPaperCorpusId": "239998472"
                },
                {
                    "start": 55,
                    "end": 76,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 140,
                    "end": 162,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1497,
                    "end": 1518,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1518,
                    "end": 1539,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1761,
                    "end": 1780,
                    "matchedPaperCorpusId": "253420623"
                },
                {
                    "start": 1780,
                    "end": 1798,
                    "matchedPaperCorpusId": "261697361"
                },
                {
                    "start": 1868,
                    "end": 1892,
                    "matchedPaperCorpusId": "251564521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.156982421875
        },
        {
            "corpus_id": "235417174",
            "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers",
            "text": "The Transformer [1] has become one of the most popular neural networks (NNs) for processing sequential data. Its success on neural machine translation quickly transferred to other problems in natural language processing, such as language modelling [2,3] or question answering [4]. Recently, it has also been applied in other domains, including image processing [5,6] or mathematical problem solving [7,8,9]. \n\nConceptually, the Transformer is a deep feedforward NN that processes all elements of a sequence in parallel: unlike in recurrent NNs (RNNs), the computations of a layer for the entire sequence can be packed into one big matrix multiplication. This scales well with the number of parallel processors. \n\nDespite the benefits of parallelisation, a major drawback of Transformers is that their computational complexity in time and space is quadratic in sequence length. Furthermore, in the auto-regressive version [1,2] -the focus of our work -the state size increases linearly with sequence length. This makes Transformers infeasible for auto-regressive settings dealing with very long or potentially infinite sequences, forcing practitioners to truncate temporal contexts and ignore long-term dependencies beyond fixed-size time windows. Although recent work tries to address this issue [10,11], this limitation makes some applications of Transformers challenging, e.g., reinforcement learning (RL) in partially observable environments [12,13], which is still dominated by RNNs such as the Long Short-Term Memory (LSTM; [14]) trained by policy gradients [15,16,17,18]. \n\nTo scale Transformers to longer sequences, recent works have proposed to linearise the softmax in the self-attention computation and reorganise the latter in a sequential way [19]. Such models include Our experiments on the language modelling dataset Wikitext-103 [28] show that our RFWPs are competitive compared to regular Transformers. We then study various properties of the proposed models on two synthetic algorithmic tasks: code execution [29] and sequential ListOps [30]. Finally, it is straightforward to apply our models to RL problems as a drop-in replacement for LSTMs.",
            "score": 0.5555648511866529,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 407
                },
                {
                    "start": 410,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1577
                },
                {
                    "start": 1580,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2161
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 19,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 248,
                    "end": 251,
                    "matchedPaperCorpusId": "52004855"
                },
                {
                    "start": 251,
                    "end": 253,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 276,
                    "end": 279,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 361,
                    "end": 364,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 364,
                    "end": 366,
                    "matchedPaperCorpusId": "222208633"
                },
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "85504763"
                },
                {
                    "start": 404,
                    "end": 406,
                    "matchedPaperCorpusId": "235613478"
                },
                {
                    "start": 921,
                    "end": 924,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 924,
                    "end": 926,
                    "matchedPaperCorpusId": "52004855"
                },
                {
                    "start": 1296,
                    "end": 1300,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 1445,
                    "end": 1449,
                    "matchedPaperCorpusId": "204578308"
                },
                {
                    "start": 1449,
                    "end": 1452,
                    "matchedPaperCorpusId": "233025422"
                },
                {
                    "start": 1529,
                    "end": 1533,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1563,
                    "end": 1567,
                    "matchedPaperCorpusId": "7162236"
                },
                {
                    "start": 1567,
                    "end": 1570,
                    "matchedPaperCorpusId": "14039355"
                },
                {
                    "start": 1573,
                    "end": 1576,
                    "matchedPaperCorpusId": "204972004"
                },
                {
                    "start": 1755,
                    "end": 1759,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1844,
                    "end": 1848,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 2054,
                    "end": 2058,
                    "matchedPaperCorpusId": "4942335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03759765625
        },
        {
            "corpus_id": "237412971",
            "title": "\\infty-former: Infinite Memory Transformer",
            "text": "Transformer models that attend to long contexts, to improve their generation quality, need large amounts of computation and memory to perform self-attention. In this paper, we propose an extension to a transformer model that makes the attention complexity independent of the length of the context being attended to. This can lead to a reduced number of parameters needed to model the same context, which can, consequently, lead to gains in efficiency and reduce energy consumption. \n\nOn the other hand, the \u221e-former, as well as the other transformer language models, can be used on questionable scenarios, such as the generation of fake news (Zellers et al., 2019), defamatory text (Wallace et al., 2019), or other undesired content.",
            "score": 0.5552577865040742,
            "section_title": "Ethics Statement",
            "char_start_offset": 26242,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 733
                }
            ],
            "ref_mentions": [
                {
                    "start": 642,
                    "end": 664,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 682,
                    "end": 704,
                    "matchedPaperCorpusId": "201698258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08929443359375
        },
        {
            "corpus_id": "273653873",
            "title": "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning",
            "text": "In the swiftly advancing field of natural language processing (NLP), large language models (LLMs) like GPT-4 and Llama have emerged as vital tools, demonstrating expertise in comprehending and producing human language. These complex scenarios often involve context lengths longer than those LLMs were trained on, posing significant challenges to Transformer-based architectures in handling long sequences. For example, LLMs are expected to read long paragraphs or books to answer questions while they are usually trained with a much smaller context length (like 8k for Llama-3 (Touvron et al., 2023)). \n\nScaling up LLMs for long sequences presents significant challenges related to limited attention windows during training and length extrapolation during inference. First, transformers usually adopt a bounded context window in the training phase due to quadratic computation costs (Vaswani et al., 2017;Tay et al., 2022). To mitigate these costs and expand the training context window, existing works usually adopt attention approximation methods like sparse, low-rank, and softmax-free attention. Despite their efficiency and scalability with relatively long sequences, these methods gain less popularity due to inefficient implementations and incompatibility with existing pre-trained LLMs. \n\nGiven such pretrained LLMs, researchers start to focus more on the length extrapolation challenge: leveraging the short-range modeling power acquired during pre-training to handle unseen longrange dependencies. Addressing this challenge requires methods that take into account both efficiency and effectiveness perspectives. One line of thought is to employ full attention with hardwareefficient implementations such as FlashAttention and quantization for efficiency, paired with positional extrapolation (Su et al., 2024) or interpolation (Chen et al., 2023a) for enhanced performance. \n\nWhile full attention captures all potential correlations, it significantly increases running time, and many correlations in long sequences are in fact unnecessary and distracting. The second line of approaches involves using segmented or sliding windows for efficiency, supplemented by additional recurrent memory modules (Bulatov et al., 2023;Wang et al., 2023) or global sinks (Xiao et al., 2023;Han et al., 2023) to integrate segments.",
            "score": 0.5551627222217529,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1883
                },
                {
                    "start": 1886,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 883,
                    "end": 905,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 905,
                    "end": 922,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1802,
                    "end": 1819,
                    "matchedPaperCorpusId": "233307138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.226318359375
        },
        {
            "corpus_id": "270370979",
            "title": "SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models",
            "text": "Enhancing the functionality of Transformer models to handle longer sequence lengths has become crucial for numerous applications, including language translation, long-context processing, chatbots, code generation, and multimedia content creation.The primary challenge lies in the self-attention mechanism, which scales quadratically with sequence length, leading to substantial computational time and memory requirements [4,41,21].To address this challenge, several approaches have been proposed.Longformer and BigBird utilize combinations of local, global, and sparse attention mechanisms to manage long contexts, reducing complexity to O(n) [4,41].Reformer introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thereby reducing computational complexity [21].LSG Attention combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead [9].Despite these advancements, managing long-context interactions in practical applications remains a significant challenge.Recent work, such as LongLoRA, extends the context window of LLaMA2 from 4096 to 32768 tokens using Position Interpolation without substantial GPU or TPU resources Figure 1: Evaluation of SinkLoRA in bridging the accuracy gap between sparse shifted attention and full attention during supervised fine-tuning, while maintaining the memory efficiency of LongLoRA, which utilizes 1.8 times less memory compared to full fine-tuning.Furthermore, SinkLoRA retains the training speed of LongLoRA, being 1.8 times faster than full fine-tuning, due to the implementation of Sink Fixed Attention.The Llama2-7B models [37] are fine-tuned to various context lengths using Flash-Attention2 [10] and DeepSpeed stage 2 [31], and are evaluated on the proof-pile test set [2] in terms of perplexity.3: Overview of the SinkLoRA inference process.Unlike LongLoRA, which retains the original standard self-attention during inference, SinkLoRA implements an optional KV cache compression method, H 2 O [43].This extension enhances inference speed without significantly compromising performance.[6].",
            "score": 0.5542683394651836,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 246,
                    "end": 431
                },
                {
                    "start": 431,
                    "end": 496
                },
                {
                    "start": 496,
                    "end": 650
                },
                {
                    "start": 650,
                    "end": 824
                },
                {
                    "start": 824,
                    "end": 963
                },
                {
                    "start": 963,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1512
                },
                {
                    "start": 1512,
                    "end": 1670
                },
                {
                    "start": 1670,
                    "end": 1866
                },
                {
                    "start": 1866,
                    "end": 1912
                },
                {
                    "start": 1912,
                    "end": 2070
                },
                {
                    "start": 2070,
                    "end": 2157
                },
                {
                    "start": 2157,
                    "end": 2161
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 427,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 646,
                    "end": 649,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 959,
                    "end": 962,
                    "matchedPaperCorpusId": "253157377"
                },
                {
                    "start": 1788,
                    "end": 1792,
                    "matchedPaperCorpusId": "221191193"
                },
                {
                    "start": 2065,
                    "end": 2069,
                    "matchedPaperCorpusId": "259263947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33544921875
        },
        {
            "corpus_id": "276259019",
            "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid",
            "text": "Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE.",
            "score": 0.5540575344551882,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046539306640625
        },
        {
            "corpus_id": "276885221",
            "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
            "text": "Hybrid linear sequence modeling models, which combine linear transformer layers (leveraging LSM methods for token mixing) with standard transformer layers (utilizing conventional selfattention for token mixing), have demonstrated substantial improvements in handling long-context tasks (Lieber et al., 2024;Ren et al., 2024;Waleffe et al., 2024). This hybrid model is particularly beneficial for tasks with high recall requirements, including five-shot MMLU (Hendrycks et al., 2020), Phone-book lookup (Jelassi et al., 2024), and Needle In A Haystack (Briakou et al., 2023), etc.. Our proposed hybrid Linear-MoE models also aim to enhance performance in areas where pure Linear-MoE models have shown limitations, specifically on tasks where recall precision is critical. \n\nApplying SP on pure Linear-MoE models is straightforward, as this form of SP operates exclusively on the LSM modules, leaving the MoE layers unaffected. In hybrid Linear-MoE models, however, implementing SP becomes more complex due to the interleaving of distinct sequence modeling layers. To effectively optimize SP for these hybrid models, we introduce an integrated approach that incorporates SP across both the linear-MoE and standard transformer layers, thus enhancing overall efficiency. We illustrate the approach in Fig. 2, and explain it as below: \n\nOn LSM Module. The SP for LSM modules is implemented via a single collective communication operation on the memory state M s \u2208 R d\u00d7d . This approach ensures that the communication complexity does not depend on either the sequence or sub-sequence length; rather, it scales only linearly with the SP size T , thereby maintaining efficiency in distributed setups. \n\nOn Standard Attention Module. Context parallelism (CP) is a SP technique used in Megatron-LM that divides input data and activations along the sequence dimension, specifically designed for standard softmax attention. Traditional CP implementations in Megatron-LM rely on a ring-like communication-computation overlap (Liu et al., 2023). In contrast, our approach for standard attention modules adopts the all-gather-based strategy used in the pretraining of Llama3 (Dubey et al., 2024).",
            "score": 0.5526490494379532,
            "section_title": "Hybrid Model Sequence Parallelism",
            "char_start_offset": 12599,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1692
                },
                {
                    "start": 1695,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2181
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.235107421875
        },
        {
            "corpus_id": "273025930",
            "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference with Partial Contexts",
            "text": "Transformer-based large language models (LLMs) capable of processing long contexts have unlocked new opportunities across a wide range of applications, such as processing long multi-turn dialogue, understanding code repositories, and answering complex queries that require synthesizing information from multiple documents. However, their quadratic complexity incurs substantial overhead for both training and inference with long contexts (Jiang et al., 2024b;Sun et al., 2024a). To address this, typical approaches involve a two-stage framework (Xiong et al., 2023;Dubey et al., 2024). \n\n\u2022 To enhance the model long context capabilities without the substantial overhead of training with long contexts, an LLM is typically pretrained on large short-context datasets (e.g., 2TB of 4K tokens) and undergoes a separate context length extension phase, usually through continual pretraining on a smaller amount of long-context data (e.g., 5B of 128K tokens). All results are measured on Llama2-7B, which consists of 32 layers in total. \"1/7\", \"1/5\", \"1/3 Full\" and \"All Full\" indicate using 5, 7, 12, and 32 full layers, respectively. \n\n\u2022 Serving a long-context LLM is also challenging due to high memory requirements for caching key and value vectors (KV cache). For example, a 7B Llama-2 model in BF16 precision uses 14 GB for model weights, while the KV cache for a single 128K sequence adds 69 GB -exceeding an H100 GPU's capacity. Recent works have sought to reduce KV cache memory overhead post-hoc by analyzing LLM attention patterns and leveraging these insights to create sparse attention mechanisms, usually without additional fine-tuning. \n\nDespite their promising performance in language modeling perplexity, these methods underperform on long-context retrieval and complex reasoning tasks ( \u00a72). This paper argues that integrating the two stages not only builds an efficient long-context LLM with improved long-context performance, but also reduces the cost of length extension training.",
            "score": 0.5513369568252967,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1643
                },
                {
                    "start": 1646,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1994
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.453857421875
        },
        {
            "corpus_id": "265034082",
            "title": "Ultra-Long Sequence Distributed Transformer",
            "text": "The LSS Transformer's sequence parallelism has three limitations. First, it still requires 2 global inter-GPU communications per attention layer, which degrades parallel efficiency at many GPUs. Second, while sequence parallelism tackles the long sequence issue, it does not address computation challenge for training large dataset. Three, sequence parallelism is only one source of parallelism. To scale to a large supercomputer for training, the LSS Transformer needs more sources of parallelism to achieve better scalability. To address these issues, this section introduces a method to integrate the LSS Transformer's sequence parallelism with data parallelism. With the integration, the parallel algorithm can (1) achieve better scalability; (2) simultaneously tackle long sequence and large dataset challenges; and (3) constrain the self-attention communications among local communicative groups for reduced overhead. \n\nDespite that sequence and data parallelisms are mostly orthogonal, one technical challenge to overcome is that both parallelisms require model parameter synchronization, but among GPUs in different communicative groups and communicate in different ways. Sequence parallelism requires model parameter synchronization among sequence parallel GPUs, but excludes positional embedding parameters from synchronization given that positional embeddings are distributed in sequence dimension. Data parallelism requires model parameter synchronization among data parallel GPUs, but must include positional embeddings given that data parallel GPUs have the same copy of the positional embedding parameters, but train them with different data batches. \n\nTo address this issue, we use an innovative double gradient averaging technique to avoid synchronization conflicts for positional embeddings. Fig. 3. illustrates an example of how the integrated sequence and data parallelism uses double gradient averaging. In this example, GPUs 1 and 2 process a sequence x 1 together using sequence parallelism, with the first segment x 1 1 assigned to GPU 1 and the second segment x 1 2 assigned to GPU 2. The positional embedding parameters are distributed in the same way with the first half P E 1 assigned to GPU 1 and the second half P E 2 assigned to GPU 2. Similarly, GPUs 3 and 4 handle a difference sequence x 2 using sequence parallelism.",
            "score": 0.550713477267798,
            "section_title": "INTEGRATED SEQUENCE & DATA PARALLELISM",
            "char_start_offset": 21806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1665
                },
                {
                    "start": 1668,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2351
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039703369140625
        },
        {
            "corpus_id": "261214487",
            "title": "Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers",
            "text": "However, the vast majority of these methods are difficult to apply to existing PLMs. Moreover, Xiong et al. (2022) proposes that many efficient-attention transformers do not even perform as well as simple local-attention models on downstream language tasks. \n\nChunking Methods for Long Sequence Another solution for long-sequence processing is to perform sequence chunking and then process them respectively (Zhong et al., 2022;Wang et al., 2023). Among chunking methods, SLED (Ivgi et al., 2023) splits the long sequence into overlapping chunks and processes each chunk with the encoder, then fuses cross-chunk information with the decoder. PageSum (Liu et al., 2022) separates the long sequence into non-overlapping chunks and effectively tackles them by the principle of locality (Denning, 2005). Unlimiformer (Bertsch et al., 2023) encodes long inputs in chunks and utilizes only the top-k input tokens for every attention head. \n\nLength Extrapolation Length extrapolation in transformers refers to their ability to handle input sequences of varying lengths during both train-ing and inference (Press et al., 2022;Sun et al., 2023). Transformers use a self-attention mechanism to capture dependencies across positions in a sequence, allowing them to generalize well to sequences of different lengths. This flexibility is essential for tasks like NLP and time series analysis, where input lengths can vary. \n\nSequence Length Reduction Reducing the length of hidden states (Guan et al., 2022;Kim et al., 2022) is the method of model compression from the width perspective, which is promising since some studies showed that there is redundant encoded information in token representations (Ethayarajh, 2019;Klafka and Ettinger, 2020). Among the redundancy, some tokens carry more task-specific information than others, suggesting that these tokens are more salient and imperative to be selected to feed into subsequent operations. Compared with model compression via layer-wise pruning, token-level pruning does not come at the expense of model performance in complex reasoning (Sanh et al., 2019;Sun et al., 2019b).",
            "score": 0.5501374872136898,
            "section_title": "Related Work",
            "char_start_offset": 14658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 257
                },
                {
                    "start": 260,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 932
                },
                {
                    "start": 935,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 95,
                    "end": 114,
                    "matchedPaperCorpusId": "245131261"
                },
                {
                    "start": 408,
                    "end": 428,
                    "matchedPaperCorpusId": "249062699"
                },
                {
                    "start": 477,
                    "end": 496,
                    "matchedPaperCorpusId": "251224058"
                },
                {
                    "start": 650,
                    "end": 668,
                    "matchedPaperCorpusId": "249063064"
                },
                {
                    "start": 1098,
                    "end": 1118,
                    "matchedPaperCorpusId": "237347130"
                },
                {
                    "start": 1118,
                    "end": 1135,
                    "matchedPaperCorpusId": "254877252"
                },
                {
                    "start": 1494,
                    "end": 1511,
                    "matchedPaperCorpusId": "258987259"
                },
                {
                    "start": 1707,
                    "end": 1733,
                    "matchedPaperCorpusId": "218502143"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09027099609375
        },
        {
            "corpus_id": "255966856",
            "title": "Transformer-based deep learning for predicting protein properties in the life sciences",
            "text": "The Transformer family of models has shown large improvements over RNNs and other DL-based models. In just a few years, they have been used for many different prediction tasks and their representations have been used with very promising results. In contrast, it took decades for conventional features based on MSAs to reach their current performances. The Transformer models have their own set of limitations, and future improvements in their architecture will likely give further boosts in their performance. \n\nFor instance, the standard attention mechanisms can only process fixed-length input sequences. For longer sequences, they need to be split into smaller fragments before being fed to a model. However, splitting a sequence up means context is being lost beyond the split boundary. Recent developments have attempted to overcome the fixed-length issue, where, for instance, some variants allow hidden states from previous fragments to be used as inputs for the current fragment (Elnaggar et al., 2020a;Dai et al., 2019). ProtT5-XL-UniRef50 model used in the section 'A proof-of-principle example' uses the same technique to pass information from one fragment to the other in the protein sequence. This allows a Transformer model to consider very long dependencies and at least in theory handle unlimited-length contexts since the information from one segment can be passed on to the next infinitely (Wang et al., 2019). Furthermore, some transformer models need the users to preprocess the sequences to adhere to a sequence length limit. This was apparent with the ESM-1b model in the 'A proof-of-principle example'. The workaround was to break the longer sequences into fragments (maximum lengths of 878 in this work) to get the Transformer representations, which was then concatenated to produce a representation for the entire sequence. That approach worked out as the best-performing features in this study out of the features compared. Fragmenting the sequence of course results in loss of some contexts, and future improvements to the sequence length limit can lead to more robust performances. \n\nThe attention mechanism, which is an integral part of Transformer models, also brings a limitation when it comes to long sequences. Since each token attends to every other token, the memory and computational complexity of the model increases quadratically in the attention layers with respect to the sequence length.",
            "score": 0.5500411154462675,
            "section_title": "Outlook",
            "char_start_offset": 54507,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2109
                },
                {
                    "start": 2112,
                    "end": 2243
                },
                {
                    "start": 2244,
                    "end": 2428
                }
            ],
            "ref_mentions": [
                {
                    "start": 987,
                    "end": 1011,
                    "matchedPaperCorpusId": "252617747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04779052734375
        },
        {
            "corpus_id": "247187613",
            "title": "Dynamic N:M Fine-Grained Structured Sparse Attention Mechanism",
            "text": "In this section, we demonstrate the speedup achieved by our method across different sequence lengths and compare it with existing studies. As our method focuses on the attention mechanism and is orthogonal to techniques (e.g. static pruning, quantization) that accelerate the other parts of transformer models, we only show the speedup achieved on the attention mechanism declared in Equation (1)   Appendix A.6. Our method achieves 1.08\u223c1.52\u00d7 end-to-end speedup and 1.41\u223c1.82\u00d7 memory footprint reduction. \n\nFor models in previous studies, We also apply the PyTorch JIT script when possible in case that their implementations are not efficient. The configuration we use is as follows: Each layer contains 4 heads, the feature dimension per head is 64. The batch size is set to be large enough to keep the GPU busy. We summarize the profiling results in Figure 5. We normalize the latency to the Transformer with full attention mechanism under each configuration. We also cut the y axis off at 2 for clarity, because some methods designed for long sequence could be more than 20\u00d7 slower than the dense transformer at moderate sequence length. \n\nFirst of all, our method achieves 1.27\u223c1.89x speedup over the transformer with full attention. It is the only method that brings consistent speedup across different sequence lengths, while other methods from previous papers suffer from high overhead at moderate and short sequence lengths. Second, under the float data type, our method achieves speedup in all the three stages with zero overhead. This accords with our arguments in Section 3. Under the data type bfloat16, the QK T in our method is a little bit slower than the dense baseline. The reason is that selecting 2 larger ones from 4 elements requires more comparisons, which results in more warp divergence.",
            "score": 0.5496138385112254,
            "section_title": "Speedup",
            "char_start_offset": 25494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1812
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004718780517578125
        },
        {
            "corpus_id": "270620100",
            "title": "DeciMamba: Exploring the Length Extrapolation Potential of Mamba",
            "text": "Lengthy sequences, which can span up to millions of tokens, are common in real-world applications including long books, high-resolution video and audio signals, and genomic data. Consequently, developing Deep Learning (DL) sequence models capable of effectively managing long contexts is a critical objective. Transformers (Vaswani et al., 2017), despite their current dominance in general DL tasks, still face challenges in processing long sequences. Specifically, their quadratic complexity in sequence length makes them computationally demanding, restricting the ability to train them over long sequences and very large datasets. \n\nIn recent years, substantial efforts have been made in order to tackle this challenge. The most significant advancements include efficient implementations that increase the model's context length during training (Dao et al., 2022;Liu et al.), and context-extension methods (Chen et al., 2023b;Peng et al., 2023b) designed to effectively expand the context after training. However, recent studies suggest that long-range processing is still an unresolved problem (Li et al., 2024a;Liu et al., 2024a). \n\nOne promising approach in this domain is the development of attention-free networks with subquadratic complexity, which can be trained more efficiently over long sequence data. In a recent line of works (Gu et al., 2021;Gu et al.;Gupta et al., 2022), the family of state-space layers has been introduced. These layers can be seen as theoretically grounded linear RNNs that can be efficiently computed in parallel via convolutions, thanks to a closed-form formulation of their linear recurrent rule. A recent advancement by Gu & Dao (2024) presented Mamba, which builds on top of an expressive variant of SSMs called Selective State-Space Layers (S6).",
            "score": 0.5494056967868223,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1787
                }
            ],
            "ref_mentions": [
                {
                    "start": 847,
                    "end": 865,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1115,
                    "end": 1133,
                    "matchedPaperCorpusId": "259360665"
                },
                {
                    "start": 1367,
                    "end": 1386,
                    "matchedPaperCorpusId": "247762199"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1417236328125
        },
        {
            "corpus_id": "275134413",
            "title": "TokenRing: An Efficient Parallelism Framework for Infinite-Context LLMs via Bidirectional Communication",
            "text": "Large Language Models (LLMs), such as GPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023a;b;Dubey et al., 2024) have emerged as a crucial part of artificial intelligence, showing outstanding performance in numerous applications like natural language processing, question answering, and code generation. As these models keep increasing in complexity and scale, there is a growing necessity for advanced parallelization techniques to enhance their training and inference efficiency. Among the key components of LLMs, the attention block (Vaswani, 2017) plays a vital role yet also brings about significant communication bottlenecks, especially when dealing with long-context sequences. \n\nEfficient parallelization methods for LLMs have been a major focus of research, with various strategies explored, each with its own strengths and limitations. Approaches such as data parallelism (Li et al., 2014), tensor parallelism (Dean et al., 2012), and pipeline parallelism (Huang et al., 2019) have been extensively studied. In data parallelism (DP), each worker maintains a complete copy of the model parameters, with training samples distributed uniquely across workers. Model computations are performed independently on each worker. However, as model size increases, DP introduces significant memory and communication overhead due to the need for parameter replication and synchronization during every iteration. Pipeline parallelism (PP) divides the model into multiple stages distributed across GPUs to improve throughput. However, it can result in pipeline stalls and idle time if the workload distribution is uneven, decreasing overall efficiency. In tensor parallelism (TP), individual operators of a model are partitioned across multiple workers. Each worker stores a portion of the operator's parameters and performs part of the computation, such as processing a tile of a matrix. While this approach reduces memory requirements per worker, it introduces complex communication patterns and incurs additional memory overhead for tensor redistribution. Sequence Parallelism (SP), a technique that involves partitioning a single input sequence, has emerged as a particularly promising approach for training or inference tasks involving long-context sequences. Notable",
            "score": 0.5490390238890355,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2264
                },
                {
                    "start": 2265,
                    "end": 2272
                }
            ],
            "ref_mentions": [
                {
                    "start": 42,
                    "end": 62,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 541,
                    "end": 556,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 887,
                    "end": 904,
                    "matchedPaperCorpusId": "4614646"
                },
                {
                    "start": 925,
                    "end": 944,
                    "matchedPaperCorpusId": "372467"
                },
                {
                    "start": 971,
                    "end": 991,
                    "matchedPaperCorpusId": "53670168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08709716796875
        },
        {
            "corpus_id": "273228958",
            "title": "Falcon Mamba: The First Competitive Attention-free 7B Language Model",
            "text": "The attention mechanism is inherently limited in processing long sequences due to the increasing compute and memory costs as sequence length grows. Leveraging the theoretical efficiency of SSM models in handling large sequences (Gu & Dao, 2023), Falcon-Mamba-7B demonstrates that these scaling limitations can indeed be overcome without compromising performance. \n\nSetup To replicate real-world use cases, we compared the memory usage and generation throughput of Falcon-Mamba-7B with popular Transformer-based models of a similar scale, including Llama3.1-8B (Dubey et al., 2024), Mistral-7B (Jiang et al., 2023), and Qwen2-7B (Yang et al., 2024). All evaluations were conducted using the Hugging Face transformers library (Wolf et al., 2020). For a fair comparison, we rescaled the vocabulary size of all transformer models to match Falcon-Mamba-7B, since it has a big impact on the memory footprint of the model. \n\nParallel Prefill and Sequential Prefill Before diving into the results, it is important to clarify the difference between the prompt (prefill) and generated (decode) parts of a sequence. For state space models (SSMs), the prefill process is more critical than for transformer models. When a transformer generates the next token, it must attend to the keys and values of all previous tokens in the context, resulting in both memory and generation time scaling linearly with context length. In contrast, an SSM only stores and attends to its recurrent state, which avoids the need for additional memory or time when generating large sequences. While this demonstrates the efficiency of SSMs during the decoding phase, the prefill phase requires additional framework optimizations to fully leverage the SSM architecture. \n\nThe standard method for prefill is processing the entire prompt in parallel, maximizing GPU utilization, referred to here as Parallel Prefill. This is the approach used in most frameworks like Optimum-Benchmark4 . In this approach, the memory usage grows with prompt length due to the need to store hidden states for each token.",
            "score": 0.5489936894864456,
            "section_title": "Throughput and memory consumption",
            "char_start_offset": 15002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 362
                },
                {
                    "start": 365,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1735
                },
                {
                    "start": 1738,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2066
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0278472900390625
        },
        {
            "corpus_id": "229923177",
            "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
            "text": "Transformers (Vaswani et al., 2017) have achieved remarkable improvements in a wide range of natural language tasks, including language modeling , text classification , and question answering (Devlin et al., 2018;. This success is largely due to the self-attention mechanism, which enables the network to capture contextual information from the *indicates equal contribution. 1 Source code and pre-trained checkpoints can be found at https://github.com/PaddlePaddle/ERNIE/ tree/repro/ernie-doc. entire input sequence. Nevertheless, the memory usage and computation complexity caused by the self-attention mechanism grows quadratically with the sequence length, incurring excessive cost when processing a long document on existing hardware. Currently, the most prominent pretrained models, such as BERT (Devlin et al., 2018), are used on fixed-length input segments of a maximum of 512 tokens owing to the aforementioned limitation. Thus, a long document input must be partitioned into smaller segments of manageable sizes. However, this leads to the loss of important crosssegment information, that is, the context fragmentation problem , as shown in Fig. 1(a). To mitigate the problem of insufficient interactions among the partitioned segments of long documents, Recurrence Transformers Rae et al., 2019) permit the use of contextual information from previous segments in computing the hidden states for a new segment by maintaining a memory component from the previous activation; this enables the modeling of long documents. In addition, Sparse Attention Transformers Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020) focus on reducing the complexity of self-attention operations to explicitly improve the modeling length, but only up to a restricted context length (4,096) due to resource limitations.\n\nWe argue that existing strategies are not sufficiently effective or reliable, because the contextual information of a complete document is still not available for each segment during the training phase. As depicted in Fig. 1, when training on segment S 2 , the model is ideally optimized by maximizing P (y | (S 1 , S 2 , S 3 )) conditioned on the contextual information of the entire document D = {S 1 , S 2 , S 3 }, in contrast to the following sub",
            "score": 0.5487270655538745,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.212890625
        },
        {
            "corpus_id": "220831004",
            "title": "Big Bird: Transformers for Longer Sequences",
            "text": "Models based on Transformers [92], such as BERT [23,64], are wildly successful for a wide variety of Natural Language Processing (NLP) tasks and consequently are mainstay of modern NLP research. Their versatility and robustness are the primary drivers behind the wide-scale adoption of Transformers. The model is easily adapted for a diverse range of sequence based tasks -as a seq2seq model for translation [92], summarization [67], generation [16], etc. or as a standalone encoders for sentiment analysis [84], POS tagging [66], machine reading comprehension [94], etc. -and it is known to vastly outperform previous sequence models like LSTM [38]. The key innovation in Transformers is the introduction of a self-attention mechanism, which can be evaluated in parallel for each token of the input sequence, eliminating the sequential dependency in recurrent neural networks, like LSTM. This parallelism enables Transformers to leverage the full power of modern SIMD hardware accelerators like GPUs/TPUs, thereby facilitating training of NLP models on datasets of unprecedented size. This ability to train on large scale data has led to surfacing of models like BERT [23] and T5 [76], which pretrain transformers on large general purpose corpora and transfer the knowledge to down-stream task. The pretraining has led to significant improvement in low data downstream tasks [52] as well as tasks with sufficient data [102] and thus have been a major force behind the ubiquity of transformers in contemporary NLP.\n\nThe self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. Using commonly available current hardware and model sizes, this requirement translates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like question answering [61], document summarization [21], etc.\n\nHowever, while we know that self-attention and Transformers are useful,",
            "score": 0.548485763254187,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 645,
                    "end": 649,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1419,
                    "end": 1424,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07965087890625
        },
        {
            "corpus_id": "277768614",
            "title": "Sequence Accumulation and Beyond: Infinite Context Length on Single GPU and Large Clusters",
            "text": "Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, have recently been recognized as potential alternatives to softmax attention thanks to their linear complexity and competitive performance. However, although their linear-memory advantage during training enables dealing with long sequences, it is still hard to handle extremely long sequences with very limited computational resources. In this paper, we propose Sequence Accumulation (SA) which leverages the common recurrence feature of linear sequence modeling methods to manage infinite context length even on a single GPU. Specifically, SA divides long input sequences into fixed-length sub-sequences and accumulates intermediate states sequentially, which reaches only constant-memory consumption. Additionally, we further propose Sequence Accumulation with Pipeline Parallelism (SAPP), to train large models with infinite context length, without incurring any additional synchronization costs in the sequence dimension. Extensive experiments with a wide range of context lengths are conducted to validate the effectiveness of SA and SAPP on both single and multiple GPUs. Results show that SA and SAPP enable the training of infinite context length on even very limited resources, and are well compatible with the out-of-the-box distributed training techniques.",
            "score": 0.5481572171196492,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06500244140625
        },
        {
            "corpus_id": "270370789",
            "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
            "text": "Large Language Models (LLMs) (Radford et al., 2018;2019;Brown et al., 2020) excel across a variety of linguistic tasks, including text generation (Goyal & Durrett, 2020;Yuan et al., 2022), program synthesis (Chen et al., 2021;Li et al., 2022), question answering (Kamalloo et al., 2023), and mathematical problem-solving (Lewkowycz et al., 2022). These tasks typically involve processing extensive sequences, often requiring the analysis of thousands of tokens to derive outcomes based on comprehensive contextual information. For example, the task of summarizing extensive government reports, as seen in the GovReport section of SCROLLS (Shaham et al., 2022), demands that LLMs efficiently sift through and distill key information from vast textual data, highlighting the need for models capable of handling long token sequences effectively. \n\nYet, transformers (Vaswani et al., 2017) struggle to process extensive token sequences due to their quadratic memory demands, which exceed the capacity of contemporary hardware. Attention computations are performed in blocks (Dai et al., 2019), with key and value states cached for subsequent encoding or decoding steps to mitigate this. However, this approach results in a Key-Value (KV) cache size that increases linearly with context length, quickly depleting GPU memory (Zhang et al., 2023b). Recently, StreamingLLM (Xiao et al., 2023) attempted to reduce KV cache size by limiting each token's receptive field and incorporating \"attention sinks\". Concurrently, H 2 O (Zhang et al., 2023b) prunes tokens based on lower accumulated attention scores to stabilize KV cache size. Despite these efforts, both methods fail to leverage full-sequence information and adequately extend the context window. StreamingLLM's exclusion of all tokens in the context middle could significantly impair the model's ability to utilize the full long context (even completely ignore), known as \"lost in the middle\" (Liu et al., 2023), while H 2 O struggles to extrapolate to longer sequences than the training context length (Han et al., 2023).",
            "score": 0.5465922136023624,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 56,
                    "end": 75,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 169,
                    "end": 187,
                    "matchedPaperCorpusId": "247585187"
                },
                {
                    "start": 226,
                    "end": 242,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 321,
                    "end": 345,
                    "matchedPaperCorpusId": "250144408"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1998291015625
        },
        {
            "corpus_id": "277510128",
            "title": "Cognitive Memory in Large Language Models",
            "text": "MEGABYTE divides the sequence into patches and employs local submodels within the patches and a global model between the patches. This approach enables subquadratic self-attention, larger feed-forward layers, and improved parallelism during decoding, thereby achieving better performance at a lower cost during both training and generation. Xie et al. [2023] introduces a simple method for handling long sequences that can be applied in Transformer models. Traditional Transformer models face limitations in computational complexity when dealing with long sequences. However, the method proposed in this paper can effectively handle long sequences while maintaining good performance on short sequences. Specifically, the paper proposes a method of dividing long inputs into manageable-length blocks and selecting the most representative tokens for decoding. In addition, the paper also introduces a selector based on policy learning to optimize the compression process of long sequence information. Ye et al. [2024] mainly introduces an attention module called ChunkAttention, which employs prefixbased KV caching and a two-stage partitioning method to improve the efficiency of self-attention. Specifically, the paper proposes using a prefix tree to implement KV caching, which can dynamically detect and remove redundant KV caches to reduce memory usage. Meanwhile, the paper also describes a two-stage self-attention computation method, consisting of chunk-first and sequence-first stages. An et al. [2024] introduces a novel training-free framework called Dual Chunk Attention (DCA) for extending the context window of large language models (LLMs). The paper first describes the three components of DCA: intra-chunk attention, inter-chunk attention, and successive-chunk attention. These attention mechanisms help the model effectively capture both long-range and short-range dependencies when processing long sequences. \n\nIn Ivgi et al. [2023], the authors propose SLED: Sliding Encoder and Decoder, a simple method for handling long sequences that leverages battle-tested pre-trained LMs for short texts. Specifically, the authors divide the input into overlapping chunks, encode each chunk using a short-text LM encoder, and fuse information between chunks using a pre-trained decoder (fusion in the decoder).",
            "score": 0.5456681928876231,
            "section_title": "MoE For Memory Parameterization",
            "char_start_offset": 106535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1924
                },
                {
                    "start": 1927,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2316
                }
            ],
            "ref_mentions": [
                {
                    "start": 1930,
                    "end": 1948,
                    "matchedPaperCorpusId": "251224058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1114501953125
        },
        {
            "corpus_id": "270703226",
            "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
            "text": "Transformer models [72] have been considered as a de facto backbone of modeling arbitrary sequences, pretraining foundation models [8,21], and more recently, constructing large language models (LLMs) [9,69].Despite the inspiring success of their wide applications on both Natural Language Processing (NLP) and Machine Learning (ML) downstream tasks, extending the context window size to long sequences with computation and memory efficiently poses significant challenges [1,20,19], owing to the quadratic computation complexity and large amounts of key/value vectors associated with self-attention, especially on resource-constrained devices.\n\nMany recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths.However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs.Clustering-based methods [39,61] allow queries to attend to different sets of KV pairs.In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81,42,78].(ii) Previous learnable sparse attention often has super-linear complexity, especially during training.For example, clustering-based methods usually cost O(n log n) to maintain clusters.Ainslie et al. [1]  KV pairs are scored by u.SPARSEK computes a threshold for each query (\u03c4 (u)) such that the sum of normalized scores is k, which is 3 in this example.We select top-k KV pairs (orange cells) to perform attention.Right: the SPARSEK attention module.We fuse selection and attention in one kernel for efficiency.incorporates a SOFTTOPK operator [41] to compute soft masks in Transformer encoders.Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SOFTTOPK for variable-length context associated with different queries requires quadratic time in total.",
            "score": 0.5441745271623469,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 207,
                    "end": 642
                },
                {
                    "start": 644,
                    "end": 777
                },
                {
                    "start": 777,
                    "end": 1008
                },
                {
                    "start": 1008,
                    "end": 1095
                },
                {
                    "start": 1095,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1408
                },
                {
                    "start": 1408,
                    "end": 1491
                },
                {
                    "start": 1491,
                    "end": 1536
                },
                {
                    "start": 1536,
                    "end": 1660
                },
                {
                    "start": 1660,
                    "end": 1721
                },
                {
                    "start": 1721,
                    "end": 1757
                },
                {
                    "start": 1757,
                    "end": 1818
                },
                {
                    "start": 1818,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 23,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 200,
                    "end": 203,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "257622671"
                },
                {
                    "start": 1037,
                    "end": 1040,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "264439578"
                },
                {
                    "start": 1506,
                    "end": 1509,
                    "matchedPaperCorpusId": "257622671"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0999755859375
        },
        {
            "corpus_id": "268384915",
            "title": "BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences",
            "text": "Transformers (Vaswani et al., 2017) have emerged as the dominant architectures for large language models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022) due to their remarkable capacities to understand complex text and generate controllable responses.Empirically, the power of Transformers lies largely in their multi-head attention modules, which enable Transformers to capture rich semantic information from textual contexts effectively.For every plus, there is a minus.Despite the success of Transformers' attention modules, these modules exhibit quadratic time and memory complexity concerning sequence length, posing challenges in terms of both computing time and memory overheads as sequence length increases.\n\nVarious efforts have been devoted to making attention modules more efficient and enabling LLMs to process longer sequences.One direction is taking full advantage of a single device's compute and storage units (e.g., a GPU) to process long sequences, such as FlashAttention (Dao et al., 2022).FlashAttention can significantly accelerate the computation of attention modules by using more efficient static random access memory (SRAM) instead of high-bandwidth memory (HBM) in devices to store intermediate attention states.Another direction is using distributed clusters containing multiple devices (e.g., multiple GPUs) to process long sequences, such as RingAttention (Li et al., 2021).RingAttention divides sequences into multiple subsequences and processes subsequences separately on different devices.\n\nAll the above improvements orienting to speedup attention operation have achieved promising results, each targeting different bottleneck.However an intuitive problem is raised -whether we can combine these improvements to achieve a more efficient attention solution.The concept is straightforward, yet in a distributed setting, simple combination of two methods may not benefit from their strength.Moreover , the RingAttention approach cannot directly incorporate with online softmax, and the FlashAttention implementation focuses exclusively on optimizing the computation of attention on a single device.To address these challenges,this paper introduces an efficient distributed attention framework to handle extremely long sequences named \"BurstAttention\".BurstAttention can take full advantage of the",
            "score": 0.5433192484390434,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 475
                },
                {
                    "start": 475,
                    "end": 718
                },
                {
                    "start": 720,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 1012
                },
                {
                    "start": 1012,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1524
                },
                {
                    "start": 1526,
                    "end": 1663
                },
                {
                    "start": 1663,
                    "end": 1792
                },
                {
                    "start": 1792,
                    "end": 1924
                },
                {
                    "start": 1924,
                    "end": 2131
                },
                {
                    "start": 2131,
                    "end": 2284
                },
                {
                    "start": 2284,
                    "end": 2329
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 35,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 112,
                    "end": 132,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 993,
                    "end": 1011,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047149658203125
        },
        {
            "corpus_id": "271212416",
            "title": "SOFA: A Compute-Memory Optimized Sparsity Accelerator via Cross-Stage Coordinated Tiling",
            "text": "Remarkable success has been witnessed recently in the development of Transformer architecture [1], for both natural language processing (NLP) [2]- [10] and computer vision (CV) tasks [11]- [19].The impressive capabilities of Transformers greatly stems from their self-attention module, which excels at extracting global context information [20].Typically, selfattention modules take three matrices as their inputs: namely, Q (query), K (key) and V (value).First, an attention matrix A\u2208 R S\u00d7S is obtained by multiplying Q and K, where S is sequence length.Next, A goes through the softmax function for normalization, then is multiplied by V for the final output.\n\nLarge language models (LLMs) have driven the transformer architecture to unprecedented levels of complexity and capability, particularly in handling extended sequence lengths [21].This evolution places heightened demands on inference capabilities and throughput [22], critically impacting the performance of key transformer components: the attention module, feed-forward network (FFN) module, and the querykey-value (QKV) computations.\n\nTraditionally, in Transformers designed for smaller sequence lengths(\u22642k), the FFN module typically presented the main bottleneck due to its dense computational requirements.However, with recent advancements in processing long text, where sequence lengths can exceed 128,000 characters [23]- [25], the performance bottleneck is shifting from the FFN to the attention module.Our detailed profiling indicates that as sequence lengths surpass 32,000 characters, the attention module becomes the dominant factor affecting inference time, as shown in Fig. 1.This shift is primarily because the complexity of the attention mechanism scales quadratically with sequence length, making it increasingly challenging to manage as sequences extend.\n\nDynamic sparsity (DS) acceleration [26]- [33] have emerged as a promising solution to mitigate the latency issue of selfattention.The key idea is to predict vital Q-K pairs at runtime and calculate attention based on these vital pairs to reduce the inference latency.Typically, it consists of three stages.A precompute stage firstly estimates the matrix A (denoted as \u00c2).",
            "score": 0.5433035613062832,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 456
                },
                {
                    "start": 456,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 661
                },
                {
                    "start": 663,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 1098
                },
                {
                    "start": 1100,
                    "end": 1274
                },
                {
                    "start": 1274,
                    "end": 1474
                },
                {
                    "start": 1474,
                    "end": 1653
                },
                {
                    "start": 1653,
                    "end": 1835
                },
                {
                    "start": 1837,
                    "end": 1967
                },
                {
                    "start": 1967,
                    "end": 2104
                },
                {
                    "start": 2104,
                    "end": 2143
                },
                {
                    "start": 2143,
                    "end": 2208
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 340,
                    "end": 344,
                    "matchedPaperCorpusId": "212996548"
                },
                {
                    "start": 1872,
                    "end": 1876,
                    "matchedPaperCorpusId": "211296403"
                },
                {
                    "start": 1878,
                    "end": 1882,
                    "matchedPaperCorpusId": "239016160"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06268310546875
        },
        {
            "corpus_id": "263608461",
            "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
            "text": "Given input sequences Q, K, V \u2208 R s\u00d7d where s is the sequence length and d is the head dimension. We compute the matrix of outputs as: \n\nwhere softmax is applied row-wise. Each self-attention sub-layer is accompanied with a feedforward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. \n\nBlockwise Parallel Transformers. Prior state-of-the-arts have led to substantial reductions in memory utilization, achieved through innovative techniques that enable attention computation without full materialization by computing attention in a block by block manner [30,9,23]. These advancements lowered the memory overhead of attention to 2bsh bytes per layer, where b represents the batch size, s denotes the sequence length, and h stands for the hidden size of the model. To further reduce memory usage, blockwise parallel transformer (BPT) [23] introduced a strategy where the feedforward network associated with each self-attention sub-layer is computed in a block-wise fashion. This approach effectively limits the maximum activation size of feedforward network from 8bsh to 2bsh. \n\nFor a more detailed analysis of memory efficiency, please refer to the discussion provided therein. In summary, the state-of-the-art transformer layer's memory cost of activation is 2bsh. \n\nLarge Output of Each Layer. While BPT significantly reduces memory demand in Transformers, it still presents a major challenge for scaling up context length because it requires storing the output of each layer. This storage is crucial due to the inherent nature of self-attention, which involves interactions among all elements (n to n interactions). Without these stored outputs, the subsequent layer's self-attention becomes computationally impractical, necessitating recomputation for each sequence element. To put it simply, processing 100 million tokens with a batch size of 1 requires over 1000GB of memory even for a modest model with a hidden size of 1024. In contrast, modern GPUs and TPUs typically provide less than 100GB of high-bandwidth memory (HBM), and the prospects for significant HBM expansion are hindered by physical limitations and high manufacturing costs.",
            "score": 0.5426473703928293,
            "section_title": "Large Context Memory Constraint",
            "char_start_offset": 5839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 134
                },
                {
                    "start": 137,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 659,
                    "end": 661,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 661,
                    "end": 664,
                    "matchedPaperCorpusId": "258987968"
                },
                {
                    "start": 933,
                    "end": 937,
                    "matchedPaperCorpusId": "258987968"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0289764404296875
        },
        {
            "corpus_id": "57759363",
            "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
            "text": "In order to apply Transformer or self-attention to language modeling, the central problem is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation. Given infinite memory and computation, a simple solution would be to process the entire context sequence using an unconditional Transformer decoder, similar to a feed-forward neural network. However, this is usually infeasible with the limited resource in practice. One feasible but crude approximation is to split the entire corpus into shorter segments of manageable sizes, and only train the model within each segment, ignoring all contextual information from previous segments. This is the idea adopted by Al-Rfou et al. (2018). We call it the vanilla model and visualize it in Fig. 1a. Under this training paradigm, information never flows across segments in either the forward or backward pass. There are two critical limitations of using a fixed-length context. First, the largest possible dependency length is upper bounded by the segment length, which is a few hundred on character-level language modeling (Al-Rfou et al., 2018). Therefore, although the self-attention mechanism is less affected by the vanishing gradient problem compared to RNNs, the vanilla model is not able to fully exploit this optimization advantage. Second, though it is possible to use padding to respect the sentence or other semantic boundaries, in practice it has been standard practice to simply chunk long text into fixed-length segments due to improved efficiency (Peters et al., 2018;Devlin et al., 2018;Al-Rfou et al., 2018). However, simply chunking a sequence into fixed-length segments will lead to the context fragmentation problem as discussed in Section 1. \n\nDuring evaluation, at each step, the vanilla model also consumes a segment of the same length as in training, but only makes one prediction at the last position. Then, at the next step, the segment is shifted to the right by only one position, and the new segment has to be processed all from scratch. As shown in Fig. 1b, this procedure ensures that each prediction utilizes the longest possible context exposed during training, and also relieves context fragmentation issue encountered in training. However, this evaluation procedure is extremely expensive. We will show that our proposed architecture is able to substantially improve the evaluation speed.",
            "score": 0.5420926891491723,
            "section_title": "VANILLA TRANSFORMER LANGUAGE MODELS",
            "char_start_offset": 7224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2261
                },
                {
                    "start": 2262,
                    "end": 2320
                },
                {
                    "start": 2321,
                    "end": 2419
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2314453125
        },
        {
            "corpus_id": "236459883",
            "title": "Multi-Head Highly Parallelized LSTM Decoder for Neural Machine Translation",
            "text": "The Transformer translation model (Vaswani et al., 2017) has achieved great success and is used extensively in the NLP community. It achieves outstanding performance compared to previous RNN/CNN based translation models (Bahdanau et al., 2015;Gehring et al., 2017) while being much faster to train. \n\nThe Transformer can be trained efficiently due to the highly parallelized self-attention network. It enables sequence-level parallelization in context modelling, as all token representations can be computed in parallel, and linear transformations are only required to compute the sequence once. On the other hand, previous RNN-based methods process a sequence in a token-by-token manner, which means that they have to compute linear layers once for each token, i.e. n times if the number of tokens in the sequence is n. \n\nHowever, the complexity of a self-attention network which compares each token with all the other tokens is O(n 2 ), while for LSTM (Hochreiter and Schmidhuber, 1997) it is only O(n). In practice, however, LSTM is slower than the self-attention network in training. This is mainly due to the fact that the computation of its current step relies on the computation output of the previous step, which prevents efficient parallelization over the sequence. As for the performance of using recurrent models in machine translation, Chen et al. (2018) shows that an LSTM-based decoder can further improve the performance over the Transformer. \n\nIn this paper, we investigate how we can efficiently parallelize all linear transformations of an LSTM at the sequence level, i.e. compute its linear transformations only once with a given input sequence. Given that linear transformations are implemented by matrix multiplication, compared to the other element-wise operations, we suggest that they take the largest part of the model's overall computation, and parallelizing the linear transformations at sequence level may significantly accelerate the training of LSTM-based models.",
            "score": 0.542026195660961,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 298
                },
                {
                    "start": 301,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1457
                },
                {
                    "start": 1460,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 56,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 220,
                    "end": 243,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 243,
                    "end": 264,
                    "matchedPaperCorpusId": "3648736"
                },
                {
                    "start": 954,
                    "end": 988,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1348,
                    "end": 1366,
                    "matchedPaperCorpusId": "13747425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034423828125
        },
        {
            "corpus_id": "276106883",
            "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
            "text": "Transformers have emerged as one of the most successful AI/ML techniques for modeling sequence data. They have drastically impacted a variety of scientific and engineering applications such as language modeling [1], molecular design [2], cancer pathology classification [3], genomic sequence modeling [4], and others. These models derive their power from an operation called the attention mechanism. Given a sequence of tokens of length L, the attention mechanism extracts pairwise similarities between the tokens [5]. \n\nHowever, the time and memory complexity of the attention mechanism is quadratic in L as it captures interactions between each pair of the sequence. This has severely limited the context length, i.e., the length of a sequence over which a transformer model can capture these interactions (also know as sequence length in the literature). For applications such as genomics, at least 4-5 orders of magnitude of increase in context length is needed [4]. \n\nA popular approach for reducing the computational complexity is to introduce sparsity in the attention mechanism [6]- [8]. In this approach, an L \u00d7 L 0-1 attention mask is used to capture the interactions between pairs. The mask is made sparse by reducing the number of interacting pairs (making the corresponding entries 0) in a structured manner. Local, dilated windowed, block, and random are popular patterns that are used to introduce sparsity while ensuring that critical interactions are preserved to maintain the accuracy of the models. [7] has demonstrated that, even with an exponentially decreasing number of connections with respect to the distance between pairs, the test perplexity scores remain comparable to dense models. This demonstrates that utilizing sparsity to achieve ultra-long sequence modeling is a feasible approach. \n\nHowever, existing implementations of sparse attention mechanisms are not \"hardware-aware\". They perform densedense matrix multiplication followed by invalidation of entries based on the attention mask [9], [10]. Optimizations have focused on partitioning the matrices into blocks, applying matrix reordering techniques to obtain denser blocks, and performing block matrix multiplication only on the blocks that have nonzero entries in the mask (see Section III for details). However, these techniques end up performing excess computations as the block may contain several 0 mask elements.",
            "score": 0.5418808000453351,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1816
                },
                {
                    "start": 1819,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2293
                },
                {
                    "start": 2294,
                    "end": 2407
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 236,
                    "matchedPaperCorpusId": "268423149"
                },
                {
                    "start": 270,
                    "end": 273,
                    "matchedPaperCorpusId": "268040015"
                },
                {
                    "start": 301,
                    "end": 304,
                    "matchedPaperCorpusId": "259274952"
                },
                {
                    "start": 514,
                    "end": 517,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 966,
                    "end": 969,
                    "matchedPaperCorpusId": "259274952"
                },
                {
                    "start": 1086,
                    "end": 1089,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.163818359375
        },
        {
            "corpus_id": "273403476",
            "title": "Flash Inference: Near Linear Time Inference for Long Convolution Sequence Models and Beyond",
            "text": "A lot of recent progress in deep learning, particularly in the form of large language models (LLMs) has been driven by the transformer architecture [Vaswani et al., 2017]. While these models have great quality, it comes at a computation cost which scales quadratically in sequence length -both during training and inference. This can become prohibitive for very long contexts and as such a number of alternative architectures with better computational scaling in context length have been proposed [Gu and Dao, 2023, Poli et al., 2023, Fu et al., 2024]. While most of these works have improved computational efficiency for training, some still scale quadratically in sequence length when it comes to inference, thus not improving asymptotically over transformers. \n\nIn this work, we propose a framework for optimizing inference efficiency for a general class of such models. As a case study, which inspired the method, we focus on long convolution sequence models (LCSMs) [Poli et al., 2023, Fu et al., 2022, Romero et al., 2021, Li et al., 2022, Karami and Ghodsi, 2024, Fu et al., 2023a]. However, our approach is not limited to LCSMs alone and we identify the properties that allow for such inference speedups in hope to guide the design of future architectures. \n\nIn the particular case of LCSMs (including Hyena), the building block of the architecture is that of convolving the input sequence with a sequence-length long, (potentially underparameterized) filter. If we let L be the sequence length (e.g. number of tokens in the case of LLMs), then a naive implementation of convolution during training would take \u2126(L 2 ) FLOPs, but one can employ FFT to bring that down to O(L log L). The issue that occurs during inference is that FFT cannot be used directly since the whole input sequence is not known ahead of time, but rather incrementally computed. Because of this, the naive inference approach goes up to \u2126(L 2 ) -this is the apparent cost of moving from a static input to a dynamic one.",
            "score": 0.5409225494221852,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1998
                }
            ],
            "ref_mentions": [
                {
                    "start": 1069,
                    "end": 1088,
                    "matchedPaperCorpusId": "256846960"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039642333984375
        },
        {
            "corpus_id": "259187506",
            "title": "Block-State Transformers",
            "text": "Vanilla transformers can be unstable when trained on long sequences [26] and token importance is concentrated in a local receptive field of around 50 tokens around the current time step [35]. \n\nAn emerging body of research suggests that State Space Models (SSMs) can serve as an alternative to Transformers because they are able to capture dependencies in extremely long sequences, while being more computationally efficient and parallelizable [14]. While still falling into the category of autoregressive sequence models, the underlying linear time-invariant dynamical system of SSMs allows the efficient processing of sequences using parallelizable convolution operators with the Fast Fourier Transform (FFT) [7], with O(L log L) complexity, where L is the length of the sequence. Moreover, retention of past information over long sequences, up to thousands of steps, can be ensured by deriving recurrent update rules by borrowing ideas from online function approximation [3,12]. SSMs have recently outperformed Transformers on long-range dependency benchmarks by a large margin [37]. Despite their success on long-range classification tasks, SSMs have not yet completely matched Transformers as an off-the-shelf sequence model for general language modeling tasks [10]. \n\nRecent findings suggest that Transformers and SSMs are complementary models for the purpose of language modeling [28]. In this work, we propose an architecture that integrates a strong local attention-based inductive bias with the long-term context modeling abilities of SSMs into a single layer, that we call Block-State Transformer (BST). Our model is able to process long input sequences, while still incorporating an attention mechanism to predict next tokens. BST is fully parallelizable, scales to much longer sequences, and offers a 10\u00d7 speedup compared to comparable Transformerbased layers. \n\nIn every BST layer, an SSM takes the entire sequence as input and maps it into a \"context\" sequence of the same length. The SSM sublayer takes advantage of FFT-based convolutions.",
            "score": 0.5402132656164299,
            "section_title": "Introduction",
            "char_start_offset": 2061,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1271
                },
                {
                    "start": 1274,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1873
                },
                {
                    "start": 1876,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2055
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 72,
                    "matchedPaperCorpusId": "252917575"
                },
                {
                    "start": 711,
                    "end": 714,
                    "matchedPaperCorpusId": "121744946"
                },
                {
                    "start": 1387,
                    "end": 1391,
                    "matchedPaperCorpusId": "250089125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.252685546875
        },
        {
            "corpus_id": "274437066",
            "title": "FlexSP: Accelerating Large Language Model Training via Flexible Sequence Parallelism",
            "text": "Extending the context length (i.e., the maximum supported sequence length) of LLMs is of paramount significance. To facilitate long context training of LLMs, sequence parallelism has emerged as an essential technique, which scatters each input sequence across multiple devices and necessitates communication to process the sequence. In essence, existing sequence parallelism methods assume homogeneous sequence lengths (i.e., all input sequences are equal in length) and therefore leverages a single, static scattering strategy for all input sequences. However, in reality, the sequence lengths in LLM training corpora exhibit substantial variability, often following a long-tail distribution, which leads to workload heterogeneity. In this paper, we show that employing a single, static strategy results in inefficiency and resource under-utilization, highlighting the need for adaptive approaches to handle the heterogeneous workloads across sequences. To address this, we propose a heterogeneity-adaptive sequence parallelism method. For each training step, our approach captures the variability in sequence lengths and assigns the optimal combination of scattering strategies based on workload characteristics. We model this problem as a linear programming optimization and design an efficient and effective solver to find the optimal solution. Furthermore, we implement our method in a high-performance system that supports adaptive parallelization in distributed LLM training. Experimental results demonstrate that our system outperforms state-of-the-art training frameworks by up to 1.98x. Source code is available at https://github.com/PKU-DAIR/Hetu-Galvatron.",
            "score": 0.5398082048980006,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10614013671875
        },
        {
            "corpus_id": "265308945",
            "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey",
            "text": "Additionally, the lack of a robust and generalizable mechanism for positional embeddings (PEs) leads to performance degradation and fluctuation during inference, particularly with longer sequences or position shifting on relevant information [139]. \n\nWith LLMs deeply ingrained in various applications that require long-context comprehension [114,248] and generation [89,142], the demand for long-context LLMs capable of comprehending and generating extremely long sequences effectively and efficiently becomes increasingly indispensable and urgent. Consequently, researchers have devoted significant efforts to enhancing the Transformer architecture to address the long-context problem in LLMs, including optimization on the efficiency of attention (Section 3), context window extension with extra memory mechanisms (Section 4), effective length generalization with extrapolative PEs (Section 5), context pre/postprocessing (Section 6), and other miscellaneous methods (Section 7) such as specific pretraining objectives, mixture of experts (MoE), quantization, parallelism, etc. \n\nExisting surveys. The field of long-context LLMs has become one of the most rapidly developing research areas on LLMs recently, with some existing surveys [65,112,137,216,270]. [112] offers an overview of long document summarization, but does not delve into techniques of long text modeling. [216] and [137] primarily concentrate on improving the computational efficiency of Transformers in long-text scenarios. Although [270] underscores the challenges LLMs face when engaging with extensive sequences, its discussed methods predominantly align with efficient Transformers, similar to [216] and [137]. A more recent survey [65] bears the closest resemblance to our study, but is considerably less comprehensive than ours. In particular, we review the advancement in breaking the barriers of context length across all Manuscript submitted to ACM stages for more intricate and scalable Transformer-based LLMs by exploring the Transformer from both an algorithmic design and system architecture perspective. This survey aims to present a panorama of literature on architecture evolution for scaling the effective context window length of the state-of-the-art Transformer-based LLMs. The main contributions are as follows.",
            "score": 0.5396556266551796,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2263
                },
                {
                    "start": 2264,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 247,
                    "matchedPaperCorpusId": "259360665"
                },
                {
                    "start": 1242,
                    "end": 1246,
                    "matchedPaperCorpusId": "250118028"
                },
                {
                    "start": 1246,
                    "end": 1250,
                    "matchedPaperCorpusId": "235368340"
                },
                {
                    "start": 1254,
                    "end": 1258,
                    "matchedPaperCorpusId": "241606169"
                },
                {
                    "start": 1260,
                    "end": 1265,
                    "matchedPaperCorpusId": "250118028"
                },
                {
                    "start": 1385,
                    "end": 1390,
                    "matchedPaperCorpusId": "235368340"
                },
                {
                    "start": 1504,
                    "end": 1509,
                    "matchedPaperCorpusId": "241606169"
                },
                {
                    "start": 1679,
                    "end": 1684,
                    "matchedPaperCorpusId": "235368340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "267027729",
            "title": "InternEvo: Efficient Long-sequence Large Language Model Training via Hybrid Parallelism and Redundant Sharding",
            "text": "Processing long sequences is crucial for supporting important applications such as dealing with longer histories in chat applications. To this end, sequence parallelism (SP) has emerged as a technique aimed at alleviating activation memory footprints during the training of Transformers. In SP, the input tensor of each Transformer layer is divided along the sequence dimension, allowing for parallel computation across multiple GPUs. This segmentation, in conjunction with activation recomputation, results in a substantial reduction in activation memory requirements by a factor of s sp . In this paper, we classify existing SP approaches into 3 stages, which correspond to slicing the sequence into Norm and Dropout modules, Linear modules, and MHA module. When enabled cumulatively: \n\nSP-1: Norm and Dropout modules. As shown in Figure 2 (b), Megatron-LM capitalizes on TP to parallelize the linear layers and MHA, which are the most time-consuming components during training. Simultaneously, it employs SP on Norm and Dropout modules, effectively reducing the activation memory of these layers by a factor of s sp , where s sp = s t p . To maintain consistency in computational results, it integrates necessary communications, including all-gather and reduce-scatter to transfer the activation in forward and backward passes. When the activation size increases with the sequence length, this way of communicating the activation will incur a high overhead. \n\nSP-2: Add Linear modules. DeepSpeed Ulysses utilizes sequence parallelism on Linear, Norm, and Dropout layers, as shown in Figure 2 (c). An all-to-all communi- cation is applied to the output of Linear qkv with a shape of B \u00d7 3 \u00d7 S/s sp \u00d7 H. This enables each GPU to receive the complete sequence of Q, K, and V (each with a shape of B \u00d7 S \u00d7 H/s sp ) for a subset of attention heads. Subsequently, FlashAttention is employed for highly efficient MHA computation with a constraint that the number of attention heads D should not exceed s sp . Another all-to-all communication is then used to gather the MHA layer results and the results are re-partitioned along the sequence dimension.",
            "score": 0.5387470003561244,
            "section_title": "Long-sequence Training",
            "char_start_offset": 9473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01971435546875
        },
        {
            "corpus_id": "275906620",
            "title": "ZETA: Leveraging Z-order Curves for Efficient Top-k Attention",
            "text": "Efficient Transformer The Transformer architecture (Vaswani et al., 2017) is foundational for sequence modeling, but its quadratic complexity limits efficiency with long sequences. Various efficient variants (Tay et al., 2022;2020;Chen et al., 2021;Qin et al., 2022b;Zhang et al., 2024) have been proposed as alternatives, mainly categorized into sparse, low-rank, and linear transformers. Sparse transformers, such as BigBird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020), restrict attention to local windows or global tokens to achieve linear complexity. SparseAxial (Ho et al., 2020) further enhances this by combining sparse attention with axial mechanisms for high-dimensional inputs. Reformer (Kitaev et al., 2020) locality-sensitive hashing to handle variable-length sequences efficiently. Low-rank transformers like Linformer (Wang et al., 2020) reduce the attention matrix to a lower-dimensional space, reducing memory and computation costs. Linear transformers such as Performer (Choromanski et al., 2021) use kernel-based approximations for linear-time complexity, while Nystr\u00f6mformer (Xiong et al., 2021) leverages Nystr\u00f6m decomposition for near-linear performance. \n\nTop-k Attention (Gupta et al., 2021) falls under the category of sparse attention, reducing attention complexity by selecting only the top-k most relevant tokens at each layer, thereby focusing computational resources on the most critical interactions. Unlimiformer (Bertsch et al., 2023) enables transformers to handle arbitrarily long sequences by chunking inputs and using a retrieval mechanism to attend to relevant past contexts. Similarly, IceFormer (Mao et al., 2024) improves transformer efficiency by integrating a k-nearest-neighbor (KNN) search mechanism that focuses on the KNN results as the most relevant tokens during inference, bypassing the need to compute the full attention matrix. However, with causal masks, these approaches can not compute the outputs of a long sequences in parallel, making them less efficient for training models from scratch by not fully exploiting the parallel computing power of GPUs.",
            "score": 0.5386810372319827,
            "section_title": "RELATED WORKS",
            "char_start_offset": 4960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 73,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 208,
                    "end": 226,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 231,
                    "end": 249,
                    "matchedPaperCorpusId": "240354799"
                },
                {
                    "start": 249,
                    "end": 267,
                    "matchedPaperCorpusId": "246904340"
                },
                {
                    "start": 267,
                    "end": 286,
                    "matchedPaperCorpusId": "267523164"
                },
                {
                    "start": 713,
                    "end": 734,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1003,
                    "end": 1029,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1110,
                    "end": 1130,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 1210,
                    "end": 1230,
                    "matchedPaperCorpusId": "235422257"
                },
                {
                    "start": 1460,
                    "end": 1482,
                    "matchedPaperCorpusId": "258436892"
                },
                {
                    "start": 1650,
                    "end": 1668,
                    "matchedPaperCorpusId": "269605811"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0261993408203125
        },
        {
            "corpus_id": "229923177",
            "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
            "text": "denotes a text sequence. To capture long dependencies, they process the text in segments from left to right based on the segment recurrence mechanism . This mechanism maintains a memory bank of past activations at each layer to preserve a history of context. Compressive Transformer (Rae et al., 2019) adds a compressive memory bank to sufficiently store old activations instead of discarding them, which facilitates long-range sequence learning. However, these methods operate from left to right, which limits their capacity for discriminative language understanding tasks that require bidirectional information. XLNet  proposed a permutation language modeling objective to construct bidirectional information and achieve superior performance in multiple NLP tasks; however, its application to long-document modeling tasks remains largely unexplored. ERNIE-DOC builds on the ideas of the Recurrence Transformers to 1) tackle the limitation of Recurrence Transformers for utilizing bidirectional contextual information and 2) improve the behavior of the segment recurrence mechanism to capture longer dependencies.  have enabled significant progress on numerous document-level tasks, such as document summarization (Zhang et al., 2019) and document ranking . Similar to Vanilla Transformers, Hierarchical Transformers also split long documents into shorter segments with manageable lengths and then feed them independently to produce corresponding segment-level semantic representations. Unlike in Vanilla Transformers, however, separate Transformer layers are used in Hierarchical Transformers to process the concatenation of these representations. Hierarchical Transformers ignore the contextual information from the remaining segments when processing each segment of a long document, thus suffering from the context fragmentation problem.",
            "score": 0.5384536433691955,
            "section_title": "Related Work",
            "char_start_offset": 6893,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1629638671875
        },
        {
            "corpus_id": "267770311",
            "title": "CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory",
            "text": "Long-range self-attention. Many efficient self-attention techniques have been proposed for long context modeling in transformer models, including low-rank factorization (Wang et al., 2020), local attention (Ramachandran et al., 2019), dilated attention (Ding et al., 2023), sparsity (Beltagy et al., 2020;Zaheer et al., 2020;Kitaev et al., 2020), and hardwareaware attention mechanisms such as FlashAttention (Dao et al., 2022;Dao, 2023). Despite notable progress, these methods are unable to handle unbounded context window sizes and struggle to retrieve information in the middle of the input (Liu et al., 2023). They can be used in tandem with our proposed approach for longer context modeling. \n\nMemory-augmented LLMs. Memory-augmented language models have emerged as a promising way to model extended context window sizes (Packer et al., 2023;Dai et al., 2019;Wu et al., 2022;Tworkowski et al., 2023;Weston et al., 2014). In particular, Wu et al. (2022) show that a kNN lookup into a memory cache bank containing (key, value) pairs of past inputs can improve language modeling. Tworkowski et al. (2023) further improved this approach using contrastive learning. In the same vein, Wang et al. (2023) addressed the memory staleness limitation of these works by training a side network model, while keeping the LLM frozen. Unlike these methods, our approach relies on consolidated representations of past tokens which are dynamically updated, enabling the context window to be arbitrarily large, without being limited by the number of memory slots. Moreover, different from these approaches, our method is training-free (memory updates occur solely at runtime), making it easier to integrate our memory module into any existing LLM architecture. (Ge et al., 2023;Mu et al., 2023;Chevalier et al., 2023) have been recently explored for extending the context length in transformer models.",
            "score": 0.5375505390820658,
            "section_title": "Related Work",
            "char_start_offset": 5301,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 325,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 409,
                    "end": 427,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55859375
        },
        {
            "corpus_id": "261101748",
            "title": "Machine learning in computational literary studies",
            "text": "While transformer models with arbitrary input lengths can theoretically be trained, the attention mechanism leads to a memory requirement that scales quadratically with the input, making much larger sizes infeasible. More recently, approaches like Longformers [77,78] have proposed methods to limit memory consumption, by limiting the attention mechanism, and thereby enable longer input sequences; Longformers, however, also only allow for input sequences up to 4096 tokens. We are not aware of work applying any of these approaches to CLS, presumably, at least in part, because even 4000 tokens are not typically sufficient to cover the entire subject of analysis. Another alternative exists in hierarchical transformers [79], which can potentially handle even longer sequences. But again, we have not yet encountered them in the application context of CLS. More recently, in the machine learning community, architectures incorporating explicit communication between segments have been proposed, outperforming existing models on long sequence tasks [80,81]. In practice, the length limitation does not always present a problem, for example, Parigini et al. [51] perform token-based annotations and process the entire text in chunks. Such windowbased approaches, however, bar the model from considering any text outside the current window, which might be an issue for some annotation tasks. \n\nAnother variant of the transformer model that was employed in the surveyed literature is the sentence encoder. Such models are trained to produce embeddings of entire sentences or even short documents representing, similarly to word embeddings, the sentences' semantics in vector space. For example, Ehrmanntraut et al. [58] use them in addition to static word embeddings for identifying similarities in poetry. They find the transformer approaches to far outperform the static embedding-based ones, with performance on the overall similarity of embeddings reaching 0.79 as compared to the accuracy of 0.72 for the fast-Text approach. Similarly, Glass explores the use of sentence encoders to analyze if one text is an adaptation of another [40].",
            "score": 0.5369448506005245,
            "section_title": "Transformer-based methods",
            "char_start_offset": 33616,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 723,
                    "end": 727,
                    "matchedPaperCorpusId": "155100086"
                },
                {
                    "start": 1055,
                    "end": 1058,
                    "matchedPaperCorpusId": "250526424"
                },
                {
                    "start": 1159,
                    "end": 1163,
                    "matchedPaperCorpusId": "254045756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03704833984375
        },
        {
            "corpus_id": "232318583",
            "title": "Linear-Time Self Attention with Codeword Histogram for Efficient Recommendation",
            "text": "Considerable efforts have been made trying to scale Transformers to long sequences. Transformer-XL in [9] captures longer-term dependency by employing a segment-level recurrent mechanism, which splits the inputs into segments to perform attention. Sukhbaatar et al. [30] limited the self-attention context to the closest samples. However, these techniques do not improve the  ( 2 ) asymptotic complexity of self-attention. \n\nIn another line of work, attempts in reducing the asymptotic complexity are made. Child et al. [6] proposed to factorize the attention computation into local and strided ones. Tay et al. [33], on the other hand, improved local attention by introducing a differentiable sorting network to re-sort the buckets. Reformer [19] hashes the query-keys into buckets via hashing functions based on random projection, and attention is computed within each bucket. In a similar manner, Roy et al. [28] assign tokens to buckets through clustering. Built on top of ETC [1], Big Bird [44] considers a mixture of various sparse patterns, including sliding window attention and random attention. Clustered Attention, introduced in [36], however, groups queries into clusters and perform attention on centroids. Linformer [37] resorts to a low-rank projection on the length dimension. However, it can only operate in a bidirectional mode without casual masking. \n\nMost of the aforementioned approaches rely on sparse attention patterns, while our method performs full contextual attention over the whole sequence. Besides, Linformer and Sinkhorn Transformer assume a fixed sequence length due to the use of sorting network and projection, while our method poses no such constraint. Our method is also notably faster than the existing approaches, enjoying an asymptotic complexity of  (), while inner product can be stored in a table.",
            "score": 0.5359394728816744,
            "section_title": "Improving Efficiency of Attention",
            "char_start_offset": 7237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1369
                },
                {
                    "start": 1372,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1841
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "159041867"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 743,
                    "end": 747,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1140,
                    "end": 1144,
                    "matchedPaperCorpusId": "220424511"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0379638671875
        },
        {
            "corpus_id": "276775997",
            "title": "From Small to Large Language Models: Revisiting the Federalist Papers",
            "text": "The computational complexity of the Transformer's attention mechanism, O(T 2 p), allows efficient modeling of relationships between tokens, as opposed to the O(T p 2 ) complexity of recurrent neural networks (RNNs) (Vaswani et al., 2017). For example, in GPT-3, T = 2, 048 is much smaller than p = 12, 288, highlighting the reliance on high-dimensional embeddings to model linguistic patterns. While this approach increases model capacity, it also introduces a computational challenge due to the quadratic scaling of cost with T . Solutions such as \"context parallelism,\" introduced by Meta (2024), extend sequence lengths up to T = 128, 000 while scaling p to 53,248. Other approaches, including linear-time sequence models (Gu and Dao, 2024) and efficient RNN-based methods (Feng et al., 2024), aim to reduce computational overhead while preserving the benefits of dimensionality expansion. \n\nMassive Training Data Large Language Models (LLMs) such as GPT (Radford, 2018;Radford et al., 2019;Brown et al., 2020), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), BART (Lewis et al., 2019), and LLaMA (Touvron et al., 2023) rely heavily on massive training datasets sourced from diverse corpora, including BookCorpus, Wikipedia, Common Crawl, and other largescale textual collections (See Table 2 for",
            "score": 0.5356423911844517,
            "section_title": "The Largeness",
            "char_start_offset": 39123,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 1307
                }
            ],
            "ref_mentions": [
                {
                    "start": 776,
                    "end": 795,
                    "matchedPaperCorpusId": "6644398"
                },
                {
                    "start": 973,
                    "end": 994,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.019866943359375
        },
        {
            "corpus_id": "267770174",
            "title": "KetGPT - Dataset Augmentation of Quantum Circuits using Transformers",
            "text": "Transformer models, as introduced in the groundbreaking work [42], have changed the landscape of natural language processing. Their applications extend to code generation [40,29] and music generation [2]. Renowned for their proficiency in capturing dependencies within sequential data, these widely adopted machinelearning models have proven effective in various domains. \n\nBefore the advent of transformers, conventional models for natural language processing tasks, such as text generation, primarily relied on Convolutional Neural Networks (CNN) [24], Recurrent Neural Networks (RNN) [37], and Long Short-Term Memory networks (LSTM) [18]. However, these models encountered several challenges, including difficulties in handling long-range dependencies and a lack of parallelizability [42]. A transformer, on the other hand, is a highly parallelizable model, well-suited for training on extensive datasets, that excels at H q[0]; CX q[0], q[1]; SWAP q[1], q[2]; qasm Tokenizer Fig. 1: Tokenization Example. A sequence of QASM operations (in text file form) is provided as input, and each statement (a line of QASM code) is assigned to a number. The number assigned to each statement does not have an intuitive meaning; rather, it just depends on how the tokenization algorithm orders its vocabulary. Consequently, tokenizing a sequence of statements will create a list of numbers. It is important to note that both gate and qubit(s), we apply the gate on, matter for the assigned token. For instance, h q[0]; and h q [1]; would have different numbers assigned as shown. \n\ncapturing longer-range dependencies and, therefore offers a significant improvement over earlier models. \n\nIn what follows, we review the three main components of the transformer model with quantum assembly language as the data.",
            "score": 0.5350829015655236,
            "section_title": "Evolution and structure of Transformers",
            "char_start_offset": 6009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 371
                },
                {
                    "start": 374,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1571
                },
                {
                    "start": 1574,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1802
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 175,
                    "matchedPaperCorpusId": "218673683"
                },
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "14542261"
                },
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1519,
                    "end": 1522,
                    "matchedPaperCorpusId": "236546858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01287078857421875
        },
        {
            "corpus_id": "267682361",
            "title": "Data Engineering for Scaling Language Models to 128K Context",
            "text": "We attribute our improvements over strong open-source baselines, as is detailed in section 5, to our careful treatments of data engineering. Our results echo the recent wisdom that in large language model research, data engineering is equally important as modeling and algorithmic innovations (Kaplan et al., 2020;Hoffmann et al., 2022;Brown et al., 2020). As we list in Table 1, many of the data details are crucial for strong long-context performance, yet may be easily overlooked. We also acknowledge that our research is made possible through utilizing the latest innovations in machine learning systems research, particularly FlashAttention (Dao, 2023) as it reduces the memory usage of the attention mechanism from quadratic to linear. Incorporating further work on sequence parallelism (Li et al., 2021;Jacobs et al., 2023) could enable brute force training of even longer context (e.g., 200K) models. The Transformer's use of position embeddings makes it difficult to generalize significantly beyond contexts seen during training even with relative positional encodings (Press et al., 2021;Sun et al., 2022;Li et al., 2023b), thus necessitating (at least a little bit of) continual pretraining. There has much recent work on RNN-like architectures, which implicitly encode positional information through their hidden states, that perform competitively with Transformers (Sun et al., 2023;Qin et al., 2023;Gu & Dao, 2023;Yang et al., 2023b). It would be interesting to test whether such models can generalize zero-shot to longer contexts than seen during training on the Needle-in-a-Haystack benchmark. \n\nLong-context language model research at the 100K-level is still a developing research area. This work only studies continual pretraining, and research on instruction finetuning language models on tasks of 100K context length (e.g., repolevel code understanding) is still limited. So far there seems to no open-source instruction-finetuned 100K context language models. We hope our work serve as a basis for future work on 100K-level long context superivsed finetuning.",
            "score": 0.5348783300345417,
            "section_title": "Discussion",
            "char_start_offset": 23592,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2080
                }
            ],
            "ref_mentions": [
                {
                    "start": 336,
                    "end": 355,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19287109375
        },
        {
            "corpus_id": "268875936",
            "title": "Linear Attention Sequence Parallelism",
            "text": "Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$ longer than existing SP methods. Code is available at: https://github.com/OpenNLPLab/LASP.",
            "score": 0.5345924504979647,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03594970703125
        },
        {
            "corpus_id": "269757781",
            "title": "USP: A Unified Sequence Parallelism Approach for Long Context Generative AI",
            "text": "Before delving into the concept of SP, let's review the computational process of the standard Transformer Block.The Notion used in this paper is shown in Table 1, where d = hc \u00d7 hs.Given input sequences Q, K, V \u2208 R L\u00d7d , where L is the sequence length and d is the head dimension, we compute the matrix of outputs as follows:\n\nEach self-attention sub-layer is accompanied by a feedforward network (FFN), which is applied to each position separately and identically.\n\nCompared to Tensor Parallelism (TP) [4] and ZeRO [5], the research on SP for Transformer models has been relatively underdeveloped for a long time.The challenge lies in the characteristic of attention computation, where the sequence dimension serves as a common dimension in the matrix multiplication after softmax, making it difficult to partition the tensors and distribute the computation across multiple nodes after slicing the sequence dimension.\n\nEarly attempts [6][7][8] at SP were not successful, often leading to redundant memory consumption [6] and inefficient communication pattern [8].For long input sequences, the best practice is to adopt the Sequence Parallelism of Megatron-LM.This method optimizes the AllReduce operation of TP, reducing the memory cost of activations while maintaining the same communication overhead.As shown in Figure 1, the principle of Megatron-LM Sequence Parallelism is the similar to ZeRO-2 [5].\n\nIt replaces the AllReduce operation on replicated tensors (the left figure) into equivalent allgather and reduce-scatter operations on partitioned data (the right figure).Since an AllReduce operation is exactly a combination of Allgather and ReduceScatter, the communication cost remains the same.The size of the input and output tensors is reduced by a factor of 1/N across N computational devices.Because of the sequence dimension in input/output tensors is partitioned, it is named as Sequence Parallelism.However, this form of Sequence Parallelism cannot be used independently without tensor parallelism and communication volume remains constant regardless of the degree of parallelism.The maturity of the standalone SP technology is marked by the publication of two milestone papers in late 2023.",
            "score": 0.5345348714995708,
            "section_title": "Sequence Parallelism Approaches",
            "char_start_offset": 2967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 112,
                    "end": 181
                },
                {
                    "start": 181,
                    "end": 325
                },
                {
                    "start": 327,
                    "end": 465
                },
                {
                    "start": 467,
                    "end": 614
                },
                {
                    "start": 614,
                    "end": 918
                },
                {
                    "start": 920,
                    "end": 1064
                },
                {
                    "start": 1064,
                    "end": 1160
                },
                {
                    "start": 1160,
                    "end": 1303
                },
                {
                    "start": 1303,
                    "end": 1404
                },
                {
                    "start": 1406,
                    "end": 1577
                },
                {
                    "start": 1577,
                    "end": 1703
                },
                {
                    "start": 1703,
                    "end": 1805
                },
                {
                    "start": 1805,
                    "end": 1915
                },
                {
                    "start": 1915,
                    "end": 2096
                },
                {
                    "start": 2096,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "231632857"
                },
                {
                    "start": 938,
                    "end": 941,
                    "matchedPaperCorpusId": "240070340"
                },
                {
                    "start": 1400,
                    "end": 1403,
                    "matchedPaperCorpusId": "231632857"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.015838623046875
        },
        {
            "corpus_id": "269626286",
            "title": "Folded Context Condensation in Path Integral Formalism for Infinite Context Transformers",
            "text": "Transformers architecture [1] has revolutionized natural language processing (NLP) by demonstrating unparalleled effectiveness in capturing long-range dependencies and enabling scalable, parallelized computation. This architecture often consists of an embedding layer, decoder layers with a selfattention mechanism, and a Feed-Forward Network (FFN), therefore, it predicts the next token in an auto-regressive manner. Unlike earlier Recurrent Neural Network (RNN) based models, which process sequences sequentially and suffer from vanishing gradient issues [2], Transformers leverage selfattention mechanisms to establish global dependencies across tokens, leading to significant improvements in contextual reasoning and coherent text generation. This architectural advantage has propelled Transformers to the forefront of modern NLP, underpinning the development of large-scale language models (LLMs) with remarkable generative capabilities. \n\nHowever, Transformers have suffered from their computational inefficiency when handling long sequences. The selfattention mechanism inherently scales quadratically with sequence length in both memory and computation, making it impractical to process extensive contexts within a single pass. This limitation hinders the retention of dependencies across long documents, dialogues, or continuous interactions, prompting the development of alternative strategies such as Infini-Attention [7], which restructures the self-attention mechanism by introducing a hybrid memory system that allows interaction between local context tokens and external memory components, leveraging a linear attention [8] framework to reduce computational overhead. Another innovative solution is Titans [9], which incorporates trainable neural long-term memory, enabling the model to retain and retrieve past contextual information beyond traditional Transformer constraints. \n\nDespite their empirical success, moreover, the theoretical foundations of Transformer architecture remain incomplete. While extensive research has explored their scaling properties [3], emergent behaviors [4], and empirical performance [5], [6], these studies provide limited insight into the fundamental mechanisms governing Transformer architectures. In particular, there is a lack of rigorous theoretical formulations explaining how information propagates through layers and how hierarchical structures are captured within Transformer models. This gap in understanding limits the interpretability and optimization of Transformer-based architectures and presents challenges in extending their capabilities systematically.",
            "score": 0.5340326483668416,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1893
                },
                {
                    "start": 1896,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2441
                },
                {
                    "start": 2442,
                    "end": 2619
                }
            ],
            "ref_mentions": [
                {
                    "start": 557,
                    "end": 560,
                    "matchedPaperCorpusId": "206457500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1676025390625
        },
        {
            "corpus_id": "245360052",
            "title": "A Literature Survey of Recent Advances in Chatbots",
            "text": "Transformers are language models based solely on the Attention mechanism.Transformers are nowadays the model of choice for NLP challenges, replacing RNN models like long short-term memory (LSTM) by differentially weighing the relevance of each portion of the input data.Furthermore, they provide training parallelization that permits training on larger datasets than was originally achievable.This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from transformers) [39] and GPT (Generative Pre-trained Transformer), which were trained with huge language datasets, such as Wikipedia Corpus and Common Crawl, and may be fine-tuned for specific applications.Several different versions of the Transformer have since been presented, such as the Reformer [40] and the Transformer XL [41].Each version of the transformer has been developed to answer to specific challenges for the task at hand.Even though transformers were introduced to answer Machine Translation challenges, they can be adapted and modified to perform dialogue Modelling tasks.\n\nIn [41], the authors propose an updated version of the Transformer called Transformer-XL.This model can go beyond the fixed-length context limitations of the Transformer, using sentence-level recurrence.Transformers show a potential of learning longer-term dependency but are constrained by fixed length context in the setting of language modelling.The authors present a unique neural architecture named Transformer-XL that enables learning dependency beyond a given length without breaking temporal coherence.It comprises a segment level recurrence mechanism and a unique positional encoding technique.This solution aims at capturing longer-term dependency and resolving the context fragmentation issue.Even though this approach has not yet been applied to dialogue modelling, it can be argued that once the appropriate and necessary modification implemented, it could prove useful in overcoming some of the issues current dialogue models present, namely context understanding.\n\nIn [40], the authors introduce the Reformer, a more efficient version of the Transformer, that makes use of two techniques to improve the Transformer in terms of efficiency.Firstly, the authors substitute dot-product attention with one that employs locality-sensitive hashing, increasing its complexity from O(L 2 )toO(LlogL), where L is the length of the sequence.",
            "score": 0.5332295982565929,
            "section_title": "Artificial Intelligence Chatbots",
            "char_start_offset": 30851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 73,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 393
                },
                {
                    "start": 393,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 831
                },
                {
                    "start": 831,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1088
                },
                {
                    "start": 1090,
                    "end": 1179
                },
                {
                    "start": 1179,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1439
                },
                {
                    "start": 1439,
                    "end": 1600
                },
                {
                    "start": 1600,
                    "end": 1693
                },
                {
                    "start": 1693,
                    "end": 1794
                },
                {
                    "start": 1794,
                    "end": 2068
                },
                {
                    "start": 2070,
                    "end": 2243
                },
                {
                    "start": 2243,
                    "end": 2435
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1319580078125
        },
        {
            "corpus_id": "277857470",
            "title": "Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation",
            "text": "Adapting transformers to enable longer context capabilities is a critical area of research in natural language processing. This effort primarily focuses on three key directions: (1) architectural modifications to the transformer model itself, (2) improvements in positional embedding techniques, and \n\n(3) the development and utilization of more extensive long-context datasets. \n\nEfficient Attention Mechanisms. To address the quadratic computational and memory demands of standard transformer self-attention, researchers have developed various architectural modifica-tions to improve efficiency and extend context lengths. Notable examples include Longformer (Beltagy et al., 2020), which combines sliding window and global attention, and BlockTransformer (Ho et al., 2024), which employs hierarchical global-to-local modeling. Linear Attention methods (Katharopoulos et al., 2020) reformulate self-attention for linear complexity, while InfiniteTransformer (Munkhdalai et al., 2024) incorporates unbounded long-term memory through continuousspace attention. State space models like Mamba (Gu & Dao, 2024) capture long-range dependencies efficiently without explicit attention mechanisms. Despite these advancements, bridging the gap with high-quality data remains a critical challenge and is the focus of this work. \n\nPosition Embedding Extension. Advances in positional encoding methods have enabled language models to handle longer sequences effectively. Techniques like RoPE (Su et al., 2023), ALiBi (Press et al., 2022), and xPos (Sun et al., 2022) have emerged as prominent solutions. RoPE has gained widespread adoption in LLaMA (Touvron et al., 2023), b) and PaLM (Anil et al., 2023), due to its ability to represent relative positions and its theoretical grounding in the complex plane.",
            "score": 0.533057279961101,
            "section_title": "RELATED WORK",
            "char_start_offset": 5526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 299
                },
                {
                    "start": 302,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1797
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5537109375
        },
        {
            "corpus_id": "220424511",
            "title": "Fast Transformers with Clustered Attention",
            "text": "In this section, we summarize techniques that seek to apply transformers to long sequences without focusing on improving the quadratic complexity of self-attention. The most important are Adaptive Attention Span Transformers [26] and Transformer-XL [7]. \n\nSukhbaatar et al. [26] propose to limit the self-attention context to the closest samples (attention span), in terms of relative distance with respect to the time step, thus reducing both the time and memory requirements of self-attention computation. This is achieved using a masking function with learnable parameters that allows the network to increase the attention span if necessary. Transformer-XL [7], on the other hand, seeks to increase the effective sequence length by introducing segment-level recurrent training, namely splitting the input into segments and attending jointly to the previous and the current segment. The above, combined with a new relative positional encoding results in models that attend to more distant positions than the length of the segment used during training. \n\nAlthough both approaches have been proven effective, the underlying limitations of self-attention still remains. Attending to an element that is N timesteps away requires O N 2 memory and computation. In contrast, our model trades-off a small error in the computation of the full attention for an improved linear asymptotic complexity. This makes processing long sequences possible.",
            "score": 0.5325476619787013,
            "section_title": "Non-asymptotic Improvements",
            "char_start_offset": 4715,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 253
                },
                {
                    "start": 256,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1053
                },
                {
                    "start": 1056,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1438
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06854248046875
        },
        {
            "corpus_id": "272423943",
            "title": "Hardware Acceleration of LLMs: A comprehensive survey and comparison",
            "text": "In 2021, Laguna et al. presented a novel in-memory architecture for the acceleration of transformer networks for long sentences called iMCAT [44]. The proposed framework uses a combination of XBars and CAMs to accelerate transformer networks. The acceleration of transformer networks is achieved by combining several techniques such as computing in-memory, thus minimizing the memory transfer overhead, caching reusable parameters to reduce the number of operations, exploiting the available parallelism in the attention mechanism, and finally using locality sensitive hashing to filter the number of sequence elements by their importance. \n\nThe performance evaluation shows that this approach achieves a 200x speedup and 41x energy improvement for a sequence length of 4098.",
            "score": 0.5316763140986648,
            "section_title": "C. iMCAT",
            "char_start_offset": 36531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 775
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "236151453"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03216552734375
        },
        {
            "corpus_id": "273850602",
            "title": "$k$NN Attention Demystified: A Theoretical Exploration for Scalable Transformers",
            "text": "Transformer models have become the dominant neural architecture across language, vision, and other domains [Vas17, DBK + 20]. However, scaling them to handle larger input sequences remains a significant challenge [TDA + 20], primarily due to the quadratic complexity of computing self-attention. Overcoming this limitation is crucial for advancing neural networks. Extending context length would enable Transformers to tackle complex tasks like book summarization [KRA + 21] and time-series forecasting [WZZ + 22, ZCZX23, ZZP + 21]. Furthermore, improving attention efficiency would reduce the computational burden of training, making these models more accessible. Bridging this \"compute divide\" is vital for democratizing AI [AW20]. \n\nEfficient computation of self-attention has been a focal point of research in recent years [FCA23]. Flash Attention [DFE + 22] and related work [SY24] optimize the exact calculation of attention by minimizing wasted computation during GPU I/O operations. However, most approaches focus on approximating the attention function. Sparse Transformers improve efficiency by allowing each token to attend to only a small subset of tokens [MLAC21]. These subsets are identified through deterministic methods [CGRS19, GQL + 19, SM20, LJX + 19, QML + 19, BPC20], randomized algorithms [KKL20, HJK + 23, ZHDK23, PPJF24], or adaptive techniques [CNM19]. Additionally, self-attention is often approximated using low-rank matrices and kernel methods [WLK + 20, TBM + 21, XZC + 21, KVPF20, CLD + 20]. On the negative side, recent finegrained complexity reductions indicate that achieving a good approximation with sub-quadratic time is not feasible across all scenarios [KWH23,AS24a]. \n\nIn this work, we focus on sparse attention methods where each token vector q i \u2208 R d attends to the k tokens k j \u2208 R d with the largest inner products q T i k j [GDG + 21, WWW + 22], a paradigm we refer to as kNN Attention.",
            "score": 0.5316143070894245,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 827,
                    "end": 834,
                    "matchedPaperCorpusId": "232380042"
                },
                {
                    "start": 1692,
                    "end": 1699,
                    "matchedPaperCorpusId": "252198880"
                },
                {
                    "start": 1699,
                    "end": 1705,
                    "matchedPaperCorpusId": "257219595"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10015869140625
        },
        {
            "corpus_id": "268875936",
            "title": "Linear Attention Sequence Parallelism",
            "text": "Linear sequence modeling methods (Katharopoulos et al., 2020;Choromanski et al., 2022;Sun et al., 2025a) including linear attention (Qin et al., 2024d), state space models (Dao & Gu, 2024) and linear RNN (Qin et al., 2024e), are becoming increasingly popular due to their faster training and inference speed and comparable modeling performance to vanilla Softmax attention-based transformer models (Vaswani et al., 2017;Zeng et al., 2022;Touvron et al., 2023a;b;Team, 2023). The hybrid architecture, which interleaves Softmax attention and linear attention Transformer layers, has proven to be an effective balance between their respective strengths. This approach has been successfully implemented in large-scale commercial models such as Minimax-01 (Li et al., 2025) and Tencent Hunyuan Turbo-S (Tencent, 2025), as well as in smaller-scale hybrid models like Samba (Ren et al., 2024), Jamba (Lieber et al., 2024). \n\nAs the size of large language models (LLMs) increases and sequence lengths extend, the capacity limitations of single GPU's memory become a significant challenge, constraining the maximum sequence length manageable by a large model. To address this, Sequence Parallelism (SP) techniques (Li et al., 2022;Korthikanti et al., 2022) are employed, which partition a long sequence into multiple sub-sequences to be processed on separate devices. However, current implementations of SP methods do not fully exploit the right-product advantages of linear-complexity attention mechanisms Qin et al. (2024b). This results in less than optimal parallelism efficiency and reduced usability on linear sequence modeling methods. \n\nIn this paper, we present Linear Attention Sequence Parallelism (LASP) approach for efficient SP on models with linear sequence modeling. Our approach takes linear attention (Katharopoulos et al., 2020) as an instance to design a sophisticated point-to-point (P2P) ring-style communication mechanism during both forward and backward among devices within a node or across multiple nodes.",
            "score": 0.5313938017298488,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1633
                },
                {
                    "start": 1636,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 61,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 86,
                    "end": 104,
                    "matchedPaperCorpusId": "267312207"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0272216796875
        },
        {
            "corpus_id": "271745235",
            "title": "PackMamba: Efficient Processing of Variable-Length Sequences in Mamba training",
            "text": "With the swift advancement of large-scale language models, the Transformer, serving as the most widely used foundational model in large models, has been applied in BERT [3], GPT-4 [1], and Llama [11]. The effectiveness of self-attention stems from its capacity to facilitate dense information exchange within a given context window, thereby enabling it to model intricate data. Nevertheless, this characteristic introduces inherent limitations, namely the incapacity to model elements outside of a bounded window [5], and the computational complexity that scales quadratically with the increase in window length. To address these drawbacks inherent to the Transformer model, an increasing amount of research is being directed towards the innovation of foundational models in natural language processing (NLP), such as linear attention [7], gated convolution [12], recurrent models [8], structured state space models [6](SSMs) and Test-Time Training [9]. Mamba [5] enjoys fast inference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. On language modeling, Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. \n\nIn contrast to Recurrent Neural Networks (RNNs), which inherently rely on the state of preceding neurons, thereby impeding parallel training, Mamba innovatively introduces a 'selective scan' mechanism that promotes parallelism during the training process. This departure from the conventional sequential dependency in RNNs empowers Mamba to harness parallel computational resources, consequently boosting training efficiency and scalability. \n\nMamba's training faces challenges with variable-length sequence inputs. Profiling shows that in single-sequence training, GPU tasks are fine-grained, with large gaps between tasks. These gaps, caused by frequent CPU-GPU synchronization and high kernel launch overhead, lead to inefficient GPU utilization. Another approach is pad to maximum length for batch training. However, these padded zeros introduce significant computation and memory overhead.",
            "score": 0.5313364961268848,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1306
                },
                {
                    "start": 1309,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1750
                },
                {
                    "start": 1753,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 835,
                    "end": 838,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 858,
                    "end": 862,
                    "matchedPaperCorpusId": "47017068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0250396728515625
        },
        {
            "corpus_id": "266164124",
            "title": "Efficient Classification of Long Documents via State-Space Models",
            "text": "Since the emergence of large-scale pre-trained language models such as BERT (Devlin et al., 2019) and GPT3 (Brown et al., 2020), these transformerbased models have become popular solutions for many text classification and generation tasks. However, their benefit is constrained to short-length inputs when the computation resource is limited because attention module requires quadratic computation time and space. More specifically, each token in a sequence of length N requires pairwise computation with all N tokens, which results in O(N 2 ) complexity. Such limitation makes transformerbased models hard to process long sequential data efficiently. There are many works aiming to improve the performance on Long Document Classi-fication for transformers (Dai et al., 2022). One of the common approaches is to simply truncate long texts to a pre-defined length, e.g. 512, which makes pre-trained models to be applicable for them. Some work demonstrated this technique is not sufficient for long documents due to the missing of important information (Dai et al., 2022). \n\nAnother sort of technique attempts to reduce the computation overhead of attention-based systems. This problem has several relevant solutions, e.g. Sparse Attention models (Beltagy et al., 2020) and Hierarchical Attention models (Chalkidis et al., 2022). One of the important sparse attention methods is Longformer, which leverages local and global attention to reduce the computational complexity of the models and increases the input length up to 4096 tokens. Another popular sparse attention method is BigBird (Zaheer et al., 2020): besides the global and local attention, it includes extra random attention modules to attend to a predefined number of random tokens. Apart from designing sparse attention mechanisms, Hierarchical Transformers (HAN) like ToBERT (Pappagari et al., 2019) propose to construct systems on top of the conventional transformer (Chalkidis et al., 2022). Basically, the long text is first split into several chunks less than a fixed number, e.g. 200. \n\nNext, every chunk is encoded by a vanilla transformer one by one to form a collection of chunk representations and then another transformer processes the sequence of chuck representations.",
            "score": 0.5311363933591969,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2051
                },
                {
                    "start": 2054,
                    "end": 2242
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 97,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 757,
                    "end": 775,
                    "matchedPaperCorpusId": "248177894"
                },
                {
                    "start": 1051,
                    "end": 1069,
                    "matchedPaperCorpusId": "248177894"
                },
                {
                    "start": 1302,
                    "end": 1326,
                    "matchedPaperCorpusId": "248177894"
                },
                {
                    "start": 1586,
                    "end": 1607,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1837,
                    "end": 1861,
                    "matchedPaperCorpusId": "204852089"
                },
                {
                    "start": 1930,
                    "end": 1954,
                    "matchedPaperCorpusId": "248177894"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0216217041015625
        },
        {
            "corpus_id": "276585307",
            "title": "CQS-Attention: Scaling Up the Standard Attention Computation for Infinitely Long Sequences",
            "text": "Since the proposal of the vanilla Transformer in 2017 [1], transformer-based models have achieved remarkable results in various language and vision tasks. The attention mechanism plays a critical role. It enables the model to capture and utilize long-range contextual information effectively. However, transformer models face serious computational challenges in time and space since the demand for longer sequences grows, such as in the application of multi-turn conversation, long document comprehension, video generation, etc. In [2], Rabe et al. pointed out that modern hardware is often memory-constrained, while computation is relatively cheap. Thus, the device memory is often the limiting factor of modern accelerators. Unfortunately, the scalability of large transformer models is hampered by heavy memory requirements. They cannot handle very long sequences, especially when the standard self-attention is used. Because \n\nThe associate editor coordinating the review of this manuscript and approving it for publication was Yangming Lee. it scales quadratically with the sequence length, which makes long sequence modeling very inefficient [3], [4], especially during the training stage.",
            "score": 0.5310019991518748,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1195
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.364013671875
        },
        {
            "corpus_id": "260440449",
            "title": "Long Range Arena: A Benchmark for Efficient Transformers",
            "text": "We believe such a side-by-side performance benchmark will be valuable to the community, providing deeper insight on the practical efficiency of these methods. Overall, we propose a unified framework for enabling easy side-by-side comparisons of efficient Transformer models and broadly speaking, long-range sequence models in general. Our framework, which we open source, is written in JAX/FLAX1 .",
            "score": 0.5310019991518748,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 397
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00646209716796875
        },
        {
            "corpus_id": "269282923",
            "title": "NLP-enabled Trajectory Map-matching in Urban Road Networks using a Transformer-based Encoder-decoder",
            "text": "g T P w 4 B p q c A d 1 a A C D E T z D K 7 w 5 i f P i v D s f 8 9 Y V J 5 8 5 g j 9 w P n 8 A 9 v S P S g = = < / l a t e x i t > r j+2 < l a t e x i t s h a 1 _ b a s e 6 4 = \" U N t 6 4 8 m x y J  RNN encoder-decoder models with attention mechanisms remain computationally demanding due to the added computational overhead required for calculating attention weights for each input token. The transformers are a newer model architecture introduced by [28] that addresses the limitations of RNN encoder-decoder models. Transformers diverge from RNNs in their architecture, opting instead for stacks of selfattention mechanisms to capture the interconnections within input sequences and their relevance to output sequences. The decoder component leverages parallelized matrix multiplications, simultaneously processing all input sequence elements, as opposed to the sequential nature of RNNs. This parallel processing is a pivotal factor contributing to transformers' enhanced speed and efficiency compared to traditional RNNbased models, which tend to have a more sequential processing approach, often leading to extended training times and slower inference speeds. Additionally, the transformers have been shown to achieve the state-of-the-art performance on a variety of tasks [28].",
            "score": 0.5310019991518748,
            "section_title": "u T I = \" >",
            "char_start_offset": 34460,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1286
                }
            ],
            "ref_mentions": [
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1281,
                    "end": 1285,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0174407958984375
        },
        {
            "corpus_id": "229924221",
            "title": "Shortformer: Better Language Modeling using Shorter Inputs",
            "text": "Training on long sequences is expensive. The computational complexity of self-attention grows quadratically with subsequence length. We observe that models trained on subsequences of length 256 are twice as fast as models trained on subsequences of 3,072 tokens, but gains for even shorter subsequence lengths are negligible.\n\nLong subsequence lengths can improve results. When using the na\u00efve evaluation approach, nonoverlapping evaluation, we see a monotonic decrease in validation perplexity when increasing L.\n\nIncreasing the minimum effective context window size is more important than increasing the maximum one. Using a sliding window for token-by-token inference offers a truer test of a model's generalization ability, and significantly improves results for all models. Here, we see negligible improvement between the models trained with subsequence lengths of 1,024 and 3,072 tokens (0.05 perplexity). This approach improves results by increasing the minimum amount of context available to each token. This indicates that long contexts may not be beneficial to transformer language models, but very short contexts are harmful. However, using sliding windows at inference time can be very expensive since each token is encoded many times. For example, token-by-token inference for the L = 3,072 model is almost 300 times slower than nonoverlapping inference.\n\nBased on these findings, we introduce methods for training and inference with short subsequence length transformers, and show that they improve both efficiency and perplexity.",
            "score": 0.530074976405213,
            "section_title": "Context Window Size Matters",
            "char_start_offset": 9528,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05865478515625
        },
        {
            "corpus_id": "273963185",
            "title": "Accelerating Large Language Model Training with 4D Parallelism and Memory Consumption Estimator",
            "text": "Unlike Sequence Parallelism (Korthikanti et al., 2023), which only parallelizes the activations of Dropout and LayerNorm, Context Parallelism enables partitioning of the model's network inputs and all activations along the sequence dimension. \n\nLlama 2 (Touvron et al., 2023) had a sequence length of 4,096 tokens, but Llama 3 (Dubey et al., 2024) increased this to 8,192 tokens, and Llama 3.1 further extended it to 131,072 tokens. As efficient training that supports long contexts is increasingly demanded, context parallelismwhich allows partitioning along the sequence dimensionis extremely useful for reducing the activations per GPU. \n\nIn components other than self-attention, there are no intertoken operations; thus, introducing context parallelism does not alter the operation. However, in self-attention layers, inter-token operations occur, necessitating the gathering of the full sequence, which requires additional all-gather communications between GPUs in the forward pass. During backpropagation, reduce-scatter is applied to the activation gradients, and each GPU stores only its sequence chunk to reduce the activation memory footprint. \n\nTo date, there is no comprehensive performance evaluation of 4D parallelism (DP, TP, PP, CP) utilizing context parallelism, and knowledge for applying it to actual LLM training is lacking.",
            "score": 0.5299117053776783,
            "section_title": "Parallelism",
            "char_start_offset": 6612,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 245,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1153
                },
                {
                    "start": 1156,
                    "end": 1344
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 54,
                    "matchedPaperCorpusId": "248693351"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039276123046875
        },
        {
            "corpus_id": "245131261",
            "title": "Simple Local Attentions Remain Competitive for Long-Context Tasks",
            "text": "The quadratic complexity of Transformer architectures make it prohibitive to apply large state-of-theart pretrained models to full-length documents. To efficiently handle longer text while still maintaining the capacity of attention-based models, a long list of low-complexity attention variants have been proposed and many claim to effectively capture longrange dependencies. Typical paradigms of these architecture innovations involve learnable sparse attention patterns (Kitaev et al., 2020;Tay et al., 2020a;, fixed local patterns (Beltagy et al., 2020;Ainslie et al., 2020;Zaheer et al., 2020) and attention matrix approximation methods Choromanski et al., 2021;Xiong et al., 2021). While most of these studies have reported numbers on long sequence inputs, they tend to adopt quite different benchmarks. For instance, Reformer (Kitaev et al., 2020) is tested on the 64k-chunk enwik8 dataset for unidirectional language modeling; Performer (Choromanski et al., 2021) reports masked language modeling (MLM) perplexity on the PG-19 book corpus and protein sequences; Linformer  shows MLP perplexity with various input length, while most of documents in their pretrain corpus are short documents. 1 The divergence of evaluation protocols make it hard to compare the relative performance of each attention variant and it is also unknown how they perform in more practical use cases, which typically involve large-scale pretraining and downstream finetuning.\n\nAnother line of work such as Longformer (Beltagy et al., 2020) and ETC (Ainslie et al., 2020) conduct experiments on real-world long-context tasks such as long document QA and summarization. These methods only test simple local attention patterns, i.e., each token can only attend a small set of nearby tokens. To reduce the pretraining cost, these models are all initialized from the RoBERTa (Liu et al., 2019) checkpoint 2 before further long-doc pretraining. While this paradigm is useful to achieve strong downstream performance, it is not ideal for a fair comparison of all available attention mechanisms since some of them use different parametrization that is incompatible",
            "score": 0.5291001897330488,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 473,
                    "end": 494,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 494,
                    "end": 512,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 557,
                    "end": 578,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 642,
                    "end": 667,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 667,
                    "end": 686,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 833,
                    "end": 854,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 945,
                    "end": 971,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1531,
                    "end": 1553,
                    "matchedPaperCorpusId": "221845203"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1527099609375
        },
        {
            "corpus_id": "271874361",
            "title": "Kraken: Inherently Parallel Transformers For Efficient Multi-Device Inference",
            "text": "By designing the model architecture to account for characteristics of the hardware, our approach increases compute utilization and allows more efficient inference. \n\nWe evaluate the improvements Kraken offers over standard Transformers in two key aspects: model quality and inference latency. For the former, we train a series of Kraken models with varying degrees of parallelism and parameter count on OpenWebText (22) and compare them with the GPT-2 (41) family of models on the SuperGLUE suite of benchmarks (48). We then implement Kraken using the TensorRT-LLM library (14) and measure the Time To First Token (TTFT) given various model sizes and context lengths to illustrate the efficiency gains when collective operators are no longer on the critical path. We find that while maintaining the language modeling capabilities of standard Transformers, Kraken models speedup the Time To First Token (TTFT) by a geomean of 35.6% when tested across a range of model sizes, context lengths, and degrees of parallelism.",
            "score": 0.5288001105100164,
            "section_title": "Introduction",
            "char_start_offset": 2196,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 166,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 1018
                }
            ],
            "ref_mentions": [
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03839111328125
        },
        {
            "corpus_id": "270710851",
            "title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs",
            "text": "My favorite team has won the championship.\n\nOutput: Joy] [3.Instruction:... Input:... Output:...] ... ... [N.Instruction:... Input:... Output:...] Please identify all the incorrectly answered questions above.\n\nAnswer: [1,4,7...] ( of key information from extended texts.We evaluate the performance of 20 different LLMs under LongIns, including GPT-4o.We observe that LLMs generally perform worse on tasks requiring understanding of complete long sequences compared to retrieval tasks of the same length.As the total length increases, the performance gap becomes more pronounced.Additionally, the models' performance is largely independent of their advertised context window length, leading us to believe that the advertised context window length for most models should be understood as the \"maximum acceptable sequence length\" rather than the \"maximum comprehensible sequence length.\"Moreover, by controlling the distribution of incorrect answers and response situations, we analyze the distribution of attention changes at different text positions.Further analysis on the density of key information within the same total length shows that, except for GPT-4 and GPT-4o, the accuracy of most models rapidly declines as the density of key information increases.In summary, the primary contributions can be summarized as: \u2022 We evaluate a series of long-context LLMs using this benchmark and find that most models fail to achieve high scores when the critical information length is only 8k.Even GPT-4 and GPT-4o score poorly at 16k length.This result is significantly different from the commonly recognized long context lengths (128k or longer), indicating that current LLMs still have considerable shortcomings in performing such tasks.We hope these results can provide a reference for research in the field of long-context LLMs.\n\nLong-context LLM The computational cost of processing sequences in Transformer-based models increases quadratically with sequence length, resulting in higher resource consumption and performance issues when handling long context inputs.",
            "score": 0.5285653860207242,
            "section_title": "Input:",
            "char_start_offset": 2631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 44,
                    "end": 60
                },
                {
                    "start": 60,
                    "end": 109
                },
                {
                    "start": 109,
                    "end": 208
                },
                {
                    "start": 210,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 351
                },
                {
                    "start": 351,
                    "end": 503
                },
                {
                    "start": 503,
                    "end": 578
                },
                {
                    "start": 578,
                    "end": 884
                },
                {
                    "start": 884,
                    "end": 1049
                },
                {
                    "start": 1049,
                    "end": 1259
                },
                {
                    "start": 1259,
                    "end": 1486
                },
                {
                    "start": 1486,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1733
                },
                {
                    "start": 1733,
                    "end": 1826
                },
                {
                    "start": 1828,
                    "end": 2064
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46044921875
        },
        {
            "corpus_id": "269032933",
            "title": "RULER: What's the Real Context Size of Your Long-Context Language Models?",
            "text": "Long-context Language Models. Numerous long-context language models have been introduced lately owing to the progress in engineering, architectural, and algorithmic designs. Flash attention (Dao et al., 2022;Dao, 2023) and Ring attention (Liu et al., 2023) significantly reduce the memory footprint required for processing long context. Various sparse attention mechanisms (Child et al., 2019;Jaszczur et al., 2021) such as shifted sparse attention (Chen et al., 2024), dilated attention (Ding et al., 2023), and attention sinks (Han et al., 2023;Xiao et al., 2024b) were employed to enable efficient context scaling. Novel position embedding methods were proposed to improve length extrapolation in Transformers (Vaswani et al., 2017), including ALiBi (Press et al., 2022), xPOS (Sun et al., 2023b), and RoPE (Su et al., 2023) variants (Chen et al., 2023;Xiong et al., 2023;Peng et al., 2024;Liu et al., 2024b;Ding et al., 2024;Zhu et al., 2024). Another line of research focuses on reducing context size. This can be achieved by caching previous context using recurrence mechanism (Zhang et al., 2024a;Bulatov et al., 2023;Martins et al., 2022;Wu et al., 2022), retrieving relevant information from context (Xu et al., 2024a;Mohtashami & Jaggi, 2023;Wang et al., 2024;Tworkowski et al., 2024;Xiao et al., 2024a), or preserving the salient information via compression (Jiang et al., 2023).",
            "score": 0.5282044729095009,
            "section_title": "Related Work",
            "char_start_offset": 4868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1390
                }
            ],
            "ref_mentions": [
                {
                    "start": 393,
                    "end": 415,
                    "matchedPaperCorpusId": "244709266"
                },
                {
                    "start": 856,
                    "end": 875,
                    "matchedPaperCorpusId": "263134982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40283203125
        },
        {
            "corpus_id": "273025930",
            "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference with Partial Contexts",
            "text": "This paper argues that integrating the two stages not only builds an efficient long-context LLM with improved long-context performance, but also reduces the cost of length extension training. \n\nOur proposed approach LONGGEN, as illustrated in Figure 1, is a simple and effective hybrid transformer architecture that could be built upon any pretrained transformer to extend its context length. LONGGEN incorporates two key designs: (1) It conducts context extension with various GPU-friendly KV cache-saving designs, including but not limited to window attention (Jiang et al., 2023), attention sink (Xiao et al., 2024), and blockwise strided attention (Zaheer et al., 2020b). \n\nCompared with delicately curated KV cache reduction methods, LONGGEN employs a simpler and more efficient KV design, which maintains uniform memory access patterns for attention heads and achieves load balance among token blocks. Practically, our customized triton training kernel inherits from FlashAttention-2 (Dao, 2023), but achieves faster speed in sparse settings, and the inference kernel is well-suited to vLLM (Kwon et al., 2023) for high-throughput serving. \n\n(2) LONGGEN uses this sparse attention in a hybrid architecture, where 2/3 of the attention layers use sparse attention while the remaining 1/3 retain full attention, which we find is essential for handling complex tasks requiring direct access to long-context information( \u00a74.4). \n\nExperiments are conducted on a Llama-2-7B base model (Touvron et al., 2023) and its 70B counterpart with group query attention. We extend their context length from 4K to 128K and perform evaluations on multiple long-context benchmarks, including the needle-in-a-haystack (NIAH) retrieval, and two more distinguishing benchmarks, BABILong (Kuratov et al., 2024) and RULER (Hsieh et al., 2024), where long-context reasoning is also needed. Through ablations on the position and number of sparse attention layers, we study how sparsity level affects long-context capability.",
            "score": 0.5281499681866358,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1818,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1428
                },
                {
                    "start": 1431,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 2002
                }
            ],
            "ref_mentions": [
                {
                    "start": 599,
                    "end": 618,
                    "matchedPaperCorpusId": "263310483"
                },
                {
                    "start": 652,
                    "end": 674,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1097,
                    "end": 1116,
                    "matchedPaperCorpusId": "261697361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.449951171875
        },
        {
            "corpus_id": "277434967",
            "title": "Generative Reliability-Based Design Optimization Using In-Context Learning Capabilities of Large Language Models",
            "text": "Natural Language Processing (NLP) has evolved from rule-based approaches to data-driven methodologies. Early statistical models, such as n-gram models, relied on word frequency statistics for text prediction but were constrained by local dependencies and the curse of dimensionality. With the advancement of deep learning, Recurrent Neural Networks (RNNs) and their variants, including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), introduced sequential modeling to enable contextual awareness. However, these models still faced challenges in capturing long-range dependencies and were limited by inefficient parallelization during training. In 2017, Vaswani et al. introduced the foundational architecture for large language models: the Transformer. This architecture consists of two main components: an encoder and a decoder. The encoder extracts contextual information from the input sequence Figure 1: LLM Architecture and Prompt Engineering using a self-attention mechanism and transforms it into context-aware representations. The decoder then utilizes these representations to generate the output sequence. Within both the encoder and decoder, each layer comprises a multi-head attention mechanism and a feed-forward network. Attention is the core component of the Transformer, enabling it to capture dependencies across long sequences effectively. The attention mechanism operates using three key matrices: a query matrix Q \u2208 R n\u00d7d k , a key matrix K \u2208 R m\u00d7d k , and a value matrixV \u2208 R m\u00d7dv , where each row of these matrices corresponds to a word in the sequence. The attention computation is performed as follows: \n\nwhere the three matrices Q, K, and V are obtained through linear transformations: \n\nHere,X q \u2208 R n\u00d7d andX k v \u2208 R m\u00d7d represent the feature matrices of the query sequence (with sequence length n) and the key-value sequence (with sequence length m), respectively. The weight matrices W q , W k , and W v map the input feature space to the corresponding query, key, and value representations. \n\nMoreover, the multi-head attention mechanism enables the model to capture diverse contextual information across different subspaces, thereby enhancing its representational capacity.",
            "score": 0.527874668051134,
            "section_title": "Background of LLMs",
            "char_start_offset": 11958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1641
                },
                {
                    "start": 1644,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2034
                },
                {
                    "start": 2037,
                    "end": 2218
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.187255859375
        },
        {
            "corpus_id": "266844488",
            "title": "Long Context Compression with Activation Beacon",
            "text": "Large language models (LLMs) need to process long contexts to accomplish many important tasks, such as long-document understanding [27], long-content creation [4], and long-term memorization/reasoning [41]. To address these needs, modern LLMs are built with extended context windows (e.g., 128K) that enable remarkable long-context processing capabilities [33; 39; 18]. Despite their effectiveness, LLMs encounter efficiency challenges in processing long contexts. On one hand, transformer-based LLMs incur substantial computational costs due to the quadratic complexity of self attention. On the other hand, they require tremendous GPU memory to hold the KV cache of the entire sequence for faster decoding. Both computation and memory costs increase as the context length grows. \n\nA wide array of studies are dedicated to alleviating efficiency issues, among which context compression is a promising direction [32; 11; 20; 24; 25]. This approach aims to compress raw input into more concise representations, allowing the generation process to be conditioned on a shorter context. Therefore, it helps to reduce both computation cost of inference and memory cost from KV cache, while also enabling the processing of longer inputs than the LLM's built-in context window. \n\nDespite the current progresses, it it remains a tough challenge to compress long contexts. Specifically, existing methods usually summarize the context into a few soft tokens [11; 20], which constitute the major bottleneck to summarize the complex information within long contexts. Besides, they try to compress the context \"all-at-once\", lacking a fine-grained handling of the detailed information. Moreover, these soft tokens must be re-encoded before generation, resulting in inferior efficiency in both training and inference. Lastly, these methods are learned to compress with a fixed number of soft tokens, thus, it's hard to customize the compression ratio for downstream tasks. While some alternamtive methods focus on deleting unimportant tokens [25; 31], they depend on the input question to estimate the token importance, limiting their efficiency in real-world multi-turn scenarios.",
            "score": 0.5274210533800986,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1269
                },
                {
                    "start": 1272,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2166
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5576171875
        },
        {
            "corpus_id": "268857023",
            "title": "Long-context LLMs Struggle with Long In-context Learning",
            "text": "Large language models have already entered the long context era.A myriad of LLMs has been released to support long context windows from 32K to 2M tokens.These methods (Hao et al., 2022;Chen et al., 2023a;Peng et al., 2023b;Ratner et al., 2023;Xiao et al., 2024;Jin et al., 2024) can unlock lots of complex real-world applications, such as long-document question-answering, multi-document summarization, long-horizon agent tasks, and repo-level code understanding.\n\nOne line of research is based on AliBi (Press et al., 2022) and RoPE (Su et al., 2024) embeddings, which allows us to train Transformers with short sequences and subsequently apply them to longer sequences during inference.Recently, different approaches (Xiong et al., 2023;Fu et al., 2024;Liu et al., 2024) help the model to extrapolate to 128K window size with continued pre-training.Later on, LongRoPE (Ding et al., 2024) was proposed to further extend the context window to 2M tokens.Another line of research also utilizes methodologies like context window sliding and segmentation to overcome the issue of the limited context window in original Transformers (Hao et al., 2022;Ratner et al., 2023).Furthermore, architectural innovations, transitioning from traditional Transformer-based designs to recurrent models or state space models, have shown promise in facilitating long-range computations naturally (Orvieto et al., 2023;Gu & Dao, 2023;Peng et al., 2023a).These techniques have been incorporated into several current open-source LLMs to enhance long sequence understanding capability (Chen et al., 2023b;Tworkowski et al., 2023).",
            "score": 0.5268066030927275,
            "section_title": "Introduction",
            "char_start_offset": 295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 64,
                    "end": 153
                },
                {
                    "start": 153,
                    "end": 463
                },
                {
                    "start": 465,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 851
                },
                {
                    "start": 851,
                    "end": 953
                },
                {
                    "start": 953,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1433
                },
                {
                    "start": 1433,
                    "end": 1606
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 243,
                    "matchedPaperCorpusId": "258686160"
                },
                {
                    "start": 243,
                    "end": 261,
                    "matchedPaperCorpusId": "263310483"
                },
                {
                    "start": 504,
                    "end": 524,
                    "matchedPaperCorpusId": "237347130"
                },
                {
                    "start": 534,
                    "end": 551,
                    "matchedPaperCorpusId": "233307138"
                },
                {
                    "start": 1146,
                    "end": 1166,
                    "matchedPaperCorpusId": "258686160"
                },
                {
                    "start": 1413,
                    "end": 1432,
                    "matchedPaperCorpusId": "258832459"
                },
                {
                    "start": 1561,
                    "end": 1581,
                    "matchedPaperCorpusId": "262084134"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64794921875
        },
        {
            "corpus_id": "258987968",
            "title": "Blockwise Parallel Transformer for Large Context Models",
            "text": "Transformers [52] have become the backbone of many state-of-the-art natural language processing models [15,43,5,35]. They have demonstrated impressive performance across a wide range of AI problems, including language modeling, machine translation, image captioning, and protein folding [39,47,32,43,5,45,9]. Transformers achieve this success through their architecture design that uses self-attention and position-wise feedforward mechanisms. These components facilitate the efficient capture of long-range dependencies between input tokens, enabling scalability in terms of context length and model size through highly parallel computations. \n\nHowever, the memory requirements of Transformers limit their ability to handle long sequences, which is necessary for many AI problems, such as high-resolution images, podcasts, code, or books and especially those that involve multiple long sequences or long-term dependencies [10,7,39,7,34,29,47,32,1]. The quadratic self-attention and the large feed forward network of Transformers require a large amount of memory, which makes it challenging to scale to longer input sequences. This limitation has led to various techniques proposed to reduce the memory requirements of Transformers, including sparse-approximation, low-rank approximation, and low precision approximation [see e.g. 51,24,22,11,25,36,54]. \n\nOne distinct line of research does not rely on approximation but instead focuses on computing exact self-attention with linear memory complexity. This approach leverages the observation that the softmax matrix in self-attention can be computed without materializing the full matrix [37]. This technique has led to the development of FlashAttention [14] and Memory Efficient Attention [42]. Both methods propose a blockwise computation of the self-attention softmax, demonstrating reduced memory requirements. (A), (B), and (C) show evaluation using one, eight A100, and 64 TPUv4, respectively, with a single sequence. Our method enables training sequences 32 times longer than vanilla attention-based Transformer [52], and 2 to 4 times longer than FlashAttention [14] and Memory Efficient Attention [42]. Section 3.1 provides a memory cost breakdown.",
            "score": 0.5265801252180842,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2206
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 110,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 110,
                    "end": 112,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "237260635"
                },
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 300,
                    "end": 302,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "231939146"
                },
                {
                    "start": 940,
                    "end": 943,
                    "matchedPaperCorpusId": "237260635"
                },
                {
                    "start": 943,
                    "end": 946,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 946,
                    "end": 948,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1331,
                    "end": 1334,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1334,
                    "end": 1337,
                    "matchedPaperCorpusId": "232110866"
                },
                {
                    "start": 1704,
                    "end": 1708,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 2119,
                    "end": 2123,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.380859375
        },
        {
            "corpus_id": "246017095",
            "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
            "text": "Sequence parallelism is designed for training Transformerbased models with longer input sequences so we investigated the maximum sequence length it can handle. Similarly, we still compared TP with and without PP. We fixed batch size as 64 for BERT Base and no PP was used.\n\nWe show the maximum sequence length of the BERT Base model in Figure 5. If we scale up to 64 GPUs, we can achieve around 3\u00d7 maximum sequence length on BERT Base. Another observation is splitting along the number of attention heads limits the input sequence length of tensor parallelism in Megatron, but our sequence parallelism can scale easily by  Table 3: Weak scaling results. P is the tensor or sequence parallel size. B and S are global batch size and sequence length, respectively. M and T denote max allocated memory/MB and tokens processed per second. OOM means that CUDA out of memory occurs. splitting a sequence into multiple chunks. When using the same 16 GPUs, our sequence parallelism still can achieve 1.4 times larger sequence length than tensor parallelism. The gap is expected to widen if we use 32GB GPUs instead of 16GB GPUs. Also, in Appendix, we investigate the maximum sequence length our system can handle when we use a smaller batch size. Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N . We leave it as our future work.",
            "score": 0.5263637279015289,
            "section_title": "Maximum sequence length",
            "char_start_offset": 15467,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00653839111328125
        },
        {
            "corpus_id": "269020190",
            "title": "Extending Context Window in Large Language Models with Segmented Base Adjustment for Rotary Position Embeddings",
            "text": "The Transformer model, since its inception by [1], has revolutionized the field of natural language processing (NLP) with its unparalleled ability to capture the intricacies of language through self-attention mechanisms. A pivotal feature of Transformer-based large language models (LLMs) is their capability for in-context learning (ICL) [2], enabling them to adapt to new tasks without explicit retraining, merely by conditioning on fewshot examples provided within their input context. This ability not only showcases the flexibility of Transformer-based models, but also underscores the importance of the context window-the span of tokens a model can consider at any given time. The size of this context window directly influences the number of examples that can be included for in-context learning, thereby impacting the model's performance on tasks requiring understanding and synthesis of information spread across longer texts. \n\nThe concept of the context window is foundational to understanding how Transformers operate. In essence, it determines the maximum scope of direct relationships and dependencies that the model can learn and leverage for prediction. A larger context window allows the inclusion of more examples for in-context learning, facilitating a richer understanding of context and enabling the model to make more informed predictions. Conversely, a smaller context window restricts the model's ability to capture long-range dependencies, potentially limiting its effectiveness in tasks that necessitate a comprehensive grasp of extended narratives or arguments. \n\nPosition encoding plays a crucial role in enabling Transformers to process sequential data. Unlike traditional sequential models such as RNN [3] and LSTM [4], Transformers do not inherently process data in sequence. Instead, they treat input as a set of tokens without any inherent order. Position encoding injects this missing sequence information, allowing the model to differentiate between the same word appearing in different positions within the text. The evolution from absolute to relative position encoding [5] has been a significant milestone in the development of Transformers, allowing models to better generalize to different sequence lengths and more effectively capture the relational dynamics within sequences. \n\nWhile techniques such as ALiBi [6] and LeX [7] enable length extrapolation, they risk insufficient long-range dependencies due to their explicit long-range decay.",
            "score": 0.5262689291842544,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1588
                },
                {
                    "start": 1591,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2317
                },
                {
                    "start": 2320,
                    "end": 2482
                }
            ],
            "ref_mentions": [
                {
                    "start": 1732,
                    "end": 1735,
                    "matchedPaperCorpusId": "2444500"
                },
                {
                    "start": 1745,
                    "end": 1748,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 2107,
                    "end": 2110,
                    "matchedPaperCorpusId": "3725815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2371826171875
        },
        {
            "corpus_id": "269294074",
            "title": "SpaceByte: Towards Deleting Tokenization from Large Language Modeling",
            "text": "The most straight-forward consequence of modeling bytes instead of subword tokens is that the length of a sequence typically increases by about a factor of four. This increased sequence length increases the training and inference compute cost for modeling a given long sequence of text for a Transformer due to the quadratic scaling of attention. \n\nMegaByte The MegaByte architecture strives to use multiscale Transformer modeling to lessen these performance issues. In particular, MegaByte groups bytes into patches of a fixed patch size P . Each patch of bytes is vectorized and then fed into a \"global\" Transformer model. The output of the global model is then fed into a \"local\" Transformer model that autoregressively outputs byte-level logits. [7] For a context size of T bytes, MegaByte's global Transformer model compresses the context into only T /P patches, which can significantly decrease the compute cost for modeling long sequences. Similar to Yu et al. [7], we also find that MegaByte outperforms a standard byte-level Transformer. However, we find that MegaByte's performance is remarkably close to a stronger byte-level Transformer baseline that simply uses a sliding window attention mechanism [29][30][31] to increase the context size without increasing the compute costs. \n\nYu et al. [7] do not compare MegaByte to subword-level Transformer in compute controlled experiments. In our compute controlled experiments, we find that MegaByte's performance significantly lags behind a subword-level Transformer. \n\nCompared to MegaByte, SpaceByte makes the crucial change that patches are dynamically sized to be commensurate with the text, e.g. with word boundaries. We also add an additional local model before the global model (while MegaByte only utilizes a single local model after the global model) to help the model deal with the dynamical patch sizes. We also use significantly longer attention windows for our local models. We find that these changes allow SpaceByte to significantly improve upon the performance of MegaByte and roughly match the performance of subword-level Transformers.",
            "score": 0.5262444011132695,
            "section_title": "Related Work",
            "char_start_offset": 6562,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2111
                }
            ],
            "ref_mentions": [
                {
                    "start": 750,
                    "end": 753,
                    "matchedPaperCorpusId": "258676582"
                },
                {
                    "start": 968,
                    "end": 971,
                    "matchedPaperCorpusId": "258676582"
                },
                {
                    "start": 1220,
                    "end": 1224,
                    "matchedPaperCorpusId": "229924221"
                },
                {
                    "start": 1304,
                    "end": 1307,
                    "matchedPaperCorpusId": "258676582"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13818359375
        },
        {
            "corpus_id": "269899568",
            "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers",
            "text": "Transformer-based models have emerged as one of the most widely used architectures for natural language processing, natural language generation, and image generation. The size of the state-of-the-art models has increased steadily reaching billions of parameters. These huge models are memory hungry and incur significant inference latency even on cutting edge AI-accelerators, such as GPUs. Specifically, the time and memory complexity of the attention operation is quadratic in terms of the total context length, i.e., prompt and output tokens. Thus, several optimizations such as key-value tensor caching and FlashAttention computation have been proposed to deliver the low latency demands of applications relying on such large models. However, these techniques do not cater to the computationally distinct nature of different phases during inference. To that end, we propose LeanAttention, a scalable technique of computing self-attention for the token-generation phase (decode-phase) of decoder-only transformer models. LeanAttention enables scaling the attention mechanism implementation for the challenging case of long context lengths by re-designing the execution flow for the decode-phase. We identify that the associative property of online softmax can be treated as a reduction operation thus allowing us to parallelize the attention computation over these large context lengths. We extend the\"stream-K\"style reduction of tiled calculation to self-attention to enable parallel computation resulting in an average of 2.6x attention execution speedup over FlashAttention-2 and up to 8.33x speedup for 512k context lengths.",
            "score": 0.5254427988432386,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26611328125
        },
        {
            "corpus_id": "215737171",
            "title": "Longformer: The Long-Document Transformer",
            "text": "Transformers (Vaswani et al., 2017) have achieved state-of-the-art results in a wide range of natural language tasks including generative language modeling (Dai et al., 2019;Radford et al., 2019) and discriminative language understanding (Devlin et al., 2019). This success is partly due to the self-attention component which enables the network to capture contextual information from the entire sequence. While powerful, the memory and computational requirements of self-attention grow quadratically with sequence length, making it infeasible (or very expensive) to process long sequences on current hardware. \n\nTo address this limitation, we present Longformer, a modified Transformer architecture with a self-attention operation that scales linearly with the sequence length, making it versatile for processing long documents (Fig. 1). This is an advantage for natural language tasks such as long document classification, question answering (QA), and coreference resolution, where existing approaches partition or shorten the long context into smaller sequences that fall within the typical 512 token limit of BERT-style pretrained models. Such partitioning could potentially result in loss of important cross-partition information, and to mitigate this problem, existing methods often rely on complex architectures to address such interactions. On the other hand, our proposed Longformer is able to build contextual representations of the entire context using multiple layers of attention, reducing the need for task-specific architectures. \n\nRecent work has addressed the computational inefficiency of Transformers on long sequences (see Tab. 1). However, they primarily focus on autoregressive language modeling, while the application of long document transformers to document-level NLP tasks in the transfer learning setting (Dai and Le, 2015;Peters et al., 2018;Howard and Ruder, 2018;Devlin et al., 2019) has remained largely unexplored. We address this gap and show that Longformer's attention mechanism can act as a arXiv:2004.05150v1 [cs.CL] 10 Apr 2020 drop-in replacement for the self-attention mechanism in pretrained Transformers, and leads to gains across a suite of document NLP tasks.",
            "score": 0.5254011112635154,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 35,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 156,
                    "end": 174,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 238,
                    "end": 259,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1832,
                    "end": 1850,
                    "matchedPaperCorpusId": "7138078"
                },
                {
                    "start": 1893,
                    "end": 1913,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365234375
        },
        {
            "corpus_id": "267740702",
            "title": "Linear Transformers with Learnable Kernel Functions are Better In-Context Models",
            "text": "The Vanilla Transformer architecture (Vaswani et al., 2017), although widely used in NLP (Radford et al., 2019;Touvron et al., 2023;Devlin et al., 2019;Jiang et al., 2023), suffers from growing computational and memory demands (O ( *  2 ) as sequence lengths () and head size () increase). While this is not much of a problem when it comes to shorter sequences, it becomes a significant bottleneck when working with longer ones. \n\nSeveral alternative architectures were proposed to address this issue. Katharopoulos et al. (2020) suggested substituting the attention mechanism's exponential function, which is meant to measure the similarity between queries and keys, with a product of kernel functions that can be separately evaluated for queries and keys. This kernel-based approach reshapes the computation within the attention mechanism, cutting the time and memory complexity to O ( 2 * ). Additionally, during inference, it supports sampling sequences with linear complexity regarding length, similar to RNNs (Hochreiter and Schmidhuber, 1997;Chung et al., 2014). \n\nIn a different approach, State Space Models (SSMs) borrow from control theory to offer a simplified structure akin to RNNs, but without activation functions across time (Gu et al., 2022;Smith et al., 2023;Gu et al., 2023). The Mamba model, also known as S6 (Gu and Dao, 2023), stands out in this category, displaying enhanced learning of shortterm dependencies in texts compared to existing pre-trained LLMs (Jiang et al., 2023;Touvron et al., 2023). \n\nDespite these advancements, there is no standard way to fully evaluate these innovative architectures to assess their performance limits. One standard evaluation method is to pre-train a language model and assess its perplexity with a given dataset, but this may not truly reflect the model's ability to manage long context dependencies. Another option is to use the Long Range Arena (LRA) benchmark, which involves classification tasks with long input sequences.",
            "score": 0.5250651833477608,
            "section_title": "Recent Work",
            "char_start_offset": 2938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1522
                },
                {
                    "start": 1525,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1988
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 59,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 132,
                    "end": 152,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1258,
                    "end": 1277,
                    "matchedPaperCorpusId": "251442769"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1951904296875
        },
        {
            "corpus_id": "263671659",
            "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training",
            "text": "In this work, we introduce DISTFLASHATTN, a distributed memory-efficient attention prototype for long-context transformer training based on sequence parallelism. DISTFLASHATTN presents novel system optimizations including load balancing for causal language modelings, overlapped communication with computation in the distributed attention computation, and a re-materialization-aware checkpointing strategy. Experiments evaluate multiple families of transformer models and on different cluster types, and over four strong distributed system baselines. In particular, DISTFLASHATTN has demonstrated up to 2.01\u00d7 speedup and scales up to 8x longer sequences, compared to the popular system, Megatron-LM with FlashAttention.",
            "score": 0.5246023733179316,
            "section_title": "Conclusion",
            "char_start_offset": 25673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 719
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.127685546875
        },
        {
            "corpus_id": "264426090",
            "title": "Extending Input Contexts of Language Models through Training on Segmented Sequences",
            "text": "Let p \u03b8 be a transformer-based language model trained to maximize the next-token-probabilities over a set of sequences D of length L t ; i.e., arg max \n\nWe will refer to L t as the model's training input context length. \n\nWe define extrapolation as the language model's ability to improve its next-token-prediction by using input contexts that are longer than those it trained on. Specifically, for k > L t , we will consider that a model can extrapolate successfully if \n\nIn practice, we consider the average perplexity on sequences of different lengths from the same dataset a suitable proxy for this. \n\nGiven p \u03b8 and L t , the problem that we want to solve is to develop resource efficient methods that allow p \u03b8 to extrapolate to input contexts of length L e that are longer than L t . We refer to L e as the extended input context length.",
            "score": 0.5245639481853431,
            "section_title": "Problem Statement",
            "char_start_offset": 9246,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 153,
                    "end": 219
                },
                {
                    "start": 222,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 843
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09368896484375
        },
        {
            "corpus_id": "258987968",
            "title": "Blockwise Parallel Transformer for Large Context Models",
            "text": "Section 3.1 provides a memory cost breakdown. \n\nDespite the resulting reduced memory requirements of the self-attention block in Transformer models, a significant challenge still arises from the feedforward network. This network contains a large number of parameters and produces high-dimensional intermediate vectors, resulting in substantial memory requirements. This issue is becomes the key memory challenge once employing memoryefficient attention mechanisms. Consequently, training Transformers on longer context lengths and scaling up Transformer models become significantly hindered due to the overwhelming memory demands imposed by the feedforward network. \n\nTo address this challenge, we make an important observation: when self-attention is computed in a blockwise manner to reduce memory requirements, it becomes feasible to merge the computation of the feedforward network. This eliminates the need to wait for the self-attention computation to finish before performing the feedforward step on the entire sequence. By computing the feedforward network on a block-by-block basis, we effectively reduce the memory cost associated with the feedforward network. This process involves the utilization of two nested loops over the input sequence blocks. In the outer loop, we iterate over each block and compute the query. In the inner loop, we iterate over each block to calculate the key and value. These key-value pairs, along with the query, are then used to compute the blockwise attention specific to the corresponding input block. This blockwise attention is subsequently used to calculate the output of the feedforward network, followed by a residual connection. This approach enables us to process longer input sequences while maintaining lower memory budget. Since our approach performs blockwise parallel computation and fuses the feedforward and self-attention computations, we name our method the Blockwise Parallel Transformer (BPT). \n\nWe evaluate the effectiveness of our approach on several benchmarks, including language modeling and reinforcement learning. Our experiments show that BPT can reduce the memory requirements of Transformers, enabling us to train 32 times longer sequence than vanilla attention [52] based GPT models and up to 4 times longer sequence than prior state-of-the-arts FlashAttention [14] and Memory Efficient Attention [42]. Furthermore, we demonstrate the application of BPT on the task of traning Transformer based RL agent.",
            "score": 0.5244824160102592,
            "section_title": "Introduction",
            "char_start_offset": 2176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 48,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1954
                },
                {
                    "start": 1957,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2374
                },
                {
                    "start": 2375,
                    "end": 2476
                }
            ],
            "ref_mentions": [
                {
                    "start": 2333,
                    "end": 2337,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34521484375
        },
        {
            "corpus_id": "261214487",
            "title": "Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers",
            "text": "Transformers (Vaswani et al., 2017) have become a fundamental model architecture for sequential data modeling (Sun et al., 2019a;Dosovitskiy et al., 2021;Wysocki et al., 2023), especially in Natural Language Processing (NLP) (Devlin et al., 2018;Brown et al., 2020), where texts are regarded as sequences of tokens. Built with transformer blocks, pre-trained language models (PLMs) have recently shown astonishing empirical performance in various NLP tasks such as question answering (Yang et al., 2019), controllable generation (Byrne et al., 2021;Cheng and Li, 2022), summarization (Rush et al., 2015;Nallapati et al., 2016) and logic reasoning (Wei et al., 2022;Cheng et al., 2024a). \n\nHowever, one fatal weakness, that has hindered transformer-based models from being applied in broader application scenarios, is the quadratically raised computational consumption of self-attention operations when the input length increases. Hence, vanilla transformers have continuously been challenged by long-context tasks, such as machine reading comprehension (Kwiatkowski et al., 2019;Gong et al., 2020;Pang et al., 2022) and longtext summarization (Huang et al., 2021;Ma et al., 2022). \n\nTo enhance transformers with more efficient long-sequence processing, prior works focus on two perspectives, efficient attention operations (Beltagy et al., 2020;Zaheer et al., 2020;Choromanski et al., 2020) and sub-sequence processing (Liu et al., 2022). Efficient attention targets on reducing the memory and calculation cost of self-attention operations while preserving transformers' empirical performance on downstream tasks. Unfortunately, most efficient attention methods require customized self-attention implementations, which demand from-scratch training instead of being directly plugged into existing pre-trained models. Moreover, some empirical studies have demonstrated that efficient-attention methods inevitably sacrifice the short-sequence processing performance compared with full-attention models (Phang et al., 2022).",
            "score": 0.5240630540242284,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 35,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 110,
                    "end": 129,
                    "matchedPaperCorpusId": "102483628"
                },
                {
                    "start": 129,
                    "end": 154,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 484,
                    "end": 503,
                    "matchedPaperCorpusId": "59604492"
                },
                {
                    "start": 529,
                    "end": 549,
                    "matchedPaperCorpusId": "229363691"
                },
                {
                    "start": 584,
                    "end": 603,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 603,
                    "end": 626,
                    "matchedPaperCorpusId": "8928715"
                },
                {
                    "start": 647,
                    "end": 665,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1053,
                    "end": 1079,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1097,
                    "end": 1115,
                    "matchedPaperCorpusId": "245218982"
                },
                {
                    "start": 1143,
                    "end": 1163,
                    "matchedPaperCorpusId": "233033613"
                },
                {
                    "start": 1163,
                    "end": 1179,
                    "matchedPaperCorpusId": "226289939"
                },
                {
                    "start": 1345,
                    "end": 1365,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1419,
                    "end": 1437,
                    "matchedPaperCorpusId": "249063064"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14697265625
        },
        {
            "corpus_id": "267949483",
            "title": "TLS-RWKV: Real-Time Online Action Detection with Temporal Label Smoothing",
            "text": "While the Transformer model has demonstrated remarkable capability in handling long-distance dependencies, it is hindered by the square compute complexity of cross self-attention. To address this challenge, a variety of approaches have been proposed. Some focus on optimizing the attention mechanism, employing techniques such as sparse self-attention [26], kernelization [21], low-rank approximations [27], and other methods [28,29]. Other researches explore alternative modules to replace attention. MLP-Mixer [30] replaces attention with Multilayer Perceptrons(MLPs), while the Attention Free Transformer(AFT) [31] introduces a computationally efficient alternative to the traditional dot-product self-attention mechanism. Inspired by AFT, RWKV [17] simplifies interaction weights to enable a RNN-style implementation for inference. For a comprehensive overview of more efficient Transformer variants, a survey by Tay et al. [32] can be referenced. Additionally, some approaches modify recurrent neural networks(RNN) to increase context length, such as the Recurrent Memory Transformer [33], Linear Recurrent Unit [34], and state space models(SSM) [35][36][37][38]. These techniques offer alternative strategies to enhance the context modeling capabilities of sequence-based models.",
            "score": 0.5233212815322117,
            "section_title": "Efficient Long Sequence Modeling",
            "char_start_offset": 4427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1285
                }
            ],
            "ref_mentions": [
                {
                    "start": 372,
                    "end": 376,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 430,
                    "end": 433,
                    "matchedPaperCorpusId": "247011581"
                },
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "233714958"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1089,
                    "end": 1093,
                    "matchedPaperCorpusId": "250526424"
                },
                {
                    "start": 1151,
                    "end": 1155,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 1155,
                    "end": 1159,
                    "matchedPaperCorpusId": "247762199"
                },
                {
                    "start": 1159,
                    "end": 1163,
                    "matchedPaperCorpusId": "260443992"
                },
                {
                    "start": 1163,
                    "end": 1167,
                    "matchedPaperCorpusId": "251442769"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12249755859375
        },
        {
            "corpus_id": "259262301",
            "title": "LongCoder: A Long-Range Pre-trained Language Model for Code Completion",
            "text": "Additionally, increasing the window length would also introduce additional computational overhead. Different from these works, LongCoder is a sparse Transformer that can take advantage of the entire file-level code context while maintaining comparable efficiency in terms of computational resources during inference. \n\nLong-Range Transformer Models The original Transformer (Vaswani et al., 2017) is inefficient for modeling long sequences since its time and space complexity is O(n2 ), where n is the length of the sequence. Prior studies focus on optimizing the complexity to enable processing of longer sequences. To name a few, Sparse Transformer (Child et al., 2019) reduces the quadratic complexity of standard selfattention by computing attention on sparse query-key pairs. Sparse Transformer uses a dilated sliding window to capture local context. Reformer (Kitaev et al., 2020) proposes locality sensitive hashing (LSH) attention to reduce the complexity and memory footprint. Longformer (Beltagy et al., 2020) uses dilated sliding windows to model longer sequences and adds global memory tokens to allow interaction with all tokens. Performer (Choromanski et al., 2021) generalizes attention calculation by introducing kernel functions. They then propose a random kernel function, namely orthogonal random features (ORF) to approximate the standard selfattention. Linformer (Wang et al., 2020) applies low-rank projection to the length dimension to reduce the complexity of self-attention. Linear Transformers (Katharopoulos et al., 2020) uses a kernel function that exploits the associativity property of matrix products to reduce complexity. BigBird (Zaheer et al., 2020) has an attention pattern comprised of random attention, window attention and global attention. CosFormer (Qin et al., 2022) proposes a linear operator and a cosine-based distance re-weighting mechanism as the substitute for softmax attention. We recommend Tay et al. (2022) as a more comprehensive survey on longrange efficient Transformer models. Different from these works, our LongCoder introduces code heuristics into the dynamic construction of global attention to imitate how human programmers code.",
            "score": 0.5230464877564839,
            "section_title": "Related Work",
            "char_start_offset": 5172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 396,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 865,
                    "end": 886,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 997,
                    "end": 1019,
                    "matchedPaperCorpusId": "215737171"
                },
                {
                    "start": 1520,
                    "end": 1548,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1662,
                    "end": 1683,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1940,
                    "end": 1957,
                    "matchedPaperCorpusId": "221702858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09979248046875
        },
        {
            "corpus_id": "273821735",
            "title": "The Evolution of RWKV: Advancements in Efficient Language Modeling",
            "text": "Across these diverse applications, several key performance characteristics of RWKV models have emerged: \n\n1. Efficiency: RWKV models consistently demonstrate lower computational and memory requirements compared to Transformer models of similar capacity, especially for long sequences. The time complexity for processing a sequence of length n can be expressed as: \n\nThis linear scaling of RWKV models allows for processing of much longer sequences than is practical with Transformer models. 2. Scalability: RWKV architectures have shown good scaling properties, with performance improving as model size increases. The relationship between model size and performance can often be approximated as: This logarithmic scaling is similar to what has been observed in Transformer models, suggesting that RWKV models can benefit from increased model size in a similar manner. 3. Adaptability: The success of RWKV across various domains demonstrates its flexibility and potential as a general-purpose architecture for sequence modeling tasks. From natural language processing to computer vision and 3D point cloud processing, RWKV has shown competitive performance across a wide range of applications. 4. Long-range Modeling: RWKV models have shown strong capabilities in capturing long-range dependencies, often outperforming Transformer models on tasks requiring understanding of extended contexts. The effective context length can be expressed as: \n\nWhere n is the sequence length, \u03b5 is a small threshold, and w is the learned decay rate vector. This formulation allows RWKV models to adaptively adjust their effective context length based on the learned parameters. 5. Inference Speed: The constant-time inference characteristics of RWKV make it particularly well-suited for real-time applications and deployment on resource-constrained devices. This is in contrast to Transformers, where inference time grows linearly with the sequence length. The time to generate a single token can be expressed as: \n\nWhere n is the current sequence length. This constant-time inference property of RWKV models is particularly advantageous for applications requiring low-latency responses, such as real-time text generation or interactive systems.",
            "score": 0.5228674640155615,
            "section_title": "Performance Analysis",
            "char_start_offset": 33506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 106,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 363
                },
                {
                    "start": 366,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 1996
                },
                {
                    "start": 1999,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2228
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12127685546875
        },
        {
            "corpus_id": "248069124",
            "title": "C-NMT: A Collaborative Inference Framework for Neural Machine Translation",
            "text": "This highlights a key problem of CI for RNN-based NMT. That is, estimating the total execution time of both encoder and decoder is key to perform correct edge/cloud mapping decisions. However, while the compute time of the encoder linearly depends on the (known) input sentence length N , the decoder RNN's execution time depends on the unknown (prior to the completion of the translation) output length M . A similar issue arises also for Transformers. These models include several layers, but the most computationally critical is self-attention [1], [14], shown in Fig. 1c. For each input element, this layer generates three vectors called query (q i ), key (k i ) and value (v i ) through learned linear mappings, omitted in the figure for space reasons. The scalar product of each query with all keys, followed by a softmax, produces the so-called attention weights w ji . Finally, the i-th output is generated by summing together all v j s, each weighted by the corresponding w ji . In the figure, the flow of operations to generate the first two outputs is shown by red and green arrows respectively. State-of-the-art transformers combine multiple of such structures (so-called attention heads) for higher accuracy. As for RNNs, transformer encoders typically use the output corresponding to the last (or first) input, further processed by fully-connected layers, as context. \n\nThe complexity of self-attention is quadratic in the input length due to query-key products; however, differently from RNNs, the processing of different sequence elements can be parallelized [14]. Consequently, for relatively short input sequences (< 100 tokens) and considering a highly parallel platform (e.g., an embedded GPU) we found that the inference time of Transformer encoders is approximately constant w.r.t. N . In contrast, autoregressive decoding, which is implemented in Transformers with masked attention [1], imposes a strict dependency among subsequent tokens, i.e., the i-th predicted word is needed as input for predicting the (i + 1)-th, limiting parallelization.",
            "score": 0.5225842592680857,
            "section_title": "A. CI for Seq2Seq Deep Learning Models",
            "char_start_offset": 7183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1381
                },
                {
                    "start": 1384,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 2068
                }
            ],
            "ref_mentions": [
                {
                    "start": 547,
                    "end": 550,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0236968994140625
        },
        {
            "corpus_id": "276079501",
            "title": "Rope to Nope and Back Again: A New Hybrid Attention Strategy",
            "text": "Long-context large language models (LLMs) have achieved remarkable advancements, driven by techniques like Rotary Position Embedding (RoPE) (Su et al., 2023) and its extensions (Chen et al., 2023; Liu et al., 2024c; Peng et al., 2023). By adjusting RoPE parameters and incorporating training data with extended contexts, we can train performant models with considerably longer input sequences. However, existing RoPE-based methods exhibit performance limitations when applied to extended context lengths. This paper presents a comprehensive analysis of various attention mechanisms, including RoPE, No Positional Embedding (NoPE), and Query-Key Normalization (QK-Norm), identifying their strengths and shortcomings in long-context modeling. Our investigation identifies distinctive attention patterns in these methods and highlights their impact on long-context performance, providing valuable insights for architectural design. Building on these findings, we propose a novel architectural based on a hybrid attention mechanism that not only surpasses conventional RoPE-based transformer models in long context tasks but also achieves competitive performance on benchmarks requiring shorter context lengths.",
            "score": 0.5214891437808409,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.378173828125
        },
        {
            "corpus_id": "207853129",
            "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning",
            "text": "BPT improves the time/space complexity of Transformer models from O(d \u2022 n2 ) to O(d \u2022 k \u2022 n log n/k) in theory, such speedup cannot be achieved by tensor-based attention operators. To address this problem, we designed a set of CUDA kernels for sparse attentions3 . \n\nWe compare the GPU memory footprint and throughput of BPT and vanilla Transformer during inference under the same setting4 for language modeling. The k is set to 1, 4, 16, 64 respectively, covering best settings for word-based tasks(k = 4) and character-based tasks(k = 64). We fix the number of tokens to 8192 each batch and varies the sequence length. Figure 6 and 7 depicts how the GPU memory and speed varies as we increases sequence length. \n\n2,000 4,000 6,000 8,000 We show that BPT consistently utilizes less GPU memory compared to Transformer, making it possible to be applied on tasks that require long sequence modeling such as time-series prediction. \n\nAs for speed, BPT increases the number of nodes from n to 2n which brings additional overhead linear to sequence length, rendering BPT not as fast as Transformer when dealing with short text. However, as the sequence length grows, the speed of BPT is steady while Transformer become too slow for practical use.",
            "score": 0.5214376832055113,
            "section_title": "Throughput and GPU Memory Footprint",
            "char_start_offset": 22136,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 264
                },
                {
                    "start": 267,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1241
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08770751953125
        },
        {
            "corpus_id": "235294151",
            "title": "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling",
            "text": "Complexity Table 3: Complexity of different methods. K is sentence length, M is the number of sentences in a document, T is the number of positions for sparse attention, and d is the hidden dimension.\n\nat both word and sentence levels. The results of these methods on the three datasets are shown in Table 2. We find that Transformers designed for long documents like Hi-Transformer and BigBird outperform the vanilla Transformer. This is because vanilla Transformer cannot handle long sequence due to the restriction of computation resources, and truncating the input sequence leads to the loss of much useful contextual information. In addition, Hi-Transformer and HI-BERT outperform Longformer and BigBird. This is because the sparse attention mechanism used in Longformer and Big-Bird cannot fully model the global contexts within a document. Besides, Hi-Transformer achieves the best performance, and the t-test results show the improvements over baselines are significant. This is because Hi-Transformer can incorporate global document contexts to enhance sentence modeling.\n\nWe also compare the computational complexity of these methods in Table 3. The complexity of Hi-Transformer is much less than the vanilla Transformer and is comparable with other Transformer variants designed for long documents. These re-sults indicate the efficiency and effectiveness of Hi-Transformer.",
            "score": 0.5213793001446333,
            "section_title": "Method",
            "char_start_offset": 8054,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07421875
        },
        {
            "corpus_id": "259187506",
            "title": "Block-State Transformers",
            "text": "Transformers have shown impressive performance on a wide range of natural language processing (NLP) tasks. While they have been primarily used for language modeling the Transformer architecture [40] has also been successfully applied to other tasks outside of the NLP and have mostly replaced Recurrent Neural Networks (RNNs). Several factors contribute to this success, including computational efficiency and architectural inductive biases that are well-suited for training on natural language tasks at scale. On the computational upside, Transformers are able to process tokens of a given input sequence in parallel, making the most of modern accelerator hardware. Moreover, the attention mechanism enables Transformers to find relationships in longer sequences by providing ready access to all the extracted information from past tokens when inferring the next token. Compared to RNNs and LSTMs [19], the benefits of self-attention are two-fold: (i) the capacity of what could be stored and directly accessible as context is drastically increased, and (ii) training on longer sequences is more stable [18,23]. \n\nGiven the remarkable achievements of Transformers in language modeling tasks, and their improved performance at scale on hard NLP tasks such as reasoning and question answering [2,39,6], the demand for deploying even deeper and larger networks is greater than ever before. An orthogonal scaling dimension, which could be potentially even more consequential, is the size of the input sequence. Despite the several advantages of Transformers over RNNs, it is still problematic to scale the input sequence length, again for both computational performance and quality reasons. Further, the Transformer's runtime is quadratic with respect to the input sequence length, which makes training these models increasingly expensive. Furthermore, Transformers with attention, that is local [8], sparse [4,43,36], low-rank approximated [41] or linearized via kernel methods [5,22], notoriously struggle on long-input classification tasks [37]. Vanilla transformers can be unstable when trained on long sequences [26] and token importance is concentrated in a local receptive field of around 50 tokens around the current time step [35].",
            "score": 0.5209488204849507,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1112
                },
                {
                    "start": 1115,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1104,
                    "end": 1108,
                    "matchedPaperCorpusId": "18452318"
                },
                {
                    "start": 1292,
                    "end": 1295,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1908,
                    "end": 1911,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1911,
                    "end": 1914,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1979,
                    "end": 1982,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.160888671875
        },
        {
            "corpus_id": "261602660",
            "title": "CombTransformers: Statement-Wise Transformers for Statement-Wise Representations",
            "text": "In this work, we designed some novel architectures (Comb-Transformers) based on the Transformer. The comb Transformers are all designed to deal with long sequences, specifically when these sequences can be seen as a sequence of sentences (e.g., methods are a sequence of statements). Comb Transformers effectively reduce the memory requirements enabling the training on longer sequences. They also achieve comparable results wrt. established architectures as the Longformer and the Transformer. There is no reason to prevent a CombTransformer architecture to be used in code-unrelated tasks. Different domains could benefit from this kind of architectures. This will be our future investigation. \n\nLastly, we provide a package to reproduce our experiments which is available at the following address: https://cazzola.di.unimi.it/comb-transformer.html",
            "score": 0.5204356573180369,
            "section_title": "IX. CONCLUSION",
            "char_start_offset": 49054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 695
                },
                {
                    "start": 698,
                    "end": 850
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.029876708984375
        },
        {
            "corpus_id": "258987968",
            "title": "Blockwise Parallel Transformer for Large Context Models",
            "text": "Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.",
            "score": 0.5204356573180369,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.424072265625
        },
        {
            "corpus_id": "250390967",
            "title": "Fine-tuning Transformers with Additional Context to Classify Discursive Moves in Mathematics Classrooms",
            "text": "The introduction of transformers has revolutionized the field of natural language processing. Unlike Recurrent Neural Networks (RNNs) and Long Short Term Memory networks (LSTMs), where training is performed sequentially, the design of transformer architecture enables parallel processing and allows for the creation of rich latent embeddings (Vaswani et al., 2017). Latent contextual representation of utterances through the self-attention mechanism makes transformers a powerful tool for various downstream applications such as question answering and text summarization (Devlin et al., 2018). \n\nResearch efforts to learn long-term dependencies with transformers were first introduced in Transformer-XL (Dai et al., 2019). Transformer-XL is a novel architecture that focuses on learning dependencies beyond the fixed length of vanilla transformers without disrupting the temporal coherence. This is achieved by saving the hidden state sequence of the previous segment to be used as context for the current segments, also known as the segment-level recurrence mechanism. In addition, to better encode the relationship between words, Transformer-XL uses relative positional embeddings. Results show that Transformer-XL can learn dependencies across the text with a window size of 900 words. Following Transformer-XL, (Yang et al., 2019) proposed XL-Net, which is a generalized autoregressive pretraining method that leverages the capabilities of Transformer-XL to solve the pre-train-finetune discrepancy commonly identified in early architectures such as BERT. XL-Net introduced two new developments. As an extension to the standard Causal Language Modeling (CLM), XL-Net uses permutation language mod-eling, which considers all possible permutations of the words within a sentence during the training phase. Also, XL-Net uses a secondary attention stream that focuses on the positional information of the predicted token. This additional attention stream led XL-Net to outperform many contemporary transformer architectures in downstream tasks, such as text classification. Similarly, to address the problem of processing long sequences with transformers, (Beltagy et al., 2020) introduced Longformer, which extends vanilla transformers with a modified self-attention mechanism to process long documents.",
            "score": 0.5202626117644533,
            "section_title": "Transformers for additional context and long-term dependencies",
            "char_start_offset": 5296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 342,
                    "end": 364,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1315,
                    "end": 1334,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12841796875
        },
        {
            "corpus_id": "276422382",
            "title": "APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs",
            "text": "The wall-time of each component is shown in Figure 5. Methods without sequence parallelism (FLASHATTN and MINFER-ENCE) exhibit substantial inference time, whereas sequence parallelism proves effective in reducing the time of both attention and FFN computations. \n\nCompared to existing sequence parallelism methods, APB achieves even lower attention time without incurring significant overhead for retaining heads and communication. When compared with RINGATTN and ULYSSES, although the introduction of the anchor blocks increases the compute cost for FFN, the additional time is smaller than the reduction in attention time, thus resulting in a faster inference speed. APB outperforming STARATTN is due to smaller anchor blocks, whose overhead in the FFN time is significantly lower. Distributed Settings. We evaluate the effectiveness of APB across various distributed settings. As mentioned in STARATTN (Acharya et al., 2024), when the context is split across more hosts, significant performance degradation occurs due to the increasing invisibility of the middle context. However, APB addresses this issue by using passing blocks, allowing each host to share the most essential KV pairs with subsequent hosts. \n\nWe test APB and STARATTN across {2, 4, 6, 8} hosts, and the results are shown in Table 4. When the sequence is long (128K tokens), both APB and STARATTN exhibit improved performance as the sequence parallelism size increases. With a shorter input length (32K tokens), the block size on each host is smaller, and APB demonstrates consistently stronger and more stable performance across all settings, while STARATTN shows significant performance loss as the host count increases.",
            "score": 0.5191621513823045,
            "section_title": "Ablation Studies",
            "char_start_offset": 23020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1693
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.051177978515625
        },
        {
            "corpus_id": "269740949",
            "title": "Linearizing Large Language Models",
            "text": "In Table 1 we report results on standard NLU evaluations using the Eleuther evaluation harness (Gao et al., 2023).We primarily compare to transformers and linear models at the 7B scale, and we train a Mamba model at 7B for comparison with RWKV-5.As our model is initialized from strong pre-trained transformers (Llama2 and Mistral-7B), it preserves performance on most benchmarks (except MMLU; see Section 4 for a discussion below).Our technique outperforms RWKV-5 with minimal uptraining and is competitive with our 7B Mamba trained from scratch on 1.2T tokens.Long Context.Recurrent models were thought to perform well on long-context tasks because of their ability to preserve performance beyond their training sequence size.However, their downstream performance on long-context tasks has not been well-documented.Prior studies either do not conduct long-context evaluations (Katharopoulos et al., 2020;Kasai et al., 2021), evaluate only on perplexity (Sun et al., 2023;De et al., 2024;Gu & Dao, 2023), or evaluate on datasets which require task-specific training (Peng et al., 2023a).Instead, we consider downstream natural language tasks from the SCROLLS benchmark (Shaham et al., 2022a).Specifically, in Table 2 we present two tasks -Qasper (Dasigi et al., 2021) and NarrativeQA (Ko\u010disk \u00fd et al., 2018) -from the set of tasks evaluated in the Llama2-Long report (Xiong et al., 2023).We evaluate both tasks with an input context cut-off at different lengths.A strong long-context model should perform better given more context.However, the training context lengths for these models do not go beyond 8k tokens.Transformer models show the strongest results up to the context length they were trained for but degrade beyond that.Interestingly, applying the YaRN trick (Peng et al., 2023b) enables transformers to scale beyond their training context quite well.RWKV shows a strong ability to handle much longer context than its training.",
            "score": 0.5188512864950516,
            "section_title": "Language Modeling.",
            "char_start_offset": 10351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 114,
                    "end": 246
                },
                {
                    "start": 246,
                    "end": 432
                },
                {
                    "start": 432,
                    "end": 562
                },
                {
                    "start": 562,
                    "end": 575
                },
                {
                    "start": 575,
                    "end": 728
                },
                {
                    "start": 728,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1389
                },
                {
                    "start": 1389,
                    "end": 1463
                },
                {
                    "start": 1463,
                    "end": 1532
                },
                {
                    "start": 1532,
                    "end": 1614
                },
                {
                    "start": 1614,
                    "end": 1731
                },
                {
                    "start": 1731,
                    "end": 1862
                },
                {
                    "start": 1862,
                    "end": 1938
                }
            ],
            "ref_mentions": [
                {
                    "start": 878,
                    "end": 906,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1170,
                    "end": 1192,
                    "matchedPaperCorpusId": "245836939"
                },
                {
                    "start": 1247,
                    "end": 1268,
                    "matchedPaperCorpusId": "234093776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2958984375
        },
        {
            "corpus_id": "267770255",
            "title": "\u221eBench: Extending Long Context Evaluation Beyond 100K Tokens",
            "text": "Extending Context Length Transformers, typically trained on text sequences under 8K tokens due to self-attention's quadratic complexity, face challenges in longer downstream tasks. To address this, two main strategies have emerged: firstly, the development of positional encodings capable of handling longer text sequences (Sun et al., 2022;Press et al., 2021), and secondly, the refinement of inference stage techniques to extend current LLMs post-training. The primary approach involves mod-ifying rotary positional encoding (Su et al., 2023) and implementing post-training adjustments to better manage the increased relative positional distances in longer sequences (Zhu et al., 2023;Peng et al., 2023b;Chen et al., 2023a). Inference Infrastructure Numerous studies aim to accelerate self-attention computation. Research primarily concentrates on refining attention mechanisms through improved IO management (Dao et al., 2022;Dao, 2023), memory optimization (Kwon et al., 2023;Shazeer, 2019;Ainslie et al., 2023), and enhanced parallelization in decoding (Dao et al., 2023;Hong et al., 2023). Approaches like Sliding Window Attention (Beltagy et al., 2020), LM-Infinite (Han et al., 2023), and StreamingLLM (Xiao et al., 2023) introduce attention variants for handling infinitely long sequences without overwhelming computation or memory overhead. However, these techniques often face challenges in maintaining historical information. \n\nLong Context Benchmarks Several benchmarks exist for evaluating long-context AI models, notably featuring context lengths of around 10K tokens. L-Eval (An et al., 2023) and Long-Bench (Bai et al., 2023) are prominent examples, aggregating pre-existing tasks (Kocisk\u00fd et al., 2017;Dasigi et al., 2021;Yang et al., 2018;Huang et al., 2021;Joshi et al., 2017)  LooGLE (Li et al., 2023), differentiates between short and long dependency examples, focusing on summary and QA tasks; its summary corpus contrasts with ours, utilizing academic papers over novels.",
            "score": 0.51881266846343,
            "section_title": "Related Work",
            "char_start_offset": 5296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1995
                }
            ],
            "ref_mentions": [
                {
                    "start": 527,
                    "end": 544,
                    "matchedPaperCorpusId": "233307138"
                },
                {
                    "start": 911,
                    "end": 929,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 961,
                    "end": 980,
                    "matchedPaperCorpusId": "261697361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.489013671875
        },
        {
            "corpus_id": "278339069",
            "title": "Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation",
            "text": "State Space Models Gu et al. (2022) proposed the S4 model, which is a promising alternative to transformers for capturing long-term dependencies. Building on S4, Gu and Dao (2023) introduced the data-dependent State Space Model (SSM) layer S6, and developed the Mamba (Gu and Dao, 2023) language model backbone. Mamba outperforms transformers of various sizes on large-scale realworld data and scales linearly with sequence length. Additionally, Dao and Gu (2024) provided insights into the performance of recent SSMs compared to transformers in the context of language modeling. Hybrid models like Jamba (Lieber et al., 2024) and Samba (Ren et al., 2024) aim to combine the strengths of attention mechanisms with Mamba's efficient long-range dependency modeling. \n\nLong-context Memory Some studies (Peng et al., 2024;Li et al., 2024;Ben-Kish et al., 2024) have highlighted that language models trained on fixed-length contexts tend to suffer performance degradation when applied to longer contexts. Transformer-based models, in particular, face significant computational challenges as the context length increases. For state space models, Deci-Mamba (Ben-Kish et al., 2024) introduced a nontraining method to filter out less important tokens, effectively reducing the input length. Additionally, Yuan et al. (2024) proposed a technique where a network is trained to compress and selectively retain essential information during the initial forward pass. Meanwhile, Ye et al. investigated the fundamental limitations of Mamba in handling long sequences and presented a principled approach to address these issues on various tasks.",
            "score": 0.5187772524582595,
            "section_title": "Related Work",
            "char_start_offset": 3954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1629
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 35,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 446,
                    "end": 463,
                    "matchedPaperCorpusId": "270199762"
                },
                {
                    "start": 799,
                    "end": 818,
                    "matchedPaperCorpusId": "261493986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.463623046875
        },
        {
            "corpus_id": "266999544",
            "title": "The What, Why, and How of Context Length Extension Techniques in Large Language Models - A Detailed Survey",
            "text": "Transformers designed for extended sequences address dual challenges: inadequate efficiency in processing or memory utilization for prolonged sequences and an intrinsic balance between efficacy and resource usage. Techniques such as linear attention (Wang et al., 2020;Katharopoulos et al., 2020;Choromanski et al., 2020), utilizing kernel-based or low-dimensional approximations, prioritize resource efficiency but often exhibit sub-optimal performance in typicallength scenarios. Sparse attention (Child et al., 2019b;Beltagy et al., 2020;Zaheer et al., 2020;Xiong et al., 2021), leveraging structured sparsity, provides a computational reduction strategy. Additionally, designs employing recurrent-style architectures (Dai et al., 2019b;Hutchins et al., 2022;Ma et al., 2022) for causal sequence modeling are contenders in handling these challenges. In this context, the emphasis is on addressing the extrapolation problem in language modeling-training on brief texts while evaluating extended texts (Press et al., 2021b). The training methodology aligns with conventional Transformers, encompassing training on brief sequences with concentrated attention computation. The advantage lies in seamlessly unlocking the potential for long-sequence modeling during inference without compromising training efficiency. This approach guarantees the retention of optimal performance for typical lengths, eliminating trade-offs associated with long-sequence modeling compared to earlier methodologies.",
            "score": 0.5185868132129892,
            "section_title": "Related work.",
            "char_start_offset": 46348,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1494
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 296,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 541,
                    "end": 561,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 740,
                    "end": 762,
                    "matchedPaperCorpusId": "247451135"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09283447265625
        },
        {
            "corpus_id": "220968768",
            "title": "FRMDN: Flow-based Recurrent Mixture Density Network",
            "text": "However, modeling long sequences is a challenging task with this basic architecture.Because long-term dependencies can be lost in this case and the context vector can not represent the knowledge well enough.\n\nTo overcome the problem of long sequences, attention-based models have been proposed in the literature.This approach lets the model focus on different parts of input at each output time-step (Bahdanau et al., 2014;Luong et al., 2015).In this model, based on the decoder network state in each time-step, a context vector is created from the hidden states of the encoder based on an attention mechanism.\n\nIt has been observed in the literature that RNN is not necessary in the encoder-decoder model and transformer networks that use simple feed-forward network together with attention mechanism works well (Vaswani et al., 2017).In many applications, such as speech recognition and machine translation, transformers are currently the-state-of-the-art model.\n\nApart from their hight accuracy, they are parallelizable, while the basic framework with RNNs can not be parallelize easily because of its sequential processing of their input.Although the transformer model has lots of advantages, it is suitable in the case of fixed-length sequences.While RNN-based models are free in terms of sequence length.This problem has been solved in the next generation of transformers named transformer-XL (Dai et al., 2019) by using a recursive module in transformers.\n\nIn classical HMMs used in discriminative tasks, the probability of the label given the observation sequence is computed using Bayes rule.However in then case when we have a sequence of target labels, a specific logic is exploited to make the training and inference tractable.HMMs that use this logic is called discriminant Markov model (Bourlard & Morgan, 1994) or sometimes conditional HMMs in the literature (Xiao et al., 2019).In discriminant HMMs, we have a HMM to model each label of the target sequence.Therefore, the model for the sequence of labels is constructed by concatenating the HMM of each label that would become HMM again.The way that neural networks are used in these models is similar to generative HMMs.",
            "score": 0.5185330120911753,
            "section_title": "Sequential generative models",
            "char_start_offset": 12089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 84,
                    "end": 207
                },
                {
                    "start": 209,
                    "end": 312
                },
                {
                    "start": 312,
                    "end": 443
                },
                {
                    "start": 443,
                    "end": 610
                },
                {
                    "start": 612,
                    "end": 836
                },
                {
                    "start": 836,
                    "end": 964
                },
                {
                    "start": 966,
                    "end": 1142
                },
                {
                    "start": 1142,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1310
                },
                {
                    "start": 1310,
                    "end": 1462
                },
                {
                    "start": 1464,
                    "end": 1601
                },
                {
                    "start": 1601,
                    "end": 1739
                },
                {
                    "start": 1739,
                    "end": 1894
                },
                {
                    "start": 1894,
                    "end": 1973
                },
                {
                    "start": 1973,
                    "end": 2103
                },
                {
                    "start": 2103,
                    "end": 2187
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 423,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 423,
                    "end": 442,
                    "matchedPaperCorpusId": "1998416"
                },
                {
                    "start": 813,
                    "end": 835,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1399,
                    "end": 1417,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1874,
                    "end": 1893,
                    "matchedPaperCorpusId": "198334710"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04620361328125
        },
        {
            "corpus_id": "270194460",
            "title": "SF-Transformer: A Mutual Information-Enhanced Transformer Model with Spot-Forward Parity for Forecasting Long-Term Chinese Stock Index Futures Prices",
            "text": "The Transformer model was originally proposed for natural language processing (NLP).After being proposed for several years, the advent of Transformers in the realm of deep learning has marked a paradigm shift, particularly in the context of NLP and various sequential data tasks [58][59][60][61][62]. Traditional sequence models for time sequence forecasting, such as recurrent neural networks (RNNs) [63][64][65] and long short-term memory networks (LSTMs) [66][67][68][69][70], faced challenges in capturing long-range dependencies and suffered from sequential processing inefficiencies.Recently, Transformers have rapidly emerged as the cornerstone of numerous cutting-edge models, owing to their capability to capture intricate patterns in sequential data.Transformers, proposed by Vaswani et al. [71], addressed these limitations by leveraging a novel mechanism called self-attention.\n\nMathematically, for a given input sequence X, the self-attention first computes its Query Q, Key K, and Value V Matrices:\n\nwhere X represents the input sequence, and W Q , W K , and W V are learnable weight matrices.\n\nBased on the compatibility between the query and key, attention scores can be computed to represent the importance assigned to each element in the sequence.\n\nwhere d k is the dimension of the key vectors, which is used to scale the dot product to mitigate issues related to vanishing gradients.The self-attention is the weighted sum that combines the values according to their corresponding attention scores, capturing the contextual information.\n\nThe self-attention mechanism allows each element within the sequence to simultaneously consider all other elements, capturing their contextual relationships in X effectively.\n\nTo enrich the expressive capacity of self-attention, Transformers utilize multi-head attention.This involves applying the attention mechanism multiple times in parallel, each with different learned linear projections.\n\nwhere W Qi , W Ki , and W Vi are learnable weight matrices specific to the i-th head.The outputs from these multiple heads are then concatenated and linearly transformed.\n\nThe outputs from the individual heads are concatenated and linearly transformed by the matrix W O to produce the final multi-head attention output.",
            "score": 0.5177810826040665,
            "section_title": "Transformer Neural Network Model",
            "char_start_offset": 13076,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 84,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 760
                },
                {
                    "start": 760,
                    "end": 889
                },
                {
                    "start": 891,
                    "end": 1012
                },
                {
                    "start": 1014,
                    "end": 1107
                },
                {
                    "start": 1109,
                    "end": 1265
                },
                {
                    "start": 1267,
                    "end": 1403
                },
                {
                    "start": 1403,
                    "end": 1555
                },
                {
                    "start": 1557,
                    "end": 1731
                },
                {
                    "start": 1733,
                    "end": 1828
                },
                {
                    "start": 1828,
                    "end": 1950
                },
                {
                    "start": 1952,
                    "end": 2037
                },
                {
                    "start": 2037,
                    "end": 2122
                },
                {
                    "start": 2124,
                    "end": 2271
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 287,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 291,
                    "end": 295,
                    "matchedPaperCorpusId": "244476905"
                },
                {
                    "start": 295,
                    "end": 299,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 405,
                    "end": 409,
                    "matchedPaperCorpusId": "158191176"
                },
                {
                    "start": 409,
                    "end": 413,
                    "matchedPaperCorpusId": "219142026"
                },
                {
                    "start": 458,
                    "end": 462,
                    "matchedPaperCorpusId": "59234048"
                },
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "14005387"
                },
                {
                    "start": 466,
                    "end": 470,
                    "matchedPaperCorpusId": "77389964"
                },
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "233580535"
                },
                {
                    "start": 474,
                    "end": 478,
                    "matchedPaperCorpusId": "252932563"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056121826171875
        },
        {
            "corpus_id": "242066144",
            "title": "One model Packs Thousands of Items with Recurrent Conditional Query Learning",
            "text": "The Transformer architecture [Vaswani et al., 2017] has been shown to excel in many seq2seq tasks, because the self-attention layer of Transformer enables the network to capture contextual information from the entire input sequence, thereby providing the relationship between the features of different input data. However, the computational and memory costs of such a network grow quadratically with the sequence length [Child et al., 2019] and thus it is hard to apply this method to long sequences. \n\nAs mentioned in Section 4, the packed state s s s p is saved in a FIFO to keep the context fresh. Although this dynamic state updating mechanism leads to smaller memory costs, for large scale packing problems, there is still a trade-off between memory cost and context length. That is, the model will overlook long-term dependencies when the context size is too small. The model simply cannot obtain the information of the state of packed boxes that are out of context. On the other hand, with a large context size the memory cost could greatly increase. \n\n(5) \n\nTo address the conflict between long-term dependence and memory cost, a recurrence feature is added to the Transformer layer to encode the packed state. In every transformer self-attention layer, a context cache of the previous hidden state h h h n\u22121 \u03c9 is concatenated with the current hidden state h h h n\u22121 \u03c9+1 , where \u03c9 denotes the context number, and n denotes the layer number. Note that this recurrence is different from that of Transformer-XL [Dai et al., 2019]. Here the context only moves one item forward per step since only one new packed box is available for the packed state in a packing step, so the previous hidden state h h h n\u22121 \u03c9 is also treated as FIFO. The recurrent attention encoder layer is produced by (4), where q q q, k k k, v v v denote query, key and value respectively in Transformer, and W W W q , W W W k , W W W v are the trainable parameters for q q q, k k k, v v v, respectively.",
            "score": 0.5176562926791737,
            "section_title": "Recurrent Attention Encoder",
            "char_start_offset": 24061,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1057
                },
                {
                    "start": 1060,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 51,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1516,
                    "end": 1534,
                    "matchedPaperCorpusId": "57759363"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07598876953125
        },
        {
            "corpus_id": "268793522",
            "title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models",
            "text": "The remainder of the paper is structured as follows. We provide a review of related literature in Section 2, before introducing the proposed ELITR-Bench in Section 3. \n\nWe then describe our experimental setup and results in Sections 4 and 5, respectively. Section 6 provides an in-depth assessment of our LLM-based evaluation methodology. Finally, Section 7 concludes the paper and provides some perspectives for future work. \n\ncontext modeling. 3 While an exhaustive survey of these methods is beyond the scope of this paper, they can generally be categorized into three main groups (excluding other distinct approaches such as retrieval-augmented generation [46] and context compression [10]): (a) the development of efficient transformer architectures to address the quadratic attention challenge, including sparse transformers [6,12,32,47], linear transformers [13,19,42], and hierarchical transformers [20,30,44]; (b) approaches like recurrent attention networks [7,14,36] and state-space models [16,41]; (c) length extrapolation or position embedding interpolation, where LLMs are fine-tuned or adapted at inference time to adjust tokens' positions to match the new context length [4,8,9,28,35,37,45]. These techniques also contributed to the context length expansion in proprietary models like GPT-4 (32K-128K), Claude-3 (200k), and Gemini-1.5 (128K-1M). \n\nLong-context benchmarks. Several benchmarks have recently emerged with the growing interest in evaluating techniques that extend the context length of LLMs. Long Range Arena [40] was proposed to assess the quality of efficient transformer models in longcontext scenarios, covering 1K-16K tokens sequences through different data types and modalities. L-Eval [1] offers a comprehensive evaluation suite with 20 subtasks and over 2,000 human-labeled query-response pairs, aggregating pre-existing datasets like Narra-tiveQA [22]. LongEval [25] proposes synthetic tasks of varying difficulty, while LongBench [3] and LongBench-Chat [4] aggregate several datasets in English and Chinese.",
            "score": 0.517488836494415,
            "section_title": "Introduction",
            "char_start_offset": 2184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 166
                },
                {
                    "start": 169,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1361
                },
                {
                    "start": 1364,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 660,
                    "end": 664,
                    "matchedPaperCorpusId": "263620134"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "258865249"
                },
                {
                    "start": 837,
                    "end": 840,
                    "matchedPaperCorpusId": "219636190"
                },
                {
                    "start": 840,
                    "end": 843,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 865,
                    "end": 869,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 869,
                    "end": 872,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 914,
                    "end": 917,
                    "matchedPaperCorpusId": "235294151"
                },
                {
                    "start": 971,
                    "end": 974,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 974,
                    "end": 977,
                    "matchedPaperCorpusId": "258832459"
                },
                {
                    "start": 1200,
                    "end": 1203,
                    "matchedPaperCorpusId": "261493986"
                },
                {
                    "start": 1538,
                    "end": 1542,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 1885,
                    "end": 1889,
                    "matchedPaperCorpusId": "2593903"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "262826014",
            "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
            "text": "Lastly, Figure 8 shows convergence of a 1.3 billion GPT model at 32K sequence length on 8 A100 GPUs with sequence parallelism degree set at 4 for both DeepSpeed-Ulysses and Megatron-LM sequence parallelism. For DeepSpeed sequence parallelism, we evaluate convergence with different ZeRO stages. DeepSpeed sequence parallelism is a purely system optimization technique that enables training of long sequence Transformer model, thus there is no (negative) on quality of trained models, this assertion is validated through experiments and is shown in Figure 8.",
            "score": 0.517469532130014,
            "section_title": "Convergence Study",
            "char_start_offset": 19401,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 557
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0117340087890625
        },
        {
            "corpus_id": "255825566",
            "title": "Rock Guitar Tablature Generation via Natural Language Processing",
            "text": "Recurrent networks. Sequence modeling is a long-standing problem in machine learning and statistics, with one of its earliest prominent efforts being recurrent neural networks [11]. While recurrent neural networks are able to leverage a form of memory to model sequences of theoretically unbounded contexts, in practice they and their recent variants [12] struggle to do so, in part due to gradient propagation issues [13]. \n\nTransformers. Meanwhile, transformers [14] circumvent the problems with recurrent neural networks by replacing recurrent operations with one feedforward attention operation that compares every element of a sequence with every other element of the sequence; such an approach could initially seem disadvantaged due to the memoryless nature, inherently finite bounded context [15], and O(N 2 ) runtime of an attention mechanism on a sequence of length N [16]. However, when combined with additional innovations such as positional embeddings and token dimensionality reduction via vectorization, transformer architectures have yielded enormous advances in sequence and image modeling [2,1,17].",
            "score": 0.5172821257043101,
            "section_title": "Sequence Models",
            "char_start_offset": 3271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1115
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 180,
                    "matchedPaperCorpusId": "51968707"
                },
                {
                    "start": 351,
                    "end": 355,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "247362443"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 799,
                    "end": 803,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "236924765"
                },
                {
                    "start": 1111,
                    "end": 1114,
                    "matchedPaperCorpusId": "225039882"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01302337646484375
        },
        {
            "corpus_id": "269033427",
            "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention",
            "text": "This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.",
            "score": 0.5171156694222137,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.466552734375
        },
        {
            "corpus_id": "226283829",
            "title": "Attending to Long-Distance Document Context for Sequence Labeling",
            "text": "When processing a single target sequence of length N words, our model must process O(N K) context sequences. If the context representation e c (x) is allowed to be trainable, O(N K) model activation copies are stored for each target sentence, which becomes prohibitively expensive for large encoders. \n\nThough optimizations can be made using GPU/TPU parallelism (e.g. Raffel et al., 2019) and/or memory-efficient encoders (e.g. Kitaev et al., 2020;Lan et al., 2019), our work adopts a different focus. Instead, we consider two simple cases which encapsulate the trade-offs inherent to this method, regardless of encoder architecture: Static. Our static variant of Doc-ARC assumes that e(\u2022) is fixed throughout training. This variant is applicable when the encoder is a memory-intensive language model such as BERT. To offset the effects of freezing BERT, we pass the context representations through a trainable 1-layer context encoder f c , which we found crucial to good performance in our experiments. \n\n(5) \n\nTo compute c(x), we first gather all of the unique sequences that x will attend over, compute the representations of the attended sequences with a We process single target sequence batches with gradient accumulation to achieve larger effective batch sizes. We do not include the context encoder f c .",
            "score": 0.5165527233348591,
            "section_title": "Static and Dynamic Doc-ARC",
            "char_start_offset": 8500,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1003
                },
                {
                    "start": 1006,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1312
                }
            ],
            "ref_mentions": [
                {
                    "start": 428,
                    "end": 448,
                    "matchedPaperCorpusId": "209315300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05059814453125
        },
        {
            "corpus_id": "264468067",
            "title": "Exploring ChatGPT Capabilities and Limitations: A Survey",
            "text": "Transformers refer to the revolutionary core technology behind ChatGPT. Transformers have transformed how sequence-to-sequence models are processed, significantly outperforming traditional models based on recurrent neural networks. Although Transformers are based on classical encoder-decoder architecture, it dramatically differs in integrating the concept of self-attention modules, which excels in capturing long-term dependencies between the elements (i.e., tokens) of the input sequence. It leverages this information to efficiently determine each element's importance in the input sequence. The importance of each element is determined through the self-attention mechanism, which computes a weight for each element based on its relevance to other tokens in the sequence. This enables Transformers to handle variable-length sequences better and capture complex relationships between the sequence elements, improving performance on various natural language processing tasks. Another critical feature is positional embedding that helps transformers learn the positional information of tokens within the sequence. It allows differentiating between tokens with the same contents but at different positions, which provides better context representation that improves the models' accuracy. These features represent a significant strength in ChatGPT for providing accurate natural language generation, as compared to its peers, particularly with being trained on large datasets of 570 GB of Internet data. \n\nIn general, a transformer comprises three featured modules: (i.) Encoder-Decoder module, (ii.) Self-Attention module, (iii.) Positional Embedding module. \n\nIn the following sub-section, we will present the core functionalities of these modules.",
            "score": 0.5165358081245663,
            "section_title": "1) TRANSFORMERS AS CORE TECHNOLOGY",
            "char_start_offset": 14019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1750
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047332763671875
        },
        {
            "corpus_id": "276813725",
            "title": "L2M: Mutual Information Scaling Law for Long-Context Language Modeling",
            "text": "Our theoretical framework allows us to analyze how different architectures' history states scale with sequence length, and what this implies for their ability to capture long-range dependencies. \n\nIn transformer-based models (excluding sparse attention and linear attention variants), the history state consists of stored key-value pairs for all previous tokens. With fixed model size, the size of key-value pairs already grows linearly with sequence length: dim(z q L L/2 ) \u223c L \u2273 L \u03b2 . This means transformer models naturally satisfy the L 2 M condition without model size scaling, as the history state dimension automatically grows with sequence length, despite the quadratic computational cost. \n\nOn the other hand, although often celebrated for their \"infinite\" context length and linear complexity, SSMs, RNNs, and linear attention models do not satisfy the L 2 M condition without scaling up model sizes as sequence length increases. When the model size is fixed, the history state dimension remains constant regardless of the sequence length. Our theory shows this constant-size state cannot capture the growing mutual information. To actually satisfy the L 2 M condition, these architectures require increasingly larger models as sequence length grows, so that their history state dimensions can increase accordingly. This requirement effectively offsets their computational efficiency advantage for long sequence length modeling. \n\nFor other architectures like sparse attention models, we can similarly analyze their history state scaling to understand when they can or cannot satisfy the L 2 M condition. \n\nWe note that the L 2 M condition only addresses a model's ability to capture long-range dependencies, not its overall language modeling capability. Therefore, the L 2 M condition remains a necessary but not sufficient condition-failing to satisfy it implies inherent limitations at longer sequences, while satisfying it does not guarantee effective language modeling. It is also distinct from neural scaling laws, which typically study how model performance scales with model size, dataset size, and compute budget at a given sequence length.",
            "score": 0.5163076037344123,
            "section_title": "Implications to Common LLM Architectures",
            "char_start_offset": 25887,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1438
                },
                {
                    "start": 1441,
                    "end": 1614
                },
                {
                    "start": 1617,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2159
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2347412109375
        },
        {
            "corpus_id": "221655697",
            "title": "Cluster-Former: Clustering-based Sparse Transformer for Question Answering",
            "text": "Efficient Transformers With Transformer models growing larger and larger, how to handle longer sequences arises as a critical challenge. Many works have been proposed to improve the computational and memory efficiency of Transformers, including Sparse Transformer (Child et al., 2019), Set Transformer (Lee et al., 2019), Routing Transformer (Roy et al., 2020), Fast Transformer (Vyas et al., 2020), Reformer (Kitaev et al., 2020), Sinkhorn Transformer (Tay et al., 2020a), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), Synthesizer (Tay et al., 2021), Performer (Choromanski et al., 2020), Linformer (Wang et al., 2020), Linear Transformer (Katharopoulos et al., 2020), and Big-Bird (Zaheer et al., 2020). Tay et al. (2020b) provided an excellent literature survey on this emerging topic. Our method falls into the setting of learnable sparse-attention patterns. \n\nAmong all these works, our method is closer to Set Transformer (Lee et al., 2019), Routing Transformer (Roy et al., 2020), and Fast Trans- former (Vyas et al., 2020), which all use cluster centroids to learn patterns. However, we target at solving a different task, question answering. And it also leads to a significant different framework to encode a short question with a long context, other than a single long sequence, such as language modeling task. Moreover, our cluster centroids are updated in a very different way by periodical centroids update with K-Means on memory bank, other than memory-based centroids (Lee et al., 2019), exponentially moving centroids (Roy et al., 2020), or online clustering (Vyas et al., 2020). \n\nLong Sequence in Question Answering For tasks such as open-domain question answering (Chen et al., 2017), a large volume of documents or paragraphs is usually retrieved to infer the answer, yielding extremely long context content.",
            "score": 0.5161849911623876,
            "section_title": "Related Work",
            "char_start_offset": 3893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1845
                }
            ],
            "ref_mentions": [
                {
                    "start": 409,
                    "end": 430,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 453,
                    "end": 472,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 513,
                    "end": 535,
                    "matchedPaperCorpusId": "215828216"
                },
                {
                    "start": 549,
                    "end": 567,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 700,
                    "end": 721,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.470458984375
        },
        {
            "corpus_id": "235097605",
            "title": "Beyond Paragraphs: NLP for Long Sequences",
            "text": "Next, we will focus on the recent transformerbased methods for efficient processing of long sequences. The key question these models are addressing is how to perform the expensive O(N 2 ) self-attention computation efficiently. All models make this computation faster by approximating the full self-attention leading to different models with different behaviors and applications. We will survey a few of the key papers summarized in Tay et al. (2020). In particular, we will talk about Transformer-XL (Dai et al., 2019), Longformer (Beltagy et al., 2020), Reformer (Kitaev et al., 2020) and Linformer (Wang et al., 2020b). We will also discuss how they apply to NLP tasks; Transformer-XL is mainly suitable for autoregressive tasks while the other three are equally suitable for autoregressive and bidirectional tasks. We will compare the performance of the other three models on various NLP tasks. \n\nThe next section discusses pretraining and finetuning of the transformer models. For pretraining, we will discuss different approaches to warm start the model weights from existing pretrained models for short sequences (Gupta and Berant, 2020;Beltagy et al., 2020). These approaches are versatile and make it possible to adapt most existing pretrained transformer models for short sequences into models that can process long sequences with a tiny pretraining cost. We will also demonstrate how to finetune such models for tasks such as question answering and classification. \n\nThe following section is a practical use case on summarization. We will show how to start from the BART (Lewis et al., 2020) checkpoint, convert it into a model that can work with a long input that's tens of thousands of tokens long, then finetune it on a long-input summarization task. It will also discuss practical techniques necessary to run the model on current hardware, including memory optimization techniques such as gradient checkpointing (Chen et al., 2016) and gradient accumulation. These are generic memory saving methods applicable to all neural models, and especially applicable in the long sequence setting.",
            "score": 0.5152441020817581,
            "section_title": "Description",
            "char_start_offset": 4171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 898
                },
                {
                    "start": 901,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1475
                },
                {
                    "start": 1478,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 501,
                    "end": 519,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1582,
                    "end": 1602,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1826171875
        },
        {
            "corpus_id": "268510227",
            "title": "DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers",
            "text": "Efficiently scaling multi-dimensional transformers to accommodate long sequences is necessary across diverse domains, including video generation (Singer et al., 2022;Blattmann et al., 2023;Ma et al., 2024), image generation (Ramesh et al., 2021;Rombach et al., 2022;Liu et al., 2024), protein structure prediction (Jumper et al., 2021), spatial-temporal information processing (Cong et al., 2021), and beyond. The long length of sequences introduces substantial activation memory costs and notable slowdown for speed, underscoring the need for employing parallelism. \n\nApart from data parallel and pipeline parallel (Huang et al., 2019) which cannot reduce memory cost and inference time, Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). \n\nsequence parallel is the only option. Current sequence parallelism, such as Megatron-LM (Shoeybi et al., 2019), Ring-Attention (Li et al., 2021;Liu et al., 2023a), Megatron-SP (Korthikanti et al., 2022), and DeepSpeed-Ulysses (Jacobs et al., 2023) are all embedded sequence parallelism methods. As shown in Figure 1, these embedded methods shard along a single sequence dimension, which are tailored to the specific pattern and introduce extra communication and complex code modification. \n\nHowever, multi-dimensional transformers calculate independently across multiple sequence dimensions. For instance, for video generation models like OpenSora (Zangwei Zheng, 2024) and Latte (Ma et al., 2024), Spatial-Temporal Attention (Yan et al., 2021) is adopted which separates attention computations to independent temporal and spatial computation. Therefore, there exists a potential space for a new sequence parallelism paradigm. \n\nTo adapt to the flexible patterns of multi-dimensional transformers, we introduce Dynamic Sequence Parallelism (DSP) as a novel abstraction of sequence parallelism, featured by its elegant design, high effectiveness, and excellent compatibility.",
            "score": 0.5152441020817581,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1756
                },
                {
                    "start": 1759,
                    "end": 2004
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 266,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 266,
                    "end": 283,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 314,
                    "end": 335,
                    "matchedPaperCorpusId": "248693351"
                },
                {
                    "start": 616,
                    "end": 636,
                    "matchedPaperCorpusId": "53670168"
                },
                {
                    "start": 1556,
                    "end": 1573,
                    "matchedPaperCorpusId": "232428140"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0099334716796875
        },
        {
            "corpus_id": "271600958",
            "title": "FINCH: Prompt-guided Key-Value Cache Compression for Large Language Models",
            "text": "To illustrate the computational benefit of our approach, we report a comparative analysis of complexity metrics between the attention-based Vanilla transformer and FINCH. We consider Complexity per Layer according to n (total number of tokens), m (chunk size), d (model's embedding dimension), a (output sequence length), Sequential Operations as the number of times the model is invoked sequentially, Cache Growth per Operation as the increment in cache size c with each sequential operation, and Initial Cache Size at the beginning of the Generation stage (0 at the beginning of the Prefill stage). Table 1 shows complexities for the Prefill stage. For large n, the Vanilla method has a higher computational complexity due to quadratic relations, while FINCH introduces sequential operations that scale according to m, hence demonstrating enhanced efficiency and potential for scalability in processing large sequences (m \u226a n). Table 2 shows complexities in the Generation stage, comparing the resource usage when synthesizing the final output. Also in this stage, the benefit for FINCH come from the reduced size of the initial cache according to the compression ratio \u03c3.",
            "score": 0.5152441020817581,
            "section_title": "Complexity Analysis",
            "char_start_offset": 20380,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1174
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00800323486328125
        },
        {
            "corpus_id": "218889852",
            "title": "CERT: Contrastive Self-supervised Learning for Language Understanding",
            "text": "Transformer (Vaswani et al., 2017) is an encode-decoder architecture for sequence-to-sequence (seq2seq) modeling (Sutskever et al., 2014). Different from seq2seq models (Sutskever et al., 2014) that are based on recurrent neural networks (e.g., LSTM (Hochreiter and Schmidhuber, 1997), GRU (Chung et al., 2014)) which model a sequence of tokens via a recurrent manner and hence is computationally inefficient. Transformer eschews recurrent computation and instead uses self-attention which not only can capture the dependency between tokens but also is amenable for parallel computation with high efficiency. Self-attention calculates the correlation among every pair of tokens and uses these correlation scores to create \"attentive\" representations by taking weighted summation of tokens' embeddings. Transformer is composed of building blocks, each consisting of a self-attention layer and a position-wise feed-forward layer. Residual connection (He et al., 2016) is applied around each of the two sub-layers, followed by layer normalization (Ba et al., 2016). Given the input sequence, an encoder, which is a stack of such building blocks, is applied to obtain a representation for each token. Then the decoder takes these representations as inputs and decodes the sequence of output tokens. To decode the i-th token, the decoder first uses selfattention to encode the already decoded sequence y 1 , \u2022 \u2022 \u2022 , y i\u22121 , then performs input-output attention between the encodings of y 1 , \u2022 \u2022 \u2022 , y i\u22121 and those of the input sequence. The \"attentive\" representations are then fed into a feed-forward layer. The three steps are repeated for multiple times. Finally, the representation is fed into a linear layer to predict the next token. The weight parameters in Transformer is learned by maximizing the conditional likelihood of output sequences conditioned on the corresponding input sequences.",
            "score": 0.5152441020817581,
            "section_title": "Transformer",
            "char_start_offset": 4912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1895
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 34,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 113,
                    "end": 137,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 169,
                    "end": 193,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 250,
                    "end": 284,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 948,
                    "end": 965,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0266571044921875
        },
        {
            "corpus_id": "268510227",
            "title": "DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers",
            "text": "Scaling multi-dimensional transformers to long sequences is indispensable across various domains. However, the challenges of large memory requirements and slow speeds of such sequences necessitate sequence parallelism. All existing approaches fall under the category of embedded sequence parallelism, which are limited to shard along a single sequence dimension, thereby introducing significant communication overhead. However, the nature of multi-dimensional transformers involves independent calculations across multiple sequence dimensions. To this end, we propose Dynamic Sequence Parallelism (DSP) as a novel abstraction of sequence parallelism. DSP dynamically switches the parallel dimension among all sequences according to the computation stage with efficient resharding strategy. DSP offers significant reductions in communication costs, adaptability across modules, and ease of implementation with minimal constraints. Experimental evaluations demonstrate DSP's superiority over state-of-the-art embedded sequence parallelism methods by remarkable throughput improvements ranging from 32.2% to 10x, with less than 25% communication volume.",
            "score": 0.5152441020817581,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00313568115234375
        },
        {
            "corpus_id": "258987968",
            "title": "Blockwise Parallel Transformer for Large Context Models",
            "text": "In conclusion, we propose a blockwise parallelization approach to reduce the memory requirements of Transformers, the backbone of state-of-the-art NLP models. Our approach enables processing longer input sequences while maintaining or improving performance. Through extensive experiments, we demonstrate its effectiveness, achieving up to 4x memory reduction than memory-efficient Transformers. Our contributions include a practical method for large context sizes in large Transformer models. With the increasing capability of hardware, larger models and longer context length are widely used in AI research. At the same time, as we are pushing up against physics and fabrication limits, it is more important to design scaling approaches as efficient as possible to scale up large models and large context size. Our approach holds promise for training and evaluating complex models with longer input sequences, potentially driving new breakthroughs in machine learning research. \n\nLimitations and Future Work. Although our method achieves state-of-the-art low memory usage for Transformer models, it does have some limitations that need to be addressed: \n\n\u2022 Optimal performance. While our implementation prioritizes simplicity with high-level Jax operations, optimizing low-level operations is crucial for achieving optimal performance. In future work, we suggest considering porting our method to CUDA and OpenAI Triton to achieve minimal memory cost and maximum speedup. \n\ntotal number of tokens) can be computationally slow. To reduce the sampling time, we limited the rollout to 16 trajectories.",
            "score": 0.5152441020817581,
            "section_title": "Conclusion",
            "char_start_offset": 22929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1153
                },
                {
                    "start": 1156,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1599
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53271484375
        },
        {
            "corpus_id": "235826270",
            "title": "You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling",
            "text": "The Transformer model (Vaswani et al., 2017) is incredibly effective across natural language processing (NLP) applications including machine translation (Vaswani et al., 2017), Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). language inference (Devlin et al., 2019) and paraphrasing (Raffel et al., 2020). Transformer-based models such as BERT (Devlin et al., 2019) are pretrained in an unsupervised manner and later finetuned on different downstream tasks, often providing state-of-the-art performance on standard benchmarks. While such models have strong empirical performance, their computational/memory requirements remain high. Consequently, in the NLP setting, many current models have certain constraints on the sequence length, e.g., BERT and other transformer-based language models (Yang et al., 2019;Liu et al., 2019) limit the sentence length to be at most 512, although recent results have reported success with longer sequences based on interesting efficiency-focused strategies (Beltagy et al., 2020;Zaheer et al., 2020;Xiong et al., 2021). \n\nMulti-Head Self-Attention is central to Transformer based models and provides a flexible global receptive field to exchange information among input tokens. While selfattention provides various benefits, it is also a bottleneck when training with long sequences. In particular, the output of self-attention is a combination of all tokens where coefficients are determined by the similarities among tokens. This is beneficial, but involves a sizable resource footprint. When the sequence length is n, one incurs a O(n2 ) complexity in both time and memory to compute pairwise similarities among all input tokens. This quadratic cost restricts its use in applications where capturing long term context dependencies is important, and has motivated many ongoing efforts to mitigate the resource needs of such models. \n\nOur work, also seeks to address the aforementioned issues, and is inspired by ideas of importance sampling via hashingbased sampling strategies (Spring & Shrivastava, 2018;Charikar & Siminelakis, 2017).",
            "score": 0.5152441020817581,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1940
                },
                {
                    "start": 1943,
                    "end": 2145
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 44,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 153,
                    "end": 175,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 316,
                    "end": 337,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 355,
                    "end": 376,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 416,
                    "end": 437,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 863,
                    "end": 882,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 882,
                    "end": 899,
                    "matchedPaperCorpusId": "46896043"
                },
                {
                    "start": 1064,
                    "end": 1086,
                    "matchedPaperCorpusId": "9222460"
                },
                {
                    "start": 1086,
                    "end": 1106,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 1106,
                    "end": 1125,
                    "matchedPaperCorpusId": "231847231"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0751953125
        },
        {
            "corpus_id": "231933701",
            "title": "Non-Autoregressive Text Generation with Pre-trained Language Models",
            "text": "means the model is trained without using context-aware objective and R-L denotes the model's ROUGE-L score. Additionally, we also show the results from transformer model for a direct comparison. Comparing the two variants of our model, we see that training with context-aware objective leads to a 42% drop on rep-3 metric (0.427 vs 0.741) and a 64% drop on rep-4 metric (0.106 vs 0.295). The ROUGE-L results also indicate that  Dynamic Length Determination Next, we examine the importance of the model's ability to dynamically determine the length of the generated output. To this end, we train another model variant by removing the two [eos] tokens from the target sequence. In this way, the model is not able to self-determine the output length throughout the generation process. To perform inference, we use length-parallel decoding (LPD) with different number of length candidates. Formally, for each length candidate l, the model generates the result\u1ef8 as\n\nThe final result is acquired by re-ranking the generated results with a transformer model.\n\nWe conduct experiments on the IWSLT14 DE-EN dataset in which we try a different number of length candidates, including top-1, top-5 and top-10. The results are shown in Table 6, from which we can see, as the number of length candidates increases, the model performance increases as well. The reason is that a larger candidates set is more likely to contain the best-suited length for the generation model, leading to better performance. However, such decoding procedure inevitably increases the required computation overhead. We can see that, when setting k as 10, the inference speedup decreases from 11.84\u00d7 to 6.01\u00d7. In contrast, our proposed model is able to determine the optimal output length by itself. Without any re-ranking process, it outperforms the model with LPD-10 decoding and achieves the inference speedup that is comparable with the model using LPD-1 decoding.",
            "score": 0.5147844195659613,
            "section_title": "Further Analysis",
            "char_start_offset": 21839,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007373809814453125
        },
        {
            "corpus_id": "259584220",
            "title": "Analysis of different transformer variants",
            "text": "Neural networks have achieved good results in sequential modeling, language modeling, machine translation, and other applications, such as RNN [1], LSTM [2], and GRU [3]. \"Recurrent\" language models and encoder-decoder architectures are making good progress. However, the inherent sequential nature of RNN hinders parallelization among training samples, and for long sequences, memory limitations will hinder batch processing of training samples. So we need a different model to avoid \"Recurrent\". This is what transformers do. Transformers [4] use a new mechanism which called selfattention and performs better in many fields compared to CNN [5], and RNN. \n\nTransformers are proposed to solve the problem of slow speed caused by serial input and serial codec in RNN, which mainly contains Longformer [6], Transformer-XL [7], BigBird [8], and Star-Transformer [9]. Longformer improves Transformer's traditional self-attention mechanism. Specifically, each token only gives local attention to tokens near a fixed window size. Moreover, Longformer adds a kind of global attention based on the former local attention for specific tasks. There are three new attention mechanisms, Sliding window attention, Dilated sliding window, and Global+sliding window; all three mechanisms can reduce the complexity of traditional self-attention very well. \n\nBefore the Transformer-XL, Vanilla Transformer [10] is proposed to deal with the long articles. However, Vanilla Transformer has three main shortcomings. First, the maximum dependent distance between characters is limited by the length of the input, and the model does not see words that appeared more than a few sentences ago. Second, there is no context dependency between segments, which makes training inefficient and affects the performance of the model. Third, in the test phase, each time the next word is predicted, the context needs to be rebuilt, and the calculation starts from scratch, which is very slow. The Transformer-XL architecture introduced two innovations to Vanilla Transformer: Recurrence Mechanism and Relative Positional Encoding, which can solve the problems above.",
            "score": 0.5146658761898486,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1340
                },
                {
                    "start": 1343,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 834,
                    "end": 837,
                    "matchedPaperCorpusId": "57759363"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0843505859375
        },
        {
            "corpus_id": "271097641",
            "title": "How Well Can a Long Sequence Model Model Long Sequences? Comparing Architechtural Inductive Biases on Long-Context Abilities",
            "text": "Nevertheless, an outstanding question remains whether or not long-context models can effectively model long contexts. While some works (Gu and Dao, 2024;Fu et al., 2023;Poli et al., 2023;Peng et al., 2024;Team, 2024) purport to be able to extrapolate towards sequences of long length (100k tokens+), further investigation has suggested differently. For example, Hsieh et al. (2024) claim modern LLMs significantly over-state true context windows on a number of synthetic tasks. Meanwhile Han et al. (2024) observe models to perform reasonably well on synthetic tasks, but struggle on real-world tasks, as do Li et al. (2023). Hence despite a consistent trend in models behaving underwhelmingly, it remains to be understood why this occurs. Yet one interesting question is whether or not linear sequence models are in fact more suited for these compared to Transformer-based ones, as has been claimed repeatedly. \n\nTo this end, we further analyze the behaviour of sequence models to observe how differently they behave compared to Transformer-based ones. We perform a more extensive study into each type of model, as well as a mixture of both, to better inves-tigate how they perform in principle and how they change in behaviour when extending to longer and longer sequences. On both synthetic and realistic data, we conduct a thorough study and observe: \n\n\u2022 All models, whether they use pure sequence layers, attention or a mix, struggle with extrapolating beyond their training context length. \n\n\u2022 The abiliy to extrapolate can vary signficantly based on the format of the sequence even if the task remains constant. \n\nThese results highlight that long sequence models suffer from significant limitations despite their theoretical soundness, highlighting a need to better understand this striking dissonance between expectation and observation and how to amend it for better long-context understanding and reasoning.",
            "score": 0.5145408406421577,
            "section_title": "Introduction",
            "char_start_offset": 1671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1618
                },
                {
                    "start": 1621,
                    "end": 1918
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 187,
                    "matchedPaperCorpusId": "257050308"
                },
                {
                    "start": 187,
                    "end": 205,
                    "matchedPaperCorpusId": "261493986"
                },
                {
                    "start": 488,
                    "end": 505,
                    "matchedPaperCorpusId": "17278462"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.315673828125
        },
        {
            "corpus_id": "270703226",
            "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
            "text": "This paper presents SPARSEK Attention, whose goal is to address both computational and memory efficiency challenges in long-range Transformer computing.We list limitations of our work below:\n\n\u2022 Limited model size and context length In this study, we validate the advantages of SPARSEK in settings involving models up to 1.1 billion parameters and context lengths up to 16,000 tokens.The primary reason for this limitation is our restricted computational resources.Despite these constraints, we consider a broad range of model sizes and context lengths to demonstrate the general applicability of our method.Nevertheless, with the advent of recent parameter-efficient fine-tuning techniques, it may be possible to experiment with our method on more limited devices.\u2022 Decoder-only We restrict our discussion to applying SPARSEK within Transformer decoders.\n\nthe incremental evaluation capability of the SPARSEK operation demonstrates superior complexity compared to previous approaches.Nonetheless, the SPARSEK operation is also applicable in various other contexts, such as Transformer encoders and routing in mixture-of-expert models, where a top-k operation might be employed.The sparse output produced by the SPARSEK operation offers a notable advantage in these scenarios because it is close to the top-k selection performed somewhere.\u2022 Text-only We focus exclusively on text tasks.However, our method is not dependent on the input modality.Future research involving vision or speech could further substantiate the robustness of our method.",
            "score": 0.5142612810552045,
            "section_title": "D Limitations",
            "char_start_offset": 40872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 152,
                    "end": 190
                },
                {
                    "start": 192,
                    "end": 383
                },
                {
                    "start": 383,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 764
                },
                {
                    "start": 764,
                    "end": 854
                },
                {
                    "start": 856,
                    "end": 984
                },
                {
                    "start": 984,
                    "end": 1177
                },
                {
                    "start": 1177,
                    "end": 1338
                },
                {
                    "start": 1338,
                    "end": 1385
                },
                {
                    "start": 1385,
                    "end": 1444
                },
                {
                    "start": 1444,
                    "end": 1543
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0343017578125
        },
        {
            "corpus_id": "174801539",
            "title": "Attention is all you need for Videos: Self-attention based Video Summarization using Universal Transformers",
            "text": "Vaswani et al. [22] use N = 8 attention heads and concatenate the outputs of each of these heads to compute the self-attention representation of the inputs. These representations are then fed to a point-wise feed forward neural network to generate the final encoded context vectors. Their complete architecture thus uses self-attention for the inputs, encoder-decoder attention for the decoder and masked self-attention to generate the outputs. \n\nWhile the Transformer improves upon the vanilla RNN and LSTM based models, it fails to generalize to unseen input lengths or learn simple tasks like Copying and Repeat copying. The Universal Transformer [23] model aims to address these issues by weight sharing across the encoder and decoder units and by using Adaptive Computation Time [24] to learn the number of steps required to learn the encoder representation. Adaptive Computation Time is an approach using which sequence models can dynamically learn the number of computation steps required to process an input. Earlier, these steps had to be explicitly defined by the architecture or were dependent on the input lengths. The Universal Transformer model uses ACT not between inputs, but across depth, and refine their self-attention distributions dynamically. \n\nThe Universal Transformer uses 4 attention heads instead of the 8 as proposed in the original architecture, and achieve significant improvements over the vanilla transformer. Their adaptation of the ACT method can indeed prove to be significant when dealing with videos of large and varying lengths. The number of parameters for both these models is the same, which has prompted us to believe that this architecture can indeed do better for tasks like video summarization, under the same computational constraints. \n\nThere have been a few other notable improvements over the vanilla transformer architecture. Transformer-XL networks [25] bring back recurrence in the transformer models and counter the problem of fixed-length context in the Transformer. \n\nThey use recurrence at a segment-level and show that the Transformer-XL model can capture long-term dependencies. They also demonstrate that by doing this, they achieve a 1800% improvement in the evaluation time when compared to the Transformer model. BERT [26] uses bidirectional Transformers and condition the representations on both left and right context, for all layers.",
            "score": 0.5137676200497411,
            "section_title": "Sequence to Sequence models using self-attention",
            "char_start_offset": 8925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2020
                },
                {
                    "start": 2023,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2274
                },
                {
                    "start": 2275,
                    "end": 2398
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.052825927734375
        },
        {
            "corpus_id": "212850031",
            "title": "Span-Based Neural Buffer: Towards Efficient and Effective Utilization of Long-Distance Context for Neural Sequence Models",
            "text": "Recent best-performing neural sequence model for language modeling use a variety of regularization and optimization techniques (Press and Wolf 2017;Inan, Khosravi, and Socher 2016). In particular, AWD-LSTM (Merity, Keskar, and Socher 2017) improves drop-out for regularization and gradient descent for optimization. Adversarial training is applied in (Wang, Gong, and Liu 2019) to improve generalization. However, all of them are not directly addressing the structural sequential recency bias problem. One direction of using long distance context is through grammar induction. It aims at extracting underlying grammar from corpus without treebank annotation (Shen et al. 2017;Yogatama et al. 2018;Shen et al. 2018). Integrating the tree shall benefits utilization of long-distance context. However, its performance is still far behind sequence model for sequence prediction tasks. \n\nNeural cache model in (Grave, Joulin, and Usunier 2016) is a non-parametric method to increase probability to reoccurring patterns during test. We observe that it performs worse than dynamic evaluation (Krause et al. 2017), which is also a method applied during test. Since parameters in our model are obtained during training, our method is orthogonal to these methods. \n\nA recent work of transformer-xl (Dai et al. 2019) extends transformer architecture (Vaswani et al. 2017) to model dependency beyond fixed lengths, using a recurrence mechanism on adjacent segments. However, the transformer-xl relies on information from long-distance context to be kept in the recurrence mechanism. In contrast, the proposed sequential neural buffer directly accesses long-distance context without using any time-recurrent mechanism. Nevertheless, the proposed SNB can be used together with transformer, in which transformer is used similarly as the LSTM to encode local information.",
            "score": 0.5136475752204482,
            "section_title": "Related Work",
            "char_start_offset": 19123,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 180,
                    "matchedPaperCorpusId": "21700944"
                },
                {
                    "start": 658,
                    "end": 676,
                    "matchedPaperCorpusId": "53034786"
                },
                {
                    "start": 697,
                    "end": 714,
                    "matchedPaperCorpusId": "53034786"
                },
                {
                    "start": 1288,
                    "end": 1304,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1339,
                    "end": 1360,
                    "matchedPaperCorpusId": "9289495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12493896484375
        },
        {
            "corpus_id": "270257771",
            "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
            "text": "The block decoder maintains global attention similar to vanilla transformers but operates at a much coarser block level, reducing context length by L B compared to the original token-level sequence. This reduction decreases position-wise computation during prefill by L B compared to vanilla transformers of the same size. The main bottleneck during batch decoding, i.e., KV cache IO, is reduced by L 2 B as it is quadratic to context length. The same savings apply to attention computation, which can become a bottleneck during prefill as context lengths grow. KV cache storage in GPU memory during decoding is also reduced linearly by L B , enabling larger batch sizes and higher parallelism. \n\nToken decoder skips prefill entirely and nearly eliminates decode IO The token decoder does not use global attention but relies on a single context embedding for global context information, applying attention within each independent block for local context. Thus, the token decoder does not need to preserve or retrieve KV cache values from previous blocks, eliminating the need to prefill input tokens. This also nearly eliminates KV cache IO overhead during decoding, as quadratic scaling applies to the small local context of L B rather than the global context L. Compared to the KV cache IO complexity of L 2 in vanilla transformers, token decoders have L 2 B complexity per block, across L/L B blocks, achieving an overall reduction of L/L B . For our main models with L = 2048 and L B = 4, this results in a 256-fold reduction in KV cache IO overhead. Asymptotically, this reduces KV cache IO overhead from quadratic to linear with respect to context length, solving a key challenge in scaling to very long contexts [29]. KV cache storage is also reduced by the same factor, enabling larger batch sizes. This significantly improves the utilization of inference hardware, which is typically as low as \u223c1% model FLOPs utilization (MFU) in vanilla transformers [61]. Thus, we can apply more FLOPs in the token decoder to improve performance, with minimal effect on inference throughput.",
            "score": 0.5130819829641495,
            "section_title": "B",
            "char_start_offset": 45111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2086
                }
            ],
            "ref_mentions": [
                {
                    "start": 1961,
                    "end": 1965,
                    "matchedPaperCorpusId": "253420623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.126953125
        },
        {
            "corpus_id": "273811349",
            "title": "Birdie: Advancing State Space Models with Reward-Driven Objectives and Curricula",
            "text": "Due to their scaling properties (Hoffmann et al., 2022) and in-context learning (Garg et al., 2023), large Transformer models using attention (Bahdanau, 2014;Vaswani et al., 2017b) are now prominent in natural language processing (NLP) and achieve effective performance in natural language generation tasks (NLG), including language mod-eling, machine translation, and question and answering (Q&A) (Yue et al., 2022;Xie et al., 2022;Kumar et al., 2021). However, the softmax attention mechanism cost scales quadratically with sequence length during training, and its key-value (KV) cache grows linearly with sequence length during inference. This leads to increasing costs for training and deployment as model providers continue to increase the context length (Dubey et al., 2024;Reid et al., 2024). \n\nThis trend in increasing context length has sparked a strong interest in developing efficient alternative sequence models. The goal is to maintain high performance while scaling effectively with longer sequences. Recent work has focused on recurrent models which offer two key advantages: subquadratic scaling for parallel processing and a fixed state size (in contrast to the growing KV cache in Transformer models) that enables constantcost inference per step. These models come in different forms, ranging from state space model (SSM)-based methods, such as S4 (Gu et al., 2022), S5 (Smith et al., 2023), or Mamba (Gu and Dao, 2023)), to linear RNNs, such as RWKV (Peng et al., 2023), HGRU (Qin et al., 2023), and Hawk (De et al., 2024), to linear attention variants, such as RetNet (Sun et al., 2023) and GLA (Yang et al., 2024). These different methods vary in their exact parameterization and parallel computation, but all have an efficient, fixed-state size recurrence for inference. For brevity, we will generally refer to all of these methods as SSMs regardless of their exact parameterization or parallel computation path.",
            "score": 0.5128034179778269,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 99,
                    "matchedPaperCorpusId": "251253368"
                },
                {
                    "start": 398,
                    "end": 416,
                    "matchedPaperCorpusId": "247518803"
                },
                {
                    "start": 416,
                    "end": 433,
                    "matchedPaperCorpusId": "246016124"
                },
                {
                    "start": 433,
                    "end": 452,
                    "matchedPaperCorpusId": "235422410"
                },
                {
                    "start": 1615,
                    "end": 1634,
                    "matchedPaperCorpusId": "266162792"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1614990234375
        },
        {
            "corpus_id": "278544600",
            "title": "Enhancing Internet Traffic Forecasting in MEC Environments With 5GT-Trans: Leveraging Synthetic Data and Transformer-Based Models",
            "text": "Originally devised for sequence-to-sequence modelling in natural-language processing, the Transformer has become canonical deep-learning framework. Its defining component self-attention updates each token embedding by contextualising it with respect to the entire sequence. Concretely, learned linear projections map the input embeddings to query, key, and value vectors; the dot-product similarities between queries and keys are then scaled, passed through a soft-max normalisation, and used to weight the corresponding values. The resulting context-aware representations form the basis for subsequent layers, enabling the model to capture both short-and long-range dependencies with high computational parallelism. \n\nDespite its effectiveness, self-attention is computationally expensive, particularly for long input sequences, posing challenges in areas such as document classification, summarization, and protein sequence analysis. To address this, various techniques, such as limiting attention to specific token ranges or approximating attention computations, have been explored. The Performer model, for instance, employs a kernel of random orthogonal features to approximate attention in sub-quadratic time, facilitating efficient long-sequence processing an approach relevant to this study.",
            "score": 0.5127617045148702,
            "section_title": "E. 5GT-TRANS ARCHITECTURE",
            "char_start_offset": 19806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1299
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.106689453125
        },
        {
            "corpus_id": "270703226",
            "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
            "text": "Long-range Transformers Self-attention is a cornerstone of Transformer success, but its quadratic complexity concerning input length poses challenges for tasks requiring long context.Numerous efficient approaches have emerged, spanning state-space models [30,62], recurrent neural networks [45,52,49], linear attention [55,38] and low-rank approximations of self-attention [75,14,53], which replace the self-attention with novel linear blocks for long-context modeling.Nonetheless, these approaches historically underperformed compared to modern Transformer models [70] in language modeling tasks until recent efforts [29,77].Besides, a few studies combine the Transformer with block-wise recurrence [17,35,36,12] or key-value compression [60,59,18].In contrast, our approach falls under sparse attention, reducing complexity by pruning the attention matrix.This approach is motivated by observations that the attention matrix in dense models naturally becomes sparse, and the performance of language models remains robust under reasonably sparse conditions [15,27,42].\n\nSparse attention Some sparse attention utilized fixed patterns to restrict the number of tokens involved, such as sliding windows [56,51], dilated sliding windows [4,22], combination of patterns [34,13], or domain-specific patterns [31].Recent studies have aimed at achieving constant memory costs during inference through predefined heuristic cache eviction policies [81,42,27].However, these static methods often prove suboptimal in various scenarios [66,2].Alternatively, sparse patterns can be learned in a data-driven manner.For example, Reformer [39] employs locality-sensitive hashing for token clustering and do attention within a cluster, while Routing Transformers [61], Cluster-Former [74] and Clustered Attention [73] use K-Means clustering on tokens.Besides, Sparse Sinkhorn Attention [68] establishes sparsity by sorting blocks of inputs.Despite achieving sub-quadratic complexity, these methods still remain above linear complexity and face challenges when handling extremely long sequences or failing to offer constant memory cost during inference.",
            "score": 0.5118613645908379,
            "section_title": "Related Work",
            "char_start_offset": 5218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 469
                },
                {
                    "start": 469,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 750
                },
                {
                    "start": 750,
                    "end": 858
                },
                {
                    "start": 858,
                    "end": 1069
                },
                {
                    "start": 1071,
                    "end": 1308
                },
                {
                    "start": 1308,
                    "end": 1450
                },
                {
                    "start": 1450,
                    "end": 1531
                },
                {
                    "start": 1531,
                    "end": 1601
                },
                {
                    "start": 1601,
                    "end": 1834
                },
                {
                    "start": 1834,
                    "end": 1923
                },
                {
                    "start": 1923,
                    "end": 2135
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "258832459"
                },
                {
                    "start": 323,
                    "end": 326,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 704,
                    "end": 707,
                    "matchedPaperCorpusId": "247011581"
                },
                {
                    "start": 1058,
                    "end": 1062,
                    "matchedPaperCorpusId": "184486746"
                },
                {
                    "start": 1205,
                    "end": 1208,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 1303,
                    "end": 1307,
                    "matchedPaperCorpusId": "259262301"
                },
                {
                    "start": 1746,
                    "end": 1750,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1869,
                    "end": 1873,
                    "matchedPaperCorpusId": "211505992"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1697998046875
        },
        {
            "corpus_id": "244278288",
            "title": "Neuro-Symbolic Speech Understanding in Aircraft Maintenance Metaverse",
            "text": "Comparing our approach with [9] in terms of architecture, we applied Transformers in contrast to LSTM, which has certain issues. First, LSTMs are inefficient in speed, because to generate embeddings for a particular item in a sequence, the representations of every single word before have to be calculated, therefore, the computation process cannot be parallelized for running on GPUs. On contrary, the Transformer model can be trained and executed across multiple GPUs using parallelism pipeline. Second, LSTM lacks contextualization, due to comprehension of the meaning of a token according to the tokens that come before it but not the ones that come after. However, in Transformers every single token in a sequence is merged with every other token in that sequence at the same time, making context to be solid. \n\nFinally, generated programs are passed the last part of NSSE processing for execution.",
            "score": 0.5117981253932219,
            "section_title": "D. TEXT-TO-PROGRAMS",
            "char_start_offset": 33515,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 903
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06646728515625
        },
        {
            "corpus_id": "248070506",
            "title": "Hardware-Software Co-Design of an In-Memory Transformer Network Accelerator",
            "text": "Transformer networks have high computational and space complexity because of the employed attention mechanism. Transformer network GPU implementations are bounded by the memory, particularly with longer sequences, because of the O (dn + dn 2 ) spatial complexity, where d represents the feature embedding dimension, and n is the sequence length. A transformer can also be computation-limited because of the O (dn 2 ) serialized time complexity of the MHA. The O (dn 2 ) complexity comes from each time step (sequence element) attending to every other time step (sequence element). However, an O (1) time complexity can be achieved with adequate parallelism. The following sections review four types of algorithm-based acceleration: quantization (Section 2.2.1), attention caching (Section 2.2.2), model parallelism (Section 2.2.3) and sparse attention (Section 2.2.4).",
            "score": 0.5112017324331432,
            "section_title": "Algorithm-Based Transformer Network Acceleration",
            "char_start_offset": 16931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 868
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0144500732421875
        },
        {
            "corpus_id": "49667762",
            "title": "Universal Transformers",
            "text": "Convolutional and fully-attentional feed-forward architectures like the Transformer have recently emerged as viable alternatives to recurrent neural networks (RNNs) for a range of sequence modeling tasks, notably machine translation (Gehring et al., 2017;Vaswani et al., 2017). These parallel-in-time architectures address a significant shortcoming of RNNs, namely their inherently sequential computation which prevents parallelization across elements of the input sequence, whilst still addressing the vanishing gradients problem as the sequence length gets longer (Hochreiter et al., 2003). The Transformer model in particular relies entirely on a self-attention mechanism (Parikh et al., 2016;Lin et al., 2017) to compute a series of context-informed vector-space representations of the symbols in its input and output, which are then used to predict distributions over subsequent symbols as the model predicts the output sequence symbol-by-symbol. Not only is this mechanism straightforward to parallelize, but as each symbol's representation is also directly informed by all other symbols' representations, this results in an effectively global receptive field across the whole sequence. This stands in contrast to e.g. convolutional architectures which typically only have a limited receptive field. \n\nNotably, however, the Transformer with its fixed stack of distinct layers foregoes RNNs' inductive bias towards learning iterative or recursive transformations. Our experiments indicate that this inductive 1 arXiv:1807.03819v3 [cs.CL] 5 Mar 2019 The Universal Transformer repeatedly refines a series of vector representations for each position of the sequence in parallel, by combining information from different positions using self-attention (see Eqn 2) and applying a recurrent transition function (see Eqn 4) across all time steps 1 \u2264 t \u2264 T . We show this process over two recurrent time-steps. Arrows denote dependencies between operations. Initially, h 0 is initialized with the embedding for each symbol in the sequence. h t i represents the representation for input symbol 1 \u2264 i \u2264 m at recurrent time-step t. With dynamic halting, T is dynamically determined for each position (Section 2.2).",
            "score": 0.5108956758206563,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2207
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.018096923828125
        },
        {
            "corpus_id": "257232619",
            "title": "A Survey on Long Text Modeling with Transformers",
            "text": "Modeling long texts has been an essential technique in the field of natural language processing (NLP). With the ever-growing number of long documents, it is important to develop effective modeling methods that can process and analyze such texts. However, long texts pose important research challenges for existing text models, with more complex semantics and special characteristics. In this paper, we provide an overview of the recent advances on long texts modeling based on Transformer models. Firstly, we introduce the formal definition of long text modeling. Then, as the core content, we discuss how to process long input to satisfy the length limitation and design improved Transformer architectures to effectively extend the maximum context length. Following this, we discuss how to adapt Transformer models to capture the special characteristics of long texts. Finally, we describe four typical applications involving long text modeling and conclude this paper with a discussion of future directions. Our survey intends to provide researchers with a synthesis and pointer to related work on long text modeling.",
            "score": 0.5104798311116674,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10211181640625
        },
        {
            "corpus_id": "277321766",
            "title": "Advancements in Natural Language Processing: Exploring Transformer-Based Architectures for Text Understanding",
            "text": "The findings of this study reaffirm the transformative impact of transformer-based architectures on natural language processing, particularly in the realm of text understanding. Models like BERT and GPT have demonstrated remarkable capabilities in capturing contextual nuances, handling longrange dependencies, and adapting to a wide range of tasks, as evidenced by their superior performance on benchmarks such as GLUE (with accuracy exceeding 80%) and SQuAD (with F1 scores above 90%). These results align with recent 2024 research, such as Li et al.'s work on enhancing multi-hop knowledge graph reasoning [8] and Liu et al.'s development of CA-BERT for context-aware chat interactions [3]. The statistical visualizations provided-probability density functions of text length distributions and feature space scatter plots-further illustrate the models' strengths in managing varying text lengths, adapting to conditional distributions, and performing classification tasks even when classes overlap in feature space. \n\nOne of the most significant insights from this analysis is the transformer models' ability to handle long-range dependencies, a limitation that plagued earlier models like RNNs and LSTMs. The self-attention mechanism allows transformers to process entire sequences simultaneously, enabling them to capture relationships between distant words effectively. This is particularly evident in the first image, which compares the overall text length distribution p(x 2 ) with the conditional distribution p(x 2 |x 2 \u2265 99). The ability to adapt to longer texts, as shown by the shift in distribution, underscores the models' suitability for tasks involving extended contexts, such as document summarization or multi-turn dialogue systems. Similarly, the second image, contrasting p(x 2 ) with p(x 2 |f (x) = 0), highlights how transformers can adjust to specific conditions, which is crucial for applications like sentiment analysis or domain-specific question answering where text characteristics may vary. \n\nLooking ahead, the future of transformer-based models in NLP appears promising, with several avenues for advancement. Efficiency optimization remains a priority, with techniques like model pruning, quantization, and efficient attention mechanisms (e.g., sparse attention) showing potential to reduce computational demands. Additionally, the expansion into multimodal understanding-integrating text with images, audio, or other data types-offers exciting opportunities.",
            "score": 0.5102975099897333,
            "section_title": "V. DISCUSSION",
            "char_start_offset": 8283,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 1018
                },
                {
                    "start": 1021,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 2020
                },
                {
                    "start": 2023,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2345
                },
                {
                    "start": 2346,
                    "end": 2491
                }
            ],
            "ref_mentions": [
                {
                    "start": 609,
                    "end": 612,
                    "matchedPaperCorpusId": "269756926"
                },
                {
                    "start": 689,
                    "end": 692,
                    "matchedPaperCorpusId": "272827760"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.134765625
        },
        {
            "corpus_id": "269004711",
            "title": "Bidirectional Long-Range Parser for Sequential Data Understanding",
            "text": "To consolidate the long-range context understanding claim, we performed a sequence length augmentation study on ListOps task in table 5. We artificially multiplied the original sequences by self-concatenating them via the MAX operator.Thus, we ensure the output is invariant w.r.t. the sequence length.This process was achieved with a multiplication factor of between 1 and 4, thus forcing the model to process sequences of up to 8192 tokens.For testing, we use the models trained on the original ListOps containing sequences of up to 2048 elements.Our approach is able to achieve high performance, although it is tested out-of-domain sequences in terms of length.Comparatively, the model of Didolkar et al. (2022) manifests a high performance drop for all sequence lengths.This observation is in line with the rightmost section of the plot from Figure 1, showcasing the capability of our approach to maintain a long-context understanding despite the high length of the processed sequence.Our proposed approach is incomplete without a study in terms of computational efficiency and scalability.To demonstrate the capabilities for our proposed BLRP with respect to this demand, we conducted an experiment in terms of self-scalability of the GPU memory consumption correlated with an increase in terms of sequence size.We measured how the model scales in terms of memory consumption, as the input sequences are increased considerably in length (i.e.exponential increase as a power of 2).Results are illustrated in Figure 4 (Right).Basically, this study highlights how the model scales with respect to the length of the sequential data being processed.Please note that the vanilla transformer Vaswani et al. (2017) is limited to sequences of up to 4096 elements due to GPU memory limitations.It is worth noticing that our scalability is directly proportional against TLB Didolkar et al. ( 2022), achieving almost linear scalability, however at a higher performance gain (see Figure 1).",
            "score": 0.5101122198941723,
            "section_title": "Forward",
            "char_start_offset": 14789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 235,
                    "end": 302
                },
                {
                    "start": 302,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 549
                },
                {
                    "start": 549,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 774
                },
                {
                    "start": 774,
                    "end": 989
                },
                {
                    "start": 989,
                    "end": 1094
                },
                {
                    "start": 1094,
                    "end": 1317
                },
                {
                    "start": 1317,
                    "end": 1447
                },
                {
                    "start": 1447,
                    "end": 1485
                },
                {
                    "start": 1485,
                    "end": 1529
                },
                {
                    "start": 1529,
                    "end": 1649
                },
                {
                    "start": 1649,
                    "end": 1789
                },
                {
                    "start": 1789,
                    "end": 1982
                }
            ],
            "ref_mentions": [
                {
                    "start": 1690,
                    "end": 1711,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061431884765625
        },
        {
            "corpus_id": "248218548",
            "title": "Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context NLP Models",
            "text": "Our main contribution is an analysis of how different sequence lengths affect the trade-off between accuracy, power, and speed in long-context Transformer models during fine-tuning and inference. Since our focus is on long-context NLP tasks, we investigated the following four input sequence lengths: 1024, 2048, 3072, and 4096.",
            "score": 0.5101122198941723,
            "section_title": "Methodology",
            "char_start_offset": 5425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 328
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.609375
        },
        {
            "corpus_id": "269293007",
            "title": "A Survey on Efficient Inference for Large Language Models",
            "text": "In addition to applying efficient techniques to the attention operation, recent studies have also innovated to design sequence modeling architectures that are efficient yet effective. Table 2 compares the efficiency of some representative non-Transformer models. These architectures exhibit sub-quadratic computational complexity with respect to sequence length during both training and inference, enabling LLMs to significantly increase their context length. \n\nWithin this research field, two prominent lines of study have garnered significant attention. One line of studies concentrates on the State Space Model (SSM), which formulates sequence modeling as a recurrence transformation based on the HiPPO theory [64]. Additionally, other studies primarily focus on employing long convolutions or designing attention-like formulations to model sequences.",
            "score": 0.5100031942029195,
            "section_title": "Transformer Alternates",
            "char_start_offset": 43979,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 854
                }
            ],
            "ref_mentions": [
                {
                    "start": 713,
                    "end": 717,
                    "matchedPaperCorpusId": "221150566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1171875
        },
        {
            "corpus_id": "237412971",
            "title": "\\infty-former: Infinite Memory Transformer",
            "text": "When reading or writing a document, it is important to keep in memory the information previously read or written. Humans have a remarkable ability to remember long-term context, keeping in memory the relevant details (Carroll, 2007;Kuhbandner, 2020). Recently, transformer-based language models have achieved impressive results by increasing the context size (Radford et al., 2018(Radford et al., , 2019;;Dai et al., 2019;Rae et al., 2019;Brown et al., 2020). However, while humans process information sequentially, updating their memories continuously, and recurrent neural networks (RNNs) update a single memory vector during time, transformers do not -they exhaustively query every representation associated to the past events. Thus, the amount of computation they need to perform grows with the length of the context, and, consequently, transformers have computational limitations about how much information can fit into memory. For example, a vanilla transformer requires quadratic time to process an input sequence and linear time to attend to the context when generating every new word. \n\nSeveral variations have been proposed to address this problem (Tay et al., 2020b). Some propose using sparse attention mechanisms, either with data-dependent patterns (Kitaev et al., 2020;Vyas et al., 2020;Tay et al., 2020a;Roy et al., 2021;Wang et al., 2021) or data-independent patterns (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020), reducing the self-attention complexity (Katharopoulos et al., 2020;Choromanski et al., 2021;Peng et al., 2021;Jaegle et al., 2021), and caching past representations in a memory (Dai et al., 2019;Rae et al., 2019). These models are able to reduce the attention complexity, and, consequently, to scale up to longer contexts. However, as their complexity still depends on the context length, they cannot deal with unbounded context.",
            "score": 0.5087693247219466,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1877
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 249,
                    "matchedPaperCorpusId": "221320806"
                },
                {
                    "start": 405,
                    "end": 422,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 422,
                    "end": 439,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 439,
                    "end": 458,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1263,
                    "end": 1284,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1284,
                    "end": 1302,
                    "matchedPaperCorpusId": "220424511"
                },
                {
                    "start": 1302,
                    "end": 1320,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1320,
                    "end": 1337,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1337,
                    "end": 1355,
                    "matchedPaperCorpusId": "221655697"
                },
                {
                    "start": 1487,
                    "end": 1515,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1515,
                    "end": 1540,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1540,
                    "end": 1558,
                    "matchedPaperCorpusId": "232105052"
                },
                {
                    "start": 1625,
                    "end": 1643,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1643,
                    "end": 1660,
                    "matchedPaperCorpusId": "207930593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58251953125
        },
        {
            "corpus_id": "266174725",
            "title": "Spectral State Space Models",
            "text": "We evaluate the stacked STU model on the Long Range Arena (LRA) benchmark [TDA + 21]. This benchmark aims to assess the performance of sequence prediction models in long-context scenarios and consists of six tasks of various modalities, including text and images. The context length for the tasks ranges from 1K to 16K, and the tasks require capabilities such as hierarchical reasoning, matching and retrieval, and visual-spatial understanding. SSMs [GGR21] have shown significantly superior performance on most of the tasks compared to Transformer architectures. In particular for the hardest task in the suite, PathX (image classification with context length of 16K), no transformer model has been able to achieve accuracy beyond random guessing. We provide the evaluation of the stacked STU model on the two hardest tasks namely PathFinder and PathX in Table 4b. \n\nWe compare our performance against the ablation carried out by [OSG + 23] who find that ring initialization, stable exponential parameterization and \u03b3-normalization are all crucial towards learning these tasks. In particular as reported by [OSG + 23] all three of the above interventions were necessary to learn on PathX to any non-trivial accuracy. This is a result of the much larger context length of 16K employed by the PathX task. On the other hand we find that the the stacked STU (with the STU component exactly as represented by (4)) is sufficient to learn on both these tasks to relatively high accuracies. Notably we do not require any other normalizations or initialization techniques We initialize all the parameters of the STU i.e. M matrices to 0. Details about our implementation as well as details about the experiments including hyperparameters can be found in the appendix (Section E. This result in particular confirms and highlights the theoretical stability afforded by the STU even under learning tasks involving large sequence lengths. In the appendix (Table 2 we provide the performance evalaution of the stacked STU on all tasks of the LRA benchmark. \n\nIn the next section we highlight a simple technique towards significantly improving the achieved accuracy for the stacked STU model. We report the median over 5 trials for our experiments.",
            "score": 0.5084105167212734,
            "section_title": "Experiments on Long Range Arena [TDA + 21]",
            "char_start_offset": 22043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2043
                },
                {
                    "start": 2046,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 450,
                    "end": 457,
                    "matchedPaperCorpusId": "239998472"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28955078125
        },
        {
            "corpus_id": "265552007",
            "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
            "text": "One significant drawback of the vanilla attention mechanism [275] is the quadratic complexity of attention computation, making it especially inefficient for handling long sequences. Although efficient / sparse attention offers some relief, its worst-case theoretical complexity remains unchanged. To address this, various attention-free methods have been proposed, providing alternatives that avoid the computation of the quadratic attention matrix [61,91,204,210,260,324]. These methods could be largely categorized into those that replace the attention mechanism with recurrent computation [204,260,324], and those that discretize state space representations [89-91, 95, 187, 253]. Notably, these new methods like RWKV [204], H3 [61], \n\nHyena [210], RetNet [260] and Mamba [89] achieve performance comparable to the standard Transformer. RWKV [204] utilizes recurrent neural networks (RNNs) to streamline sequence processing, thereby reducing the complexity of handling long sequences. H3 [61], based on state space models (SSMs), offers an efficient alternative for data representation and processing. \n\nHyena [210] presents itself as a drop-in replacement for traditional attention mechanisms, simplifying the Transformer architecture. RetNet [260] introduces a multi-scale retention module coupled with a feed-forward network module, enhancing parallelism and recurrence, which significantly improves efficiency in both training and inference phases. Mamba [89] includes the selection operation based on state space models for as compression and further improve the efficiency with hardware-related optimization. We include the complexity analysis of these methods compared with the vanilla Transformer with an input query of length  in Table 1. It provides an overview of how each method scales in complexity, offering insights into the advancements in the attention-free technologies.",
            "score": 0.5082562630862539,
            "section_title": "Attention-free",
            "char_start_offset": 47416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 736
                },
                {
                    "start": 739,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1891
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 65,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03338623046875
        },
        {
            "corpus_id": "53208380",
            "title": "Blockwise Parallel Decoding for Deep Autoregressive Models",
            "text": "Standard greedy decoding takes m steps to produce an output of length m, even for models that can efficiently score sequences using a constant number of sequential operations. While brute-force enumeration of output extensions longer than one token is intractable when the size of the vocabulary is large, we can still attempt to exploit parallelism within the model by training a set of auxiliary models to propose candidate extensions. \n\nLet the original model be p 1 = p, and suppose that we have also learned a collection of auxiliary models p 2 , . . . , p k for which p i (y j+i | y \u2264j , x) is the probability of the (j + i)th token being y j+i given the first j tokens. We propose the following blockwise parallel decoding algorithm (illustrated in Figure 1), which is guaranteed to produce the same prediction \u0177 that would be found under greedy decoding but uses as few as m/k steps. As before, we start with an empty prediction \u0177 and set j = 0. Then we repeat the following three substeps until the termination condition is met: \n\n\u2022 Predict: Get the block predictions \u0177j+i = argmax yj+i p i (y j+i | \u0177\u2264j , x) for i = 1, . . . , k. \n\n\u2022 Verify: Find the largest k such that \u0177j+i = argmax yj+i p 1 (y j+i | \u0177\u2264j+i\u22121 , x) for all 1 \u2264 i \u2264 k. \n\nNote that k \u2265 1 by the definition of \u0177j+1 . \n\n\u2022 Accept: Extend \u0177 with \u0177j+1 , . . . , \u0177j+ k and set j \u2190 j + k.  1: The three substeps of blockwise parallel decoding. In the predict substep, the greedy model and two proposal models independently and in parallel predict \"in\", \"the\", and \"bus\". In the verify substep, the greedy model scores each of the three independent predictions, conditioning on the previous independent predictions where applicable. When using a Transformer or convolutional sequence-to-sequence model, these three computations can be done in parallel.",
            "score": 0.5082287944526661,
            "section_title": "Blockwise Parallel Decoding",
            "char_start_offset": 4401,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1037
                },
                {
                    "start": 1040,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1244
                },
                {
                    "start": 1247,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1819
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01262664794921875
        },
        {
            "corpus_id": "267574347",
            "title": "Deep social neuroscience: the promise and peril of using artificial neural networks to study the social brain",
            "text": "However, recurrent maintenance of context memory has an important limitation: the further apart two tokens are, the more likely an intervening step will disrupt the context memory that tracks their dependence. \n\n'Transformers' address this limitation by tracking longdistance dependencies with 'attention' and 'positional encoding' (Vaswani et al., 2017). Because sequence-to-sequence LSTMs build a context memory by moving step-by-step over the input, longer sequences are subject to accumulating noise and vanishing gradients. In contrast, transformers use an attention mechanism to represent each token in the input as a complicated weighted transformation of all the other tokens at once, capturing many non-linear dependencies between tokens. Learning attention weights that minimize the loss is like learning how to look at different parts of a sentence as you read through it; for example, inferring who is referred to as 'them' by looking earlier in the sentence for their name. By adding a positional encoding vector to every input, each token is 'tagged' with a value representing both its absolute and relative positions. Because attention and positional encoding allow transformers to represent long-distance dependencies more crisply, they can exceed the performance of LSTMs on sequence-based tasks like natural language processing. When trained on very large datasets, the performance of transformers can be remarkable: the success of large language models like GPT4 (Achiam et al., 2023), as well as text-to-image models like Stable Diffusion (Rombach et al., 2022), can be partially attributed to the transformer architecture.",
            "score": 0.5081970805580509,
            "section_title": "LSTMs and transformers for sequence processing",
            "char_start_offset": 25626,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 212,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1643
                }
            ],
            "ref_mentions": [
                {
                    "start": 332,
                    "end": 354,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1559,
                    "end": 1581,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.129638671875
        },
        {
            "corpus_id": "271097641",
            "title": "How Well Can a Long Sequence Model Model Long Sequences? Comparing Architechtural Inductive Biases on Long-Context Abilities",
            "text": "Datasets. We conduct an initial evaluation using RULER (Hsieh et al., 2024), a set of synthetic benchmarks that test long-context information retention, before conductin a more fine-grained evaluation on a general needle-in-the-haystack task. We use this benchmark as for more granular control over the exact information that must be retained. Results are measured in terms of accuracy based on exact matching of predicted tokens. \n\nBaselines. Our main objective is to compare how long-sequence models fare on long context tasks. \n\nTo this end, we compare models with the same number of parameters that are evenly trained on the same data. Hence we first use Mamba2 (Dao and Gu, 2024) as well as a Transformer variant (Transformer++) as well as a hybrid Mamba2Attn, each with 2.7 billion parameters. We further add Sheared-LLaMA (Xia et al., 2024) and Recurrent-Gemma (Botev et al., 2024) baselines (with and without intruction-tuning) as same-sized baselines trained under different conditions. We finally add a 3 billion RWKV (Peng et al., 2023) variant as another sequence model baseline.",
            "score": 0.5079067052579768,
            "section_title": "Experiments and Results",
            "char_start_offset": 7985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1091
                }
            ],
            "ref_mentions": [
                {
                    "start": 666,
                    "end": 684,
                    "matchedPaperCorpusId": "270199762"
                },
                {
                    "start": 1028,
                    "end": 1047,
                    "matchedPaperCorpusId": "258832459"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1973876953125
        },
        {
            "corpus_id": "232069031",
            "title": "Automated essay scoring using efficient transformer-based language models",
            "text": "Initially, seq2seq models were used for natural machine translation between multiple languages. The seq2seq model has an encoder-decoder component; an encoder analyzes the input sequence and creates a context vector while the decoder is initialized with the context vector and is trainer to produce the transformed output. Previous language models were based on a fixed-length context vector and suffered from an inability to infer context over long sentences and text in general. An attention mechanism was used to improve the performance in translation for long sentences. \n\nThe use of a self-attention mechanism turns out to be very useful in th context of machine translation [3]. This mechanism, and it various derivatives, have been responsible for a large number of accuracy gains over a wide range of NLP tasks more broadly. The form of attention for this paper can be found in [41]. Given a query matrix Q, key matrix K, and value matrix V , then the resulting sequence is given by ( 2) \n\nThese matrices are obtained by linear transformations of either the output of a neural network, the output of a previous attention mechanism, or embeddings. The overall success of attention has led to the development of the transformer [41]. The architecture of the transformer is outlined in Figure 1. \n\nIn the context of efficient models, if we consider all sequences up to length L and if each query is of size d, then each keys are also of length d, hence, the matrix QK T of size L \u00d7 L. The implication is that the memory and computational power required to implement this mechanism scales quadratically with length. The above-mentioned models often adhere to a size restriction by letting L = 512.",
            "score": 0.5072469784915559,
            "section_title": "Methodology",
            "char_start_offset": 8292,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 995
                },
                {
                    "start": 998,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1701
                }
            ],
            "ref_mentions": [
                {
                    "start": 886,
                    "end": 890,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1234,
                    "end": 1238,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09552001953125
        },
        {
            "corpus_id": "276724714",
            "title": "ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length on More Than 12,000 GPUs",
            "text": "Scaling long-context ability is essential for Large Language Models (LLMs). To amortize the memory consumption across multiple devices in long-context training, inter-data partitioning (a.k.a. Data Parallelism) and intra-data partitioning (a.k.a. Context Parallelism) are commonly used. Current training frameworks predominantly treat the two techniques as orthogonal, and establish static communication groups to organize the devices as a static mesh (e.g., a 2D mesh). However, the sequences for LLM training typically vary in lengths, no matter for texts, multi-modalities or reinforcement learning. The mismatch between data heterogeneity and static mesh causes redundant communication and imbalanced computation, degrading the training efficiency. In this work, we introduce ByteScale, an efficient, flexible, and scalable LLM training framework for large-scale mixed training of long and short sequences. The core of ByteScale is a novel parallelism strategy, namely Hybrid Data Parallelism (HDP), which unifies the inter- and intra-data partitioning with a dynamic mesh design. In particular, we build a communication optimizer, which eliminates the redundant communication for short sequences by data-aware sharding and dynamic communication, and further compresses the communication cost for long sequences by selective offloading. Besides, we also develop a balance scheduler to mitigate the imbalanced computation by parallelism-aware data assignment. We evaluate ByteScale with the model sizes ranging from 7B to 141B, context lengths from 256K to 2048K, on a production cluster with more than 12,000 GPUs. Experiment results show that ByteScale outperforms the state-of-the-art training system by up to 7.89x.",
            "score": 0.5067267401479705,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06488037109375
        },
        {
            "corpus_id": "270710851",
            "title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs",
            "text": "Long-context LLM The computational cost of processing sequences in Transformer-based models increases quadratically with sequence length, resulting in higher resource consumption and performance issues when handling long context inputs.\n\nMany studies explore various strategies to address this challenge, including the use of new positional encodings to achieve position interpolation or extrapolation (Peng et al., 2023;Chen et al., 2023;Liu et al., 2024b).For example, Rope (Su et al., 2021) extends the positional knowledge learned during the pre-training phase to longer sequence lengths through extrapolation, including various variants such as NTK-RoPE (NormXU, 2021).Alibi (Press et al., 2021), on the other hand, maps long sequences to within recognizable lengths through interpolation.Some studies attempt to fine-tune LLMs to give the model a longer context window.\n\nLongLoRA (Chen et al., 2024)is an efficient finetuning approach that significantly extends the context sizes of pre-trained LLMs with limited computational cost by using sparse local attention and improved parameter-efficient fine-tuning techniques.RingAttention (Liu et al., 2023a) can reduce the memory requirements of the Transformer model, allowing it to train sequences over 500 times longer than previous memory-efficient methods, without needing to approximate the attention mechanism.Additionally, traditional engineering techniques such as sliding windows or RAG (Lewis et al., 2020) are also solutions for addressing long context scenarios in LLMs.These methods improve the performance of LLMs in handling long contexts in certain aspects.\n\nLong-context Benchmark To evaluate the performance of different LLMs on long texts, various benchmarks are often used to test different aspects of LLMs' capabilities.For instance, Zeroscrolls (Shaham et al., 2023) is a zero-shot benchmark for natural language understanding over long texts, which contains only test and small validation sets, without training data.LongBench (Bai et al., 2023b) is a bilingual, multi-task benchmark for long context understanding, which also provides a subset with uniformly distributed data called LongBench-E.",
            "score": 0.5066865808993154,
            "section_title": "Input:",
            "char_start_offset": 4459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 238,
                    "end": 458
                },
                {
                    "start": 458,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 794
                },
                {
                    "start": 794,
                    "end": 875
                },
                {
                    "start": 877,
                    "end": 1126
                },
                {
                    "start": 1126,
                    "end": 1369
                },
                {
                    "start": 1369,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1626
                },
                {
                    "start": 1628,
                    "end": 1794
                },
                {
                    "start": 1794,
                    "end": 1993
                },
                {
                    "start": 1993,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 1449,
                    "end": 1469,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.689453125
        },
        {
            "corpus_id": "264426505",
            "title": "Sentiment analysis with adaptive multi-head attention in Transformer",
            "text": "Transformer has demonstrated its abilities of sentence processing and recognition, including text classification, machine translation and text generation. It is a state-of-art architecture to take care of semantic analysis via parallel computing computations. Compared with traditional RNN structure, the multi-head attention mechanism can process the whole paragraph or multiple sentences in total to obtain their interrelationship. In Transformer, encoder positioner, multi-head attention mechanism and feedforward comprise a complete unit. The encoder position is for marking the word position in each sentence, which can effectively ensure the position of each word can be fully considered in sentence analysis. When it comes to the multi-head attention mechanism, there are three trainable matrices: k,q,v for dividing samples into multiple heads and analyzing their attention relationship separately under parallel computing. In the meanwhile, the word dependence will be preserved due to former positional encoding. Finally, the processed samples pass the feed-forward, which is composed of feed-forward networks to serve as gradient descent during the backpropagation. The multi-head attention mechanism may cause overfitting or computing waste for short sentences, especially in the final training stage. Consequently, adaptive mechanisms have been widely applied in statistical language models for tackling various interesting issues. The core objective of the mechanism is to dynamically tune the architecture of the model to accommodate the context states, especially the issue that context states emphasize more flexible hyper parameters in the model. Inspired by the two issues, we introduce a state-of-art adaptive Transformer, which is able to process sentences with different lengths using different numbers of heads. For text classification, the length of sentence is pivotal to the performance of the model, especially the Transformer. Transformer outperforms in short sentences; however, it is not fully adaptable to long sentences. On the other hand, if far more condensed attention is imposed on short sentences, overfitting will negatively impact the final recognition accuracy with training going through. To be specific, we initially extend the distribution of sentence length in our dataset and divide them into three groups : Small, Medium, Large with closed group size. After that, we apply sparse Attention to process Small Groups, Medium Attention to process Medium Groups, condense Attention to process Large Groups. The figure is shown below for Attention mechanism.",
            "score": 0.5058511943424292,
            "section_title": "The Model",
            "char_start_offset": 3564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2229
                },
                {
                    "start": 2230,
                    "end": 2397
                },
                {
                    "start": 2398,
                    "end": 2547
                },
                {
                    "start": 2548,
                    "end": 2598
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05792236328125
        },
        {
            "corpus_id": "256847015",
            "title": "A Unified View of Long-Sequence Models towards Modeling Million-Scale Dependencies",
            "text": "In this section, we conduct experiments to investigate the impact of varying context lengths in sequence modeling. One of the applications of long-context learning is on high-resolution images, e.g., fMRI, and satellite images. Therefore, we first study the behaviors of Vision Transformers (ViT) [14] when varying the sequence length. To this end, we pretrain a base ViT on ImageNet (21k+) and fine-tune it on four datasets: CIFAR 10, CIFAR 100, EuroSAT [23], and So2Sat [49]. The latter two are satellite datasets of higher resolution. The downstream task for all four datasets is classification. \n\nViT partitions an image into patches and treats each patch as a token. Therefore, the smaller the patches are, the longer the context the model gets access to in each batch. Note that, although the dependencies between patches are preserved among patches, the structural information within a patch is destroyed as it is serialized during embedding. We thus expect better performance when using smaller patch sizes since more structural information is kept. Also, this performance increase is expected to be task-dependent, for example, predicting a dog needs far fewer details than identifying the type of vehicle in a satellite image. We repeat the experiments for six runs on the ETH Euler Note that the y-axes do not start from zero. cluster with two GeForce RTX 3090 GPUs. As expected, model quality does increase as the patch size gets smaller (Fig. 1), and the magnitudes of such increase differ by task. However, we notice that performance improvement is not significant in this experiment, and the trends tend to plateau at the end. Therefore further investigations with different tasks and more find-grained increments of context length are needed. \n\nTo avoid being constrained by the fixed architecture of pretrained models, we write a vanilla Transformer for the following experiments so that we can vary the sequence length at will. The Transformer model consists of three layers of encoder blocks and one MLP layer as the prediction head, the decoder. We implement the positional embedding described in the original work [40] and keep it autoregressive by implementing attention masks.",
            "score": 0.5050081258485977,
            "section_title": "Impact of Context Length",
            "char_start_offset": 19485,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2199
                }
            ],
            "ref_mentions": [
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "11810992"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3828125
        },
        {
            "corpus_id": "263671659",
            "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training",
            "text": "Memory-efficient attention. Dao et al. (2022) and Lefaudeux et al. (2022) propose to use an online normalizer (Milakov & Gimelshein, 2018) to compute the attention in a blockwise and memory-efficient way. It reduces peak memory usage by not materializing large intermediate states, e.g. the attention softmax matrix. In addition, research on sparse attention computes only a sparse subset of the attention score, which also reduces the memory footprints yet may lead to inferior performance (Beltagy et al., 2020;Sun et al., 2022;Zaheer et al., 2020). In this work, we limit our scope to exact attention. \n\nSequence parallelism and ring attention Ring Self-Attention (Li et al., 2021) is among the first to parallelize Transformers in the sequence dimension. However, its distributed attention design is not optimized for causal language modeling and incompatible with memory-efficient attention, which are crucial for long-context LLM training. Ring Attention (Liu et al., 2023) proposes to compute distributed attention in a memory-efficient blockwise pattern. However, it is also not optimized for causal language modeling, leading to 2\u00d7 extra computation. DISTFLASHATTN optimizes for both memory-efficient attention and causal language modeling. More recently, DeepSpeed Ulysses (Jacobs et al., 2023) proposes a hybrid parallelism strategy. It computes distributed attention in the tensor model parallelism to address these two problems and utilizes sequence parallelism elsewhere (Shoeybi et al., 2019). We provide head-to-head comparison in Table 4. \n\nModel Parallelism and FSDP Tensor Model parallelism (Korthikanti et al., 2023) partitions model parameters and also distributes the activation in parallel LLM training. Pipeline model parallelism (Huang et al., 2019) also partitions the activations. However, it applies high memory pressure to the first pipeline stage. We show in \u00a7 4.2 that this leads to a less effective support for long sequences. Thus, we focus on comparing with tensor model parallelism and only consider pipeline parallelism when the number of heads is insufficient for tensor parallelism.",
            "score": 0.5046567444530019,
            "section_title": "Related work",
            "char_start_offset": 5333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1555
                },
                {
                    "start": 1558,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2120
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 45,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 530,
                    "end": 550,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1610,
                    "end": 1636,
                    "matchedPaperCorpusId": "248693351"
                },
                {
                    "start": 1754,
                    "end": 1773,
                    "matchedPaperCorpusId": "53670168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.181640625
        },
        {
            "corpus_id": "278129492",
            "title": "EcoServe: Enabling Cost-effective LLM Serving with Proactive Intra- and Inter-Instance Orchestration",
            "text": "As illustrated in Figure 2, the LLM predicts the next token with the accumulated context iteratively until it encounters the end-of-sequence (EoS). By saving the key and value \n\nembedding in the memory (i.e. KV cache), redundant computations are avoided in subsequent steps, thus the inference process is divided into prefill and decode phases. Here we conclude their computation and memory features. \n\n(1) \n\nModern LLMs primarily adopt the Transformer architecture [44], which leverages the self-attention mechanism to model complex dependencies in sequences. Its computation involves three main steps: \n\n\u2022 QKV projection (Equation 1): Each input token is projected into query (Q), key (K), and value (V) embeddings. \u2022 QKV attention (Equation 2): A weighed aggregation is performed using the scaled dot-product of queries and keys, determining how each token attends to others in the sequence. The resulting weights are applied to the value vectors, capturing contextual information. \u2022 Output projection (Equation 3): A position-wise feedforward layer further transforms each token's representation with a nonlinear activation, enhancing the model's capacity to learn complex patterns. \n\nDistinct arithmetic intensity. In Transformer-based LLM inference, matrix multiplications dominate the overall computation time, while softmax and layer normalization account for only a small fraction of the total execution time. As listed in Table 2, there are 6 major matrix multiplication operations and we compute their arithmetic intensities separately for both prefill and decode phases, using the hyperparameters defined in Table 1. The arithmetic intensity is computed by dividing the total number of floating-point operations by the total amount of memory access. Since some terms, such as 1/ , are negligible, we omit them and present the approximate arithmetic intensity. Although certain optimization techniques, such as FlashAttention [14], can affect the arithmetic intensity, our calculations closely reflect real-world scenarios. As shown in Table 2, the arithmetic intensity of the prefill phase depends on both the sequence length  and batch size , while the decode phase primarily depends on only the batch size .",
            "score": 0.5045571152188661,
            "section_title": "Computation and Memory of LLM Inferences",
            "char_start_offset": 6526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 175
                },
                {
                    "start": 178,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 1937,
                    "end": 1941,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0247039794921875
        },
        {
            "corpus_id": "269772997",
            "title": "Challenges in Deploying Long-Context Transformers: A Theoretical Peak Performance Analysis",
            "text": "Suppose one has successfully turned an open-source GPT-3.5 to GPT-4 level language models (e.g., LLaMA 3 [25], DeepSeek [7], QWen [3] and Mixtral [14]) into a long-context variant (e.g., through continual pretraining [9] and instruction tuning [4]) then wants to deploy this model for a wide spectrum of applications like repository-level coding and hour-long video understanding.These applications typically require the input-context to be of 100K to 10M tokens where the paradigm changes substantially from the 4K short-context regime.We are interested in an ambitious goal:\n\nHow to reduce the deployment of 1M context production-level transformers to be as cheap as 4K?\n\nWhy serving long-context transformers is expensive?Most of the cost overhead trace back to one single source: the size of the KV cache.Consider a 30+B 100K context GPT-3.5 quality open-source models like QWen or Yi, the differences between KV cache for 4K v.s.100K   Here we use the Yi-34B 200K [29] configuration (60 layers, 8 kv heads and 128 hidden dimension).Suppose we use 2 \u00d7 80G A100 tensor parallelism to serve this model in bf16, then we have 2 \u00d7 80 \u2212 34 \u00d7 2 = 122 GB spare space for storing the KV cache.From this first glance, we immediately realize that under this setting, we can achieve about 100+ users concurrency of 4K context, but only 5 users of 100K context.If we were able to deploy 1M context models as cheap as 4K, we can substantially democratize long-context and multimodal generative models and foster an emerging application ecosystem.\n\nThis work gives a full-stack quantitative analysis of challenges in deploying long-context transformers, from hardware to system, from model architecture to user preference.We first describe a concurrent programming framework (Fig 1 ) for serving multiple long-context user requests under limited GPU HBM size.",
            "score": 0.5042963196264131,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 380
                },
                {
                    "start": 380,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 576
                },
                {
                    "start": 578,
                    "end": 672
                },
                {
                    "start": 674,
                    "end": 725
                },
                {
                    "start": 725,
                    "end": 809
                },
                {
                    "start": 809,
                    "end": 934
                },
                {
                    "start": 934,
                    "end": 1037
                },
                {
                    "start": 1037,
                    "end": 1188
                },
                {
                    "start": 1188,
                    "end": 1352
                },
                {
                    "start": 1352,
                    "end": 1536
                },
                {
                    "start": 1538,
                    "end": 1711
                },
                {
                    "start": 1711,
                    "end": 1848
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326904296875
        },
        {
            "corpus_id": "271915857",
            "title": "FocusLLM: Precise Understanding of Long Context by Dynamic Condensing",
            "text": "Empowering LLMs with the ability to precisely understand long contexts is crucial for many downstream applications. However, handling long contexts with conventional transformer architecture requires substantial training and inference resources. Existing context condensing methods cannot accurately understand the full context, as there is a considerable amount of information loss in the condensing process. To address these issues, we present FocusLLM, a framework designed to extend the fixed context length of any decoder-only LLM, allowing the model to focus on relevant information from very long sequences. FocusLLM first divides long text input into chunks based on the model's original context length. It then employs the dynamic condensing process to distill crucial information from each chunk. Ultimately, through the novel parallel decoding mechanism, FocusLLM can integrate the extracted information into its local context. FocusLLM stands out for great training efficiency and versatility: trained with an 8K input length and with much less training cost than previous methods, FocusLLM exhibits superior performance across downstream tasks and maintains strong language modeling ability when handling extensive long texts, even up to 400K tokens. Our code is available at https://github.com/leezythu/FocusLLM.",
            "score": 0.5042353357378954,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280517578125
        },
        {
            "corpus_id": "250526200",
            "title": "Forming Trees with Treeformers",
            "text": "The CKY algorithm, which uses a similar chart structure to Treeformer, has a worst case runtime complexity of O(n 3 |G|) where |G| is the size of the context-free grammar. Similarly, the Treeformer encoding algorithm is also O(n 3 ) assuming constant model dimension and sequential operations. In this section, we show this calculation as well as two key optimizations which are necessary for tractable training and improve the time and space complexity to O(n) and O(nmH) respectively. See \u00a7 6.8 for empirical results. \n\nSequential Algorithm Starting with a sequence of length n, we encode phrases of length h for 1 \u2264 h \u2264 n. There are n \u2212 h + 1 phrases of length h, each having h \u2212 1 pairs of children. Each pair will be composed together exactly once in the entire algorithm, giving us \n\ntotal compositions. As our composition function runs in constant time (with respect to n), our total complexity for compositions is O(n 3 ). For pooling, we have O(n 2 ) total nodes each with O(n) pairs of children each. Since the scaled dot-product attention scales linearly in its arguments, we again get a complexity of O(n 3 ) for pooling, and thus for the entire algorithm as well. \n\nParallel Algorithm While encoding phrases of length h is dependent on the encodings for all lengths less than h, there is no dependency on other phrases of the same length, allowing us to compute them in parallel. Parallelization removes the factor of n \u2212 h + 1 in Equation 3, leaving \n\ntotal compositions. Likewise, we can pool O(n) sets of children in parallel, reducing the pooling (and thus overall) parallel complexity to O(n 2 ). \n\nLimiting Tree Height In practice the space complexity turns out to be a bottleneck. Decoding involves calculating and storing cross attention to O(n 2 ) vectors (compared to O(n) for Transformers) for each of the m tokens in the output, resulting in a space complexity of O(n 2 m). To reduce this, we introduce a hyperparameter H which limits the maximum tree height (or phrase length). This results in O(n) and O(nmH) complexities respectively.",
            "score": 0.5041754862612882,
            "section_title": "Parallelization",
            "char_start_offset": 11762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1176
                },
                {
                    "start": 1179,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1614
                },
                {
                    "start": 1617,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2062
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.005100250244140625
        },
        {
            "corpus_id": "270562409",
            "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling",
            "text": "Long sequence language modeling have attracted more and more research interests in recent years (Tiezzi et al., 2024), as large language models continue to advance (Li et al., 2024a). Various long document processing tasks are proposed to evaluate the long sequence modeling of language models (Zhao et al., 2021;Luo et al., 2021;Bai et al., 2023). Longformer, leveraging sparse self-attention pattern, save the memory cost to make the model process long document (Beltagy et al., 2020). Memorizing transformer uses a external memory to save the information during the long sequence modeling process (Wu et al., 2022). Mistral applied Pre-fill and chunking sliding window methods to model longer sequences (Jiang et al., 2023). State space models and their variations are also popular recently (Gu et al., 2022;Gu and Dao, 2023;Wang et al., 2022). Unlimitedformer wraps pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor index, while the returned kNN distances are the attention dotproduct scores (Bertsch et al., 2024). Nawrot et al. (2024) propose to compress the key-value cache to make the model process longer sequences. Xiong et al. (2023)    positional interpolation based on it are also used enable the model process longer sequences (Su et al., 2024;Chen et al., 2023). Text summarization has also been known by its relation with long sequence processing area (Du and Gao, 2023;Gao et al., 2024;Li et al., 2024b). ReadAgent are proposed by using a large language model agent to process long sequences (Lee et al., 2024). LONG-HEADS enhances the long-context processing of large language models by allowing multi-head attention to attend to selected important context chunks within the trained length (Lu et al., 2024).",
            "score": 0.5041727370819218,
            "section_title": "M More Related Work M.1 Long Sequence Processing",
            "char_start_offset": 43185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1792
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 182,
                    "matchedPaperCorpusId": "271923338"
                },
                {
                    "start": 313,
                    "end": 330,
                    "matchedPaperCorpusId": "267740432"
                },
                {
                    "start": 828,
                    "end": 846,
                    "matchedPaperCorpusId": "254877218"
                },
                {
                    "start": 1062,
                    "end": 1084,
                    "matchedPaperCorpusId": "258436892"
                },
                {
                    "start": 1191,
                    "end": 1210,
                    "matchedPaperCorpusId": "263134982"
                },
                {
                    "start": 1307,
                    "end": 1324,
                    "matchedPaperCorpusId": "233307138"
                },
                {
                    "start": 1434,
                    "end": 1452,
                    "matchedPaperCorpusId": "260009920"
                },
                {
                    "start": 1469,
                    "end": 1486,
                    "matchedPaperCorpusId": "270688452"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0885009765625
        },
        {
            "corpus_id": "270620100",
            "title": "DeciMamba: Exploring the Length Extrapolation Potential of Mamba",
            "text": "We find that vanilla Transformers of equivalent size (trained on the same dataset with a similar training recipe), have inferior length generalization abilities compared to Mamba. This is evident in multiple long-context tasks (Tables 4, 5). \n\nTable 4: Comparison With Transformers -Passkey Retrieval. The setting is the same as in AppendixA.1. All models were trained on sequences of length 2k. For each context length we test performance for 5 different needle locations, and report the success rate (between 0 and 1). \u2713stands for 100% success, \u2717 for 0% success. For LongBench, the equivalent Transformer model (Pythia-2.8B) repeatedly causes an OOM error on our GPU (A6000, 48GB of RAM).",
            "score": 0.5038330329568117,
            "section_title": "B ADDITIONAL EVALUATIONS B.1 COMPARISON WITH TRANSFORMERS",
            "char_start_offset": 28319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 241
                },
                {
                    "start": 244,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 690
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07342529296875
        },
        {
            "corpus_id": "259063776",
            "title": "A Novel Vision Transformer with Residual in Self-attention for Biomedical Image Classification",
            "text": "The transformer introduced in the seminal paper \"Attention is All You Need\" by Vaswani et. al. [21] is a powerful architecture that revolutionized natural language processing (NLP) tasks. The transformer architecture has since become the foundation for various state-of-the-art NLP models as well as computer vision models. It leverages the self-attention mechanism to efficiently capture the inter-dependencies in long sequential inputs. This mechanism enables the model to consider the context of each word based on the entire input sequence, resulting in better representation learning. The schematic of transformer architecture has been shown in Fig. 1. The main components of the transformer are encoder and decoder. The encoder processes the input sequence, while the decoder generates an output sequence. Each encoder and decoder layer in a transformer contains multiple selfattention heads, allowing the model to attend to different parts of the input sequence simultaneously. The traditional recurrent models, like RNNs, suffer from vanishing or exploding gradients when dealing with long sequences. On the other hand, transformers overcome this limitation by employing attention mechanisms, allowing them to capture dependencies between distant words.",
            "score": 0.5035614518073895,
            "section_title": "Transformer in NLP",
            "char_start_offset": 6091,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1261
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1195068359375
        },
        {
            "corpus_id": "268378933",
            "title": "StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses",
            "text": "StreamingDialogue efficiently handles long context, improving the model's long-term memory for conversation history. Existing methods for processing long context in transformer-based models broadly fall into three categories: efficient transformer design, long-term memory enhancement, and length extrapolation techniques.",
            "score": 0.5029520418081233,
            "section_title": "Related work",
            "char_start_offset": 409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 322
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63525390625
        },
        {
            "corpus_id": "119296728",
            "title": "Dynamic Evaluation of Transformer Language Models",
            "text": "Transformers with a large memory cache also potentially have the capability of adapting to the style of the recent sequence history, although it is unclear to what extent they learn to do this in practice. Dynamic evaluation and Transformers have each shown their respective capabilities to use thousands of timesteps of context to improve predictions (Krause et al., 2018;Dai et al., 2019), but it is unclear how much overlap there is between the type of long-range dependencies exploited by Transformers and dynamic evaluation. If Transformers are able to fully adapt to the style of the recent sequence history, there should be little to no advantage of using dynamic evaluation. Therefore, in this work, we explore the utility of applying dynamic evaluation to Transformers. \n\nA number of variants of Transformers have been suggested for language modeling (Al-Rfou et al., 2018;Liu et al., 2018;Baevski and Auli, 2019;Radford et al., 2018), but in this work, we focus on the Transformer-XL architecture of Dai et al. (2019), which uses segment-level attention recurrence and a relative positional encoding mechanism to generalize to longer attention lengths than seen during training. Transformer-XL has recently improved state-of-the-art results on a number of common language modeling benchmarks. \n\nThe Transformer-XL, like the regular Transformer, contains stacked self-attention layers and positionwise feedforward operations. The Transformer-XL processes sequence segments in parallel across time in each forward pass. The hidden states from these sequence segments are cached in a memory so that future sequence segments can apply attention over them. We refer to Dai et al. (2019) for the full details of the model.",
            "score": 0.5026921380099423,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1302
                },
                {
                    "start": 1305,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1726
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.089599609375
        },
        {
            "corpus_id": "224818050",
            "title": "Emformer: Efficient Memory Transformer Based Acoustic Model for Low Latency Streaming Speech Recognition",
            "text": "Transformers [1] have achieved dominated performance for various tasks in natural language processing area [2,3,4]. Rather than using memory state to capture long-range dependencies in recurrent neural networks, the multi-head self-attention method connects arbitrary positions in the whole sequence directly in parallel. \n\nRecently, transformer-based model architectures have also been successfully applied to automatic speech recognition (ASR) area across various modeling paradigms, including sequence-tosequence [5,6,7,8,9], neural transducer [10,11,12], Connectionist temporal classification (CTC) [13,14] and traditional hybrid [15,16] systems. \n\nUnlike most natural language processing tasks, many ASR applications deal with streaming scenarios challenging for vanilla transformers. The streaming recognizer needs to produce output given partially available speech utterance rather than entire utterance. Several methods advance the transformer for streaming speech recognition. The work [15,10,17] proposed to constrain the attention computation with a limited length of look-ahead inputs. However, these methods have a significant delay due to the look-ahead context leaking issue where essential look-ahead context grows linearly with the number of transformer layers stacking on top of one another. A scout network is proposed in [9] to detect the word boundary. In scout networks, only the context information before the word boundary is used by the transformer to make predictions. However, the scout network does not address the heavy self-attention computation that grows quadratically with the left context length. A streaming transformer with augmented memory (AM-TRF) is proposed in [18] to reduce latency and the self-attention computation. \n\nAM-TRF uses a similar block processing method as [19]. The block processing chunks the whole utterance into multiple segments. To reduce the computation in capturing the long-range left context, AM-TRF introduces a memory bank. Each vector in the memory bank is an abstract embedding from the previous one segment. The direct left context block from the current segment and look-ahead context block provides context information for current segment recognition in addition to the memory bank. However, AM-TRF has duplicated computations for the direct left context block in both training and decoding.",
            "score": 0.5026732804873283,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1759
                },
                {
                    "start": 1762,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2253
                },
                {
                    "start": 2254,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 16,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 107,
                    "end": 110,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 110,
                    "end": 112,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 547,
                    "end": 551,
                    "matchedPaperCorpusId": "211066611"
                },
                {
                    "start": 603,
                    "end": 607,
                    "matchedPaperCorpusId": "59336132"
                },
                {
                    "start": 634,
                    "end": 638,
                    "matchedPaperCorpusId": "46974195"
                },
                {
                    "start": 638,
                    "end": 641,
                    "matchedPaperCorpusId": "204823802"
                },
                {
                    "start": 995,
                    "end": 999,
                    "matchedPaperCorpusId": "46974195"
                },
                {
                    "start": 999,
                    "end": 1002,
                    "matchedPaperCorpusId": "211066611"
                },
                {
                    "start": 1811,
                    "end": 1815,
                    "matchedPaperCorpusId": "62841563"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.125732421875
        },
        {
            "corpus_id": "212996548",
            "title": "Lite Transformer with Long-Short Range Attention",
            "text": "RNNs and CNNs. Recurrent neural networks (RNNs) have prevailed various sequence modeling tasks for a long time (Sutskever et al., 2014;Luong et al., 2015;Bahdanau et al., 2015;Wu et al., 2016). However, RNNs are not easy to parallelize across the sequence due to its temporal dependency. \n\nRecently, some work has demonstrated that RNN is not an essential component to achieve stateof-the-art performance. For instance, researchers have proposed highly-efficient convolution-based models (Kalchbrenner et al., 2016;Gehring et al., 2017;Kaiser et al., 2018;Wu et al., 2019b). \n\nConvolution is an ideal primitive to model the local context information; however, it lacks the ability to capture the long-distance relationship, which is critical in many sequence modeling tasks. \n\nTransformers. As an alternative, attention is able to capture global-context information by pairwise correlation. Transformer (Vaswani et al., 2017)   self-attentions to achieve state-of-the-art performance. Recently, there have been a lot of variants to the transformer (Ahmed et al., 2017;Ott et al., 2018;Chen et al., 2018;Paulus et al., 2018;Shaw et al., 2018;Sukhbaatar et al., 2019a;b;Child et al., 2019). Among them, Ott et al. (2018) proposed to scale up the batch size; Shaw et al. (2018) leverages the relative position representations; Ahmed et al. (2017) introduces the weighted multi-head attention; Sukhbaatar et al. (2019a) applies adaptive masks for long-range information on character-level language modeling with very long sequences. All these attempts are orthogonal to our work, as their methods can also be applied in our architecture. \n\nAutomated Model Design. Due to the vast architecture design space, automating the design with neural architecture search (NAS) becomes popular (Zoph & Le, 2017;Zoph et al., 2018;Pham et al., 2018;Cai et al., 2019a).",
            "score": 0.5025471060953514,
            "section_title": "RELATED WORK",
            "char_start_offset": 4350,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1633
                },
                {
                    "start": 1636,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1851
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 135,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 135,
                    "end": 154,
                    "matchedPaperCorpusId": "1998416"
                },
                {
                    "start": 154,
                    "end": 176,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 515,
                    "end": 536,
                    "matchedPaperCorpusId": "3648736"
                },
                {
                    "start": 536,
                    "end": 556,
                    "matchedPaperCorpusId": "3508167"
                },
                {
                    "start": 556,
                    "end": 573,
                    "matchedPaperCorpusId": "59310641"
                },
                {
                    "start": 903,
                    "end": 925,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1068,
                    "end": 1085,
                    "matchedPaperCorpusId": "44131019"
                },
                {
                    "start": 1103,
                    "end": 1123,
                    "matchedPaperCorpusId": "21850704"
                },
                {
                    "start": 1123,
                    "end": 1141,
                    "matchedPaperCorpusId": "3725815"
                },
                {
                    "start": 1141,
                    "end": 1166,
                    "matchedPaperCorpusId": "159041867"
                },
                {
                    "start": 1201,
                    "end": 1218,
                    "matchedPaperCorpusId": "44131019"
                },
                {
                    "start": 1256,
                    "end": 1274,
                    "matchedPaperCorpusId": "3725815"
                },
                {
                    "start": 1390,
                    "end": 1415,
                    "matchedPaperCorpusId": "159041867"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044769287109375
        },
        {
            "corpus_id": "271310083",
            "title": "TorchGT: A Holistic System for Large-Scale Graph Transformer Training",
            "text": "There have been extensive studies in sequence parallelism technologies [37]- [39] for LLMs to support efficient long sequence training. However, current parallelism methods for language models trigger two challenges when applied to graph transformers: (1) failing to leverage graph properties; (2) not supporting various graph encodings. In traditional language models, the input sequence encodes the context of a specific sentence. As such, training the language model requires tokens in the input sequence concatenated in a pre-defined order. In contrast, we observe that for graph learning tasks, there is no need for graph transformers to predict sequences (graphlevel task) or tokens (node-level task) within a position-fixed context, since they only rely on the graph topology to construct the structural encodings. A motivating example of parallelizing graphs with graph cluster is the graph-level task, where only the global token is critical for inferring the graph type and other node tokens can be arranged in any order. \n\nBased on this insight, we are the first to design a Clusteraware Graph Parallelism specialized for graph transformers, as shown in Figure 4. Specifically, the raw input sequences S and graph encodings B are randomly partitioned across P devices. Each local sub-sequence S sub and sub-encodings are projected to local matrices: Q sub , K sub , V sub , B sub \u2208 R S P \u00d7d , assuming they have the same dimensionality. Then in each graph transformer layer, all subspaces are combined together into complete matrices Q, K, V , and B via the highly efficient all-to-all collective communication operation. All-toall operation owns an advantage over other communication operations (e.g., all-gather and reduce-scatter) in terms of much smaller communication volume and overall better scalability, which is also proved in [38]. All-to-all gathers matrices in sequence dimension and splits in the head, resulting in Q, K, V , and B \u2208 R S\u00d7 d P . Now that since matrices are complete in the sequence dimension, TORCHGT reorganizes the layout according to the clustering nature of graphs discussed before.",
            "score": 0.5025097762195329,
            "section_title": "C. Cluster-aware Graph Parallelism",
            "char_start_offset": 29149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1031
                },
                {
                    "start": 1034,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 75,
                    "matchedPaperCorpusId": "246017095"
                },
                {
                    "start": 77,
                    "end": 81,
                    "matchedPaperCorpusId": "248693351"
                },
                {
                    "start": 1847,
                    "end": 1851,
                    "matchedPaperCorpusId": "262826014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0280609130859375
        },
        {
            "corpus_id": "262826014",
            "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
            "text": "Two challenges with existing parallelism approach come to the fore. First, existing parallelism approach such as data, tensor and pipeline parallelism cannot address the scaling along sequence dimension. Second, existing sequence parallelism approaches are not effective because of memory-communication inefficiencies. Furthermore, existing approaches have limited usability requiring intrusive and error prone code refactoring. \n\nIn this paper, we introduce DeepSpeed-Ulysses (or Ulysses, a very long novel), a simple, portable, and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence lengths. DeepSpeed-Ulysses partitions individual samples along the sequence dimension among participating GPUs. Then right before the attention computation, it employs all-to-all communication collective on the partitioned queries, keys and values such that each GPU receives the full sequence but only for a non-overlapping subset of the attention heads. This allows the participating GPUs to compute attention for different attention heads in parallel. Finally, DeepSpeed-Ulysses employs another all-to-all to gather the results along the attention heads while re-partitioning along the sequence dimension. \n\nIn this work, we put forward the following contributions of DeepSpeed-Ulysses to advance state of the art in long sequence parallelism: \n\n\u2022 DeepSpeed-Ulysses trains Transformer models 4x larger sequence lengths than existing systems, while enabling training with sequences with over a million tokens. \n\n\u2022 Communication reduction of over 10x compared to existing systems, resulting in throughput improvements of up to 2.5x, and sustained throughput of over 175 TFlops/GPU (over 54% of hardware peak). \n\n\u2022 Fully general and implementation agnostic attention: DeepSpeed sequence parallelism (Ulysses) supports dense as well as sparse attention, and it works with efficient attention implementations such as FlashAttention v2 [Dao, 2023]. \n\n\u2022 Support for massive model training: DeepSpeed sequence parallelism works together with ZeRO-3 to not only support large sequence lengths but also massive model sizes. \n\n\u2022 Easy-to-use and portable, requiring minimal code changes to the existing training frameworks.",
            "score": 0.5023539332417916,
            "section_title": "Introduction",
            "char_start_offset": 2188,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1387
                },
                {
                    "start": 1390,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1751
                },
                {
                    "start": 1754,
                    "end": 1986
                },
                {
                    "start": 1989,
                    "end": 2157
                },
                {
                    "start": 2160,
                    "end": 2255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.012969970703125
        },
        {
            "corpus_id": "273098476",
            "title": "How to Train Long-Context Language Models (Effectively)",
            "text": "For the final model, we mix 3% textbooks into the longcontext training data. The textbooks are open-source resources from libretexts.org, collected and made available by Chevalier et al. (2024). We pre-process the data by concatenating chapters from the same text books, as well as books from the same subject areas. This results in extremely long sequences which we pack into contexts of either 64K or 512K tokens. Though we do not have an ablation for adding this data due to limited resources, we believe that it should have a slight positive effect to the final model performance as textbooks are highly educational long-context data. \n\nTable 14: % Proportion of long documents for the short data components used in Table 6. These statistics are computed after packing and truncation and therefore correspond to the document lengths as seen by the model. We highlight that the proportion of documents beyond 32K is below 1% for ShortMix. (Paszke et al., 2019) and Hugging Face transformers (Wolf et al., 2020) for the model training. We use mosaic-streaming (Mosaic ML, 2022) for loading and mixing the data and FlashAttention 2 (Dao, 2024) for efficient attention implementation. We implement sequence parallelism based on DeepSpeed-Ulysses (Jacobs et al., 2023) across groups of 8 GPUs on the same node. We only perform distributed attention if it is necessary, i.e., only on sequences of 512K length. For long-context evaluation, we use HELMET (Yen et al., 2025) and for short-context evaluation, we use lm-eval-harness (Gao et al., 2021).",
            "score": 0.5019082463596616,
            "section_title": "A.2 Data processing",
            "char_start_offset": 35043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 638
                },
                {
                    "start": 641,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1546
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 193,
                    "matchedPaperCorpusId": "267750153"
                },
                {
                    "start": 942,
                    "end": 963,
                    "matchedPaperCorpusId": "202786778"
                },
                {
                    "start": 994,
                    "end": 1013,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 1133,
                    "end": 1144,
                    "matchedPaperCorpusId": "259936734"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2186279296875
        },
        {
            "corpus_id": "276408054",
            "title": "HybriDNA: A Hybrid Transformer-Mamba2 Long-Range DNA Language Model",
            "text": "To evaluate the computational efficiency of the HybriDNA model compared to a standard Transformer model with a similar parameter size, particularly during the training phase, we use the following two metrics:  We evaluate both models using four NVIDIA A100 GPUs (80G memory) with DeepSpeed Zero-1 Stage optimization and BF16 mixed-precision training. Both models comprise approximately 300M parameters. The Transformer model uses Flash Attention 2 optimization, while the Mamba2 layers in the HybriDNA model are implemented using CUDA. We test the pretraining of both models at various context lengths, ranging from 2k tokens to 65k tokens, doubling the sequence length at each step. \n\nAs shown in Fig. 4, the HybriDNA model achieves significantly higher training throughput than standard Transformer models, especially when processing context lengths exceeding 32,000 tokens. For instance, at a context length of 49,000 tokens, the throughput of HybriDNA is approximately 3.4 times higher than that of Transformers. This performance gap widens as context length increases, highlighting the superior efficiency of our model compared to Transformers. \n\nIn terms of GPU memory usage, HybriDNA consistently demonstrates greater efficiency than Transformer models, even those optimized with advanced techniques such as Flash Attention 2 [35]. Notably, at context lengths around 65,000 tokens, a standard Transformer model encounters Out-Of-Memory (OOM) issues on A100 GPUs. These findings underscore the exceptional capability of our hybrid model to effectively manage larger context lengths, a crucial advantage for complex long-range DNA-related tasks. \n\n5 Related Work",
            "score": 0.5016507224404219,
            "section_title": "Computational efficiency",
            "char_start_offset": 36415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1650
                },
                {
                    "start": 1653,
                    "end": 1667
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.064208984375
        },
        {
            "corpus_id": "207853129",
            "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning",
            "text": "The power of Transformer comes from relating every pair of tokens. To reduce the computation complexity while retaining the ability to capture long-range context, we model the context with a fine-to-coarse strategy. \n\nFor a leaf node, to model its local context, we connect it to neighbor token nodes or lower-level span nodes. Similarly, we connect it to higher-level span nodes for long-range context. \n\nIn detail, for a leaf node u 0,i , we add the incoming edges from the different granularity. For simplicity, we describe the process of constructing edges from its right context of node u 0,i . The edges from the left context is conducted similarly. \n\nWe use a hyper-parameter k to determine the connection density of the graph. We add k edges per level to capture the information from the right context. \n\nFor node u 0,i , its contextual nodes are \n\nwhere p l is the start index at level l and can be computed recursively: \n\nFor the sake of computation efficiency, when the index p l +k\u22121 is odd, we also add its next node in the same layer as the contextual nodes. Thus, the start index at next level is p l+1 = parent(p l + k + 1). \n\nIn practice, it is easy to find the contextual nodes in a recursive fashion. Given a leaf node u, the whole procedure is described in Algorithm 1. \n\nAfter collect all the contextual nodes, we add a directed edge from each contextual node to node u 0,i . \n\nuntil l and r reach the boundary return N end function \n\nFinally, for a sequence with length n, we can construct a directed graph G. The number of nodes is O(2n), the number of edges is O(kn log n/k). We can see that the distances between any two token nodes are no greater than 2 in graph G. This property enables our model to learn long-term dependencies easily.",
            "score": 0.5013599002050253,
            "section_title": "Contextual Edges",
            "char_start_offset": 9756,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 215
                },
                {
                    "start": 218,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 655
                },
                {
                    "start": 658,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 854
                },
                {
                    "start": 857,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1140
                },
                {
                    "start": 1143,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1289
                },
                {
                    "start": 1292,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1763
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06805419921875
        },
        {
            "corpus_id": "57759363",
            "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
            "text": "We propose a novel architecture, Transformer-XL, for language modeling with self-attention architectures beyond a fixed-length context. Our main technical contributions include introducing the notion of recurrence in a purely self-attentive model and deriving a novel positional encoding scheme. These two techniques form a complete set of solutions, as any one of them alone does not address the issue of fixed-length contexts. Transformer-XL is the first self-attention model that achieves substantially better results than RNNs on both character-level and word-level language modeling. Transformer-XL is also able to model longer-term dependency than RNNs and Transformer, and achieves substantial speedup during evaluation compared to vanilla Transformers. As we discussed in section 3.3, the naive way of computing the W k,R R i\u2212j for all pairs (i, j) is subject to a quadratic cost. Here, we present a simple method with only a linear cost. Firstly, notice that the relative distance i \u2212 j can only be integer from 0 to M + L \u2212 1, where M and L are the memory length and segment length respectively. Hence, the rows of the matrix \n\nconsist of all possible vector outputs of W k,R R i\u2212j for any (i, j). Note that we have defined Q in a reversed order, i.e., Q k = W k,R R M +L\u22121\u2212k , to make further discussion easier. \n\nNext, we collect the term (b) for all possible i, j into the following L \u00d7 (M + L) matrix, \n\nThen, we further define \n\n. Now, it is easy to see an immediate relationship between B and B, where the i-th row of B is simply a left-shifted version of i-th row of B. Hence, the computation of B only requires a matrix multiplication qQ to compute B and then a set of left-shifts. \n\nSimilarly, we can collect all term (d) for all possible i, j into another L \u00d7 (M + L) matrix D, \n\n. \n\nThen, we can follow the same procedure to define In this section, we describe the details of the metric RECL.",
            "score": 0.5012200641033286,
            "section_title": "CONCLUSIONS",
            "char_start_offset": 22052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1135
                },
                {
                    "start": 1138,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1322
                },
                {
                    "start": 1325,
                    "end": 1415
                },
                {
                    "start": 1418,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1699
                },
                {
                    "start": 1702,
                    "end": 1797
                },
                {
                    "start": 1800,
                    "end": 1801
                },
                {
                    "start": 1804,
                    "end": 1913
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048492431640625
        },
        {
            "corpus_id": "265158147",
            "title": "To Transformers and Beyond: Large Language Models for the Genome",
            "text": "Originally the Hyena layer type was introduced for NLP 75 , but more recently has been applied to genomic data in HyenaDNA 49 . The Hyena layer 75 was created to answer the demand for a transformer-like model to scale in terms of context length. The attention mechanism is bounded by quadratic cost in sequence length, and while adaptations have been made to improve the efficiency of the transformer 47,48,79 , novel architectures have been proposed to replace it entirely. These new model architectures must be able to be pre-trained like the transformer and exhibit an attention-like mechanism, without the cost of attention. Model types claiming to be the \"next transformer\" include the linear transformer (an adaptation of the transformer to calculate attention in linear-time, likening it to an RNN) 80 , RetNet (a linear transformer which removes the softmax in the attention calculation) 81 , and the Hyena layer 75 . \n\nThe Hyena layer is so far the only one of these new model types to be applied to genomic data. The authors of Hyena 75 noted that there are two main properties of the attention mechanism that contribute to its success: global context, and data dependency. In attention, global context is achieved by having every token attend to every other token. The data dependency property of the attention mechanism is enforced by the softmax equation being applied to different projections of the inputted data. In contrast, a convolutional operator is not data dependent in this way, the same kernel is applied to every section of the inputted sequence. In attention, the input itself changes how the operation is applied. \n\nThe Hyena authors proposed that the global context and data-dependency properties of attention could be satisfied by another approach, one that would have better scalability in terms of context length. To achieve this, the authors introduced a subquadratic drop-in replacement for attention, composed of implicitly parameterized long convolutions and data-controlled gating 75 . An implicitly parameterized convolution is the opposite of an explicit convolution, in which there are n parameters for the kernel size n. Instead, an implicit convolution is parameter-efficient.",
            "score": 0.5010893741092479,
            "section_title": "Hyena",
            "char_start_offset": 33435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2217
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07696533203125
        },
        {
            "corpus_id": "278033756",
            "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
            "text": "The ability to model long sequences in large language models (LLMs) lies at the heart of long-context processing (Liu et al., 2025a) and inference-time scaling (Snell et al., 2024;Muennighoff et al., 2025). The fundamental bottleneck for this ability is the self-attention mechanism (Bahdanau et al., 2015) in Transformer (Vaswani et al., 2017) LLMs: during inference-time prefilling, the complexity of attention scales quadratically with sequence length-hence, ballooning time-to-first-token and deployment cost (Jiang et al., 2024). During inference-time decoding, dense attention results in a linearly growing cache of key-value (KV) items, whose size is proportional to the sequence length. Hence, runtime is dominated by high bandwidth memory access for loading these KV pairs from memory (Nawrot et al., 2024). \n\nSparse attention mechanisms aim to address the aforementioned challenges and reduce computational overhead by approximating dense attention outputs with a subset of key-query interactions (Fu, 2024). \n\nThough such methods promise to accelerate long sequence modelling and alleviate memory load while maintaining dense model performance (Jiang et al., 2024), their viability and robustness remain unclear due to a lack of comprehensive large-scale evaluation of state-of-the-art methods. While Li et al. (2025b), Liu et al. (2025b), and Yuan et al. (2024) provide a preliminary exploration of this question, they are limited to a narrow range of configurations (model sizes, sequence lengths, and sparsity levels), specific use cases (such as multi-turn decoding), and rely on datasets with variable sequence lengths that prevent a systematic analysis of length-dependent effects. \n\nIn this work, we conduct the largest-scale empirical analysis of training-free2 sparse attention methods to date, covering models between 7B and 72B parameters, sequences between 16K to 128K tokens, and sparsity between 0% and 95%. To enable a controlled analysis, we first survey existing approaches, addressing the challenge of comparing rapidly evolving methods whose implementation details often obscure their core design principles.",
            "score": 0.5010776300834311,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 1018
                },
                {
                    "start": 1021,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1698
                },
                {
                    "start": 1701,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 306,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 322,
                    "end": 344,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 513,
                    "end": 533,
                    "matchedPaperCorpusId": "270877928"
                },
                {
                    "start": 794,
                    "end": 815,
                    "matchedPaperCorpusId": "268384862"
                },
                {
                    "start": 1155,
                    "end": 1175,
                    "matchedPaperCorpusId": "270877928"
                },
                {
                    "start": 1312,
                    "end": 1329,
                    "matchedPaperCorpusId": "274763141"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07574462890625
        },
        {
            "corpus_id": "272368245",
            "title": "Benchmarking the Performance of Large Language Models on the Cerebras Wafer Scale Engine",
            "text": "Large Language Models (LLMs) have been gaining growing interest and led to significant breakthroughs in natural language processing, enabling applications from automated text generation [1] and machine translation [2] to conversational AI [3]. Their ability to understand human inquiry and generate human-like text has enabled advancements in fields such as education [4] and healthcare [5], fostering human and AI to have closer and more direct interactions. \n\nBefore transformer models, Recurrent Neural Networks (RNNs) [6] and Long Short-Term Memory (LSTMs) were the state-of-the-art models for sequential data processing tasks. RNNs operate on sequential data processing tasks by maintaining a hidden state that captures information from previous inputs and handles one input element at a time. This enables RNNs to be able to handle inputs of arbitrary length, therefore making it suitable for language-related tasks. RNNs have been deployed in applications such as machine translation [7] and speech recognition [8]. However, RNNs suffer from vanishing/exploding gradients [9] and struggle at handling inputs that exhibit long-range dependencies among input words. LSTMs address these issues by introducing a gating mechanism, allowing important information to be preserved over long sequences. This makes LSTMs to be more effective at handling input sequences with long-term dependencies [10]. Despite the success of RNNs and LSTMs at handling textual and sequential data, they struggle with parallelization. This limits RNNs and LSTMs from scaling to large model sizes without sacrificing efficiency, therefore making them struggle to handle complex inputs and inputs with extremely long-range relationships. These limitations led to the development of transformer models [1], which utilize a Multi-Head Self-Attention (MHSA) mechanism to process input tokens simultaneously in order to find relationships among input tokens. The parallel nature of MHSA enables the model to efficiently capture relationships among very long input sequences. For example, in GPT-3 [11], the context window size is 2048 tokens, meaning the model will find relationships among the previous 2048 tokens. Moreover, the parallel nature of transformer models allows them to scale in size and excel at handling complex input sentences.",
            "score": 0.5010058782347511,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2191
                },
                {
                    "start": 2192,
                    "end": 2319
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 189,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 214,
                    "end": 217,
                    "matchedPaperCorpusId": "9447219"
                },
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "269776852"
                },
                {
                    "start": 522,
                    "end": 525,
                    "matchedPaperCorpusId": "2763403"
                },
                {
                    "start": 991,
                    "end": 994,
                    "matchedPaperCorpusId": "207970020"
                },
                {
                    "start": 1018,
                    "end": 1021,
                    "matchedPaperCorpusId": "206514100"
                },
                {
                    "start": 1780,
                    "end": 1783,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 2072,
                    "end": 2076,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1387939453125
        },
        {
            "corpus_id": "269757406",
            "title": "Length-Aware Multi-Kernel Transformer for Long Document Classification",
            "text": "To enable transformers to accept longer sequences, two primary approaches have been employed in long document modeling: efficient transformers (e.g., sparse attention transformers) and hierarchical transformers (Dong et al., 2023).Hierarchical transformer models (Li et al., 2023a;Ruan et al., 2022;Chalkidis et al., 2023) rely on chunking the text into slices of equal size and obtaining the document representation based on the representations of these slices, ensuring that the model's input does not exceed the limit in each instance.For example, HiPool (Li et al., 2023a) employs Transformers for sentence modeling and then uses Graph Convolutional Neural Networks for document information modeling.HiStruct+ (Ruan et al., 2022) encodes the hierarchical structure information of the document and infuses it into the hierarchical attention model.Due to the full-rank attention mechanism in transformer models leading to quadratic computational complexity, efficient transformers (Beltagy et al., 2020;Zaheer et al., 2020;Choromanski et al., 2021;Zhang et al., 2023) aim to use sparse attention or low-rank methods to reduce the complexity and minimize context fragmentation caused by segmentation.For instance, to reduce computational complexity from O(n 2 ) to O(n), Longformer (Beltagy et al., 2020) employs a mix of local attention (through a sliding window) and global attention on certain special tokens.Similarly, BigBird (Zaheer et al., 2020) incorporates both these attention mechanisms and introduces an additional random attention strategy.Both models have expanded their input limits to 4096 tokens.However, they do not perform well on documents of all lengths.\n\nPrior research (Li et al., 2023a) has noted that document lengths differ among datasets, and model performance can be inconsistent across corpora with varying lengths.Studies (Dai et al., 2022) have also shown that segmenting documents inevitably leads to issues of context fragmentation.However, no previous work has centered on the aforementioned two inherent issues of long document models: context fragmentation and generalizability across varying text lengths.",
            "score": 0.5007915640615277,
            "section_title": "Long Document Modeling",
            "char_start_offset": 26126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 231,
                    "end": 538
                },
                {
                    "start": 538,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 1201
                },
                {
                    "start": 1201,
                    "end": 1413
                },
                {
                    "start": 1413,
                    "end": 1554
                },
                {
                    "start": 1554,
                    "end": 1614
                },
                {
                    "start": 1614,
                    "end": 1676
                },
                {
                    "start": 1678,
                    "end": 1845
                },
                {
                    "start": 1845,
                    "end": 1966
                },
                {
                    "start": 1966,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 281,
                    "matchedPaperCorpusId": "258546884"
                },
                {
                    "start": 281,
                    "end": 299,
                    "matchedPaperCorpusId": "247594288"
                },
                {
                    "start": 299,
                    "end": 322,
                    "matchedPaperCorpusId": "258676095"
                },
                {
                    "start": 558,
                    "end": 576,
                    "matchedPaperCorpusId": "258546884"
                },
                {
                    "start": 714,
                    "end": 733,
                    "matchedPaperCorpusId": "247594288"
                },
                {
                    "start": 1005,
                    "end": 1025,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1025,
                    "end": 1050,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1050,
                    "end": 1069,
                    "matchedPaperCorpusId": "259858862"
                },
                {
                    "start": 1432,
                    "end": 1453,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1693,
                    "end": 1711,
                    "matchedPaperCorpusId": "258546884"
                },
                {
                    "start": 1853,
                    "end": 1871,
                    "matchedPaperCorpusId": "248177894"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.097900390625
        },
        {
            "corpus_id": "271974334",
            "title": "ReMamba: Equip Mamba with Effective Long-Sequence Modeling",
            "text": "Varying Length To complement our main results, which employ a maximum sequence length of 6k tokens to align with training settings, we further evaluate the model performance at varying input lengths ranging from 2k to 9k tokens. This evaluation is conducted using the LongBench and L-Eval benchmarks. As depicted in Figure 3, our ReMamba consistently outperforms the baseline Mamba model across all tested context lengths on LongBench. Notably, the performance gap between our model and the baseline widens as the context length increases. Furthermore, our model extends the efficient context length (the length at which greatest performance is observed) to 6k tokens, compared to 4k tokens for the finetuned Mamba baseline. In Figure 4, we observe performance improvements across all context lengths for our model on L-Eval. Our ReMamba even surpasses the transformers baseline. \n\nSpeed Performance and Memory Expense Our model introduces a single additional forward pass during inference, resulting in no additional memory consumption. To evaluate the speed performance, we varys the input sequence length from 1k to 8k tokens while fixing the output length at 1k tokens. For each configuration, we use a batch size of 1 and measure the speed on an NVIDIA A100 80GB GPU. We compare the performance of ReMamba, Mamba, and the vanilla transformer model (llama2-3b), as illustrated in Figure 6. The speed metric is given in tokens per second. \n\nOur experiments indicate that ReMamba operates at speeds comparable to the original baseline, maintaining a significant speed advantage over traditional transformers. \n\nGeneralizing to Mamba2 While our method is specifically tailored for Mamba, we also conduct experiments to verify its applicability to Mamba2. As is shown in Table 3, the same method applied to Mamba2 (we call Re-Mamba2 here) achieves 1.6 improved performance on averaged scores of LongBench. Here we use s = 0, p = 0.25 and \u03c1 = 0.05. The max length is still 6k. It is noteworthy that Mamba2 exhibits nearly no performance improvement over Mamba on LongBench, suggesting potential limitations within the Mamba model series.",
            "score": 0.5007591754967021,
            "section_title": "Analyses and Discussions",
            "char_start_offset": 15981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1610
                },
                {
                    "start": 1613,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2136
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.032470703125
        },
        {
            "corpus_id": "259274952",
            "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
            "text": "To test this, we explore two questions: (i.) Can a convolutional long-context model be used effectively at single nucleotide resolution? (ii.) What new capabilities could long-context genomic foundations models enable? \n\nHyenaDNA The result of our investigation is HyenaDNA, a genomic FM pretrained on the human reference genome at context lengths up to 1 million tokens at single nucleotide resolution -an up to 500x increase over existing genomic FMs using dense-attention. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than attention at sequence length 1M), uses single nucleotide tokens, and has a global receptive field at each layer. Our contributions include a \"full-stack\" recipe for building genomic FMs, including architecture design, a warm-up schedule to speed up training on ultralong sequences, and an efficient downstream adaptation procedure based on soft prompting and in-context learning.",
            "score": 0.5006305385976967,
            "section_title": "Introduction",
            "char_start_offset": 3431,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 218
                },
                {
                    "start": 221,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 941
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.051849365234375
        },
        {
            "corpus_id": "263608461",
            "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
            "text": "Transformers [37] have become the backbone of many state-of-the-art AI systems that have demonstrated impressive performance across a wide range of AI problems. Transformers achieve this success through their architecture design that uses self-attention and position-wise feedforward mechanisms. However, scaling up the context length of Transformers is a challenge [29], since the inherited architecture design of Transformers, i.e. the self-attention has memory cost quadratic in the input sequence length, which makes it challenging to scale to longer input sequences. Large context Transformers are essential for tackling a diverse array of AI challenges, ranging from processing books and high-resolution images to analyzing long videos and complex codebases. They excel at extracting information from the interconnected web and hyperlinked content, and are crucial for handling complex scientific experiment data. There have been emerging use cases of language models with significantly expanded context than before: GPT-3.5 [32] with context length 16K, GPT-4 [29] with context length 32k, MosaicML's MPT [25] with context length 65k, and Anthropic's Claude [1] with context length 100k. \n\nDriven by the significance, there has been surging research interests in reducing memory cost. One line of research leverages the observation that the softmax matrix in self-attention can be computed without materializing the full matrix [24] which has led to the development of blockwise computation of self-attention and feedforward [30,9,23] without making approximations. Despite the reduced memory, a significant challenge still arises from storing the output of each layer. This necessity arises from self-attention's inherent nature, involving interactions among all elements (n to n interactions). The subsequent layer's self-attention relies on accessing all of the prior layer's outputs. Failing to do so would increase computational costs cubically, as every output must be recomputed for each sequence element, rendering it impractical for longer sequences. [37], memory efficient transformers [30], and memory efficient attention and feedforward (blockwise parallel transformers) [23].",
            "score": 0.5000234060481228,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2195
                }
            ],
            "ref_mentions": [
                {
                    "start": 1536,
                    "end": 1538,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1538,
                    "end": 1541,
                    "matchedPaperCorpusId": "258987968"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30859375
        },
        {
            "corpus_id": "258212838",
            "title": "Towards a Neural Lambda Calculus: Neurosymbolic AI Applied to the Foundations of Functional Programming",
            "text": "The Transformer model is a type of neural network architecture that was introduced in [11]. It is designed to handle sequential data, such as natural language, and has quickly become one of the most popular models for tasks such as natural language processing, machine translation, text classification, and question answering. One of the key innovations of the Transformer model is its use of a self-attention mechanism, which allows the model to dynamically weigh the importance of different parts of the input sequence. This allows the Transformer to capture long-range dependencies in the data, which is particularly useful for processing sequences of variable lengths. Another advantage of the Transformer is its parallelization capacity, which allows it to be trained efficiently on large amounts of data. The Transformer model can be trained in parallel on multiple sequences, which is not possible with other traditional sequence-tosequence models. \n\nThe Transformer was the model chosen for this work for several reasons. First, it has parallelization features, which significantly speed up the training time. Also, it presents better performance than all other seq2seq models on a variety of natural language processing. But, besides the better technical features, the main reason we chose the model is for its self-attention mechanism. To perform the \u03b2-reduction over lambda terms, it is necessary to substitute every occurrence of the variable in question with the term. So, we think that the self-attention can be used to \"pay attention\" to every occurrence of the variable in question on the \u03bb-term when performing the task.",
            "score": 0.5000234060481228,
            "section_title": "The Transformer Model",
            "char_start_offset": 20723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1637
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05108642578125
        },
        {
            "corpus_id": "261947877",
            "title": "HBert: A Long Text Processing Method Based on BERT and Hierarchical Attention Mechanisms",
            "text": "The Transformer (Vaswani et al., 2017) model achieves excellent results in many natural language processing tasks, including classification, text generation, etc., while advancing the birth of many Transformer-based large-scale pre-trained models such as BERT (Devlin et al., 2018). The Transformer model computes the attention between each token in a sentence by self-attention, obtains the semantic information of each word and the semantic relations between words, and uses positional encoding to obtain the positional information of each word in each sentence. It can capture the whole context of a sequence in these two ways, which also leads to the success of the Transformer model. However, due to Transformer's self-attention mechanism complexity, the time complexity of the Transformer is O n d 2 ( ) (where n is the sequence length and d is the dimension of the hidden layer), which results in a limited length of the text that can be processed. Theoretically, the Transformer model can input text of arbitrary length, but due to its large complexity the Transformer cannot handle excessively long text in practical applications. The maximum length that can be processed depends on the actual situation. This also leads to a limited sequence length that Transformer-based models such as BERT can process, which is generally limited by the performance of computer hardware. While the BERT model limits the maximum input sequence length to 512 tokens, this does not mean that the Bert model can handle sentences of 512 words. The BERT model's tokenizer divides an input word into multiple subwords and adds various special tags, such as [CLS] and [SEP], which results in the length of text that BERT can handle being much less than 512 words. Since the number of subwords divided per word is not certain, the BERT does not have a fixed maximum input text length. It is certain that the maximum processable text length is less than 512 words. However, most of the texts, such as press releases, patent texts, etc., are much longer than 512 words. These longer texts are not as easy to process and cannot be processed directly by the BERT model, which limits its use in long texts processing. \n\nGenerally, the BERT processes the long text using four types of methods.",
            "score": 0.4999643636513451,
            "section_title": "INTRodUCTIoN",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2198
                },
                {
                    "start": 2201,
                    "end": 2273
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 38,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.135986328125
        },
        {
            "corpus_id": "269921290",
            "title": "Continuous Predictive Modeling of Clinical Notes and ICD Codes in Patient Health Records",
            "text": "As the computation can be performed in separate batches and then combined, this allows for considerably longer sequences to be used as input during inference.Unlike other methods for extending the context of transformers that rely on reducing or compressing long-distance attention (Beltagy et al., 2020;Munkhdalai et al., 2024), this proposed method is also exact -the result is always the same as it would be with a single pass using infinite memory.\n\n5 Experiment Set-up",
            "score": 0.49995777928988283,
            "section_title": "Extending the Context",
            "char_start_offset": 13645,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 452
                },
                {
                    "start": 454,
                    "end": 473
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.164306640625
        },
        {
            "corpus_id": "259342724",
            "title": "Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation",
            "text": "Simultaneous speech translation (SimulST) refers to the process of producing an output translation concurrently with an oncoming source speech input. For humans, performing accurate SimulST is extremely difficult and becomes nearly impossible to perform over long periods of time. Given the potential broad applications of SimulST in industry and government sectors, there is a strong need for machine learning models to perform the task to a level above the capabilities of humans. \n\nOne branch of machine learning models that have been effective in SimulST is transformers (Vaswani et al., 2017) using block processing, a process that breaks an input sequence into segments which the encoder processes sequentially and individually (Dong et al., 2019). As later segments may lose earlier information in a sentence (i.e., context fragmentation), techniques known as left context and memory banks have been introduced. The concept of left context was idealized with the Transformer-XL (Dai et al., 2019), a model optimized for language modeling, which was later adapted for streaming automatic speech recognition (ASR). The Transformer-XL generated left context by saving the previous segment to each encoder layer, so the subsequent segment could include it in the attention calculation at the same encoder layer. Memory banks were later introduced in the self-attention calculation of the Augmented Memory Transformer (Wu et al., 2020), allowing it to outperform the Transformer-XL in streaming ASR and also be state-of-the-art in SimulST (Ma et al., 2021). These memory banks were token summarizations of previous segments and helped retain explicit long-term dependencies. The Augmented Memory Transformer also included the left context alongside the center (main) segment tokens with an additional right context, all of which add computational cost. We argue that the methods to generate and use left context and/or memory banks in both the Transformer-XL and Augmented Memory Transformer are naive, costing both models' performance at a given computational budget. \n\nIn this paper, we propose a computationally efficient architecture, the Implicit Memory Transformer, that implicitly retains memory through a novel left context generation method, thereby removing the need for memory banks entirely.",
            "score": 0.49994838561979205,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2070
                },
                {
                    "start": 2073,
                    "end": 2305
                }
            ],
            "ref_mentions": [
                {
                    "start": 575,
                    "end": 597,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1541,
                    "end": 1558,
                    "matchedPaperCorpusId": "226227140"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08929443359375
        },
        {
            "corpus_id": "174801539",
            "title": "Attention is all you need for Videos: Self-attention based Video Summarization using Universal Transformers",
            "text": "Recurrent Neural Networks (RNN's) and Long Short Term Memory cells have been posed for sequence to sequence tasks for a long time. While RNN's and LSTM's are naturally suited for these tasks, they fail to capture long-term dependencies or adapt to sequence lengths not encountered in training. Machine Translation systems use an encoderdecoder architecture, in which, the outputs of the decoder at each time step are conditioned on the encoder. The inability of RNN's or LSTM's to capture long-term dependencies is well exposed in these encoder-decoder based translation system. The encoded context vector generated by the encoder fails to capture information about the tokens seen in the beginning of the sequence, especially when the sequences are long. This then makes vanilla RNN's and LSTM's unsuitable for modeling translation based tasks having long sequences. \n\nBengio et al. [21] introduced the scaled-dot-product attention mechanism which saw an improvement over the encoderdecoder based architectures. The key idea behind their success was rather than using just the context vector generated by the encoder, soft-attention can help improve the performance of the decoder, by providing it the hidden states of the encoder. In a way, the decoder peeks at the input sequence using an attention distribution to decode the sequence. Several other improvements over this architecture have been proposed, but all of these rely on using either RNN's and LSTM's, and hence, are unable to capture long-term dependencies and cannot be parallelized across training examples. \n\nThe transformer model [22] addressed the shortcomings of recurrent machine translation systems by proposing an architecture that relies only on self-attention. Since they take recurrence completely out of the picture, the Transformer model allows parallelization across training samples and generates a feature representation in a fixed number of steps, which are chosen empirically. They also use the scaled dot product attention mechanism over the Keys K, Queries Q and Values V and compute the representation using softmax as: \n\nVaswani et al. [22] use N = 8 attention heads and concatenate the outputs of each of these heads to compute the self-attention representation of the inputs.",
            "score": 0.49990025775491903,
            "section_title": "Sequence to Sequence models using self-attention",
            "char_start_offset": 6817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1573
                },
                {
                    "start": 1576,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2105
                },
                {
                    "start": 2108,
                    "end": 2264
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043701171875
        },
        {
            "corpus_id": "276885221",
            "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
            "text": "While the Training subsystem is designed to achieve efficient training of Linear-MoE models on modern accelerators. In addition to supporting state-of-the-art training techniques, we incorporate a specialized Sequence Parallelism (SP) technique for LSM modules, which is particularly effective for handling extremely long input sequences on Linear-MoE architecture. Importantly, the system is designed to be extensible, enables more advanced sequence modeling methods or training techniques integrated in the future. Furthermore, we also explore efficient modeling and training for hybrid Linear-MoE models, which combine Linear-MoE layers with standard Transformer-MoE layers. For hybrid models, we introduce an SP method that employs distinct computational and communication strategies tailored to the different types of layers. \n\nOur contributions can be summarized as follows: \n\n\u2022 Production-level System. We introduce Linear-MoE, a production-level system designing for efficient modeling and training of large-scale MoE models with LSM modules integrated. The standard softmax attention (Vaswani et al., 2017), commonly used in transformer models, whose parallel computation form during training can typically be expressed as: \n\nHere, the matrices Q, K, V, O \u2208 R N \u00d7d correspond to the query, key, value, and output matrices, respectively. The matrices Q, K, and V are linear projections of the input matrix X \u2208 R N \u00d7d , defined as Q = XW Q , K = XW K , and V = XW V , where W Q , W K , W V \u2208 R d\u00d7d are learnable weight matrices. Here, N and d represent the sequence length and hidden dimension. Linear Attention (Katharopoulos et al., 2020) as one of the representative LSM methods, has emerged as a viable alternative to traditional softmax attention by implementing two primary modifications. First, it eliminates the Softmax(\u2022) operation, instead embedding it within a kernel feature map. Second, it leverages the associative property of matrix multiplication, reconfiguring (QK \u22a4 )V into Q(K \u22a4 V). These changes reduce both the computational and memory complexity from O(N 2 d) to O(N d 2 ).",
            "score": 0.4995480384296197,
            "section_title": "Introduction",
            "char_start_offset": 3676,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1232
                },
                {
                    "start": 1235,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 1093,
                    "end": 1115,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1619,
                    "end": 1647,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0202178955078125
        },
        {
            "corpus_id": "269213989",
            "title": "Length Generalization of Causal Transformers without Position Encoding",
            "text": "Generalizing to longer sentences is important for recent Transformer-based language models. Besides algorithms manipulating explicit position features, the success of Transformers without position encodings (NoPE) provides a new way to overcome the challenge. In this paper, we study the length generalization property of NoPE. We find that although NoPE can extend to longer sequences than the commonly used explicit position encodings, it still has a limited context length. We identify a connection between the failure of NoPE's generalization and the distraction of attention distributions. We propose a parameter-efficient tuning for searching attention heads' best temperature hyper-parameters, which substantially expands NoPE's context size. Experiments on long sequence language modeling, the synthetic passkey retrieval task and real-world long context tasks show that NoPE can achieve competitive performances with state-of-the-art length generalization algorithms. The source code is publicly accessible",
            "score": 0.49952514405198656,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303466796875
        },
        {
            "corpus_id": "270738084",
            "title": "LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism",
            "text": "With the emergence of Large Language Models (LLM) in recent years, researchers have investigated and proposed many advanced training methodologies in a distributed way, such as data parallelism (DP) [23,25,26,36], tensor parallelism (TP) [15], pipeline parallelism (PP) [4,20], PyTorch FSDP [52], and automatic parallelization frameworks [53].Recently, LLMs with long sequences have driven the development of novel applications that are essential in our daily lives, including generative AI [33] and long-context understanding [5,16,54].With the increased popularity of ChatGPT, long dialogue processing tasks have become more important for chatbot applications than ever [45].In addition to these scenarios for language processing, Transformer-based giant models also achieve impressive performance in computer vision [3,49,50] and AI for science [6,30], where inputs with long sequences are critical for complex tasks such as video stream processing [41] and protein property prediction [9].\n\nTraining LLMs with long sequences requires massive memory resources and computation.To tackle these challenges, sequence parallelism (SP) has been proposed [21,24,29,34], which can be basically divided into two categories: head parallelism (HP) [21] and context parallelism (CP) [29,34].In Attention blocks, HP methods keep the whole sequence and compute attention for different heads in parallel, while CP methods split the QKV (Query, Key, and Value) tensors into chunks along the sequence dimension.However, both face limitations when applied to extremely-long-sequence LLMs at a large scale.First, HP meets the scalability issue.In HP, the degree of SP inherently cannot exceed the number of attention heads [21].Therefore, there is an upper bound for the degree that HP can scale out.Second, CP meets the communication inefficiency issue.CP [29,34] employs a peer-to-peer (P2P) communication primitive.However, P2P encounters issues of low intra-node bandwidth utilization and low inter-node network resource utilization.",
            "score": 0.4987960884924531,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 677
                },
                {
                    "start": 677,
                    "end": 993
                },
                {
                    "start": 995,
                    "end": 1079
                },
                {
                    "start": 1079,
                    "end": 1282
                },
                {
                    "start": 1282,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1628
                },
                {
                    "start": 1628,
                    "end": 1712
                },
                {
                    "start": 1712,
                    "end": 1784
                },
                {
                    "start": 1784,
                    "end": 1838
                },
                {
                    "start": 1838,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 203,
                    "end": 206,
                    "matchedPaperCorpusId": "4614646"
                },
                {
                    "start": 206,
                    "end": 209,
                    "matchedPaperCorpusId": "2235165"
                },
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "372467"
                },
                {
                    "start": 270,
                    "end": 273,
                    "matchedPaperCorpusId": "243847496"
                },
                {
                    "start": 291,
                    "end": 295,
                    "matchedPaperCorpusId": "258297871"
                },
                {
                    "start": 491,
                    "end": 495,
                    "matchedPaperCorpusId": "234342691"
                },
                {
                    "start": 533,
                    "end": 536,
                    "matchedPaperCorpusId": "225039888"
                },
                {
                    "start": 819,
                    "end": 822,
                    "matchedPaperCorpusId": "232417054"
                },
                {
                    "start": 822,
                    "end": 825,
                    "matchedPaperCorpusId": "231719476"
                },
                {
                    "start": 825,
                    "end": 828,
                    "matchedPaperCorpusId": "216641891"
                },
                {
                    "start": 848,
                    "end": 851,
                    "matchedPaperCorpusId": "261918830"
                },
                {
                    "start": 952,
                    "end": 956,
                    "matchedPaperCorpusId": "237581134"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24072265625
        },
        {
            "corpus_id": "274464890",
            "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
            "text": "Transformer Architecture. Large language models (LLMs) are predominantly built upon the Transformer architecture [61], which is used to capture complex long-range dependencies in sequential data. At the core of Transformer architecture is the attention mechanism, which allows each token in a sequence to weigh the significance of other tokens when constructing its contextualized representation. Mathematically, the attention computation is defined as: \n\nQ and K are matrices of size l \u00d7 d, representing the queries and keys, respectively, where l denotes the number of tokens and d represents the feature dimensionality. Vectors q i , k i , and v i correspond to the query, key, and value of the i th token. The dot product between Q and K measures the relevance between each pair of query and key. The softmax function normalizes these dot products to form a probability distribution, referred to as attention scores, which are used to compute a weighted sum of the value vectors and produce the output. \n\nTo capture a broader range of interactions between tokens, Transformer models employ multi-head attention (MHA), where each head independently computes attention using distinct projections of queries, keys, and values. The outputs from different heads are concatenated and linearly transformed to create a more expressive representation. Grouped-query attention (GQA) [5] improves the efficiency of MHA by allowing a group of query heads to share the same projection of keys and values, referred to as a KV head. Autoregressive Generation and KV Cache. The autoregressive generation process of LLMs consists of two phases: the prompt phase, during which the model computes latent representations for all tokens in the prompt, the user-provided context, and generates the first new token, and the generation phase, where the model iteratively generates subsequent tokens by attending to both the prompt and previously generated tokens. To avoid redundant computations across generation steps, KV cache is introduced to store the keys and values of all previous tokens. However, the size of the KV cache grows linearly with both the sequence length and batch size, quickly becoming a bottleneck for inference throughput [32,67]. Therefore, efficient KV cache management is critical for alleviating memory bottlenecks and improving LLM serving efficiency.",
            "score": 0.4983869579421514,
            "section_title": "Large Language Models",
            "char_start_offset": 7234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2361
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 117,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 2227,
                    "end": 2231,
                    "matchedPaperCorpusId": "261697361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047607421875
        },
        {
            "corpus_id": "273403508",
            "title": "SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs",
            "text": "Powerful but Complex Attention in Transformer. The advent of attention mechanisms, particularly within the Transformer architecture (Vaswani, 2017), marked a significant advancement in natural language processing. Attention enables improved handling of long-range dependencies and a better understanding of context by attending each token to every other token in the sequence, resulting in a quadratic memory and time complexity O(n 2 ), where n is the sequence length. This presents a significant challenge as the community moves towards LLMs that can process increasingly longer contexts. Many studies explore alternative attention mechanisms to mitigate this complexity. The Reformer architecture (Kitaev et al., 2020) reduces the complexity to O(n log n) and the linear attention mechanism (Katharopoulos et al., 2020) further decreases complexity to O(n). \n\nRecently, there has been a trend of revisiting recurrent neural networks, leading to the proposal of new architectural frameworks such as RWKV (Peng et al., 2023), RetNet (Sun et al., 2023), and Mamba (Gu & Dao, 2023). Despite their promise of efficiency, these methods struggle to match the performance of full attention mechanisms, particularly with larger models and longer contexts. \n\nIntrinsic but Dynamic Sparsity in Attention. Attention mechanisms inherently exhibit sparsity, which arises from the attention map A generated by Q and K: A = softmax(QK T / \u221a d). The softmax function often produces a multitude of negligible scores that can be treated as zeros without impacting model accuracy (Zaheer et al., 2020;Liu et al., 2021;Wang et al., 2021;Child et al., 2019). Attention sparsity becomes more pronounced with longer contexts, presenting opportunities to optimize inference speed. Unfortunately, this sparsity is dynamic, varying across different inputs and attention heads, each displaying distinct sparsity locations and ratios. Prior research has attempted to approximate attention sparsity using predefined patterns and heuristics (Fu et al., 2024;Jiang et al., 2024). Yet, these methods lack generality and often rely on handcrafted features, struggling to fully capture the sparsity behavior of attention mechanisms.",
            "score": 0.498372193791012,
            "section_title": "BACKGROUND AND MOTIVATION",
            "char_start_offset": 5255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2200
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 147,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 794,
                    "end": 822,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1563,
                    "end": 1584,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1601,
                    "end": 1619,
                    "matchedPaperCorpusId": "229298088"
                },
                {
                    "start": 2013,
                    "end": 2030,
                    "matchedPaperCorpusId": "231573431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1275634765625
        },
        {
            "corpus_id": "222291274",
            "title": "SJTU-NICT\u2019s Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task",
            "text": "In addition, we argue that since long sequences encoding is easier than decoding, truly whole document-level translation is still a long way off, since the bidirectional context is available in the encoder, but only the past is visible by the decoder. \n\nLongformer To make the long documents processed with Transformer (Vaswani et al., 2017) architecture feasible or easier, a modified Transformer architecture named Longformer was proposed by Beltagy et al. (2020), in which the limitation for memory and computational requirements is addressed with a novel self-attention operation scales linearly with the sequence length. \n\nIn Longformer, the original full self-attention (O(n 2 ) time and memory complexity) is sparsified to makes it efficient for longer sequences. There are three \"attention patterns\" for specifying pairs of input locations attending to one another. \n\n\u2022 Sliding Window Self-attention is performed in a fixed-size window w and multiple stacked layers of such sliding windowed attention results in a large receptive field as analogs to CNNs. \n\n\u2022 Dilated Sliding Window Inspired by the dilated CNNs (Oord et al., 2016), dilation gaps of size d is introduced to the window to further increase the receptive field without increasing computation. \n\n\u2022 Global Attention Though the receptive field is enlarged by stacking multiple layers and dilation in sliding window and dilated sliding window attention patterns, some part of the long sequence has the requirement for keeping the full and global receptive field due to the downstream tasks, so global attention is introduced to make up this need. \n\nIn our document-enhanced NMT model, some heads in multi-head attention are set to use the sliding window pattern to focus on the local context which was revealed very important (Kovaleva et al., 2019), while others with dilation focus on longer context. Besides, as Longformer is incorporated into the NMT model, we perform global attention on the position of [CLS] token in which the representation of the whole sequence (i.e., the document embedding) is generated. This makes the previous document-enhanced model with document embedding as a special case of ours.",
            "score": 0.49834827044002356,
            "section_title": "Document-enhanced NMT",
            "char_start_offset": 12841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 254,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1614
                },
                {
                    "start": 1617,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2182
                }
            ],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 341,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08343505859375
        },
        {
            "corpus_id": "271974334",
            "title": "ReMamba: Equip Mamba with Effective Long-Sequence Modeling",
            "text": "Conversely, a substantial performance degradation is observed for Mamba on long-context tasks relative to Transformers. This performance disparity underscores a significant limitation of Mamba models in practical long-context applications. \n\nThis long-context deficency issue of Mamba is usually attributed to its RNN-like nature. This kind of architecture exhibits limitations in preserving crucial information from earlier input sequences as the context length increases due to the fixed-size memory (Wen, Dang, and Lyu 2024;Yang et al. 2024b). Hybrid architectures (Lieber et al. 2024;Ren et al. 2024;Park et al. 2024) have sought to mitigate this issue by integrating attention mechanisms from transformers. However, these approaches often lead to decreased computational efficiency and increased memory consumption. A parallel study, DeciMamba (Ben-Kish et al. 2024), also attributes Mamba's limitations to a restricted effective recep-arXiv:2408.15496v3 [cs.CL] 1 Sep 2024 tive field and proposes a method for discarding less important tokens in specific layers to extend the length capabilities of Mamba. However, their focus is on improving the length extrapolation ability without training, and they still achieve limited performance. \n\nTo improve the long-context performance of Mamba, we introduce ReMamba. The core intention in ReMamba is straightforward. The distant information within Mamba undergoes excessive degradation. An effective compression strategy to condense information and reduce the distance can be of help. Our approach achieves this compression by selecting the top-k hidden states during the first forward pass and leverages Mamba's selective mechanism to incorporate them into the state space during the second forward pass. ReMamba incurs minimal additional computational overhead (a single extra forward pass). Experimental results demonstrate that our approach significantly improves Mamba's long-context performance, bringing it close to the performance of transformers. Our ReMamba model achieves a 3.2 improvement over the baseline on LongBench (Bai et al. 2024) and 1.6 improvement on L-Eval (An et al. 2023).",
            "score": 0.4982538107409628,
            "section_title": "Introduction",
            "char_start_offset": 2042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2148
                }
            ],
            "ref_mentions": [
                {
                    "start": 588,
                    "end": 604,
                    "matchedPaperCorpusId": "267499935"
                },
                {
                    "start": 604,
                    "end": 621,
                    "matchedPaperCorpusId": "267499935"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11279296875
        },
        {
            "paperId": "353539616338018f4a381b685e719fbf64aba37c",
            "corpusId": 270870290,
            "title": "WallFacer: Guiding Transformer Model Training Out of the Long-Context Dark Forest with N-body Problem",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 51,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2407.00611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2407.00611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280088379",
                    "name": "Ziming Liu"
                },
                {
                    "authorId": "2309188857",
                    "name": "Shaoyu Wang"
                },
                {
                    "authorId": "1454003795",
                    "name": "Shenggan Cheng"
                },
                {
                    "authorId": "2309195825",
                    "name": "Zhongkai Zhao"
                },
                {
                    "authorId": "2280071587",
                    "name": "Xuanlei Zhao"
                },
                {
                    "authorId": "2261562755",
                    "name": "Jim Demmel"
                },
                {
                    "authorId": "2285076067",
                    "name": "Yang You"
                }
            ],
            "abstract": "In recent years, Transformer-based Large Language Models (LLMs) have garnered significant attention due to their exceptional performance across a variety of tasks. However, training these models on long sequences presents a substantial challenge in terms of efficiency and scalability. Current methods are constrained either by the number of attention heads, limiting scalability, or by excessive communication overheads. In this paper, we propose an insight that Attention Computation can be considered as a special case of n-body problem with direct interactions. Based on this concept, this paper introduces WallFacer, an efficient long-sequence training system with a novel multi-dimensional ring sequence parallelism, fostering an efficient communication paradigm and extra tuning space for communication arrangement. Through comprehensive experiments under diverse environments and model settings, we demonstrate that WallFacer significantly surpasses state-of-the-art method that supports near-infinite sequence length, achieving performance improvements of up to 77.12%.",
            "corpus_id": "270870290",
            "text": "In recent years, Transformer-based Large Language Models (LLMs) have garnered significant attention due to their exceptional performance across a variety of tasks. However, training these models on long sequences presents a substantial challenge in terms of efficiency and scalability. Current methods are constrained either by the number of attention heads, limiting scalability, or by excessive communication overheads. In this paper, we propose an insight that Attention Computation can be considered as a special case of n-body problem with direct interactions. Based on this concept, this paper introduces WallFacer, an efficient long-sequence training system with a novel multi-dimensional ring sequence parallelism, fostering an efficient communication paradigm and extra tuning space for communication arrangement. Through comprehensive experiments under diverse environments and model settings, we demonstrate that WallFacer significantly surpasses state-of-the-art method that supports near-infinite sequence length, achieving performance improvements of up to 77.12%.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1666259765625
        },
        {
            "paperId": "eb06e95dd3eb5a916e52d2e463f474ef4967d8ca",
            "corpusId": 269149599,
            "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with Elastic Sequence Parallelism",
            "venue": "Symposium on Operating Systems Principles",
            "year": 2024,
            "referenceCount": 54,
            "citationCount": 63,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695948",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.09526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2216328376",
                    "name": "Bingya Wu"
                },
                {
                    "authorId": "2279874847",
                    "name": "Shengyu Liu"
                },
                {
                    "authorId": "2203414303",
                    "name": "Yinmin Zhong"
                },
                {
                    "authorId": "2297146205",
                    "name": "Peng Sun"
                },
                {
                    "authorId": "2237080638",
                    "name": "Xuanzhe Liu"
                },
                {
                    "authorId": "2279869979",
                    "name": "Xin Jin"
                }
            ],
            "abstract": "The context window of large language models (LLMs) is rapidly increasing, leading to a huge variance in resource usage between different requests as well as between different phases of the same request. Restricted by static parallelism strategies, existing LLM serving systems cannot efficiently utilize the underlying resources to serve variable-length requests in different phases. To address this problem, we propose a new parallelism paradigm, elastic sequence parallelism (ESP), to elastically adapt to the variance across different requests and phases. Based on ESP, we design and build LoongServe, an LLM serving system that (1) improves computation efficiency by elastically adjusting the degree of parallelism in real-time, (2) improves communication efficiency by reducing key-value cache migration overhead and overlapping partial decoding communication with computation, and (3) improves GPU memory efficiency by reducing key-value cache fragmentation across instances. Our evaluation under diverse real-world datasets shows that LoongServe improves the throughput by up to 3.85\u00d7 compared to chunked prefill and 5.81\u00d7 compared to prefill-decoding disaggregation.",
            "corpus_id": "269149599",
            "text": "The context window of large language models (LLMs) is rapidly increasing, leading to a huge variance in resource usage between different requests as well as between different phases of the same request. Restricted by static parallelism strategies, existing LLM serving systems cannot efficiently utilize the underlying resources to serve variable-length requests in different phases. To address this problem, we propose a new parallelism paradigm, elastic sequence parallelism (ESP), to elastically adapt to the variance across different requests and phases. Based on ESP, we design and build LoongServe, an LLM serving system that (1) improves computation efficiency by elastically adjusting the degree of parallelism in real-time, (2) improves communication efficiency by reducing key-value cache migration overhead and overlapping partial decoding communication with computation, and (3) improves GPU memory efficiency by reducing key-value cache fragmentation across instances. Our evaluation under diverse real-world datasets shows that LoongServe improves the throughput by up to 3.85\u00d7 compared to chunked prefill and 5.81\u00d7 compared to prefill-decoding disaggregation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0214080810546875
        },
        {
            "paperId": "5e424004958853f4e366e7a86a1c3a56a76cb2a4",
            "corpusId": 270973564,
            "title": "LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 27,
            "citationCount": 23,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.03294",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2310.03294?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2310.03294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2117961435",
                    "name": "Dacheng Li"
                },
                {
                    "authorId": "2254264970",
                    "name": "Rulin Shao"
                },
                {
                    "authorId": "2254221083",
                    "name": "Anze Xie"
                },
                {
                    "authorId": "2243336934",
                    "name": "Eric P. Xing"
                },
                {
                    "authorId": "2308794329",
                    "name": "Joseph Gonzalez"
                },
                {
                    "authorId": "2055174324",
                    "name": "Ion Stoica"
                },
                {
                    "authorId": "2309897086",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "2257340589",
                    "name": "Hao Zhang"
                }
            ],
            "abstract": "Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LIGHTSEQ, for long-context LLMs training. LIGHTSEQ has many notable advantages. First, LIGHTSEQ partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LIGHTSEQ not only requires up to 4.7\u00d7 less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LIGHTSEQ features a novel gradient checkpointing scheme to bypass an forward computation for memory-efficient attention. We evaluate LIGHTSEQ on Llama-7B and its variants with sequence lengths from 32K to 512K. Through comprehensive experiments on single and cross-node training, we show that LIGHTSEQ achieves up to 1.24-2.01\u00d7 end-to-end speedup, and a 2-8\u00d7 longer sequence length on models with fewer heads, compared to Megatron-LM. Codes will be available at https://github.com/RulinShao/LightSeq.",
            "corpus_id": "270973564",
            "text": "Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LIGHTSEQ, for long-context LLMs training. LIGHTSEQ has many notable advantages. First, LIGHTSEQ partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LIGHTSEQ not only requires up to 4.7\u00d7 less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LIGHTSEQ features a novel gradient checkpointing scheme to bypass an forward computation for memory-efficient attention. We evaluate LIGHTSEQ on Llama-7B and its variants with sequence lengths from 32K to 512K. Through comprehensive experiments on single and cross-node training, we show that LIGHTSEQ achieves up to 1.24-2.01\u00d7 end-to-end speedup, and a 2-8\u00d7 longer sequence length on models with fewer heads, compared to Megatron-LM. Codes will be available at https://github.com/RulinShao/LightSeq.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.29150390625
        },
        {
            "paperId": "03fc1872ee0a2e284f14386aab69a18b416289b6",
            "corpusId": 276712539,
            "title": "WeiPipe: Weight Pipeline Parallelism for Communication-Effective Long-Context Large Model Training",
            "venue": "ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming",
            "year": 2025,
            "referenceCount": 33,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3710848.3710869?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3710848.3710869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2238399650",
                    "name": "Junfeng Lin"
                },
                {
                    "authorId": "2259092494",
                    "name": "Zi-li Liu"
                },
                {
                    "authorId": "2348095401",
                    "name": "Yang You"
                },
                {
                    "authorId": "2274795791",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "2170672707",
                    "name": "Weihao Zhang"
                },
                {
                    "authorId": "2155262353",
                    "name": "Rong Zhao"
                }
            ],
            "abstract": "Training large language models (LLMs) has become increasingly expensive due to the rapid expansion in model size. Pipeline parallelism is a widely used distributed training technique. However, as LLMs with larger context become prevalent and memory optimization techniques advance, traditional PP methods encounter greater communication challenges due to the increased size of activations and gradients of activations. To address this issue, we introduce weight-pipeline parallelism (WeiPipe) that transitions from an activation-passing pipeline to a weight-passing pipeline. WeiPipe reduces communication costs and achieves a more balanced utilization by transmitting only weights and their gradients between workers in a pipeline manner. WeiPipe does not rely on collective communication primitives, thus ensuring scalability. We present four variations of WeiPipe parallelism, including WeiPipe-Interleave, which emphasizes communication efficiency, and WeiPipe-zero-bubble, discussing the potential for minimal bubble ratios. Our implementation of WeiPipe-Interleave, performed on up to 32 GPUs and tested in various model configurations, including large-context LLM training, demonstrates a significant improvement in throughput compared to state-of-the-art pipeline parallelism and fully sharded data parallelism with different underlying infrastructures, including NVLink connections within cluster with Ethernet among cluster, and PCIe within cluster and Ethernet among cluster. Additionally, WeiPipe also shows greater scalability in communication-constrained scenarios compared to state-of-art strategies.",
            "corpus_id": "276712539",
            "text": "Training large language models (LLMs) has become increasingly expensive due to the rapid expansion in model size. Pipeline parallelism is a widely used distributed training technique. However, as LLMs with larger context become prevalent and memory optimization techniques advance, traditional PP methods encounter greater communication challenges due to the increased size of activations and gradients of activations. To address this issue, we introduce weight-pipeline parallelism (WeiPipe) that transitions from an activation-passing pipeline to a weight-passing pipeline. WeiPipe reduces communication costs and achieves a more balanced utilization by transmitting only weights and their gradients between workers in a pipeline manner. WeiPipe does not rely on collective communication primitives, thus ensuring scalability. We present four variations of WeiPipe parallelism, including WeiPipe-Interleave, which emphasizes communication efficiency, and WeiPipe-zero-bubble, discussing the potential for minimal bubble ratios. Our implementation of WeiPipe-Interleave, performed on up to 32 GPUs and tested in various model configurations, including large-context LLM training, demonstrates a significant improvement in throughput compared to state-of-the-art pipeline parallelism and fully sharded data parallelism with different underlying infrastructures, including NVLink connections within cluster with Ethernet among cluster, and PCIe within cluster and Ethernet among cluster. Additionally, WeiPipe also shows greater scalability in communication-constrained scenarios compared to state-of-art strategies.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0225067138671875
        },
        {
            "paperId": "31372726dbfcd9ea92a2e08a62ae5439d989c824",
            "corpusId": 269982244,
            "title": "Equipping Transformer with Random-Access Reading for Long-Context Understanding",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 47,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.13216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2302927288",
                    "name": "Chenghao Yang"
                },
                {
                    "authorId": "2278970280",
                    "name": "Zi Yang"
                },
                {
                    "authorId": "2278831818",
                    "name": "Nan Hua"
                }
            ],
            "abstract": "Long-context modeling presents a significant challenge for transformer-based large language models (LLMs) due to the quadratic complexity of the self-attention mechanism and issues with length extrapolation caused by pretraining exclusively on short inputs. Existing methods address computational complexity through techniques such as text chunking, the kernel approach, and structured attention, and tackle length extrapolation problems through positional encoding, continued pretraining, and data engineering. These approaches typically require $\\textbf{sequential access}$ to the document, necessitating reading from the first to the last token. We contend that for goal-oriented reading of long documents, such sequential access is not necessary, and a proficiently trained model can learn to omit hundreds of less pertinent tokens. Inspired by human reading behaviors and existing empirical observations, we propose $\\textbf{random access}$, a novel reading strategy that enables transformers to efficiently process long documents without examining every token. Experimental results from pretraining, fine-tuning, and inference phases validate the efficacy of our method.",
            "corpus_id": "269982244",
            "text": "Long-context modeling presents a significant challenge for transformer-based large language models (LLMs) due to the quadratic complexity of the self-attention mechanism and issues with length extrapolation caused by pretraining exclusively on short inputs. Existing methods address computational complexity through techniques such as text chunking, the kernel approach, and structured attention, and tackle length extrapolation problems through positional encoding, continued pretraining, and data engineering. These approaches typically require $\\textbf{sequential access}$ to the document, necessitating reading from the first to the last token. We contend that for goal-oriented reading of long documents, such sequential access is not necessary, and a proficiently trained model can learn to omit hundreds of less pertinent tokens. Inspired by human reading behaviors and existing empirical observations, we propose $\\textbf{random access}$, a novel reading strategy that enables transformers to efficiently process long documents without examining every token. Experimental results from pretraining, fine-tuning, and inference phases validate the efficacy of our method.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.59033203125
        },
        {
            "paperId": "ba916d7d2bba90b571b9df27cdb4be773d1a25a4",
            "corpusId": 274023420,
            "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 49,
            "citationCount": 16,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.09688, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2029486869",
                    "name": "Coleman Hooper"
                },
                {
                    "authorId": "2262511276",
                    "name": "Sehoon Kim"
                },
                {
                    "authorId": "2259959157",
                    "name": "Hiva Mohammadzadeh"
                },
                {
                    "authorId": "2330588600",
                    "name": "Monishwaran Maheswaran"
                },
                {
                    "authorId": "2330588188",
                    "name": "June Paik"
                },
                {
                    "authorId": "2271923589",
                    "name": "Michael W. Mahoney"
                },
                {
                    "authorId": "2242659602",
                    "name": "Kurt Keutzer"
                },
                {
                    "authorId": "2303682290",
                    "name": "Amir Gholami"
                }
            ],
            "abstract": "Emerging Large Language Model (LLM) applications require long input context in order to perform complex tasks like document analysis and code generation. For these long context length applications, the length of the input prompt poses a significant challenge in terms of inference efficiency since the inference costs increase linearly with sequence length. However, for many of these applications, much of the context in the prompt is fixed across different user inputs, thereby providing the opportunity to perform offline optimizations in order to process user inputs quickly, as they are received. We propose Squeezed Attention to accelerate LLM applications where a large portion of the input context is fixed. We first leverage K-means clustering offline to group the keys for the fixed context based on semantic similarity and represent each cluster with a single centroid value. During inference, we compare query tokens from the user input with the centroids to predict which keys from the fixed context are semantically relevant, and then compute exact attention using only the important keys, thereby reducing bandwidth and computational costs. We also present a hierarchical version of our algorithm which can reduce the complexity of attention from linear to logarithmic with respect to the fixed context length. We evaluate our method on long-context benchmarks including LongBench, where it achieves a 3.1$\\times$ reduction in KV budget with no noticeable accuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy gap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models. Futhermore, we implement kernels for centroid comparison and sparse FlashAttention with important keys, achieving more than 4$\\times$ speedups during both the prefill and generation phases for long-context inference. Our code is available at https://github.com/SqueezeAILab/SqueezedAttention.",
            "corpus_id": "274023420",
            "text": "Emerging Large Language Model (LLM) applications require long input context in order to perform complex tasks like document analysis and code generation. For these long context length applications, the length of the input prompt poses a significant challenge in terms of inference efficiency since the inference costs increase linearly with sequence length. However, for many of these applications, much of the context in the prompt is fixed across different user inputs, thereby providing the opportunity to perform offline optimizations in order to process user inputs quickly, as they are received. We propose Squeezed Attention to accelerate LLM applications where a large portion of the input context is fixed. We first leverage K-means clustering offline to group the keys for the fixed context based on semantic similarity and represent each cluster with a single centroid value. During inference, we compare query tokens from the user input with the centroids to predict which keys from the fixed context are semantically relevant, and then compute exact attention using only the important keys, thereby reducing bandwidth and computational costs. We also present a hierarchical version of our algorithm which can reduce the complexity of attention from linear to logarithmic with respect to the fixed context length. We evaluate our method on long-context benchmarks including LongBench, where it achieves a 3.1$\\times$ reduction in KV budget with no noticeable accuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy gap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models. Futhermore, we implement kernels for centroid comparison and sparse FlashAttention with important keys, achieving more than 4$\\times$ speedups during both the prefill and generation phases for long-context inference. Our code is available at https://github.com/SqueezeAILab/SqueezedAttention.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.21142578125
        },
        {
            "paperId": "3f03876b23b491bdc161816024044e13b02b46e5",
            "corpusId": 271903601,
            "title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 75,
            "citationCount": 98,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.10188, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313476433",
                    "name": "Fuzhao Xue"
                },
                {
                    "authorId": "2316441683",
                    "name": "Yukang Chen"
                },
                {
                    "authorId": "2316444311",
                    "name": "Dacheng Li"
                },
                {
                    "authorId": "2316787431",
                    "name": "Qinghao Hu"
                },
                {
                    "authorId": "20515689",
                    "name": "Ligeng Zhu"
                },
                {
                    "authorId": "2141625113",
                    "name": "Xiuyu Li"
                },
                {
                    "authorId": "2312873153",
                    "name": "Yunhao Fang"
                },
                {
                    "authorId": "150127950",
                    "name": "Haotian Tang"
                },
                {
                    "authorId": "2202210853",
                    "name": "Shang Yang"
                },
                {
                    "authorId": "2316442094",
                    "name": "Zhijian Liu"
                },
                {
                    "authorId": "2316430247",
                    "name": "Ethan He"
                },
                {
                    "authorId": "1989015",
                    "name": "Hongxu Yin"
                },
                {
                    "authorId": "2824500",
                    "name": "Pavlo Molchanov"
                },
                {
                    "authorId": "2273651410",
                    "name": "Jan Kautz"
                },
                {
                    "authorId": "2257381161",
                    "name": "Linxi Fan"
                },
                {
                    "authorId": "2258068214",
                    "name": "Yuke Zhu"
                },
                {
                    "authorId": "2274926912",
                    "name": "Yao Lu"
                },
                {
                    "authorId": "2273855886",
                    "name": "Song Han"
                }
            ],
            "abstract": "Long-context capability is critical for multi-modal foundation models, especially for long video understanding. We introduce LongVILA, a full-stack solution for long-context visual-language models by co-designing the algorithm and system. For model training, we upgrade existing VLMs to support long video understanding by incorporating two additional stages, i.e., long context extension and long video supervised fine-tuning. However, training on long video is computationally and memory intensive. We introduce the long-context Multi-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes long video training and inference, enabling 2M context length training on 256 GPUs without any gradient checkpointing. LongVILA efficiently extends the number of video frames of VILA from 8 to 2048, achieving 99.8% accuracy in 6,000-frame (more than 1 million tokens) video needle-in-a-haystack. LongVILA-7B demonstrates strong accuracy on 9 popular video benchmarks, e.g. 65.1% VideoMME with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence parallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and tensor parallelism. Moreover, it seamlessly integrates with Hugging Face Transformers.",
            "corpus_id": "271903601",
            "text": "Long-context capability is critical for multi-modal foundation models, especially for long video understanding. We introduce LongVILA, a full-stack solution for long-context visual-language models by co-designing the algorithm and system. For model training, we upgrade existing VLMs to support long video understanding by incorporating two additional stages, i.e., long context extension and long video supervised fine-tuning. However, training on long video is computationally and memory intensive. We introduce the long-context Multi-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes long video training and inference, enabling 2M context length training on 256 GPUs without any gradient checkpointing. LongVILA efficiently extends the number of video frames of VILA from 8 to 2048, achieving 99.8% accuracy in 6,000-frame (more than 1 million tokens) video needle-in-a-haystack. LongVILA-7B demonstrates strong accuracy on 9 popular video benchmarks, e.g. 65.1% VideoMME with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence parallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and tensor parallelism. Moreover, it seamlessly integrates with Hugging Face Transformers.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2440185546875
        },
        {
            "paperId": "9803d83bbb28d02fb01f00e0e05aa3c192a87255",
            "corpusId": 270877928,
            "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "referenceCount": 99,
            "citationCount": 122,
            "influentialCitationCount": 34,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.02490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2181120463",
                    "name": "Huiqiang Jiang"
                },
                {
                    "authorId": "1527099159",
                    "name": "Yucheng Li"
                },
                {
                    "authorId": "2284970741",
                    "name": "Chengruidong Zhang"
                },
                {
                    "authorId": "2108728536",
                    "name": "Qianhui Wu"
                },
                {
                    "authorId": "13289447",
                    "name": "Xufang Luo"
                },
                {
                    "authorId": "2309738728",
                    "name": "Surin Ahn"
                },
                {
                    "authorId": "2281867465",
                    "name": "Zhenhua Han"
                },
                {
                    "authorId": "2309244780",
                    "name": "Amir H. Abdi"
                },
                {
                    "authorId": "2305587638",
                    "name": "Dongsheng Li"
                },
                {
                    "authorId": "2257359863",
                    "name": "Chin-Yew Lin"
                },
                {
                    "authorId": "2125051198",
                    "name": "Yuqing Yang"
                },
                {
                    "authorId": "2160727304",
                    "name": "Lili Qiu"
                }
            ],
            "abstract": "The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.",
            "corpus_id": "270877928",
            "text": "The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.01450347900390625
        },
        {
            "paperId": "e8982f1478986a617ffbeebdc6e0482ce2bc6811",
            "corpusId": 273186757,
            "title": "Forgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-Context Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "referenceCount": 40,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.04727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319080033",
                    "name": "Xinyu Liu"
                },
                {
                    "authorId": "2322471369",
                    "name": "Runsong Zhao"
                },
                {
                    "authorId": "2312173315",
                    "name": "Pengcheng Huang"
                },
                {
                    "authorId": "2312102409",
                    "name": "Chunyang Xiao"
                },
                {
                    "authorId": "2291200534",
                    "name": "Bei Li"
                },
                {
                    "authorId": "2324838018",
                    "name": "Jingang Wang"
                },
                {
                    "authorId": "2261739712",
                    "name": "Tong Xiao"
                },
                {
                    "authorId": "2240940961",
                    "name": "Jingbo Zhu"
                }
            ],
            "abstract": "Numerous recent works target to extend effective context length for language models and various methods, tasks and benchmarks exist to measure model\u2019s effective memory length. However, through thorough investigations, we find limitations for currently existing evaluations on model\u2019s memory. We provide an extensive survey for limitations in this work and propose a new method called forgetting curve to measure the memorization capability of long-context models. We show that forgetting curve has the advantage of being robust to the tested corpus and the experimental settings, of not relying on prompt and can be applied to any model size. We apply our forgetting curve to a large variety of models involving both transformer and RNN/SSM based architectures. Our measurement provides empirical evidence for the effectiveness of transformer extension techniques while raises questions for the effective length of RNN/SSM based models. We also examine the difference between our measurement and existing benchmarks as well as popular metrics for various models.",
            "corpus_id": "273186757",
            "text": "Numerous recent works target to extend effective context length for language models and various methods, tasks and benchmarks exist to measure model\u2019s effective memory length. However, through thorough investigations, we find limitations for currently existing evaluations on model\u2019s memory. We provide an extensive survey for limitations in this work and propose a new method called forgetting curve to measure the memorization capability of long-context models. We show that forgetting curve has the advantage of being robust to the tested corpus and the experimental settings, of not relying on prompt and can be applied to any model size. We apply our forgetting curve to a large variety of models involving both transformer and RNN/SSM based architectures. Our measurement provides empirical evidence for the effectiveness of transformer extension techniques while raises questions for the effective length of RNN/SSM based models. We also examine the difference between our measurement and existing benchmarks as well as popular metrics for various models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.318115234375
        },
        {
            "paperId": "54f4ce7ff3390d9b8ffff90ff9be4f6e14046cd2",
            "corpusId": 271097687,
            "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 36,
            "citationCount": 34,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.08454, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308276343",
                    "name": "Zheng Wang"
                },
                {
                    "authorId": "2310701486",
                    "name": "Boxiao Jin"
                },
                {
                    "authorId": "2310779652",
                    "name": "Zhongzhi Yu"
                },
                {
                    "authorId": "2253896522",
                    "name": "Minjia Zhang"
                }
            ],
            "abstract": "How to efficiently serve Large Language Models (LLMs) has become a pressing issue because of their huge computational cost in their autoregressive generation process. To mitigate computational costs, LLMs often employ the KV Cache technique to improve the generation speed. While improving the computational efficiency, the storage requirements of the KV cache are substantial, particularly in long-context scenarios, leading to significant memory consumption. Existing KV cache eviction methods often degrade the performance of LLMs in long-context scenarios due to the information loss introduced by eviction. In this paper, we propose a novel KV cache merging approach, called KVMerger, to achieve adaptive KV cache compression for long-context tasks without significant performance degradation under constrained memory budgets. Our approach is inspired by the intriguing observation that key states exhibit high similarity at the token level within a single sequence. To facilitate merging, we develop an effective yet straightforward merging set identification algorithm to identify suitable KV states for merging. Our merging set identification algorithm stimulates the second observation that KV cache sparsity, from similarity perspective, is independent of the dataset and remains persistent at the model level. Subsequently, we propose a Gaussian kernel weighted merging algorithm to selectively merge all states within each merging set. We conduct extensive experiments to demonstrate the effectiveness of KVMerger for long-context tasks under constrained memory budgets, applying it to models including Llama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll benchmarks, we compare our method with other KV cache compression techniques, including H2O and CaM, showing that our method achieves superior performance across tasks with both 50% and 35% KV cache budgets.",
            "corpus_id": "271097687",
            "text": "How to efficiently serve Large Language Models (LLMs) has become a pressing issue because of their huge computational cost in their autoregressive generation process. To mitigate computational costs, LLMs often employ the KV Cache technique to improve the generation speed. While improving the computational efficiency, the storage requirements of the KV cache are substantial, particularly in long-context scenarios, leading to significant memory consumption. Existing KV cache eviction methods often degrade the performance of LLMs in long-context scenarios due to the information loss introduced by eviction. In this paper, we propose a novel KV cache merging approach, called KVMerger, to achieve adaptive KV cache compression for long-context tasks without significant performance degradation under constrained memory budgets. Our approach is inspired by the intriguing observation that key states exhibit high similarity at the token level within a single sequence. To facilitate merging, we develop an effective yet straightforward merging set identification algorithm to identify suitable KV states for merging. Our merging set identification algorithm stimulates the second observation that KV cache sparsity, from similarity perspective, is independent of the dataset and remains persistent at the model level. Subsequently, we propose a Gaussian kernel weighted merging algorithm to selectively merge all states within each merging set. We conduct extensive experiments to demonstrate the effectiveness of KVMerger for long-context tasks under constrained memory budgets, applying it to models including Llama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll benchmarks, we compare our method with other KV cache compression techniques, including H2O and CaM, showing that our method achieves superior performance across tasks with both 50% and 35% KV cache budgets.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.05633544921875
        },
        {
            "paperId": "65a1f6c82840a98d0830b40e348f7243e9afb89a",
            "corpusId": 272910674,
            "title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 25,
            "citationCount": 33,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17422, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "113515522",
                    "name": "Zhenmei Shi"
                },
                {
                    "authorId": "2321601631",
                    "name": "Yifei Ming"
                },
                {
                    "authorId": "1399659909",
                    "name": "Xuan-Phi Nguyen"
                },
                {
                    "authorId": "2260827689",
                    "name": "Yingyu Liang"
                },
                {
                    "authorId": "2313536726",
                    "name": "Shafiq Joty"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in handling long context inputs, but this comes at the cost of increased computational resources and latency. Our research introduces a novel approach for the long context bottleneck to accelerate LLM inference and reduce GPU memory consumption. Our research demonstrates that LLMs can identify relevant tokens in the early layers before generating answers to a query. Leveraging this insight, we propose an algorithm that uses early layers of an LLM as filters to select and compress input tokens, significantly reducing the context length for subsequent processing. Our method, GemFilter, demonstrates substantial improvements in both speed and memory efficiency compared to existing techniques, such as standard attention and SnapKV/H2O. Notably, it achieves a 2.4$\\times$ speedup and 30\\% reduction in GPU memory usage compared to SOTA methods. Evaluation on the Needle in a Haystack task shows that GemFilter significantly outperforms standard attention, SnapKV and demonstrates comparable performance on the LongBench challenge. GemFilter is simple, training-free, and broadly applicable across different LLMs. Crucially, it provides interpretability by allowing humans to inspect the selected input sequence. These findings not only offer practical benefits for LLM deployment, but also enhance our understanding of LLM internal mechanisms, paving the way for further optimizations in LLM design and inference. Our code is available at \\url{https://github.com/SalesforceAIResearch/GemFilter}.",
            "corpus_id": "272910674",
            "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities in handling long context inputs, but this comes at the cost of increased computational resources and latency. Our research introduces a novel approach for the long context bottleneck to accelerate LLM inference and reduce GPU memory consumption. Our research demonstrates that LLMs can identify relevant tokens in the early layers before generating answers to a query. Leveraging this insight, we propose an algorithm that uses early layers of an LLM as filters to select and compress input tokens, significantly reducing the context length for subsequent processing. Our method, GemFilter, demonstrates substantial improvements in both speed and memory efficiency compared to existing techniques, such as standard attention and SnapKV/H2O. Notably, it achieves a 2.4$\\times$ speedup and 30\\% reduction in GPU memory usage compared to SOTA methods. Evaluation on the Needle in a Haystack task shows that GemFilter significantly outperforms standard attention, SnapKV and demonstrates comparable performance on the LongBench challenge. GemFilter is simple, training-free, and broadly applicable across different LLMs. Crucially, it provides interpretability by allowing humans to inspect the selected input sequence. These findings not only offer practical benefits for LLM deployment, but also enhance our understanding of LLM internal mechanisms, paving the way for further optimizations in LLM design and inference. Our code is available at \\url{https://github.com/SalesforceAIResearch/GemFilter}.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1708984375
        },
        {
            "paperId": "3c7ceac213a90773cb34b15e9e1fe6e28674df57",
            "corpusId": 277953354,
            "title": "Data-Centric and Heterogeneity-Adaptive Sequence Parallelism for Efficient LLM Training",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 44,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2412.01523?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2412.01523, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2167500394",
                    "name": "Yujie Wang"
                },
                {
                    "authorId": "2333521042",
                    "name": "Shiju Wang"
                },
                {
                    "authorId": "2313331985",
                    "name": "Shenhan Zhu"
                },
                {
                    "authorId": "46182701",
                    "name": "Fangcheng Fu"
                },
                {
                    "authorId": "2334151423",
                    "name": "Xinyi Liu"
                },
                {
                    "authorId": "2319391688",
                    "name": "Xuefeng Xiao"
                },
                {
                    "authorId": "2108525422",
                    "name": "Huixia Li"
                },
                {
                    "authorId": "2219863430",
                    "name": "Jiashi Li"
                },
                {
                    "authorId": "2333458941",
                    "name": "Faming Wu"
                },
                {
                    "authorId": "2313408987",
                    "name": "Bin Cui"
                }
            ],
            "abstract": "Extending the context length (i.e., the maximum supported sequence length) of LLMs is of paramount significance. To facilitate long context training of LLMs, sequence parallelism has emerged as an essential technique, which scatters each input sequence across multiple devices and necessitates communication to process the sequence. In essence, existing sequence parallelism methods assume homogeneous sequence lengths (i.e., all input sequences are equal in length) and therefore leverages a single, static scattering strategy for all input sequences. However, in reality, the sequence lengths in LLM training corpora exhibit substantial variability, often following a long-tail distribution, which leads to workload heterogeneity. Inthispaper, we show that employing a single, static strategy results in inefficiency and resource under-utilization, highlighting the need for adaptive approaches to handle the heterogeneous workloads across sequences. To address this, we propose a heterogeneity-adaptive sequence parallelism method. For each training step, our approach captures the variability in sequence lengths and assigns the optimal combination of scattering strategies based on workload characteristics. We model this problem as a linear programming optimization and design an efficient and effective solver to find the optimal",
            "corpus_id": "277953354",
            "text": "Extending the context length (i.e., the maximum supported sequence length) of LLMs is of paramount significance. To facilitate long context training of LLMs, sequence parallelism has emerged as an essential technique, which scatters each input sequence across multiple devices and necessitates communication to process the sequence. In essence, existing sequence parallelism methods assume homogeneous sequence lengths (i.e., all input sequences are equal in length) and therefore leverages a single, static scattering strategy for all input sequences. However, in reality, the sequence lengths in LLM training corpora exhibit substantial variability, often following a long-tail distribution, which leads to workload heterogeneity. Inthispaper, we show that employing a single, static strategy results in inefficiency and resource under-utilization, highlighting the need for adaptive approaches to handle the heterogeneous workloads across sequences. To address this, we propose a heterogeneity-adaptive sequence parallelism method. For each training step, our approach captures the variability in sequence lengths and assigns the optimal combination of scattering strategies based on workload characteristics. We model this problem as a linear programming optimization and design an efficient and effective solver to find the optimal",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.08447265625
        },
        {
            "paperId": "abdedf38a9153646f014384ecf4822f8cd136aab",
            "corpusId": 277883071,
            "title": "Mnemosyne: Parallelization Strategies for Efficiently Serving Multi-Million Context Length LLM Inference Requests Without Approximations",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 46,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2409.17264?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2409.17264, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "70203043",
                    "name": "Amey Agrawal"
                },
                {
                    "authorId": "2322906162",
                    "name": "Junda Chen"
                },
                {
                    "authorId": "2294723848",
                    "name": "\u00cd\u00f1igo Goiri"
                },
                {
                    "authorId": "2794488",
                    "name": "R. Ramjee"
                },
                {
                    "authorId": "2256775267",
                    "name": "Chaojie Zhang"
                },
                {
                    "authorId": "2290021009",
                    "name": "Alexey Tumanov"
                },
                {
                    "authorId": "3449187",
                    "name": "Esha Choukse"
                }
            ],
            "abstract": "As large language models (LLMs) evolve to handle increasingly longer contexts, serving inference requests for context lengths in the range of millions of tokens presents unique challenges. While existing techniques are effective for training, they fail to address the unique challenges of inference, such as varying prefill and decode phases and their associated latency constraints \u2013 like Time to First Token (TTFT) and Time Between Tokens (TBT). Furthermore, there are no long context inference solutions that allow batching requests to increase the hardware utilization today. In this paper, we propose three key innovations for efficient interactive long context LLM inference, without resorting to any approximation: adaptive chunking to reduce prefill overheads in mixed batching, Sequence Pipeline Parallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize TBT. These contributions are combined into a 3D parallelism strategy, enabling Mnemosyne to scale interactive inference to context lengths at least up to 10 million tokens with high throughput enabled with batching. To our knowledge, Mnemosyne is the first to be able to achieve support for 10 million long context inference efficiently, while satisfying production-grade SLOs on TBT (30ms) on contexts up to and including 10 million.",
            "corpus_id": "277883071",
            "text": "As large language models (LLMs) evolve to handle increasingly longer contexts, serving inference requests for context lengths in the range of millions of tokens presents unique challenges. While existing techniques are effective for training, they fail to address the unique challenges of inference, such as varying prefill and decode phases and their associated latency constraints \u2013 like Time to First Token (TTFT) and Time Between Tokens (TBT). Furthermore, there are no long context inference solutions that allow batching requests to increase the hardware utilization today. In this paper, we propose three key innovations for efficient interactive long context LLM inference, without resorting to any approximation: adaptive chunking to reduce prefill overheads in mixed batching, Sequence Pipeline Parallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize TBT. These contributions are combined into a 3D parallelism strategy, enabling Mnemosyne to scale interactive inference to context lengths at least up to 10 million tokens with high throughput enabled with batching. To our knowledge, Mnemosyne is the first to be able to achieve support for 10 million long context inference efficiently, while satisfying production-grade SLOs on TBT (30ms) on contexts up to and including 10 million.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.057373046875
        },
        {
            "paperId": "812c6882c8ff9d6c6bd7204d53f72ea16276113e",
            "corpusId": 272911295,
            "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference Requests Without Approximations",
            "venue": "",
            "year": 2024,
            "referenceCount": 57,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17264, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "70203043",
                    "name": "Amey Agrawal"
                },
                {
                    "authorId": "2322906162",
                    "name": "Junda Chen"
                },
                {
                    "authorId": "2279540149",
                    "name": "'Inigo Goiri"
                },
                {
                    "authorId": "2794488",
                    "name": "R. Ramjee"
                },
                {
                    "authorId": "2256775267",
                    "name": "Chaojie Zhang"
                },
                {
                    "authorId": "2290021009",
                    "name": "Alexey Tumanov"
                },
                {
                    "authorId": "3449187",
                    "name": "Esha Choukse"
                }
            ],
            "abstract": "As large language models (LLMs) handle increasingly longer contexts, serving long inference requests of millions of tokens presents unique challenges. We show that existing work for long context inference is largely based on techniques from long context training, and does not handle the high variability in input lengths during inference. This leads to inefficient resource utilization, server fragmentation, and head-of-line (HOL) blocking. We present Medha, an end-to-end system for efficient long-context LLM inference that addresses these challenges through fine-grained time sharing. Medha introduces three key innovations: (1) the mechanism of adaptive prefill chunking to help mitigate HOL blocking with preemption; (2) two new parallelism strategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token by pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower time-peroutput-token by distributing decoding across servers; and (3) a novel input-length aware least remaining slack scheduling to meet Service Level Objectives (SLOs). Medha enables exact inference scaling beyond 10 million tokens, maintaining high throughput and low latency across mixed-length workloads. Compared to state-of-the-art systems, Medha reduces server fragmentation, cuts median latency by up to 30x, and improves throughput by over 5x, delivering production-scale long-context inference without compromising performance on shorter requests.",
            "corpus_id": "272911295",
            "text": "As large language models (LLMs) handle increasingly longer contexts, serving long inference requests of millions of tokens presents unique challenges. We show that existing work for long context inference is largely based on techniques from long context training, and does not handle the high variability in input lengths during inference. This leads to inefficient resource utilization, server fragmentation, and head-of-line (HOL) blocking. We present Medha, an end-to-end system for efficient long-context LLM inference that addresses these challenges through fine-grained time sharing. Medha introduces three key innovations: (1) the mechanism of adaptive prefill chunking to help mitigate HOL blocking with preemption; (2) two new parallelism strategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token by pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower time-peroutput-token by distributing decoding across servers; and (3) a novel input-length aware least remaining slack scheduling to meet Service Level Objectives (SLOs). Medha enables exact inference scaling beyond 10 million tokens, maintaining high throughput and low latency across mixed-length workloads. Compared to state-of-the-art systems, Medha reduces server fragmentation, cuts median latency by up to 30x, and improves throughput by over 5x, delivering production-scale long-context inference without compromising performance on shorter requests.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.04638671875
        }
    ],
    "quotes": {
        "cost": 0.08965499999999998,
        "quotes": [
            {
                "idx": 0,
                "key": "[237412971 | Martins et al. | 2021 | Citations: 11]",
                "snippets": "Recently, transformer-based language models have achieved impressive results by increasing the context size (Radford et al., 2018(Radford et al., , 2019;;(Dai et al., 2019)(Rae et al., 2019)(Brown et al., 2020). However, while humans process information sequentially, updating their memories continuously, and recurrent neural networks (RNNs) update a single memory vector during time, transformers do not -they exhaustively query every representation associated to the past events. Thus, the amount of computation they need to perform grows with the length of the context, and, consequently, transformers have computational limitations about how much information can fit into memory. For example, a vanilla transformer requires quadratic time to process an input sequence and linear time to attend to the context when generating every new word.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207930593 | Rae et al. | 2019 | Citations: 653]": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.",
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[57759363 | Dai et al. | 2019 | Citations: 3746]": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 251,
                        "end": 1093,
                        "sentence_offsets": [
                            {
                                "start": 251,
                                "end": 459
                            },
                            {
                                "start": 460,
                                "end": 730
                            },
                            {
                                "start": 731,
                                "end": 932
                            },
                            {
                                "start": 933,
                                "end": 1093
                            }
                        ],
                        "ref_mentions": [
                            "57759363",
                            "207930593",
                            "218971783"
                        ],
                        "quote": "Recently, transformer-based language models have achieved impressive results by increasing the context size (Radford et al., 2018(Radford et al., , 2019;;(Dai et al., 2019)(Rae et al., 2019)(Brown et al., 2020). However, while humans process information sequentially, updating their memories continuously, and recurrent neural networks (RNNs) update a single memory vector during time, transformers do not -they exhaustively query every representation associated to the past events. Thus, the amount of computation they need to perform grows with the length of the context, and, consequently, transformers have computational limitations about how much information can fit into memory. For example, a vanilla transformer requires quadratic time to process an input sequence and linear time to attend to the context when generating every new word."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[248218548 | Ang et al. | 2022 | Citations: 6]",
                "snippets": "With many real-world applications of Natural Language Processing (NLP) comprising of long texts, there has been a rise in NLP benchmarks that measure the accuracy of models that can handle longer input sequences.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "With many real-world applications of Natural Language Processing (NLP) comprising of long texts, there has been a rise in NLP benchmarks that measure the accuracy of models that can handle longer input sequences.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[261245264 | Bai et al. | 2023 | Citations: 603]",
                "snippets": "Long Context Modeling Techniques. We first discuss some popular lines of methods that aim to tackle long context understanding. These studies are mainly aimed at solving two key challenges in long text modeling, including the high runtime overhead on longer context, and the catastrophic forgetting phenomenon when processing long sequence. A series of studies focus on how to make Transformers more efficient and unforgetful (Tay et al., 2020), with designs such as sparse and efficient computation (Child et al., 2019;(Kitaev et al., 2020)Beltagy et al., 2020;(Yu et al., 2023)Wang et al., 2020;(Fedus et al., 2021)Ding et al., 2023), recurrent and memory modules (Dai et al., 2019)(Rae et al., 2019)(Wu et al., 2022)Martins et al., 2022;(Bulatov et al., 2022)Orvieto et al., 2023;Liang et al., 2023;Zhou et al., 2023). More recently, several methods (Press et al., 2021)Sun et al., 2022;Chen et al., 2023) have been proposed to enable length extrapolation of Transformers, and have been adopted in the training process of long context LLMs such as ChatGLM2-32k (Zeng et al., 2022) and LongChat-32k (Li et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207930593 | Rae et al. | 2019 | Citations: 653]": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.",
                    "[209315300 | Kitaev et al. | 2020 | Citations: 2333]": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.",
                    "[221702858 | Tay et al. | 2020 | Citations: 1128]": "Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \u201cX-former\u201d models have been proposed\u2014Reformer, Linformer, Performer, Longformer, to name a few\u2014which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored \u201cX-former\u201d models, providing an organized and comprehensive overview of existing work and models across multiple domains.",
                    "[231573431 | Fedus et al. | 2021 | Citations: 2224]": "In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the\"Colossal Clean Crawled Corpus\"and achieve a 4x speedup over the T5-XXL model.",
                    "[237347130 | Press et al. | 2021 | Citations: 775]": "Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.",
                    "[247519194 | Wu et al. | 2022 | Citations: 178]": "Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.",
                    "[250526424 | Bulatov et al. | 2022 | Citations: 110]": "Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.",
                    "[252715691 | Zeng et al. | 2022 | Citations: 1094]": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \\url{https://github.com/THUDM/GLM-130B/}.",
                    "[259165244 | Yu et al. | 2023 | Citations: 69]": "The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For \\textbf{ability modeling}, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering $19$ tasks. (2) For \\textbf{data}, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For \\textbf{evaluation criteria}, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate $28$ open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset and open-participation leaderboard are publicly released at https://kola.xlore.cn and will be continuously updated to provide references for developing LLMs and knowledge-related systems.",
                    "[57759363 | Dai et al. | 2019 | Citations: 3746]": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1117,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 33
                            },
                            {
                                "start": 34,
                                "end": 127
                            },
                            {
                                "start": 128,
                                "end": 340
                            },
                            {
                                "start": 341,
                                "end": 819
                            },
                            {
                                "start": 820,
                                "end": 1117
                            }
                        ],
                        "ref_mentions": [
                            "221702858",
                            "209315300",
                            "259165244",
                            "231573431",
                            "57759363",
                            "207930593",
                            "247519194",
                            "250526424",
                            "237347130",
                            "252715691"
                        ],
                        "quote": "Long Context Modeling Techniques. We first discuss some popular lines of methods that aim to tackle long context understanding. These studies are mainly aimed at solving two key challenges in long text modeling, including the high runtime overhead on longer context, and the catastrophic forgetting phenomenon when processing long sequence. A series of studies focus on how to make Transformers more efficient and unforgetful (Tay et al., 2020), with designs such as sparse and efficient computation (Child et al., 2019;(Kitaev et al., 2020)Beltagy et al., 2020;(Yu et al., 2023)Wang et al., 2020;(Fedus et al., 2021)Ding et al., 2023), recurrent and memory modules (Dai et al., 2019)(Rae et al., 2019)(Wu et al., 2022)Martins et al., 2022;(Bulatov et al., 2022)Orvieto et al., 2023;Liang et al., 2023;Zhou et al., 2023). More recently, several methods (Press et al., 2021)Sun et al., 2022;Chen et al., 2023) have been proposed to enable length extrapolation of Transformers, and have been adopted in the training process of long context LLMs such as ChatGLM2-32k (Zeng et al., 2022) and LongChat-32k (Li et al., 2023)."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[267770311 | He et al. | 2024 | Citations: 11]",
                "snippets": "Many efficient self-attention techniques have been proposed for long context modeling in transformer models, including low-rank factorization (Wang et al., 2020), local attention (Ramachandran et al., 2019), dilated attention (Ding et al., 2023), sparsity (Beltagy et al., 2020;(Zaheer et al., 2020)Kitaev et al., 2020), and hardwareaware attention mechanisms such as FlashAttention (Dao et al., 2022)Dao, 2023). Despite notable progress, these methods are unable to handle unbounded context window sizes and struggle to retrieve information in the middle of the input (Liu et al., 2023). They can be used in tandem with our proposed approach for longer context modeling.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                    "[249151871 | Dao et al. | 2022 | Citations: 2285]": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 27,
                        "end": 697,
                        "sentence_offsets": [
                            {
                                "start": 27,
                                "end": 438
                            },
                            {
                                "start": 439,
                                "end": 614
                            },
                            {
                                "start": 615,
                                "end": 697
                            }
                        ],
                        "ref_mentions": [
                            "220831004",
                            "249151871"
                        ],
                        "quote": "Many efficient self-attention techniques have been proposed for long context modeling in transformer models, including low-rank factorization (Wang et al., 2020), local attention (Ramachandran et al., 2019), dilated attention (Ding et al., 2023), sparsity (Beltagy et al., 2020;(Zaheer et al., 2020)Kitaev et al., 2020), and hardwareaware attention mechanisms such as FlashAttention (Dao et al., 2022)Dao, 2023). Despite notable progress, these methods are unable to handle unbounded context window sizes and struggle to retrieve information in the middle of the input (Liu et al., 2023). They can be used in tandem with our proposed approach for longer context modeling."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[268378933 | Li et al. | 2024 | Citations: 4]",
                "snippets": "Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[268385246 | Athiwaratkun et al. | 2024 | Citations: 4]",
                "snippets": "To effectively handle longer context sequences, optimizing memory I/O and reducing computational overhead are critical. Currently, the dominant approaches to addressing this challenge have been to make the attention computation less expensive. Beltagy et al. (2020) proposed to sparsify self-attention using various attention patterns. Wang et al. (2020) explores low-rank approximation of self-attention. In addition to the compute bound improvements, advancements in memory-efficient attention mechanisms and techniques for reducing memory I/O will continue to propel the field forward, facilitating the handling of longer context sequences in language models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B.2. Supporting Long Context Requires IO-Efficient Attention",
                        "pdf_hash": "",
                        "start": 1188,
                        "end": 1850,
                        "sentence_offsets": [
                            {
                                "start": 1144,
                                "end": 1339
                            },
                            {
                                "start": 1341,
                                "end": 1520
                            },
                            {
                                "start": 1520,
                                "end": 1639
                            },
                            {
                                "start": 1639,
                                "end": 1762
                            },
                            {
                                "start": 1762,
                                "end": 1853
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To effectively handle longer context sequences, optimizing memory I/O and reducing computational overhead are critical. Currently, the dominant approaches to addressing this challenge have been to make the attention computation less expensive. Beltagy et al. (2020) proposed to sparsify self-attention using various attention patterns. Wang et al. (2020) explores low-rank approximation of self-attention. In addition to the compute bound improvements, advancements in memory-efficient attention mechanisms and techniques for reducing memory I/O will continue to propel the field forward, facilitating the handling of longer context sequences in language models."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[268793522 | Thonet et al. | 2024 | Citations: 3]",
                "snippets": "context modeling. 3 While an exhaustive survey of these methods is beyond the scope of this paper, they can generally be categorized into three main groups (excluding other distinct approaches such as retrieval-augmented generation (Xu et al., 2023) and context compression (Chevalier et al., 2023)): (a) the development of efficient transformer architectures to address the quadratic attention challenge, including sparse transformers [6,12,(Martins et al., 2020)(Zaheer et al., 2020), linear transformers (Choromanski et al., 2020)(Katharopoulos et al., 2020)[42], and hierarchical transformers [20,30,(Wu et al., 2021); (b) approaches like recurrent attention networks [7,(Dai et al., 2019)(Peng et al., 2023) and state-space models [16,41]; (c) length extrapolation or position embedding interpolation, where LLMs are fine-tuned or adapted at inference time to adjust tokens' positions to match the new context length [4,8,9,28,35,(Peng et al., 2023)[45]. These techniques also contributed to the context length expansion in proprietary models like GPT-4 (32K-128K), Claude-3 (200k), and Gemini-1.5 (128K-1M).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                    "[219636190 | Martins et al. | 2020 | Citations: 41]": "Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and alpha-entmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: first, we extend alpha-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for alpha in {1,2}. Experiments on attention-based text classification, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions.",
                    "[220250819 | Katharopoulos et al. | 2020 | Citations: 1790]": "Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.",
                    "[222067132 | Choromanski et al. | 2020 | Citations: 1602]": "We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.",
                    "[235294151 | Wu et al. | 2021 | Citations: 68]": "Transformer is important for text modeling. However, it has difficulty in handling long documents due to the quadratic complexity with input text length. In order to handle this problem, we propose a hierarchical interactive Transformer (Hi-Transformer) for efficient and effective long document modeling. Hi-Transformer models documents in a hierarchical way, i.e., first learns sentence representations and then learns document representations. It can effectively reduce the complexity and meanwhile capture global document context in the modeling of each sentence. More specifically, we first use a sentence Transformer to learn the representations of each sentence. Then we use a document Transformer to model the global document context from these sentence representations. Next, we use another sentence Transformer to enhance sentence modeling using the global document context. Finally, we use hierarchical pooling method to obtain document embedding. Extensive experiments on three benchmark datasets validate the efficiency and effectiveness of Hi-Transformer in long document modeling.",
                    "[258832459 | Peng et al. | 2023 | Citations: 608]": "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.",
                    "[258865249 | Chevalier et al. | 2023 | Citations: 190]": "Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments and summary vectors from all previous segments are used in language modeling. We fine-tune OPT models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations. We find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference cost. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrieval-augmented language modeling. Overall, AutoCompressors emerge as a simple and inexpensive solution for extending the context window of LMs while speeding up inference over long contexts.",
                    "[261493986 | Peng et al. | 2023 | Citations: 264]": "Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at https://github.com/jquesnelle/yarn",
                    "[263620134 | Xu et al. | 2023 | Citations: 85]": "Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.",
                    "[57759363 | Dai et al. | 2019 | Citations: 3746]": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 428,
                        "end": 1361,
                        "sentence_offsets": [
                            {
                                "start": 428,
                                "end": 447
                            },
                            {
                                "start": 448,
                                "end": 1207
                            },
                            {
                                "start": 1208,
                                "end": 1361
                            }
                        ],
                        "ref_mentions": [
                            "263620134",
                            "258865249",
                            "219636190",
                            "220831004",
                            "222067132",
                            "220250819",
                            "235294151",
                            "57759363",
                            "258832459",
                            "261493986"
                        ],
                        "quote": "context modeling. 3 While an exhaustive survey of these methods is beyond the scope of this paper, they can generally be categorized into three main groups (excluding other distinct approaches such as retrieval-augmented generation (Xu et al., 2023) and context compression (Chevalier et al., 2023)): (a) the development of efficient transformer architectures to address the quadratic attention challenge, including sparse transformers [6,12,(Martins et al., 2020)(Zaheer et al., 2020), linear transformers (Choromanski et al., 2020)(Katharopoulos et al., 2020)[42], and hierarchical transformers [20,30,(Wu et al., 2021); (b) approaches like recurrent attention networks [7,(Dai et al., 2019)(Peng et al., 2023) and state-space models [16,41]; (c) length extrapolation or position embedding interpolation, where LLMs are fine-tuned or adapted at inference time to adjust tokens' positions to match the new context length [4,8,9,28,35,(Peng et al., 2023)[45]. These techniques also contributed to the context length expansion in proprietary models like GPT-4 (32K-128K), Claude-3 (200k), and Gemini-1.5 (128K-1M)."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[268857023 | Li et al. | 2024 | Citations: 192]",
                "snippets": "One line of research is based on AliBi (Press et al., 2021) and RoPE (Su et al., 2021) embeddings, which allows us to train Transformers with short sequences and subsequently apply them to longer sequences during inference.Recently, different approaches (Xiong et al., 2023;Fu et al., 2024;Liu et al., 2024) help the model to extrapolate to 128K window size with continued pre-training.Later on, LongRoPE (Ding et al., 2024) was proposed to further extend the context window to 2M tokens.Another line of research also utilizes methodologies like context window sliding and segmentation to overcome the issue of the limited context window in original Transformers (Hao et al., 2022;(Ratner et al., 2022).Furthermore, architectural innovations, transitioning from traditional Transformer-based designs to recurrent models or state space models, have shown promise in facilitating long-range computations naturally (Orvieto et al., 2023;Gu & Dao, 2023;(Peng et al., 2023).These techniques have been incorporated into several current open-source LLMs to enhance long sequence understanding capability (Chen et al., 2023)Tworkowski et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[237347130 | Press et al. | 2021 | Citations: 775]": "Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.",
                    "[258686160 | Ratner et al. | 2022 | Citations: 75]": "When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (\u201cwindows\u201d), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ ai21labs/parallel-context-windows.",
                    "[258832459 | Peng et al. | 2023 | Citations: 608]": "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.",
                    "[262084134 | Chen et al. | 2023 | Citations: 167]": "We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 465,
                        "end": 1606,
                        "sentence_offsets": [
                            {
                                "start": 465,
                                "end": 688
                            },
                            {
                                "start": 688,
                                "end": 851
                            },
                            {
                                "start": 851,
                                "end": 953
                            },
                            {
                                "start": 953,
                                "end": 1167
                            },
                            {
                                "start": 1167,
                                "end": 1433
                            },
                            {
                                "start": 1433,
                                "end": 1606
                            }
                        ],
                        "ref_mentions": [
                            "237347130",
                            "233307138",
                            "258686160",
                            "258832459",
                            "262084134"
                        ],
                        "quote": "One line of research is based on AliBi (Press et al., 2021) and RoPE (Su et al., 2021) embeddings, which allows us to train Transformers with short sequences and subsequently apply them to longer sequences during inference.Recently, different approaches (Xiong et al., 2023;Fu et al., 2024;Liu et al., 2024) help the model to extrapolate to 128K window size with continued pre-training.Later on, LongRoPE (Ding et al., 2024) was proposed to further extend the context window to 2M tokens.Another line of research also utilizes methodologies like context window sliding and segmentation to overcome the issue of the limited context window in original Transformers (Hao et al., 2022;(Ratner et al., 2022).Furthermore, architectural innovations, transitioning from traditional Transformer-based designs to recurrent models or state space models, have shown promise in facilitating long-range computations naturally (Orvieto et al., 2023;Gu & Dao, 2023;(Peng et al., 2023).These techniques have been incorporated into several current open-source LLMs to enhance long sequence understanding capability (Chen et al., 2023)Tworkowski et al., 2023)."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[270710851 | Gavin et al. | 2024 | Citations: 3]",
                "snippets": "Many studies explore various strategies to address this challenge, including the use of new positional encodings to achieve position interpolation or extrapolation (Peng et al., 2023;Chen et al., 2023;Liu et al., 2024b)...RingAttention (Liu et al., 2023a) can reduce the memory requirements of the Transformer model, allowing it to train sequences over 500 times longer than previous memory-efficient methods, without needing to approximate the attention mechanism.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Input:",
                        "pdf_hash": "",
                        "start": 238,
                        "end": 457,
                        "sentence_offsets": [
                            {
                                "start": 238,
                                "end": 458
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Many studies explore various strategies to address this challenge, including the use of new positional encodings to achieve position interpolation or extrapolation (Peng et al., 2023;Chen et al., 2023;Liu et al., 2024b)"
                    },
                    {
                        "section_title": "Input:",
                        "pdf_hash": "",
                        "start": 1126,
                        "end": 1370,
                        "sentence_offsets": [
                            {
                                "start": 1126,
                                "end": 1369
                            },
                            {
                                "start": 1369,
                                "end": 1535
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "RingAttention (Liu et al., 2023a) can reduce the memory requirements of the Transformer model, allowing it to train sequences over 500 times longer than previous memory-efficient methods, without needing to approximate the attention mechanism."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[277857470 | He et al. | 2025 | Citations: 1]",
                "snippets": "To address the quadratic computational and memory demands of standard transformer self-attention, researchers have developed various architectural modifica-tions to improve efficiency and extend context lengths. Notable examples include Longformer (Beltagy et al., 2020), which combines sliding window and global attention, and BlockTransformer (Ho et al., 2024), which employs hierarchical global-to-local modeling. Linear Attention methods (Katharopoulos et al., 2020) reformulate self-attention for linear complexity, while InfiniteTransformer (Munkhdalai et al., 2024) incorporates unbounded long-term memory through continuousspace attention. State space models like Mamba (Gu & Dao, 2024) capture long-range dependencies efficiently without explicit attention mechanisms.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 413,
                        "end": 1190,
                        "sentence_offsets": [
                            {
                                "start": 413,
                                "end": 624
                            },
                            {
                                "start": 625,
                                "end": 829
                            },
                            {
                                "start": 830,
                                "end": 1060
                            },
                            {
                                "start": 1061,
                                "end": 1190
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To address the quadratic computational and memory demands of standard transformer self-attention, researchers have developed various architectural modifica-tions to improve efficiency and extend context lengths. Notable examples include Longformer (Beltagy et al., 2020), which combines sliding window and global attention, and BlockTransformer (Ho et al., 2024), which employs hierarchical global-to-local modeling. Linear Attention methods (Katharopoulos et al., 2020) reformulate self-attention for linear complexity, while InfiniteTransformer (Munkhdalai et al., 2024) incorporates unbounded long-term memory through continuousspace attention. State space models like Mamba (Gu & Dao, 2024) capture long-range dependencies efficiently without explicit attention mechanisms."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.017619,
        "cot": "To effectively answer this query about how people extend context length in Transformer models using sequence parallelism techniques, I'll organize the quotes into meaningful dimensions.\n\n1. \"Introduction to Long Context in Transformers\" - This should be a synthesis section to provide background on why long context is important, what challenges exist with standard Transformers, and why this is a significant area of research. Quotes 0, 1, and 4 discuss the importance and challenges of long context.\n\n2. \"Categories of Long Context Modeling Techniques\" - This should be a synthesis section that provides an overview of the major approaches to extending context length. Quotes 2, 6, and 7 provide good taxonomies of different methods, showing how sequence parallelism fits into the broader landscape.\n\n3. \"Efficiency-Focused Approaches\" - This should be a list format to enumerate specific techniques that focus on computational efficiency, as sequence parallelism relates to efficiency. Quotes 3, 5, 8, and 9 describe various efficiency techniques including sparse attention, low-rank factorization, and memory-efficient approaches.\n\n4. \"Position Encoding Approaches\" - This should be a list format to cover the position encoding-based methods, which is another major category mentioned in the quotes. Quotes 7 and 8 specifically mention these approaches.\n\n5. \"Recent Advancements and Implementations\" - This should be a synthesis section to discuss how these techniques are being implemented in modern models. Several quotes mention applications in specific models.",
        "plan": {
            "Introduction to Long Context in Transformers (synthesis)": [
                0,
                1,
                4
            ],
            "Categories of Long Context Modeling Techniques (synthesis)": [
                2,
                6,
                7
            ],
            "Efficiency-Focused Approaches (list)": [
                3,
                5,
                9
            ],
            "Position Encoding Approaches (list)": [
                7,
                8
            ],
            "Recent Advancements and Implementations (synthesis)": [
                2,
                6,
                7,
                8,
                9
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Long Context in Transformers",
                "tldr": "Transformer models have shown impressive capabilities but face computational challenges when processing long contexts. Extending context length is a critical area of research as many real-world NLP applications involve processing lengthy documents or dialogues. (6 sources)",
                "text": "\nTransformer-based language models have achieved remarkable results in recent years by increasing their ability to handle larger context windows <Paper corpusId=\"237412971\" paperTitle=\"(Martins et al., 2021)\" isShortName></Paper>. Unlike humans and recurrent neural networks that process information sequentially with continuous memory updates, transformers process information differently - they query every representation from past events, which creates computational challenges as context length grows <Paper corpusId=\"237412971\" paperTitle=\"(Martins et al., 2021)\" isShortName></Paper>. This fundamental limitation means that vanilla transformers require quadratic time complexity with respect to input sequence length, significantly constraining how much information they can effectively process <Paper corpusId=\"237412971\" paperTitle=\"(Martins et al., 2021)\" isShortName></Paper>.\n\nDespite these limitations, researchers have made significant progress in extending transformer context capabilities. Models like Transformer-XL introduced segment-level recurrence mechanisms that enable learning dependencies that are 80% longer than RNNs and 450% longer than vanilla Transformers while maintaining temporal coherence <Paper corpusId=\"57759363\" paperTitle=\"(Dai et al., 2019)\" isShortName></Paper>. Other advancements like the Compressive Transformer have further pushed the boundaries of long-range sequence learning through memory compression techniques <Paper corpusId=\"207930593\" paperTitle=\"(Rae et al., 2019)\" isShortName></Paper>. The development of GPT-3 with 175 billion parameters represented another leap forward in the field, demonstrating strong performance across many NLP tasks even in few-shot settings <Paper corpusId=\"218971783\" paperTitle=\"(Brown et al., 2020)\" isShortName></Paper>.\n\nThe drive to extend context length in transformers is fueled by practical needs. Many real-world NLP applications involve processing long documents, making this an increasingly important area of research <Paper corpusId=\"248218548\" paperTitle=\"(Ang et al., 2022)\" isShortName></Paper>. Standard Large Language Models (LLMs) particularly struggle with handling extended dialogues due to efficiency and consistency issues when dealing with long contexts <Paper corpusId=\"268378933\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This challenge has spurred the development of various techniques for extending context capabilities in transformer architectures.",
                "citations": [
                    {
                        "id": "(Martins et al., 2021)",
                        "snippets": [
                            "Recently, transformer-based language models have achieved impressive results by increasing the context size (Radford et al., 2018(Radford et al., , 2019;;(Dai et al., 2019)(Rae et al., 2019)(Brown et al., 2020). However, while humans process information sequentially, updating their memories continuously, and recurrent neural networks (RNNs) update a single memory vector during time, transformers do not -they exhaustively query every representation associated to the past events. Thus, the amount of computation they need to perform grows with the length of the context, and, consequently, transformers have computational limitations about how much information can fit into memory. For example, a vanilla transformer requires quadratic time to process an input sequence and linear time to attend to the context when generating every new word."
                        ],
                        "paper": {
                            "corpus_id": 237412971,
                            "title": "\\infty-former: Infinite Memory Transformer",
                            "authors": [
                                {
                                    "authorId": "144869806",
                                    "name": "Pedro Henrique Martins"
                                },
                                {
                                    "authorId": "2566656",
                                    "name": "Zita Marinho"
                                },
                                {
                                    "authorId": "145644643",
                                    "name": "Andr\u00e9 F. T. Martins"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 11
                        },
                        "score": 0.58251953125
                    },
                    {
                        "id": "(Dai et al., 2019)",
                        "snippets": [
                            "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
                        ],
                        "paper": {
                            "corpus_id": 57759363,
                            "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
                            "authors": [
                                {
                                    "authorId": "3422912",
                                    "name": "Zihang Dai"
                                },
                                {
                                    "authorId": "2109512754",
                                    "name": "Zhilin Yang"
                                },
                                {
                                    "authorId": "35729970",
                                    "name": "Yiming Yang"
                                },
                                {
                                    "authorId": "143712374",
                                    "name": "J. Carbonell"
                                },
                                {
                                    "authorId": "2827616",
                                    "name": "Quoc V. Le"
                                },
                                {
                                    "authorId": "145124475",
                                    "name": "R. Salakhutdinov"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 3746
                        },
                        "score": 0
                    },
                    {
                        "id": "(Rae et al., 2019)",
                        "snippets": [
                            "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19."
                        ],
                        "paper": {
                            "corpus_id": 207930593,
                            "title": "Compressive Transformers for Long-Range Sequence Modelling",
                            "authors": [
                                {
                                    "authorId": "34269227",
                                    "name": "Jack W. Rae"
                                },
                                {
                                    "authorId": "13759734",
                                    "name": "Anna Potapenko"
                                },
                                {
                                    "authorId": "35880964",
                                    "name": "Siddhant M. Jayakumar"
                                },
                                {
                                    "authorId": "2542999",
                                    "name": "T. Lillicrap"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 653
                        },
                        "score": 0
                    },
                    {
                        "id": "(Brown et al., 2020)",
                        "snippets": [
                            "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                        ],
                        "paper": {
                            "corpus_id": 218971783,
                            "title": "Language Models are Few-Shot Learners",
                            "authors": [
                                {
                                    "authorId": "31035595",
                                    "name": "Tom B. Brown"
                                },
                                {
                                    "authorId": "2056658938",
                                    "name": "Benjamin Mann"
                                },
                                {
                                    "authorId": "39849748",
                                    "name": "Nick Ryder"
                                },
                                {
                                    "authorId": "2065894334",
                                    "name": "Melanie Subbiah"
                                },
                                {
                                    "authorId": "152724169",
                                    "name": "J. Kaplan"
                                },
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "2072676",
                                    "name": "Arvind Neelakantan"
                                },
                                {
                                    "authorId": "67311962",
                                    "name": "Pranav Shyam"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "1404060687",
                                    "name": "Ariel Herbert-Voss"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "103143311",
                                    "name": "T. Henighan"
                                },
                                {
                                    "authorId": "48422824",
                                    "name": "R. Child"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "2052152920",
                                    "name": "Daniel M. Ziegler"
                                },
                                {
                                    "authorId": "49387725",
                                    "name": "Jeff Wu"
                                },
                                {
                                    "authorId": "2059411355",
                                    "name": "Clemens Winter"
                                },
                                {
                                    "authorId": "144239765",
                                    "name": "Christopher Hesse"
                                },
                                {
                                    "authorId": "2108828435",
                                    "name": "Mark Chen"
                                },
                                {
                                    "authorId": "2064673055",
                                    "name": "Eric Sigler"
                                },
                                {
                                    "authorId": "1380985420",
                                    "name": "Ma-teusz Litwin"
                                },
                                {
                                    "authorId": "145565184",
                                    "name": "Scott Gray"
                                },
                                {
                                    "authorId": "1490681878",
                                    "name": "Benjamin Chess"
                                },
                                {
                                    "authorId": "2115193883",
                                    "name": "Jack Clark"
                                },
                                {
                                    "authorId": "133740015",
                                    "name": "Christopher Berner"
                                },
                                {
                                    "authorId": "52238703",
                                    "name": "Sam McCandlish"
                                },
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                },
                                {
                                    "authorId": "2698777",
                                    "name": "Dario Amodei"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 42437
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ang et al., 2022)",
                        "snippets": [
                            "With many real-world applications of Natural Language Processing (NLP) comprising of long texts, there has been a rise in NLP benchmarks that measure the accuracy of models that can handle longer input sequences."
                        ],
                        "paper": {
                            "corpus_id": 248218548,
                            "title": "Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context NLP Models",
                            "authors": [
                                {
                                    "authorId": "2089185954",
                                    "name": "Phyllis Ang"
                                },
                                {
                                    "authorId": "2060730422",
                                    "name": "Bhuwan Dhingra"
                                },
                                {
                                    "authorId": "1820939409",
                                    "name": "L. Wills"
                                }
                            ],
                            "year": 2022,
                            "venue": "NLPPOWER",
                            "n_citations": 6
                        },
                        "score": 0.609375
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues."
                        ],
                        "paper": {
                            "corpus_id": 268378933,
                            "title": "StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses",
                            "authors": [
                                {
                                    "authorId": "2291078805",
                                    "name": "Jia-Nan Li"
                                },
                                {
                                    "authorId": "2071635049",
                                    "name": "Quan Tu"
                                },
                                {
                                    "authorId": "34754580",
                                    "name": "Cunli Mao"
                                },
                                {
                                    "authorId": "2285331933",
                                    "name": "Zhengtao Yu"
                                },
                                {
                                    "authorId": "2263887786",
                                    "name": "Ji-Rong Wen"
                                },
                                {
                                    "authorId": "2277448117",
                                    "name": "Rui Yan"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 4
                        },
                        "score": 0.63525390625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Categories of Long Context Modeling Techniques",
                "tldr": "Long context modeling approaches can be categorized into three main groups: efficiency-focused architectures that address quadratic attention complexity, memory/recurrence-based approaches that maintain state across segments, and length extrapolation techniques that enable models to handle sequences longer than seen during training. (21 sources)",
                "text": "\nThe rapid advancement of transformer models has led to various approaches for handling long contexts, which can be organized into distinct categories based on their underlying mechanisms. These categories address specific challenges in long-context modeling, particularly the computational overhead and the tendency for models to \"forget\" information from earlier parts of long sequences <Paper corpusId=\"261245264\" paperTitle=\"(Bai et al., 2023)\" isShortName></Paper> <Paper corpusId=\"221702858\" paperTitle=\"(Tay et al., 2020)\" isShortName></Paper>.\n\nThe first major category consists of **efficiency-focused architectures** that aim to reduce the quadratic complexity of standard transformers. These include sparse attention mechanisms <Paper corpusId=\"209315300\" paperTitle=\"(Kitaev et al., 2020)\" isShortName></Paper> <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper> <Paper corpusId=\"219636190\" paperTitle=\"(Martins et al., 2020)\" isShortName></Paper>, linear transformers <Paper corpusId=\"220250819\" paperTitle=\"(Katharopoulos et al., 2020)\" isShortName></Paper> <Paper corpusId=\"222067132\" paperTitle=\"(Choromanski et al., 2020)\" isShortName></Paper>, and mixture-of-experts approaches <Paper corpusId=\"231573431\" paperTitle=\"(Fedus et al., 2021)\" isShortName></Paper>. These methods restructure the attention mechanism to achieve linear or log-linear complexity with respect to sequence length, significantly reducing computational requirements for long contexts <Paper corpusId=\"268793522\" paperTitle=\"(Thonet et al., 2024)\" isShortName></Paper>.\n\nThe second category includes **memory and recurrence-based approaches** that maintain state across sequence segments. Notable examples include Transformer-XL <Paper corpusId=\"57759363\" paperTitle=\"(Dai et al., 2019)\" isShortName></Paper>, which introduced segment-level recurrence, and the Compressive Transformer <Paper corpusId=\"207930593\" paperTitle=\"(Rae et al., 2019)\" isShortName></Paper>, which compresses past memories for efficient long-range learning. More recent developments include Recurrent Memory Transformer <Paper corpusId=\"250526424\" paperTitle=\"(Bulatov et al., 2022)\" isShortName></Paper> and RWKV <Paper corpusId=\"258832459\" paperTitle=\"(Peng et al., 2023)\" isShortName></Paper>, which combines efficient parallel training of transformers with the linear scaling of RNNs at inference time. These architectures enable models to remember information across longer contexts by incorporating memory mechanisms or recurrent structures <Paper corpusId=\"247519194\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper>.\n\nThe third major category focuses on **length extrapolation techniques**, which enable models to handle sequences longer than those seen during training. This approach primarily involves position encoding innovations like ALiBi (Attention with Linear Biases) <Paper corpusId=\"237347130\" paperTitle=\"(Press et al., 2021)\" isShortName></Paper> and RoPE (Rotary Position Embeddings) extensions <Paper corpusId=\"261493986\" paperTitle=\"(Peng et al._1, 2023)\" isShortName></Paper>. These methods have been adopted in models like ChatGLM2-32k <Paper corpusId=\"252715691\" paperTitle=\"(Zeng et al., 2022)\" isShortName></Paper> and LongChat-32k <Paper corpusId=\"261245264\" paperTitle=\"(Bai et al., 2023)\" isShortName></Paper>. Other approaches in this category include context window sliding and segmentation techniques <Paper corpusId=\"258686160\" paperTitle=\"(Ratner et al., 2022)\" isShortName></Paper>.\n\nSome researchers have explored alternative approaches such as retrieval-augmented generation <Paper corpusId=\"263620134\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper> and context compression <Paper corpusId=\"258865249\" paperTitle=\"(Chevalier et al., 2023)\" isShortName></Paper>, which offer complementary solutions to the long-context challenge. Recent implementations like LongLoRA <Paper corpusId=\"262084134\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> combine multiple techniques, such as sparse attention and parameter-efficient fine-tuning, to efficiently extend context windows to impressive lengths (up to 100k tokens for 7B parameter models).\n\nThese advances have contributed to significant context length expansions in recent models, with proprietary systems like GPT-4, Claude-3, and Gemini-1.5 now supporting context windows ranging from 32K to 1 million tokens <Paper corpusId=\"268793522\" paperTitle=\"(Thonet et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Bai et al., 2023)",
                        "snippets": [
                            "Long Context Modeling Techniques. We first discuss some popular lines of methods that aim to tackle long context understanding. These studies are mainly aimed at solving two key challenges in long text modeling, including the high runtime overhead on longer context, and the catastrophic forgetting phenomenon when processing long sequence. A series of studies focus on how to make Transformers more efficient and unforgetful (Tay et al., 2020), with designs such as sparse and efficient computation (Child et al., 2019;(Kitaev et al., 2020)Beltagy et al., 2020;(Yu et al., 2023)Wang et al., 2020;(Fedus et al., 2021)Ding et al., 2023), recurrent and memory modules (Dai et al., 2019)(Rae et al., 2019)(Wu et al., 2022)Martins et al., 2022;(Bulatov et al., 2022)Orvieto et al., 2023;Liang et al., 2023;Zhou et al., 2023). More recently, several methods (Press et al., 2021)Sun et al., 2022;Chen et al., 2023) have been proposed to enable length extrapolation of Transformers, and have been adopted in the training process of long context LLMs such as ChatGLM2-32k (Zeng et al., 2022) and LongChat-32k (Li et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 261245264,
                            "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",
                            "authors": [
                                {
                                    "authorId": "2141377570",
                                    "name": "Yushi Bai"
                                },
                                {
                                    "authorId": "48574888",
                                    "name": "Xin Lv"
                                },
                                {
                                    "authorId": "2107983722",
                                    "name": "Jiajie Zhang"
                                },
                                {
                                    "authorId": "2220304036",
                                    "name": "Hong Lyu"
                                },
                                {
                                    "authorId": "2204826271",
                                    "name": "Jiankai Tang"
                                },
                                {
                                    "authorId": "2234629967",
                                    "name": "Zhidian Huang"
                                },
                                {
                                    "authorId": "66395694",
                                    "name": "Zhengxiao Du"
                                },
                                {
                                    "authorId": "2111312892",
                                    "name": "Xiao Liu"
                                },
                                {
                                    "authorId": "2051712753",
                                    "name": "Aohan Zeng"
                                },
                                {
                                    "authorId": "2055765060",
                                    "name": "Lei Hou"
                                },
                                {
                                    "authorId": "2047998",
                                    "name": "Yuxiao Dong"
                                },
                                {
                                    "authorId": "2148911975",
                                    "name": "Jie Tang"
                                },
                                {
                                    "authorId": "2133353675",
                                    "name": "Juanzi Li"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 603
                        },
                        "score": 0.5009765625
                    },
                    {
                        "id": "(Tay et al., 2020)",
                        "snippets": [
                            "Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \"X-former\" models have been proposed\u2014Reformer, Linformer, Performer, Longformer, to name a few\u2014which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored \"X-former\" models, providing an organized and comprehensive overview of existing work and models across multiple domains."
                        ],
                        "paper": {
                            "corpus_id": 221702858,
                            "title": "Efficient Transformers: A Survey",
                            "authors": [
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "3226635",
                                    "name": "Mostafa Dehghani"
                                },
                                {
                                    "authorId": "11774695",
                                    "name": "Dara Bahri"
                                },
                                {
                                    "authorId": "1680617",
                                    "name": "Donald Metzler"
                                }
                            ],
                            "year": 2020,
                            "venue": "ACM Computing Surveys",
                            "n_citations": 1128
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kitaev et al., 2020)",
                        "snippets": [
                            "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences."
                        ],
                        "paper": {
                            "corpus_id": 209315300,
                            "title": "Reformer: The Efficient Transformer",
                            "authors": [
                                {
                                    "authorId": "143808231",
                                    "name": "Nikita Kitaev"
                                },
                                {
                                    "authorId": "40527594",
                                    "name": "Lukasz Kaiser"
                                },
                                {
                                    "authorId": "6639036",
                                    "name": "Anselm Levskaya"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 2333
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zaheer et al., 2020)",
                        "snippets": [
                            "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."
                        ],
                        "paper": {
                            "corpus_id": 220831004,
                            "title": "Big Bird: Transformers for Longer Sequences",
                            "authors": [
                                {
                                    "authorId": "1771307",
                                    "name": "M. Zaheer"
                                },
                                {
                                    "authorId": "1947314",
                                    "name": "Guru Guruganesh"
                                },
                                {
                                    "authorId": "89890133",
                                    "name": "Kumar Avinava Dubey"
                                },
                                {
                                    "authorId": "1643737606",
                                    "name": "J. Ainslie"
                                },
                                {
                                    "authorId": "114577307",
                                    "name": "Chris Alberti"
                                },
                                {
                                    "authorId": "1722671",
                                    "name": "Santiago Onta\u00f1\u00f3n"
                                },
                                {
                                    "authorId": "38552691",
                                    "name": "Philip Pham"
                                },
                                {
                                    "authorId": "101210026",
                                    "name": "Anirudh Ravula"
                                },
                                {
                                    "authorId": "145196279",
                                    "name": "Qifan Wang"
                                },
                                {
                                    "authorId": "113906155",
                                    "name": "Li Yang"
                                },
                                {
                                    "authorId": "143629707",
                                    "name": "Amr Ahmed"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2103
                        },
                        "score": 0.383544921875
                    },
                    {
                        "id": "(Martins et al., 2020)",
                        "snippets": [
                            "Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and alpha-entmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: first, we extend alpha-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for alpha in {1,2}. Experiments on attention-based text classification, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions."
                        ],
                        "paper": {
                            "corpus_id": 219636190,
                            "title": "Sparse and Continuous Attention Mechanisms",
                            "authors": [
                                {
                                    "authorId": "145644643",
                                    "name": "Andr\u00e9 F. T. Martins"
                                },
                                {
                                    "authorId": "145188499",
                                    "name": "Marcos Vin\u00edcius Treviso"
                                },
                                {
                                    "authorId": "1748971692",
                                    "name": "Ant\u00f3nio Farinhas"
                                },
                                {
                                    "authorId": "2114966",
                                    "name": "Vlad Niculae"
                                },
                                {
                                    "authorId": "2075330932",
                                    "name": "M\u00e1rio A. T. Figueiredo"
                                },
                                {
                                    "authorId": "35537344",
                                    "name": "P. Aguiar"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 41
                        },
                        "score": 0
                    },
                    {
                        "id": "(Katharopoulos et al., 2020)",
                        "snippets": [
                            "Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences."
                        ],
                        "paper": {
                            "corpus_id": 220250819,
                            "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
                            "authors": [
                                {
                                    "authorId": "3493855",
                                    "name": "Angelos Katharopoulos"
                                },
                                {
                                    "authorId": "2992087",
                                    "name": "Apoorv Vyas"
                                },
                                {
                                    "authorId": "143958923",
                                    "name": "Nikolaos Pappas"
                                },
                                {
                                    "authorId": "116272138",
                                    "name": "Franccois Fleuret"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1790
                        },
                        "score": 0
                    },
                    {
                        "id": "(Choromanski et al., 2020)",
                        "snippets": [
                            "We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers."
                        ],
                        "paper": {
                            "corpus_id": 222067132,
                            "title": "Rethinking Attention with Performers",
                            "authors": [
                                {
                                    "authorId": "1805203",
                                    "name": "K. Choromanski"
                                },
                                {
                                    "authorId": "52314889",
                                    "name": "Valerii Likhosherstov"
                                },
                                {
                                    "authorId": "35363891",
                                    "name": "David Dohan"
                                },
                                {
                                    "authorId": "32725720",
                                    "name": "Xingyou Song"
                                },
                                {
                                    "authorId": "3071104",
                                    "name": "Andreea Gane"
                                },
                                {
                                    "authorId": "2227764",
                                    "name": "Tam\u00e1s Sarl\u00f3s"
                                },
                                {
                                    "authorId": "2052793706",
                                    "name": "Peter Hawkins"
                                },
                                {
                                    "authorId": "29827891",
                                    "name": "Jared Davis"
                                },
                                {
                                    "authorId": "1579862074",
                                    "name": "Afroz Mohiuddin"
                                },
                                {
                                    "authorId": "40527594",
                                    "name": "Lukasz Kaiser"
                                },
                                {
                                    "authorId": "2636941",
                                    "name": "David Belanger"
                                },
                                {
                                    "authorId": "2654847",
                                    "name": "Lucy J. Colwell"
                                },
                                {
                                    "authorId": "145689461",
                                    "name": "Adrian Weller"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 1602
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fedus et al., 2021)",
                        "snippets": [
                            "In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the\"Colossal Clean Crawled Corpus\"and achieve a 4x speedup over the T5-XXL model."
                        ],
                        "paper": {
                            "corpus_id": 231573431,
                            "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
                            "authors": [
                                {
                                    "authorId": "26958176",
                                    "name": "W. Fedus"
                                },
                                {
                                    "authorId": "2368067",
                                    "name": "Barret Zoph"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                }
                            ],
                            "year": 2021,
                            "venue": "Journal of machine learning research",
                            "n_citations": 2224
                        },
                        "score": 0
                    },
                    {
                        "id": "(Thonet et al., 2024)",
                        "snippets": [
                            "context modeling. 3 While an exhaustive survey of these methods is beyond the scope of this paper, they can generally be categorized into three main groups (excluding other distinct approaches such as retrieval-augmented generation (Xu et al., 2023) and context compression (Chevalier et al., 2023)): (a) the development of efficient transformer architectures to address the quadratic attention challenge, including sparse transformers [6,12,(Martins et al., 2020)(Zaheer et al., 2020), linear transformers (Choromanski et al., 2020)(Katharopoulos et al., 2020)[42], and hierarchical transformers [20,30,(Wu et al., 2021); (b) approaches like recurrent attention networks [7,(Dai et al., 2019)(Peng et al., 2023) and state-space models [16,41]; (c) length extrapolation or position embedding interpolation, where LLMs are fine-tuned or adapted at inference time to adjust tokens' positions to match the new context length [4,8,9,28,35,(Peng et al., 2023)[45]. These techniques also contributed to the context length expansion in proprietary models like GPT-4 (32K-128K), Claude-3 (200k), and Gemini-1.5 (128K-1M)."
                        ],
                        "paper": {
                            "corpus_id": 268793522,
                            "title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models",
                            "authors": [
                                {
                                    "authorId": "2955690",
                                    "name": "Thibaut Thonet"
                                },
                                {
                                    "authorId": "120419790",
                                    "name": "Jos Rozen"
                                },
                                {
                                    "authorId": "2259359878",
                                    "name": "Laurent Besacier"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0.56689453125
                    },
                    {
                        "id": "(Dai et al., 2019)",
                        "snippets": [
                            "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
                        ],
                        "paper": {
                            "corpus_id": 57759363,
                            "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
                            "authors": [
                                {
                                    "authorId": "3422912",
                                    "name": "Zihang Dai"
                                },
                                {
                                    "authorId": "2109512754",
                                    "name": "Zhilin Yang"
                                },
                                {
                                    "authorId": "35729970",
                                    "name": "Yiming Yang"
                                },
                                {
                                    "authorId": "143712374",
                                    "name": "J. Carbonell"
                                },
                                {
                                    "authorId": "2827616",
                                    "name": "Quoc V. Le"
                                },
                                {
                                    "authorId": "145124475",
                                    "name": "R. Salakhutdinov"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 3746
                        },
                        "score": 0
                    },
                    {
                        "id": "(Rae et al., 2019)",
                        "snippets": [
                            "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19."
                        ],
                        "paper": {
                            "corpus_id": 207930593,
                            "title": "Compressive Transformers for Long-Range Sequence Modelling",
                            "authors": [
                                {
                                    "authorId": "34269227",
                                    "name": "Jack W. Rae"
                                },
                                {
                                    "authorId": "13759734",
                                    "name": "Anna Potapenko"
                                },
                                {
                                    "authorId": "35880964",
                                    "name": "Siddhant M. Jayakumar"
                                },
                                {
                                    "authorId": "2542999",
                                    "name": "T. Lillicrap"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 653
                        },
                        "score": 0
                    },
                    {
                        "id": "(Bulatov et al., 2022)",
                        "snippets": [
                            "Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning."
                        ],
                        "paper": {
                            "corpus_id": 250526424,
                            "title": "Recurrent Memory Transformer",
                            "authors": [
                                {
                                    "authorId": "2176183932",
                                    "name": "Aydar Bulatov"
                                },
                                {
                                    "authorId": "51114080",
                                    "name": "Yuri Kuratov"
                                },
                                {
                                    "authorId": "3359236",
                                    "name": "M. Burtsev"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 110
                        },
                        "score": 0
                    },
                    {
                        "id": "(Peng et al., 2023)",
                        "snippets": [
                            "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks."
                        ],
                        "paper": {
                            "corpus_id": 258832459,
                            "title": "RWKV: Reinventing RNNs for the Transformer Era",
                            "authors": [
                                {
                                    "authorId": "145560079",
                                    "name": "Bo Peng"
                                },
                                {
                                    "authorId": "79046907",
                                    "name": "Eric Alcaide"
                                },
                                {
                                    "authorId": "1404060481",
                                    "name": "Quentin G. Anthony"
                                },
                                {
                                    "authorId": "2044198106",
                                    "name": "Alon Albalak"
                                },
                                {
                                    "authorId": "2322500848",
                                    "name": "Samuel Arcadinho"
                                },
                                {
                                    "authorId": "103476203",
                                    "name": "Stella Biderman"
                                },
                                {
                                    "authorId": "47709883",
                                    "name": "Huanqi Cao"
                                },
                                {
                                    "authorId": "2193630544",
                                    "name": "Xin Cheng"
                                },
                                {
                                    "authorId": "2218291950",
                                    "name": "Michael Chung"
                                },
                                {
                                    "authorId": "2425906",
                                    "name": "Matteo Grella"
                                },
                                {
                                    "authorId": "101433524",
                                    "name": "G. Kranthikiran"
                                },
                                {
                                    "authorId": "46214809",
                                    "name": "Xingjian Du"
                                },
                                {
                                    "authorId": "33913193",
                                    "name": "Xuming He"
                                },
                                {
                                    "authorId": "9397636",
                                    "name": "Haowen Hou"
                                },
                                {
                                    "authorId": "1724788",
                                    "name": "Przemyslaw Kazienko"
                                },
                                {
                                    "authorId": "2905929",
                                    "name": "Jan Koco\u0144"
                                },
                                {
                                    "authorId": "35171548",
                                    "name": "Jiaming Kong"
                                },
                                {
                                    "authorId": "2208962106",
                                    "name": "Bartlomiej Koptyra"
                                },
                                {
                                    "authorId": "1598440603",
                                    "name": "Hayden Lau"
                                },
                                {
                                    "authorId": "2209207087",
                                    "name": "Krishna Sri Ipsit Mantri"
                                },
                                {
                                    "authorId": "2218334866",
                                    "name": "Ferdinand Mom"
                                },
                                {
                                    "authorId": "2186861874",
                                    "name": "Atsushi Saito"
                                },
                                {
                                    "authorId": "47274259",
                                    "name": "Xiangru Tang"
                                },
                                {
                                    "authorId": "2153213619",
                                    "name": "Bolun Wang"
                                },
                                {
                                    "authorId": "1388112865",
                                    "name": "J. S. Wind"
                                },
                                {
                                    "authorId": "2218377273",
                                    "name": "Stansilaw Wozniak"
                                },
                                {
                                    "authorId": "2218987422",
                                    "name": "Ruichong Zhang"
                                },
                                {
                                    "authorId": "2144400204",
                                    "name": "Zhenyuan Zhang"
                                },
                                {
                                    "authorId": "2110483969",
                                    "name": "Qihang Zhao"
                                },
                                {
                                    "authorId": "1994721202",
                                    "name": "P. Zhou"
                                },
                                {
                                    "authorId": "144549416",
                                    "name": "Jian Zhu"
                                },
                                {
                                    "authorId": "144649570",
                                    "name": "Rui Zhu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 608
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2022)",
                        "snippets": [
                            "Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time."
                        ],
                        "paper": {
                            "corpus_id": 247519194,
                            "title": "Memorizing Transformers",
                            "authors": [
                                {
                                    "authorId": "3374063",
                                    "name": "Yuhuai Wu"
                                },
                                {
                                    "authorId": "1800714",
                                    "name": "M. Rabe"
                                },
                                {
                                    "authorId": "32913644",
                                    "name": "DeLesley S. Hutchins"
                                },
                                {
                                    "authorId": "2574060",
                                    "name": "Christian Szegedy"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 178
                        },
                        "score": 0
                    },
                    {
                        "id": "(Press et al., 2021)",
                        "snippets": [
                            "Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark."
                        ],
                        "paper": {
                            "corpus_id": 237347130,
                            "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
                            "authors": [
                                {
                                    "authorId": "40170001",
                                    "name": "Ofir Press"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 775
                        },
                        "score": 0
                    },
                    {
                        "id": "(Peng et al._1, 2023)",
                        "snippets": [
                            "Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at https://github.com/jquesnelle/yarn"
                        ],
                        "paper": {
                            "corpus_id": 261493986,
                            "title": "YaRN: Efficient Context Window Extension of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2237425757",
                                    "name": "Bowen Peng"
                                },
                                {
                                    "authorId": "31964220",
                                    "name": "Jeffrey Quesnelle"
                                },
                                {
                                    "authorId": "2273749369",
                                    "name": "Honglu Fan"
                                },
                                {
                                    "authorId": "2261554775",
                                    "name": "Enrico Shippole"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 264
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zeng et al., 2022)",
                        "snippets": [
                            "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \\url{https://github.com/THUDM/GLM-130B/}."
                        ],
                        "paper": {
                            "corpus_id": 252715691,
                            "title": "GLM-130B: An Open Bilingual Pre-trained Model",
                            "authors": [
                                {
                                    "authorId": "2051712753",
                                    "name": "Aohan Zeng"
                                },
                                {
                                    "authorId": "2111312892",
                                    "name": "Xiao Liu"
                                },
                                {
                                    "authorId": "66395694",
                                    "name": "Zhengxiao Du"
                                },
                                {
                                    "authorId": null,
                                    "name": "Zihan Wang"
                                },
                                {
                                    "authorId": "2051311700",
                                    "name": "Hanyu Lai"
                                },
                                {
                                    "authorId": "145573466",
                                    "name": "Ming Ding"
                                },
                                {
                                    "authorId": "2109506541",
                                    "name": "Zhuoyi Yang"
                                },
                                {
                                    "authorId": "2125063007",
                                    "name": "Yifan Xu"
                                },
                                {
                                    "authorId": "2163967642",
                                    "name": "Wendi Zheng"
                                },
                                {
                                    "authorId": "2186982651",
                                    "name": "Xiao Xia"
                                },
                                {
                                    "authorId": "1403621152",
                                    "name": "W. Tam"
                                },
                                {
                                    "authorId": "2124489983",
                                    "name": "Zixuan Ma"
                                },
                                {
                                    "authorId": "2114921664",
                                    "name": "Yufei Xue"
                                },
                                {
                                    "authorId": "2467444",
                                    "name": "Jidong Zhai"
                                },
                                {
                                    "authorId": "1712168",
                                    "name": "Wenguang Chen"
                                },
                                {
                                    "authorId": "47243067",
                                    "name": "P. Zhang"
                                },
                                {
                                    "authorId": "2047998",
                                    "name": "Yuxiao Dong"
                                },
                                {
                                    "authorId": "2148911956",
                                    "name": "Jie Tang"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 1094
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ratner et al., 2022)",
                        "snippets": [
                            "When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (\"windows\"), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ ai21labs/parallel-context-windows."
                        ],
                        "paper": {
                            "corpus_id": 258686160,
                            "title": "Parallel Context Windows for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2148471161",
                                    "name": "Nir Ratner"
                                },
                                {
                                    "authorId": "152754428",
                                    "name": "Yoav Levine"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                },
                                {
                                    "authorId": "73775461",
                                    "name": "Ori Ram"
                                },
                                {
                                    "authorId": "2158996542",
                                    "name": "Inbal Magar"
                                },
                                {
                                    "authorId": "2769805",
                                    "name": "Omri Abend"
                                },
                                {
                                    "authorId": "12621100",
                                    "name": "Ehud Karpas"
                                },
                                {
                                    "authorId": "3140335",
                                    "name": "A. Shashua"
                                },
                                {
                                    "authorId": "2066411743",
                                    "name": "Kevin Leyton-Brown"
                                },
                                {
                                    "authorId": "1701353",
                                    "name": "Y. Shoham"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 75
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2023)",
                        "snippets": [
                            "Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners."
                        ],
                        "paper": {
                            "corpus_id": 263620134,
                            "title": "Retrieval meets Long Context Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2254989105",
                                    "name": "Peng Xu"
                                },
                                {
                                    "authorId": "2253664013",
                                    "name": "Wei Ping"
                                },
                                {
                                    "authorId": "2253618149",
                                    "name": "Xianchao Wu"
                                },
                                {
                                    "authorId": "20957879",
                                    "name": "Lawrence C. McAfee"
                                },
                                {
                                    "authorId": "2283871700",
                                    "name": "Chen Zhu"
                                },
                                {
                                    "authorId": "2256582287",
                                    "name": "Zihan Liu"
                                },
                                {
                                    "authorId": "2253531461",
                                    "name": "Sandeep Subramanian"
                                },
                                {
                                    "authorId": "32867948",
                                    "name": "E. Bakhturina"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "2301680",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 85
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chevalier et al., 2023)",
                        "snippets": [
                            "Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments and summary vectors from all previous segments are used in language modeling. We fine-tune OPT models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations. We find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference cost. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrieval-augmented language modeling. Overall, AutoCompressors emerge as a simple and inexpensive solution for extending the context window of LMs while speeding up inference over long contexts."
                        ],
                        "paper": {
                            "corpus_id": 258865249,
                            "title": "Adapting Language Models to Compress Contexts",
                            "authors": [
                                {
                                    "authorId": "2161343103",
                                    "name": "Alexis Chevalier"
                                },
                                {
                                    "authorId": "2127066887",
                                    "name": "Alexander Wettig"
                                },
                                {
                                    "authorId": "2218438150",
                                    "name": "Anirudh Ajith"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 190
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset."
                        ],
                        "paper": {
                            "corpus_id": 262084134,
                            "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2109297557",
                                    "name": "Yukang Chen"
                                },
                                {
                                    "authorId": "152230789",
                                    "name": "Shengju Qian"
                                },
                                {
                                    "authorId": "150127950",
                                    "name": "Haotian Tang"
                                },
                                {
                                    "authorId": "2237802684",
                                    "name": "Xin Lai"
                                },
                                {
                                    "authorId": "47781592",
                                    "name": "Zhijian Liu"
                                },
                                {
                                    "authorId": "2243400730",
                                    "name": "Song Han"
                                },
                                {
                                    "authorId": "2237811040",
                                    "name": "Jiaya Jia"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 167
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Efficiency-Focused Approaches",
                "tldr": "Efficiency-focused approaches tackle the quadratic complexity problem of transformer attention mechanisms through various techniques like sparse attention patterns, linear reformulations, and optimized memory operations. These innovations allow models to process longer sequences with significantly reduced computational and memory requirements. (5 sources)",
                "text": "\n- **Sparse Attention Mechanisms**: Researchers have developed multiple sparse attention patterns to reduce the quadratic complexity of standard transformers. BigBird implements a sparse attention mechanism that reduces the quadratic dependency to linear while maintaining the model's capabilities as a universal approximator of sequence functions <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>. Longformer combines sliding window and global attention patterns to efficiently process longer sequences <Paper corpusId=\"267770311\" paperTitle=\"(He et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>.\n\n- **Low-Rank Approximations**: These methods approximate the self-attention matrix using low-rank factorization, significantly reducing computational demands while preserving most of the attention mechanism's effectiveness <Paper corpusId=\"267770311\" paperTitle=\"(He et al., 2024)\" isShortName></Paper> <Paper corpusId=\"268385246\" paperTitle=\"(Athiwaratkun et al., 2024)\" isShortName></Paper>.\n\n- **Linear Attention Formulations**: Linear Attention methods reformulate the self-attention mechanism to achieve linear complexity with respect to sequence length, making it feasible to process much longer contexts <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>.\n\n- **Memory-Efficient Implementations**: FlashAttention stands out as an IO-aware exact attention algorithm that uses tiling to reduce memory transfers between GPU high bandwidth memory and on-chip SRAM. This approach achieves substantial speedups: 15% faster end-to-end wall-clock performance on BERT-large, 3\u00d7 speedup on GPT-2, and 2.4\u00d7 speedup on long-range arena tasks <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>.\n\n- **Hierarchical Approaches**: BlockTransformer employs hierarchical global-to-local modeling to efficiently handle longer contexts by processing information at different levels of granularity <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>.\n\n- **State Space Models**: Models like Mamba capture long-range dependencies efficiently without using explicit attention mechanisms, offering an alternative approach to the long context challenge <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>.\n\n- **Unbounded Context Models**: InfiniteTransformer incorporates unbounded long-term memory through continuous-space attention, potentially allowing for unlimited context length <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>.\n\nDespite these advances, many of these methods still struggle with retrieving information from the middle of very long inputs and cannot handle truly unbounded context window sizes <Paper corpusId=\"267770311\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>. However, they can often be combined with other approaches for more effective long context modeling.",
                "citations": [
                    {
                        "id": "(Zaheer et al., 2020)",
                        "snippets": [
                            "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."
                        ],
                        "paper": {
                            "corpus_id": 220831004,
                            "title": "Big Bird: Transformers for Longer Sequences",
                            "authors": [
                                {
                                    "authorId": "1771307",
                                    "name": "M. Zaheer"
                                },
                                {
                                    "authorId": "1947314",
                                    "name": "Guru Guruganesh"
                                },
                                {
                                    "authorId": "89890133",
                                    "name": "Kumar Avinava Dubey"
                                },
                                {
                                    "authorId": "1643737606",
                                    "name": "J. Ainslie"
                                },
                                {
                                    "authorId": "114577307",
                                    "name": "Chris Alberti"
                                },
                                {
                                    "authorId": "1722671",
                                    "name": "Santiago Onta\u00f1\u00f3n"
                                },
                                {
                                    "authorId": "38552691",
                                    "name": "Philip Pham"
                                },
                                {
                                    "authorId": "101210026",
                                    "name": "Anirudh Ravula"
                                },
                                {
                                    "authorId": "145196279",
                                    "name": "Qifan Wang"
                                },
                                {
                                    "authorId": "113906155",
                                    "name": "Li Yang"
                                },
                                {
                                    "authorId": "143629707",
                                    "name": "Amr Ahmed"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2103
                        },
                        "score": 0.383544921875
                    },
                    {
                        "id": "(He et al., 2024)",
                        "snippets": [
                            "Many efficient self-attention techniques have been proposed for long context modeling in transformer models, including low-rank factorization (Wang et al., 2020), local attention (Ramachandran et al., 2019), dilated attention (Ding et al., 2023), sparsity (Beltagy et al., 2020;(Zaheer et al., 2020)Kitaev et al., 2020), and hardwareaware attention mechanisms such as FlashAttention (Dao et al., 2022)Dao, 2023). Despite notable progress, these methods are unable to handle unbounded context window sizes and struggle to retrieve information in the middle of the input (Liu et al., 2023). They can be used in tandem with our proposed approach for longer context modeling."
                        ],
                        "paper": {
                            "corpus_id": 267770311,
                            "title": "CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory",
                            "authors": [
                                {
                                    "authorId": "2116458151",
                                    "name": "Zexue He"
                                },
                                {
                                    "authorId": "2142741421",
                                    "name": "Leonid Karlinsky"
                                },
                                {
                                    "authorId": "2266361991",
                                    "name": "Donghyun Kim"
                                },
                                {
                                    "authorId": "2258962117",
                                    "name": "Julian McAuley"
                                },
                                {
                                    "authorId": "2284865239",
                                    "name": "Dmitry Krotov"
                                },
                                {
                                    "authorId": "2239199018",
                                    "name": "Rog\u00e9rio Feris"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 11
                        },
                        "score": 0.55859375
                    },
                    {
                        "id": "(He et al., 2025)",
                        "snippets": [
                            "To address the quadratic computational and memory demands of standard transformer self-attention, researchers have developed various architectural modifica-tions to improve efficiency and extend context lengths. Notable examples include Longformer (Beltagy et al., 2020), which combines sliding window and global attention, and BlockTransformer (Ho et al., 2024), which employs hierarchical global-to-local modeling. Linear Attention methods (Katharopoulos et al., 2020) reformulate self-attention for linear complexity, while InfiniteTransformer (Munkhdalai et al., 2024) incorporates unbounded long-term memory through continuousspace attention. State space models like Mamba (Gu & Dao, 2024) capture long-range dependencies efficiently without explicit attention mechanisms."
                        ],
                        "paper": {
                            "corpus_id": 277857470,
                            "title": "Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation",
                            "authors": [
                                {
                                    "authorId": "2356154321",
                                    "name": "Linda He"
                                },
                                {
                                    "authorId": "2252087284",
                                    "name": "Jue Wang"
                                },
                                {
                                    "authorId": "2110605429",
                                    "name": "Maurice Weber"
                                },
                                {
                                    "authorId": "2356004549",
                                    "name": "Shang Zhu"
                                },
                                {
                                    "authorId": "2304481349",
                                    "name": "Ben Athiwaratkun"
                                },
                                {
                                    "authorId": "2305565297",
                                    "name": "Ce Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 1
                        },
                        "score": 0.5537109375
                    },
                    {
                        "id": "(Athiwaratkun et al., 2024)",
                        "snippets": [
                            "To effectively handle longer context sequences, optimizing memory I/O and reducing computational overhead are critical. Currently, the dominant approaches to addressing this challenge have been to make the attention computation less expensive. Beltagy et al. (2020) proposed to sparsify self-attention using various attention patterns. Wang et al. (2020) explores low-rank approximation of self-attention. In addition to the compute bound improvements, advancements in memory-efficient attention mechanisms and techniques for reducing memory I/O will continue to propel the field forward, facilitating the handling of longer context sequences in language models."
                        ],
                        "paper": {
                            "corpus_id": 268385246,
                            "title": "Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs",
                            "authors": [
                                {
                                    "authorId": "2095707",
                                    "name": "Ben Athiwaratkun"
                                },
                                {
                                    "authorId": "1913939",
                                    "name": "Sujan Kumar Gonugondla"
                                },
                                {
                                    "authorId": "40892818",
                                    "name": "Sanjay Krishna Gouda"
                                },
                                {
                                    "authorId": "2287922898",
                                    "name": "Haifeng Qian"
                                },
                                {
                                    "authorId": "2113455281",
                                    "name": "Hantian Ding"
                                },
                                {
                                    "authorId": "2291224391",
                                    "name": "Qing Sun"
                                },
                                {
                                    "authorId": "2291153804",
                                    "name": "Jun Wang"
                                },
                                {
                                    "authorId": "2291418208",
                                    "name": "Jiacheng Guo"
                                },
                                {
                                    "authorId": "2282351608",
                                    "name": "Liangfu Chen"
                                },
                                {
                                    "authorId": "50339091",
                                    "name": "Parminder Bhatia"
                                },
                                {
                                    "authorId": "1701451",
                                    "name": "Ramesh Nallapati"
                                },
                                {
                                    "authorId": "2072419570",
                                    "name": "Sudipta Sengupta"
                                },
                                {
                                    "authorId": "2258965075",
                                    "name": "Bing Xiang"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 4
                        },
                        "score": 0.5205078125
                    },
                    {
                        "id": "(Dao et al., 2022)",
                        "snippets": [
                            "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."
                        ],
                        "paper": {
                            "corpus_id": 249151871,
                            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                            "authors": [
                                {
                                    "authorId": "24593911",
                                    "name": "Tri Dao"
                                },
                                {
                                    "authorId": "49577833",
                                    "name": "Daniel Y. Fu"
                                },
                                {
                                    "authorId": "2490652",
                                    "name": "Stefano Ermon"
                                },
                                {
                                    "authorId": "1755572",
                                    "name": "A. Rudra"
                                },
                                {
                                    "authorId": "2061444681",
                                    "name": "Christopher R'e"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2285
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Position Encoding Approaches",
                "tldr": "Position encoding innovations have made significant strides in extending transformer context lengths beyond training limits. These approaches modify how models perceive token positions, enabling them to process sequences many times longer than they were trained on without additional architectural changes. (5 sources)",
                "text": "\n- **Attention with Linear Biases (ALiBi)**: This position method does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty proportional to token distance. ALiBi enables models trained on shorter sequences (e.g., 1024 tokens) to effectively extrapolate to longer ones (e.g., 2048 tokens) at inference time, achieving the same perplexity as models specifically trained on longer sequences while training 11% faster and using 11% less memory. <Paper corpusId=\"237347130\" paperTitle=\"(Press et al., 2021)\" isShortName></Paper>\n\n- **Rotary Position Embeddings (RoPE)**: RoPE has become a foundation for length extrapolation techniques, allowing models to generalize to longer contexts than seen during training. Recent advances like LongRoPE have pushed the boundaries even further, enabling context windows up to an impressive 2 million tokens. <Paper corpusId=\"268857023\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Parallel Context Windows (PCW)**: This approach addresses context limitations in off-the-shelf LLMs without requiring additional training. PCW works by dividing long inputs into chunks (\"windows\"), restricting attention to operate only within each window, and reusing positional embeddings across windows. This method has shown substantial improvements for various tasks requiring long context processing. <Paper corpusId=\"258686160\" paperTitle=\"(Ratner et al., 2022)\" isShortName></Paper> <Paper corpusId=\"268857023\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Position Interpolation/Extrapolation Techniques**: Numerous studies have explored strategies to enable models to handle positions beyond their training range through interpolation or extrapolation techniques. These methods modify how models interpret token positions, allowing them to process much longer sequences at inference time. <Paper corpusId=\"270710851\" paperTitle=\"(Gavin et al., 2024)\" isShortName></Paper>\n\n- **Combined Approaches**: Innovations like LongLoRA combine position encoding improvements with other techniques such as sparse attention and parameter-efficient fine-tuning. This has enabled dramatic context extensions, such as expanding Llama2 7B from 4K to 100K context length, or Llama2 70B to 32K, while maintaining the original architecture. <Paper corpusId=\"262084134\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268857023\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\nThese position encoding approaches are particularly valuable because they often require minimal architectural changes to existing models, making them relatively straightforward to implement compared to completely redesigning model architectures. Many current open-source LLMs incorporate these techniques to enhance their long sequence understanding capabilities. <Paper corpusId=\"268857023\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Press et al., 2021)",
                        "snippets": [
                            "Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark."
                        ],
                        "paper": {
                            "corpus_id": 237347130,
                            "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
                            "authors": [
                                {
                                    "authorId": "40170001",
                                    "name": "Ofir Press"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 775
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al._1, 2024)",
                        "snippets": [
                            "One line of research is based on AliBi (Press et al., 2021) and RoPE (Su et al., 2021) embeddings, which allows us to train Transformers with short sequences and subsequently apply them to longer sequences during inference.Recently, different approaches (Xiong et al., 2023;Fu et al., 2024;Liu et al., 2024) help the model to extrapolate to 128K window size with continued pre-training.Later on, LongRoPE (Ding et al., 2024) was proposed to further extend the context window to 2M tokens.Another line of research also utilizes methodologies like context window sliding and segmentation to overcome the issue of the limited context window in original Transformers (Hao et al., 2022;(Ratner et al., 2022).Furthermore, architectural innovations, transitioning from traditional Transformer-based designs to recurrent models or state space models, have shown promise in facilitating long-range computations naturally (Orvieto et al., 2023;Gu & Dao, 2023;(Peng et al., 2023).These techniques have been incorporated into several current open-source LLMs to enhance long sequence understanding capability (Chen et al., 2023)Tworkowski et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 268857023,
                            "title": "Long-context LLMs Struggle with Long In-context Learning",
                            "authors": [
                                {
                                    "authorId": "2157466550",
                                    "name": "Tianle Li"
                                },
                                {
                                    "authorId": "2143853895",
                                    "name": "Ge Zhang"
                                },
                                {
                                    "authorId": "2294572942",
                                    "name": "Quy Duc Do"
                                },
                                {
                                    "authorId": "2284988933",
                                    "name": "Xiang Yue"
                                },
                                {
                                    "authorId": "2249847177",
                                    "name": "Wenhu Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 192
                        },
                        "score": 0.64794921875
                    },
                    {
                        "id": "(Ratner et al., 2022)",
                        "snippets": [
                            "When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (\"windows\"), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ ai21labs/parallel-context-windows."
                        ],
                        "paper": {
                            "corpus_id": 258686160,
                            "title": "Parallel Context Windows for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2148471161",
                                    "name": "Nir Ratner"
                                },
                                {
                                    "authorId": "152754428",
                                    "name": "Yoav Levine"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                },
                                {
                                    "authorId": "73775461",
                                    "name": "Ori Ram"
                                },
                                {
                                    "authorId": "2158996542",
                                    "name": "Inbal Magar"
                                },
                                {
                                    "authorId": "2769805",
                                    "name": "Omri Abend"
                                },
                                {
                                    "authorId": "12621100",
                                    "name": "Ehud Karpas"
                                },
                                {
                                    "authorId": "3140335",
                                    "name": "A. Shashua"
                                },
                                {
                                    "authorId": "2066411743",
                                    "name": "Kevin Leyton-Brown"
                                },
                                {
                                    "authorId": "1701353",
                                    "name": "Y. Shoham"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 75
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gavin et al., 2024)",
                        "snippets": [
                            "Many studies explore various strategies to address this challenge, including the use of new positional encodings to achieve position interpolation or extrapolation (Peng et al., 2023;Chen et al., 2023;Liu et al., 2024b)",
                            "RingAttention (Liu et al., 2023a) can reduce the memory requirements of the Transformer model, allowing it to train sequences over 500 times longer than previous memory-efficient methods, without needing to approximate the attention mechanism."
                        ],
                        "paper": {
                            "corpus_id": 270710851,
                            "title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs",
                            "authors": [
                                {
                                    "authorId": "2308098543",
                                    "name": "Shawn Gavin"
                                },
                                {
                                    "authorId": "2300091474",
                                    "name": "Tuney Zheng"
                                },
                                {
                                    "authorId": "2294523552",
                                    "name": "Jiaheng Liu"
                                },
                                {
                                    "authorId": "2303653097",
                                    "name": "Quehry Que"
                                },
                                {
                                    "authorId": "2303796897",
                                    "name": "Noah Wang"
                                },
                                {
                                    "authorId": "2308240515",
                                    "name": "Jian Yang"
                                },
                                {
                                    "authorId": "2214538677",
                                    "name": "Chenchen Zhang"
                                },
                                {
                                    "authorId": "2239245627",
                                    "name": "Wenhao Huang"
                                },
                                {
                                    "authorId": "2249847177",
                                    "name": "Wenhu Chen"
                                },
                                {
                                    "authorId": "2308233656",
                                    "name": "Ge Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.689453125
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset."
                        ],
                        "paper": {
                            "corpus_id": 262084134,
                            "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2109297557",
                                    "name": "Yukang Chen"
                                },
                                {
                                    "authorId": "152230789",
                                    "name": "Shengju Qian"
                                },
                                {
                                    "authorId": "150127950",
                                    "name": "Haotian Tang"
                                },
                                {
                                    "authorId": "2237802684",
                                    "name": "Xin Lai"
                                },
                                {
                                    "authorId": "47781592",
                                    "name": "Zhijian Liu"
                                },
                                {
                                    "authorId": "2243400730",
                                    "name": "Song Han"
                                },
                                {
                                    "authorId": "2237811040",
                                    "name": "Jiaya Jia"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 167
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Recent Advancements and Implementations",
                "tldr": "Recent years have seen dramatic innovations in extending context windows from thousands to millions of tokens through both architectural innovations and position encoding techniques. Commercial models have pushed boundaries even further, with context windows now reaching up to 1 million tokens, while open-source implementations like LongLoRA and LongRoPE have made extended context capabilities more accessible. (6 sources)",
                "text": "\nThe field of long context modeling has evolved rapidly, with researchers achieving remarkable breakthroughs in extending transformer models' context windows. These advancements have culminated in a new generation of models capable of handling sequences that would have been unimaginable just a few years ago.\n\nCommercial language models have rapidly expanded their context capabilities, with GPT-4, Claude-3, and Gemini-1.5 now supporting context windows ranging from 32K to an impressive 1 million tokens <Paper corpusId=\"268793522\" paperTitle=\"(Thonet et al., 2024)\" isShortName></Paper>. This expansion reflects the practical importance of long context understanding for real-world applications.\n\nOpen-source implementations have also made significant strides in long context modeling. LongLoRA, for instance, can extend Llama2 7B from 4K to 100K context length or Llama2 70B to 32K context length while maintaining the original architecture <Paper corpusId=\"268857023\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"262084134\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This approach combines parameter-efficient fine-tuning with sparse attention mechanisms, allowing models to be extended with relatively modest computational resources\u2014as little as a single 8\u00d7A100 machine <Paper corpusId=\"262084134\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\nEven more impressive is LongRoPE, which has pushed context windows to an extraordinary 2 million tokens <Paper corpusId=\"268857023\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>. This represents a remarkable advancement in position encoding techniques that enable models to handle extremely long documents without redesigning their core architecture.\n\nMemory efficiency has also been a focus of recent research. RingAttention can reduce memory requirements dramatically, allowing models to train on sequences over 500 times longer than previous memory-efficient methods without approximating the attention mechanism <Paper corpusId=\"270710851\" paperTitle=\"(Gavin et al., 2024)\" isShortName></Paper>. This addresses one of the major bottlenecks in scaling context length.\n\nArchitectural innovations continue to emerge, with models like BlockTransformer employing hierarchical global-to-local modeling for efficient long context processing <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>. InfiniteTransformer incorporates unbounded long-term memory through continuous-space attention, potentially allowing for unlimited context length <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>.\n\nState space models like Mamba represent another promising direction, capturing long-range dependencies efficiently without explicit attention mechanisms <Paper corpusId=\"277857470\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>. These models offer an alternative approach to the long context challenge that avoids some of the fundamental limitations of attention-based architectures.\n\nThe practical benefits of these advances are now being realized in multiple domains. For instance, a study comparing retrieval-augmentation versus long context windows found that LLMs with 4K context using simple retrieval-augmentation can achieve comparable performance to fine-tuned LLMs with 16K context <Paper corpusId=\"268793522\" paperTitle=\"(Thonet et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263620134\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper>. Moreover, combining both approaches\u2014retrieval-augmented Llama2-70B with 32K context\u2014outperformed GPT-3.5-turbo-16k and Davinci003 across various long context tasks <Paper corpusId=\"263620134\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper>.\n\nAs these methods continue to mature, we're witnessing the democratization of long context modeling capabilities, with many techniques now being incorporated into open-source LLMs to enhance their long sequence understanding abilities <Paper corpusId=\"268857023\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"262084134\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This accessibility marks a significant step forward in making advanced long context processing available to a broader range of researchers and developers.",
                "citations": [
                    {
                        "id": "(Thonet et al., 2024)",
                        "snippets": [
                            "context modeling. 3 While an exhaustive survey of these methods is beyond the scope of this paper, they can generally be categorized into three main groups (excluding other distinct approaches such as retrieval-augmented generation (Xu et al., 2023) and context compression (Chevalier et al., 2023)): (a) the development of efficient transformer architectures to address the quadratic attention challenge, including sparse transformers [6,12,(Martins et al., 2020)(Zaheer et al., 2020), linear transformers (Choromanski et al., 2020)(Katharopoulos et al., 2020)[42], and hierarchical transformers [20,30,(Wu et al., 2021); (b) approaches like recurrent attention networks [7,(Dai et al., 2019)(Peng et al., 2023) and state-space models [16,41]; (c) length extrapolation or position embedding interpolation, where LLMs are fine-tuned or adapted at inference time to adjust tokens' positions to match the new context length [4,8,9,28,35,(Peng et al., 2023)[45]. These techniques also contributed to the context length expansion in proprietary models like GPT-4 (32K-128K), Claude-3 (200k), and Gemini-1.5 (128K-1M)."
                        ],
                        "paper": {
                            "corpus_id": 268793522,
                            "title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models",
                            "authors": [
                                {
                                    "authorId": "2955690",
                                    "name": "Thibaut Thonet"
                                },
                                {
                                    "authorId": "120419790",
                                    "name": "Jos Rozen"
                                },
                                {
                                    "authorId": "2259359878",
                                    "name": "Laurent Besacier"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0.56689453125
                    },
                    {
                        "id": "(Li et al._1, 2024)",
                        "snippets": [
                            "One line of research is based on AliBi (Press et al., 2021) and RoPE (Su et al., 2021) embeddings, which allows us to train Transformers with short sequences and subsequently apply them to longer sequences during inference.Recently, different approaches (Xiong et al., 2023;Fu et al., 2024;Liu et al., 2024) help the model to extrapolate to 128K window size with continued pre-training.Later on, LongRoPE (Ding et al., 2024) was proposed to further extend the context window to 2M tokens.Another line of research also utilizes methodologies like context window sliding and segmentation to overcome the issue of the limited context window in original Transformers (Hao et al., 2022;(Ratner et al., 2022).Furthermore, architectural innovations, transitioning from traditional Transformer-based designs to recurrent models or state space models, have shown promise in facilitating long-range computations naturally (Orvieto et al., 2023;Gu & Dao, 2023;(Peng et al., 2023).These techniques have been incorporated into several current open-source LLMs to enhance long sequence understanding capability (Chen et al., 2023)Tworkowski et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 268857023,
                            "title": "Long-context LLMs Struggle with Long In-context Learning",
                            "authors": [
                                {
                                    "authorId": "2157466550",
                                    "name": "Tianle Li"
                                },
                                {
                                    "authorId": "2143853895",
                                    "name": "Ge Zhang"
                                },
                                {
                                    "authorId": "2294572942",
                                    "name": "Quy Duc Do"
                                },
                                {
                                    "authorId": "2284988933",
                                    "name": "Xiang Yue"
                                },
                                {
                                    "authorId": "2249847177",
                                    "name": "Wenhu Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 192
                        },
                        "score": 0.64794921875
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset."
                        ],
                        "paper": {
                            "corpus_id": 262084134,
                            "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2109297557",
                                    "name": "Yukang Chen"
                                },
                                {
                                    "authorId": "152230789",
                                    "name": "Shengju Qian"
                                },
                                {
                                    "authorId": "150127950",
                                    "name": "Haotian Tang"
                                },
                                {
                                    "authorId": "2237802684",
                                    "name": "Xin Lai"
                                },
                                {
                                    "authorId": "47781592",
                                    "name": "Zhijian Liu"
                                },
                                {
                                    "authorId": "2243400730",
                                    "name": "Song Han"
                                },
                                {
                                    "authorId": "2237811040",
                                    "name": "Jiaya Jia"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 167
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gavin et al., 2024)",
                        "snippets": [
                            "Many studies explore various strategies to address this challenge, including the use of new positional encodings to achieve position interpolation or extrapolation (Peng et al., 2023;Chen et al., 2023;Liu et al., 2024b)",
                            "RingAttention (Liu et al., 2023a) can reduce the memory requirements of the Transformer model, allowing it to train sequences over 500 times longer than previous memory-efficient methods, without needing to approximate the attention mechanism."
                        ],
                        "paper": {
                            "corpus_id": 270710851,
                            "title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs",
                            "authors": [
                                {
                                    "authorId": "2308098543",
                                    "name": "Shawn Gavin"
                                },
                                {
                                    "authorId": "2300091474",
                                    "name": "Tuney Zheng"
                                },
                                {
                                    "authorId": "2294523552",
                                    "name": "Jiaheng Liu"
                                },
                                {
                                    "authorId": "2303653097",
                                    "name": "Quehry Que"
                                },
                                {
                                    "authorId": "2303796897",
                                    "name": "Noah Wang"
                                },
                                {
                                    "authorId": "2308240515",
                                    "name": "Jian Yang"
                                },
                                {
                                    "authorId": "2214538677",
                                    "name": "Chenchen Zhang"
                                },
                                {
                                    "authorId": "2239245627",
                                    "name": "Wenhao Huang"
                                },
                                {
                                    "authorId": "2249847177",
                                    "name": "Wenhu Chen"
                                },
                                {
                                    "authorId": "2308233656",
                                    "name": "Ge Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.689453125
                    },
                    {
                        "id": "(He et al., 2025)",
                        "snippets": [
                            "To address the quadratic computational and memory demands of standard transformer self-attention, researchers have developed various architectural modifica-tions to improve efficiency and extend context lengths. Notable examples include Longformer (Beltagy et al., 2020), which combines sliding window and global attention, and BlockTransformer (Ho et al., 2024), which employs hierarchical global-to-local modeling. Linear Attention methods (Katharopoulos et al., 2020) reformulate self-attention for linear complexity, while InfiniteTransformer (Munkhdalai et al., 2024) incorporates unbounded long-term memory through continuousspace attention. State space models like Mamba (Gu & Dao, 2024) capture long-range dependencies efficiently without explicit attention mechanisms."
                        ],
                        "paper": {
                            "corpus_id": 277857470,
                            "title": "Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation",
                            "authors": [
                                {
                                    "authorId": "2356154321",
                                    "name": "Linda He"
                                },
                                {
                                    "authorId": "2252087284",
                                    "name": "Jue Wang"
                                },
                                {
                                    "authorId": "2110605429",
                                    "name": "Maurice Weber"
                                },
                                {
                                    "authorId": "2356004549",
                                    "name": "Shang Zhu"
                                },
                                {
                                    "authorId": "2304481349",
                                    "name": "Ben Athiwaratkun"
                                },
                                {
                                    "authorId": "2305565297",
                                    "name": "Ce Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 1
                        },
                        "score": 0.5537109375
                    },
                    {
                        "id": "(Xu et al., 2023)",
                        "snippets": [
                            "Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners."
                        ],
                        "paper": {
                            "corpus_id": 263620134,
                            "title": "Retrieval meets Long Context Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2254989105",
                                    "name": "Peng Xu"
                                },
                                {
                                    "authorId": "2253664013",
                                    "name": "Wei Ping"
                                },
                                {
                                    "authorId": "2253618149",
                                    "name": "Xianchao Wu"
                                },
                                {
                                    "authorId": "20957879",
                                    "name": "Lawrence C. McAfee"
                                },
                                {
                                    "authorId": "2283871700",
                                    "name": "Chen Zhu"
                                },
                                {
                                    "authorId": "2256582287",
                                    "name": "Zihan Liu"
                                },
                                {
                                    "authorId": "2253531461",
                                    "name": "Sandeep Subramanian"
                                },
                                {
                                    "authorId": "32867948",
                                    "name": "E. Bakhturina"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "2301680",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 85
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.153039
    }
}