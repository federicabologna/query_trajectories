{
    "query": "What are the advantages and limitations of using size-matched or identically-architected teacher and student models in knowledge distillation, and how does this setup impact student model performance?",
    "user_id": "lib_user",
    "task_id": "814ccb07-5c80-442e-adee-079140df3db0",
    "timestamp": "2025-06-24T00:26:15.828626",
    "n_retrieval": 256,
    "n_retrieved": 273,
    "n_candidates": 44,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.37642200000000003,
    "decomposed_query": {
        "rewritten_query": "Advantages and limitations of using size-matched or identically-architected teacher and student models in knowledge distillation, and how this setup impacts student model performance.",
        "keyword_query": "size-matched identical architecture teacher student models knowledge distillation impact performance",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009972,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights",
            "venue": "International Conference on Applications of Natural Language to Data Bases",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.12586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491169373",
                    "name": "Mohamad Ballout"
                },
                {
                    "authorId": "1751765",
                    "name": "U. Krumnack"
                },
                {
                    "authorId": "2238205582",
                    "name": "Gunther Heidemann"
                },
                {
                    "authorId": "1743582",
                    "name": "Kai-Uwe K\u00fchnberger"
                }
            ],
            "abstract": "Enhancing small language models for real-life application deployment is a significant challenge facing the research community. Due to the difficulties and costs of using large language models, researchers are seeking ways to effectively deploy task-specific small models. In this work, we introduce a simple yet effective knowledge distillation method to improve the performance of small language models. Our approach utilizes a teacher model with approximately 3 billion parameters to identify the most influential tokens in its decision-making process. These tokens are extracted from the input based on their attribution scores relative to the output, using methods like saliency maps. These important tokens are then provided as rationales to a student model, aiming to distill the knowledge of the teacher model. This method has proven to be effective, as demonstrated by testing it on four diverse datasets, where it shows improvement over both standard fine-tuning methods and state-of-the-art knowledge distillation models. Furthermore, we explore explanations of the success of the model by analyzing the important tokens extracted from the teacher model. Our findings reveal that in 68\\% of cases, specifically in datasets where labels are part of the answer, such as multiple-choice questions, the extracted tokens are part of the ground truth.",
            "corpus_id": 272753230,
            "sentences": [
                {
                    "corpus_id": "272753230",
                    "title": "Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights",
                    "text": "Our research demonstrates an effective and straightforward approach for generating rationales from a large model to enhance a smaller model. However, a significant performance gap between the teacher model (the large model) and the student model (the small model) still exists. As illustrated in table 6, the flan-t5-3b, which serves as the teacher model, significantly outperforms the smaller t5-small model across all four datasets we tested. \n\nThis limitation underscores the inherent challenge in knowledge distillation and model scaling. While our method effectively transfers knowledge from a large to a small model, the reduced capacity of the smaller model limits its ability to fully replicate the performance of its larger counterpart. This performance discrepancy highlights the trade-offs involved in model downsizing, where gains in efficiency and deployability often come at the cost of reduced accuracy and overall capability. \n\nAddressing this gap remains a key area for future research. Efforts could focus on developing more advanced distillation techniques or optimizing small models to better capture and utilize the knowledge transferred from larger models. Our findings lay the groundwork for such exploration, offering insights into the dynamics of knowledge transfer between models of varying sizes and complexities.",
                    "score": 0.6887177270358885,
                    "section_title": "Limitation",
                    "char_start_offset": 23879,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 140
                        },
                        {
                            "start": 141,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 444
                        },
                        {
                            "start": 447,
                            "end": 542
                        },
                        {
                            "start": 543,
                            "end": 745
                        },
                        {
                            "start": 746,
                            "end": 941
                        },
                        {
                            "start": 944,
                            "end": 1003
                        },
                        {
                            "start": 1004,
                            "end": 1178
                        },
                        {
                            "start": 1179,
                            "end": 1340
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92626953125
                },
                {
                    "corpus_id": "272753230",
                    "title": "Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights",
                    "text": "In this work, we demonstrated an effective approach to distilling knowledge into student models by calculating the attribution scores of the teacher model's input and using them as rationales for the student model. This method is based on the theory that a high gradient value for a token in the input indicates that minor changes in that token can significantly impact the model's prediction, emphasizing its importance in influencing the model's output. Conversely, a low gradient value suggests that the token has minimal effect on the prediction, signifying its lesser importance. We tested our method on four diverse datasets, which included tasks in natural language inference, commonsense question answering, and arithmetic word problems. The results showed that using t5-small as the student model, our method enhances performance across these datasets. Additionally, we explored why this method is effective by analyzing the top important tokens used as rationales. Particularly in the multiple-choice CQA dataset, where the answer is included in the input, we found that the top-5 extracted tokens contain the answer or a part of it 68% of the time. \n\nWhile our method offers a simple and effective means of knowledge distillation, there is still a significant performance gap between the teacher and student models, with the teacher model outperforming the student by a large margin. We hope that this work contributes to the field of knowledge distillation and inspires further research to narrow the gap between large and small models. This endeavor is crucial for advancing the efficiency and applicability of language models in various real-world scenarios.",
                    "score": 0.5249649286507436,
                    "section_title": "Conclusion and Outlook",
                    "char_start_offset": 25246,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 584
                        },
                        {
                            "start": 585,
                            "end": 745
                        },
                        {
                            "start": 746,
                            "end": 861
                        },
                        {
                            "start": 862,
                            "end": 974
                        },
                        {
                            "start": 975,
                            "end": 1159
                        },
                        {
                            "start": 1162,
                            "end": 1394
                        },
                        {
                            "start": 1395,
                            "end": 1548
                        },
                        {
                            "start": 1549,
                            "end": 1672
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76904296875
                }
            ],
            "relevance_judgement": 0.92626953125,
            "relevance_judgment_input_expanded": "# Title: Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights\n# Venue: International Conference on Applications of Natural Language to Data Bases\n# Authors: Mohamad Ballout, U. Krumnack, Gunther Heidemann, Kai-Uwe K\u00fchnberger\n## Abstract\nEnhancing small language models for real-life application deployment is a significant challenge facing the research community. Due to the difficulties and costs of using large language models, researchers are seeking ways to effectively deploy task-specific small models. In this work, we introduce a simple yet effective knowledge distillation method to improve the performance of small language models. Our approach utilizes a teacher model with approximately 3 billion parameters to identify the most influential tokens in its decision-making process. These tokens are extracted from the input based on their attribution scores relative to the output, using methods like saliency maps. These important tokens are then provided as rationales to a student model, aiming to distill the knowledge of the teacher model. This method has proven to be effective, as demonstrated by testing it on four diverse datasets, where it shows improvement over both standard fine-tuning methods and state-of-the-art knowledge distillation models. Furthermore, we explore explanations of the success of the model by analyzing the important tokens extracted from the teacher model. Our findings reveal that in 68\\% of cases, specifically in datasets where labels are part of the answer, such as multiple-choice questions, the extracted tokens are part of the ground truth.\n## Limitation\nOur research demonstrates an effective and straightforward approach for generating rationales from a large model to enhance a smaller model. However, a significant performance gap between the teacher model (the large model) and the student model (the small model) still exists. As illustrated in table 6, the flan-t5-3b, which serves as the teacher model, significantly outperforms the smaller t5-small model across all four datasets we tested. \n\nThis limitation underscores the inherent challenge in knowledge distillation and model scaling. While our method effectively transfers knowledge from a large to a small model, the reduced capacity of the smaller model limits its ability to fully replicate the performance of its larger counterpart. This performance discrepancy highlights the trade-offs involved in model downsizing, where gains in efficiency and deployability often come at the cost of reduced accuracy and overall capability. \n\nAddressing this gap remains a key area for future research. Efforts could focus on developing more advanced distillation techniques or optimizing small models to better capture and utilize the knowledge transferred from larger models. Our findings lay the groundwork for such exploration, offering insights into the dynamics of knowledge transfer between models of varying sizes and complexities.\n\n## Conclusion and Outlook\nIn this work, we demonstrated an effective approach to distilling knowledge into student models by calculating the attribution scores of the teacher model's input and using them as rationales for the student model. This method is based on the theory that a high gradient value for a token in the input indicates that minor changes in that token can significantly impact the model's prediction, emphasizing its importance in influencing the model's output. Conversely, a low gradient value suggests that the token has minimal effect on the prediction, signifying its lesser importance. We tested our method on four diverse datasets, which included tasks in natural language inference, commonsense question answering, and arithmetic word problems. The results showed that using t5-small as the student model, our method enhances performance across these datasets. Additionally, we explored why this method is effective by analyzing the top important tokens used as rationales. Particularly in the multiple-choice CQA dataset, where the answer is included in the input, we found that the top-5 extracted tokens contain the answer or a part of it 68% of the time. \n\nWhile our method offers a simple and effective means of knowledge distillation, there is still a significant performance gap between the teacher and student models, with the teacher model outperforming the student by a large margin. We hope that this work contributes to the field of knowledge distillation and inspires further research to narrow the gap between large and small models. This endeavor is crucial for advancing the efficiency and applicability of language models in various real-world scenarios.",
            "reference_string": "[272753230 | Ballout et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Can a student Large Language Model perform as well as it's teacher?",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 29,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.02421",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.02421, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2141639835",
                    "name": "Sia Gholami"
                },
                {
                    "authorId": "2239197098",
                    "name": "Marwan Omar"
                }
            ],
            "abstract": "The burgeoning complexity of contemporary deep learning models, while achieving unparalleled accuracy, has inadvertently introduced deployment challenges in resource-constrained environments. Knowledge distillation, a technique aiming to transfer knowledge from a high-capacity\"teacher\"model to a streamlined\"student\"model, emerges as a promising solution to this dilemma. This paper provides a comprehensive overview of the knowledge distillation paradigm, emphasizing its foundational principles such as the utility of soft labels and the significance of temperature scaling. Through meticulous examination, we elucidate the critical determinants of successful distillation, including the architecture of the student model, the caliber of the teacher, and the delicate balance of hyperparameters. While acknowledging its profound advantages, we also delve into the complexities and challenges inherent in the process. Our exploration underscores knowledge distillation's potential as a pivotal technique in optimizing the trade-off between model performance and deployment efficiency.",
            "corpus_id": 263620565,
            "sentences": [
                {
                    "corpus_id": "263620565",
                    "title": "Can a student Large Language Model perform as well as it's teacher?",
                    "text": "While knowledge distillation is a powerful tool, it's not without its limitations and challenges. Here are a few to consider: \n\n1. Performance Gap: The performance of the distilled student model, although better than a similarly-sized model trained from scratch, typically doesn't reach the performance level of the larger teacher model. There is usually a trade-off between model size and accuracy. \n\n2. Dependence on a Good Teacher Model: The effectiveness of knowledge distillation heavily depends on the quality of the teacher model. The student model can only be as good as the teacher model allows. If the teacher model isn't well-trained or doesn't perform well, the student model is unlikely to perform well either. \n\n3. Hyperparameter Sensitivity: The process of knowledge distillation requires careful tuning of several hyperparameters, such as the temperature parameter and the weighting between the original loss and the distillation loss. Finding the right settings can be tricky and might require a lot of experimentation. \n\n4. Computational Overhead: Although the student model is smaller and more efficient, the distillation process itself requires the teacher model to generate predictions for the data, which could be computationally expensive, especially for large models and datasets. \n\n5. Opaque Process: The process of knowledge distillation is somewhat opaque and difficult to interpret. It's not always clear why a certain student model learns effectively from a teacher model, or why certain hyperparameters work better than others.",
                    "score": 0.5678563413109863,
                    "section_title": "Limitations",
                    "char_start_offset": 17792,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 97
                        },
                        {
                            "start": 98,
                            "end": 125
                        },
                        {
                            "start": 128,
                            "end": 337
                        },
                        {
                            "start": 338,
                            "end": 399
                        },
                        {
                            "start": 402,
                            "end": 537
                        },
                        {
                            "start": 538,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 723
                        },
                        {
                            "start": 726,
                            "end": 951
                        },
                        {
                            "start": 952,
                            "end": 1036
                        },
                        {
                            "start": 1039,
                            "end": 1304
                        },
                        {
                            "start": 1307,
                            "end": 1410
                        },
                        {
                            "start": 1411,
                            "end": 1557
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.890625
                },
                {
                    "corpus_id": "263620565",
                    "title": "Can a student Large Language Model perform as well as it's teacher?",
                    "text": "Capacity: There's also a limit to how much a smaller student model can learn from a large teacher model. If the student model's capacity is too small, it may not effectively learn the teacher's knowledge. \n\n7. Overfitting Risk: If the teacher model has overfit to the training data, it's possible that the student model might learn these overfit predictions, leading to poor generalization to new data. \n\nDespite these limitations, knowledge distillation can still be a very useful technique, especially when dealing with constraints on computational resources or when deploying models in real-world applications where efficiency is key. \n\n7 Future Work \n\nThere are several directions that future work on knowledge distillation could take to further improve this technique and its application in various fields: \n\n\u2022 Improved Understanding of Distillation Dynamics: Further research is needed to understand the dynamics of knowledge transfer during distillation. For example, understanding which aspects of the teacher's knowledge are most effectively transferred and why could help optimize the process. \n\n\u2022 Automated Hyperparameter Tuning: Given the sensitivity of the distillation process to hyperparameters like the temperature and the weighting between the original loss and distillation loss, developing methods for automatic or more efficient hyperparameter tuning could be beneficial. \n\n\u2022 Advanced Distillation Techniques: Exploring advanced distillation techniques beyond the standard approach could lead to better results. This could involve novel loss functions, training methods, or types of teacher-student relationships. \n\n\u2022 Multi-Teacher Distillation: The idea of distilling knowledge from multiple teacher models into a single student model is an interesting area for exploration. This could potentially combine the strengths of various models into a single efficient student model. \n\n\u2022 Domain-Specific Adaptations: Adapting and optimizing knowledge distillation techniques for specific domains or tasks could also be a valuable avenue for future work. Different tasks might benefit from different distillation strategies. \n\n\u2022 Privacy and Security in Distillation: As distillation involves transferring knowledge from a teacher model, there could be concerns about privacy and security, especially when the teacher model has been trained on sensitive data. Future work could look at how to ensure that distillation does not leak sensitive information.",
                    "score": 0.5606240415003465,
                    "section_title": "Student Model",
                    "char_start_offset": 19367,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 104
                        },
                        {
                            "start": 105,
                            "end": 204
                        },
                        {
                            "start": 207,
                            "end": 402
                        },
                        {
                            "start": 405,
                            "end": 637
                        },
                        {
                            "start": 640,
                            "end": 653
                        },
                        {
                            "start": 656,
                            "end": 811
                        },
                        {
                            "start": 814,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1103
                        },
                        {
                            "start": 1106,
                            "end": 1391
                        },
                        {
                            "start": 1394,
                            "end": 1531
                        },
                        {
                            "start": 1532,
                            "end": 1633
                        },
                        {
                            "start": 1636,
                            "end": 1795
                        },
                        {
                            "start": 1796,
                            "end": 1897
                        },
                        {
                            "start": 1900,
                            "end": 2067
                        },
                        {
                            "start": 2068,
                            "end": 2137
                        },
                        {
                            "start": 2140,
                            "end": 2371
                        },
                        {
                            "start": 2372,
                            "end": 2466
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88134765625
                },
                {
                    "corpus_id": "263620565",
                    "title": "Can a student Large Language Model perform as well as it's teacher?",
                    "text": "Knowledge distillation is a technique used in machine learning to transfer knowledge from a larger, more complex model (called the teacher model) to a smaller, simpler model (called the student model). The goal is to create a lightweight, computationally efficient student model that retains as much of the teacher model's performance as possible. This is especially useful for deploying machine learning models on resource-constrained devices or in situations where inference time and energy consumption are critical. \n\nThe knowledge distillation approach entails training a smaller, more focused model to replicate the results of a bigger, more broad language model, like GPT-3 [Brown et al., 2020]. The bigger model's high-level symbolic knowledge should be reduced into a smaller, more effective model that can accurately carry out specific tasks. This method involves training a student Transformer model using knowledge distillation from a larger teacher model. The teacher model provides soft labels for the training data, which are used to train the student model. This allows the student model to learn from the teacher model's rich representation while being more efficient due to its smaller size [Freitag et al., 2017]. In our experiments we used the model introduced by Gholami and Omar [2023] (GPT-Efficio) as the teacher. \n\nHere we study a specific approach called The figurative distiller (FD) process that involves three main components: a teacher model, a student model, and a set of rules. The teacher model is the larger, more general language model, while the student model is the smaller, more specialized model being trained. The rules define the constraints and relationships between different concepts in the modeled domain. There are three steps in the FD procedure. The student model is first trained using a collection of training examples produced by the instructor model. In order to enhance the student model's performance, a task-specific dataset is used to fine-tune it. The output of the student model is then further refined using symbolic rules to ensure it adheres to the limitations and relationships specified by the rules. \n\nBoth supervised, and unsupervised learning can be included in the FD process. The teacher model creates a collection of labeled examples in the supervised environment, subsequently utilized for training the student model.",
                    "score": 0.5255930619873028,
                    "section_title": "Approach",
                    "char_start_offset": 7499,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 518
                        },
                        {
                            "start": 521,
                            "end": 701
                        },
                        {
                            "start": 702,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1072
                        },
                        {
                            "start": 1073,
                            "end": 1231
                        },
                        {
                            "start": 1232,
                            "end": 1336
                        },
                        {
                            "start": 1339,
                            "end": 1508
                        },
                        {
                            "start": 1509,
                            "end": 1648
                        },
                        {
                            "start": 1649,
                            "end": 1749
                        },
                        {
                            "start": 1750,
                            "end": 1792
                        },
                        {
                            "start": 1793,
                            "end": 1901
                        },
                        {
                            "start": 1902,
                            "end": 2003
                        },
                        {
                            "start": 2004,
                            "end": 2162
                        },
                        {
                            "start": 2165,
                            "end": 2242
                        },
                        {
                            "start": 2243,
                            "end": 2386
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70849609375
                }
            ],
            "relevance_judgement": 0.890625,
            "relevance_judgment_input_expanded": "# Title: Can a student Large Language Model perform as well as it's teacher?\n# Venue: arXiv.org\n# Authors: Sia Gholami, Marwan Omar\n## Abstract\nThe burgeoning complexity of contemporary deep learning models, while achieving unparalleled accuracy, has inadvertently introduced deployment challenges in resource-constrained environments. Knowledge distillation, a technique aiming to transfer knowledge from a high-capacity\"teacher\"model to a streamlined\"student\"model, emerges as a promising solution to this dilemma. This paper provides a comprehensive overview of the knowledge distillation paradigm, emphasizing its foundational principles such as the utility of soft labels and the significance of temperature scaling. Through meticulous examination, we elucidate the critical determinants of successful distillation, including the architecture of the student model, the caliber of the teacher, and the delicate balance of hyperparameters. While acknowledging its profound advantages, we also delve into the complexities and challenges inherent in the process. Our exploration underscores knowledge distillation's potential as a pivotal technique in optimizing the trade-off between model performance and deployment efficiency.\n## Approach\nKnowledge distillation is a technique used in machine learning to transfer knowledge from a larger, more complex model (called the teacher model) to a smaller, simpler model (called the student model). The goal is to create a lightweight, computationally efficient student model that retains as much of the teacher model's performance as possible. This is especially useful for deploying machine learning models on resource-constrained devices or in situations where inference time and energy consumption are critical. \n\nThe knowledge distillation approach entails training a smaller, more focused model to replicate the results of a bigger, more broad language model, like GPT-3 [Brown et al., 2020]. The bigger model's high-level symbolic knowledge should be reduced into a smaller, more effective model that can accurately carry out specific tasks. This method involves training a student Transformer model using knowledge distillation from a larger teacher model. The teacher model provides soft labels for the training data, which are used to train the student model. This allows the student model to learn from the teacher model's rich representation while being more efficient due to its smaller size [Freitag et al., 2017]. In our experiments we used the model introduced by Gholami and Omar [2023] (GPT-Efficio) as the teacher. \n\nHere we study a specific approach called The figurative distiller (FD) process that involves three main components: a teacher model, a student model, and a set of rules. The teacher model is the larger, more general language model, while the student model is the smaller, more specialized model being trained. The rules define the constraints and relationships between different concepts in the modeled domain. There are three steps in the FD procedure. The student model is first trained using a collection of training examples produced by the instructor model. In order to enhance the student model's performance, a task-specific dataset is used to fine-tune it. The output of the student model is then further refined using symbolic rules to ensure it adheres to the limitations and relationships specified by the rules. \n\nBoth supervised, and unsupervised learning can be included in the FD process. The teacher model creates a collection of labeled examples in the supervised environment, subsequently utilized for training the student model.\n\n## Limitations\nWhile knowledge distillation is a powerful tool, it's not without its limitations and challenges. Here are a few to consider: \n\n1. Performance Gap: The performance of the distilled student model, although better than a similarly-sized model trained from scratch, typically doesn't reach the performance level of the larger teacher model. There is usually a trade-off between model size and accuracy. \n\n2. Dependence on a Good Teacher Model: The effectiveness of knowledge distillation heavily depends on the quality of the teacher model. The student model can only be as good as the teacher model allows. If the teacher model isn't well-trained or doesn't perform well, the student model is unlikely to perform well either. \n\n3. Hyperparameter Sensitivity: The process of knowledge distillation requires careful tuning of several hyperparameters, such as the temperature parameter and the weighting between the original loss and the distillation loss. Finding the right settings can be tricky and might require a lot of experimentation. \n\n4. Computational Overhead: Although the student model is smaller and more efficient, the distillation process itself requires the teacher model to generate predictions for the data, which could be computationally expensive, especially for large models and datasets. \n\n5. Opaque Process: The process of knowledge distillation is somewhat opaque and difficult to interpret. It's not always clear why a certain student model learns effectively from a teacher model, or why certain hyperparameters work better than others.\n\n## Student Model\nCapacity: There's also a limit to how much a smaller student model can learn from a large teacher model. If the student model's capacity is too small, it may not effectively learn the teacher's knowledge. \n\n7. Overfitting Risk: If the teacher model has overfit to the training data, it's possible that the student model might learn these overfit predictions, leading to poor generalization to new data. \n\nDespite these limitations, knowledge distillation can still be a very useful technique, especially when dealing with constraints on computational resources or when deploying models in real-world applications where efficiency is key. \n\n7 Future Work \n\nThere are several directions that future work on knowledge distillation could take to further improve this technique and its application in various fields: \n\n\u2022 Improved Understanding of Distillation Dynamics: Further research is needed to understand the dynamics of knowledge transfer during distillation. For example, understanding which aspects of the teacher's knowledge are most effectively transferred and why could help optimize the process. \n\n\u2022 Automated Hyperparameter Tuning: Given the sensitivity of the distillation process to hyperparameters like the temperature and the weighting between the original loss and distillation loss, developing methods for automatic or more efficient hyperparameter tuning could be beneficial. \n\n\u2022 Advanced Distillation Techniques: Exploring advanced distillation techniques beyond the standard approach could lead to better results. This could involve novel loss functions, training methods, or types of teacher-student relationships. \n\n\u2022 Multi-Teacher Distillation: The idea of distilling knowledge from multiple teacher models into a single student model is an interesting area for exploration. This could potentially combine the strengths of various models into a single efficient student model. \n\n\u2022 Domain-Specific Adaptations: Adapting and optimizing knowledge distillation techniques for specific domains or tasks could also be a valuable avenue for future work. Different tasks might benefit from different distillation strategies. \n\n\u2022 Privacy and Security in Distillation: As distillation involves transferring knowledge from a teacher model, there could be concerns about privacy and security, especially when the teacher model has been trained on sensitive data. Future work could look at how to ensure that distillation does not leak sensitive information.",
            "reference_string": "[263620565 | Gholami et al. | 2023 | Citations: 12]"
        },
        {
            "title": "Efficient image classification through collaborative knowledge distillation: A novel AlexNet modification approach",
            "venue": "Heliyon",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cell.com/article/S2405844024104070/pdf",
                "status": "GOLD",
                "license": "CCBYNC",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11305255, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311413816",
                    "name": "Avazov Kuldashboy"
                },
                {
                    "authorId": "2240683787",
                    "name": "Sabina Umirzakova"
                },
                {
                    "authorId": "1582039294",
                    "name": "Sharofiddin Allaberdiev"
                },
                {
                    "authorId": "2102335285",
                    "name": "Rashid Nasimov"
                },
                {
                    "authorId": "2090021583",
                    "name": "A. Abdusalomov"
                },
                {
                    "authorId": "2298414610",
                    "name": "Y. Cho"
                }
            ],
            "abstract": null,
            "corpus_id": 271208532,
            "sentences": [
                {
                    "corpus_id": "271208532",
                    "title": "Efficient image classification through collaborative knowledge distillation: A novel AlexNet modification approach",
                    "text": "Knowledge distillation mitigates this risk by enabling the student model to learn from the detailed and nuanced knowledge encoded in the teacher model. This includes learning from the teacher intermediate representations and final outputs, which helps the student model achieve comparable accuracy to the teacher model despite having a simpler architecture. \n\nAnother significant aspect is the generalization and robustness of models. The process of knowledge distillation often leads to models that generalize better to new data, as the student model learns not just the final predictions but also the intermediate processes and feature representations from the teacher model. This comprehensive learning process makes the student model more robust and better equipped to handle variations and noise in the input data. Deploying large models can be impractical due to hardware limitations in real-world scenarios. Knowledge distillation allows for the creation of smaller, efficient models that can be easily deployed across various devices and platforms without sacrificing performance. This scalability is crucial for the widespread adoption of deep learning technologies in consumer electronics, autonomous systems, and other applications where resources are constrained. The versatility of knowledge distillation is also noteworthy. It is not limited to a single type of neural network or application but has been successfully applied across various domains, including image classification, natural language processing, and speech recognition. This makes it a valuable tool for improving the efficiency of machine-learning models across a wide range of tasks. In addition to facilitating model compression and optimization, knowledge distillation complements other techniques like pruning, quantization, and low-rank factorization. It helps create lightweight models that retain high performance, essential for applications requiring real-time processing and low latency. \n\nKnowledge distillation bridges the gap between the high performance of complex models and the practical constraints of deploying these models in real-world environments. It ensures models remain efficient, accurate, and robust while being suitable for various applications with limited computational resources. This ability to create high-performing yet resource-efficient models underscores the critical importance of knowledge distillation in advancing the field of machine learning and artificial intelligence. \n\nMost extant knowledge distillation strategies prompt the student network to emulate the teacher network representational domain to match its adept performance [9].",
                    "score": 0.5265478097092589,
                    "section_title": "Introduction",
                    "char_start_offset": 2291,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 357
                        },
                        {
                            "start": 360,
                            "end": 434
                        },
                        {
                            "start": 435,
                            "end": 677
                        },
                        {
                            "start": 678,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 914
                        },
                        {
                            "start": 915,
                            "end": 1088
                        },
                        {
                            "start": 1089,
                            "end": 1275
                        },
                        {
                            "start": 1276,
                            "end": 1337
                        },
                        {
                            "start": 1338,
                            "end": 1548
                        },
                        {
                            "start": 1549,
                            "end": 1664
                        },
                        {
                            "start": 1665,
                            "end": 1836
                        },
                        {
                            "start": 1837,
                            "end": 1976
                        },
                        {
                            "start": 1979,
                            "end": 2148
                        },
                        {
                            "start": 2149,
                            "end": 2289
                        },
                        {
                            "start": 2290,
                            "end": 2492
                        },
                        {
                            "start": 2495,
                            "end": 2658
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8359375
                }
            ],
            "relevance_judgement": 0.8359375,
            "relevance_judgment_input_expanded": "# Title: Efficient image classification through collaborative knowledge distillation: A novel AlexNet modification approach\n# Venue: Heliyon\n# Authors: Avazov Kuldashboy, Sabina Umirzakova, Sharofiddin Allaberdiev, Rashid Nasimov, A. Abdusalomov, Y. Cho\n## Abstract\nNone\n## Introduction\nKnowledge distillation mitigates this risk by enabling the student model to learn from the detailed and nuanced knowledge encoded in the teacher model. This includes learning from the teacher intermediate representations and final outputs, which helps the student model achieve comparable accuracy to the teacher model despite having a simpler architecture. \n\nAnother significant aspect is the generalization and robustness of models. The process of knowledge distillation often leads to models that generalize better to new data, as the student model learns not just the final predictions but also the intermediate processes and feature representations from the teacher model. This comprehensive learning process makes the student model more robust and better equipped to handle variations and noise in the input data. Deploying large models can be impractical due to hardware limitations in real-world scenarios. Knowledge distillation allows for the creation of smaller, efficient models that can be easily deployed across various devices and platforms without sacrificing performance. This scalability is crucial for the widespread adoption of deep learning technologies in consumer electronics, autonomous systems, and other applications where resources are constrained. The versatility of knowledge distillation is also noteworthy. It is not limited to a single type of neural network or application but has been successfully applied across various domains, including image classification, natural language processing, and speech recognition. This makes it a valuable tool for improving the efficiency of machine-learning models across a wide range of tasks. In addition to facilitating model compression and optimization, knowledge distillation complements other techniques like pruning, quantization, and low-rank factorization. It helps create lightweight models that retain high performance, essential for applications requiring real-time processing and low latency. \n\nKnowledge distillation bridges the gap between the high performance of complex models and the practical constraints of deploying these models in real-world environments. It ensures models remain efficient, accurate, and robust while being suitable for various applications with limited computational resources. This ability to create high-performing yet resource-efficient models underscores the critical importance of knowledge distillation in advancing the field of machine learning and artificial intelligence. \n\nMost extant knowledge distillation strategies prompt the student network to emulate the teacher network representational domain to match its adept performance [9].",
            "reference_string": "[271208532 | Kuldashboy et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation on BERT",
            "venue": "PLoS ONE",
            "year": 2020,
            "reference_count": 21,
            "citation_count": 11,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0263592&type=printable",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8856529, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3215210",
                    "name": "Ikhyun Cho"
                },
                {
                    "authorId": "2288811020",
                    "name": "ID UKang"
                }
            ],
            "abstract": "Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model\u2019s level of performance as much as possible. However, existing KD methods suffer from the following limitations. First, since the student model is smaller in absolute size, it inherently lacks model capacity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations. In this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher\u2019s Predictions (PTP). Using this combination, we are capable of alleviating the KD\u2019s limitations. SPS is a new parameter sharing method that increases the student model capacity. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model\u2019s performance. Experiments conducted on BERT with different datasets and tasks show that the proposed approach improves the student model\u2019s performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.",
            "corpus_id": 222066807,
            "sentences": [
                {
                    "corpus_id": "222066807",
                    "title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation on BERT",
                    "text": "Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model\u2019s level of performance as much as possible. However, existing KD methods suffer from the following limitations. First, since the student model is smaller in absolute size, it inherently lacks model capacity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations. In this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher\u2019s Predictions (PTP). Using this combination, we are capable of alleviating the KD\u2019s limitations. SPS is a new parameter sharing method that increases the student model capacity. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model\u2019s performance. Experiments conducted on BERT with different datasets and tasks show that the proposed approach improves the student model\u2019s performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.",
                    "score": 0.4946253700021551,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81640625
                }
            ],
            "relevance_judgement": 0.81640625,
            "relevance_judgment_input_expanded": "# Title: Pea-KD: Parameter-efficient and accurate Knowledge Distillation on BERT\n# Venue: PLoS ONE\n# Authors: Ikhyun Cho, ID UKang\n## Abstract\nKnowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model\u2019s level of performance as much as possible. However, existing KD methods suffer from the following limitations. First, since the student model is smaller in absolute size, it inherently lacks model capacity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations. In this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher\u2019s Predictions (PTP). Using this combination, we are capable of alleviating the KD\u2019s limitations. SPS is a new parameter sharing method that increases the student model capacity. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model\u2019s performance. Experiments conducted on BERT with different datasets and tasks show that the proposed approach improves the student model\u2019s performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.\n",
            "reference_string": "[222066807 | Cho et al. | 2020 | Citations: 11]"
        },
        {
            "title": "Fair Visual Recognition in Limited Data Regime using Self-Supervision and Self-Distillation",
            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
            "year": 2021,
            "reference_count": 35,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2107.00067",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.00067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "31222412",
                    "name": "Pratik Mazumder"
                },
                {
                    "authorId": "144377059",
                    "name": "Pravendra Singh"
                },
                {
                    "authorId": "145460361",
                    "name": "Vinay P. Namboodiri"
                }
            ],
            "abstract": "Deep learning models generally learn the biases present in the training data. Researchers have proposed several approaches to mitigate such biases and make the model fair. Bias mitigation techniques assume that a sufficiently large number of training examples are present. However, we observe that if the training data is limited, then the effectiveness of bias mitigation methods is severely degraded. In this paper, we propose a novel approach to address this problem. Specifically, we adapt self-supervision and self-distillation to reduce the impact of biases on the model in this setting. Self-supervision and self-distillation are not used for bias mitigation. However, through this work, we demonstrate for the first time that these techniques are very effective in bias mitigation. We empirically show that our approach can significantly reduce the biases learned by the model. Further, we experimentally demonstrate that our approach is complementary to other bias mitigation strategies. Our approach significantly improves their performance and further reduces the model biases in the limited data regime. Specifically, on the L-CIFAR-10S skewed dataset, our approach significantly reduces the bias score of the baseline model by 78.22% and outperforms it in terms of accuracy by a significant absolute margin of 8.89%. It also significantly reduces the bias score for the state-of-the-art domain independent bias mitigation method by 59.26% and improves its performance by a significant absolute margin of 7.08%.",
            "corpus_id": 235694419,
            "sentences": [
                {
                    "corpus_id": "235694419",
                    "title": "Fair Visual Recognition in Limited Data Regime using Self-Supervision and Self-Distillation",
                    "text": "The authors in [13] propose to use knowledge distillation for knowledge transfer from a teacher network to a student network. This process involves training the student network to match the output logits/soft predictions of the teacher network in addition to the training objective of the primary task. When the teacher and student architectures are the same, the knowledge distillation process is referred to as self-distillation. The authors in [20] demonstrate that self-distillation improves the test set performance of the network. The distillation process increases the generalization ability of the network without requiring additional labeled data for training. We use self-distillation in our approach to reduce the impact of biases in the model (see Sec. 4.1).",
                    "score": 0.5130278248120534,
                    "section_title": "Self-Distillation",
                    "char_start_offset": 7755,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 302
                        },
                        {
                            "start": 303,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 536
                        },
                        {
                            "start": 537,
                            "end": 669
                        },
                        {
                            "start": 670,
                            "end": 770
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 15,
                            "end": 19,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80712890625
                }
            ],
            "relevance_judgement": 0.80712890625,
            "relevance_judgment_input_expanded": "# Title: Fair Visual Recognition in Limited Data Regime using Self-Supervision and Self-Distillation\n# Venue: IEEE Workshop/Winter Conference on Applications of Computer Vision\n# Authors: Pratik Mazumder, Pravendra Singh, Vinay P. Namboodiri\n## Abstract\nDeep learning models generally learn the biases present in the training data. Researchers have proposed several approaches to mitigate such biases and make the model fair. Bias mitigation techniques assume that a sufficiently large number of training examples are present. However, we observe that if the training data is limited, then the effectiveness of bias mitigation methods is severely degraded. In this paper, we propose a novel approach to address this problem. Specifically, we adapt self-supervision and self-distillation to reduce the impact of biases on the model in this setting. Self-supervision and self-distillation are not used for bias mitigation. However, through this work, we demonstrate for the first time that these techniques are very effective in bias mitigation. We empirically show that our approach can significantly reduce the biases learned by the model. Further, we experimentally demonstrate that our approach is complementary to other bias mitigation strategies. Our approach significantly improves their performance and further reduces the model biases in the limited data regime. Specifically, on the L-CIFAR-10S skewed dataset, our approach significantly reduces the bias score of the baseline model by 78.22% and outperforms it in terms of accuracy by a significant absolute margin of 8.89%. It also significantly reduces the bias score for the state-of-the-art domain independent bias mitigation method by 59.26% and improves its performance by a significant absolute margin of 7.08%.\n## Self-Distillation\nThe authors in [13] propose to use knowledge distillation for knowledge transfer from a teacher network to a student network. This process involves training the student network to match the output logits/soft predictions of the teacher network in addition to the training objective of the primary task. When the teacher and student architectures are the same, the knowledge distillation process is referred to as self-distillation. The authors in [20] demonstrate that self-distillation improves the test set performance of the network. The distillation process increases the generalization ability of the network without requiring additional labeled data for training. We use self-distillation in our approach to reduce the impact of biases in the model (see Sec. 4.1).",
            "reference_string": "[235694419 | Mazumder et al. | 2021 | Citations: 2]"
        },
        {
            "title": "Aligning in a Compact Space: Contrastive Knowledge Distillation between Heterogeneous Architectures",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.18524, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2243295129",
                    "name": "Hongjun Wu"
                },
                {
                    "authorId": "2303803900",
                    "name": "Li Xiao"
                },
                {
                    "authorId": "2303860883",
                    "name": "Xingkuo Zhang"
                },
                {
                    "authorId": "2303653169",
                    "name": "Yining Miao"
                }
            ],
            "abstract": "Knowledge distillation is commonly employed to compress neural networks, reducing the inference costs and memory footprint. In the scenario of homogenous architecture, feature-based methods have been widely validated for their effectiveness. However, in scenarios where the teacher and student models are of heterogeneous architectures, the inherent differences in feature representation significantly degrade the performance of these methods. Recent studies have highlighted that low-frequency components constitute the majority of image features. Motivated by this, we propose a Low-Frequency Components-based Contrastive Knowledge Distillation (LFCC) framework that significantly enhances the performance of feature-based distillation between heterogeneous architectures. Specifically, we designe a set of multi-scale low-pass filters to extract the low-frequency components of intermediate features from both the teacher and student models, aligning them in a compact space to overcome architectural disparities. Moreover, leveraging the intrinsic pairing characteristic of the teacher-student framework, we design an innovative sample-level contrastive learning framework that adeptly restructures the constraints of within-sample feature similarity and between-sample feature divergence into a contrastive learning task. This strategy enables the student model to capitalize on intra-sample feature congruence while simultaneously enhancing the discrimination of features among disparate samples. Consequently, our LFCC framework accurately captures the commonalities in feature representation across heterogeneous architectures. Extensive evaluations and empirical analyses across three architectures (CNNs, Transformers, and MLPs) demonstrate that LFCC achieves superior performance on the challenging benchmarks of ImageNet-1K and CIFAR-100. All codes will be publicly available.",
            "corpus_id": 270094801,
            "sentences": [
                {
                    "corpus_id": "270094801",
                    "title": "Aligning in a Compact Space: Contrastive Knowledge Distillation between Heterogeneous Architectures",
                    "text": "Knowledge distillation has emerged as an extensively utilized strategy for model compression.It typically employs a teacher-student architecture, where the student model is guided to mimic the teacher's predictions [1,2,3,4] or feature representations [5,6,7,8], thereby enhancing the performance of compact models.Current methods predominantly focus on scenarios where both the teacher and student models share the same architectural lineage, often involving pairing a largerscale teacher model with a smaller-scale student, exemplified by transitions such as ResNet18 to ResNet34 [9] or ViT-S to ViT-G [10].\n\nHowever, the performance ceilings of models vary due to the differing scalability of the architectures.This limitation narrows the applicability of knowledge distillation homogeneous architectures.For instance, it is challenging for a student model from the MobileNet [11] family to find a teacher model within its own lineage that can match the capabilities of ViT-G [10].Knowledge distillation in heterogeneous architectures addresses this limitation by expanding the pool of potential teacherstudent pairings.While some research has ventured into exploring teacher-student combinations in heterogeneous architectures [12,13,14,4,15], the field is still in its early stages and holds substantial potential for further exploration and advancement.\n\nExisting knowledge distillation methods primarily employ logit [1,2,3,4] or intermediate feature maps [5,6,7,8] as the medium for knowledge transfer.The logit space serves as a task-specific shared domain, applicable to all knowledge distillation tasks.In contrast to logit-based approaches, feature-based methods transmit knowledge via aligning intermediate feature representations point-topoint.While these methods often excel in knowledge distillation with homogeneous architectures due to the intrinsic similarity of feature representations, they face serious setback in heterogeneous settings [4].The substantial differences in meta-architecture, input format, and spatial representation between heterogeneous models preclude the use of simple similarity metrics for aligning stage-wise feature representations.Moreover, mimicing the local details in feature representations may lead to detrimental guidance.",
                    "score": 0.557591667147001,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 93,
                            "end": 315
                        },
                        {
                            "start": 315,
                            "end": 609
                        },
                        {
                            "start": 611,
                            "end": 714
                        },
                        {
                            "start": 714,
                            "end": 808
                        },
                        {
                            "start": 808,
                            "end": 984
                        },
                        {
                            "start": 984,
                            "end": 1123
                        },
                        {
                            "start": 1123,
                            "end": 1359
                        },
                        {
                            "start": 1361,
                            "end": 1510
                        },
                        {
                            "start": 1510,
                            "end": 1614
                        },
                        {
                            "start": 1614,
                            "end": 1758
                        },
                        {
                            "start": 1758,
                            "end": 1963
                        },
                        {
                            "start": 1963,
                            "end": 2177
                        },
                        {
                            "start": 2177,
                            "end": 2274
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 218,
                            "end": 220,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 220,
                            "end": 222,
                            "matchedPaperCorpusId": "248986690"
                        },
                        {
                            "start": 222,
                            "end": 224,
                            "matchedPaperCorpusId": "265871679"
                        },
                        {
                            "start": 252,
                            "end": 255,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 255,
                            "end": 257,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 257,
                            "end": 259,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 259,
                            "end": 261,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 582,
                            "end": 585,
                            "matchedPaperCorpusId": "206594692"
                        },
                        {
                            "start": 604,
                            "end": 608,
                            "matchedPaperCorpusId": "225039882"
                        },
                        {
                            "start": 979,
                            "end": 983,
                            "matchedPaperCorpusId": "225039882"
                        },
                        {
                            "start": 1231,
                            "end": 1235,
                            "matchedPaperCorpusId": "195584436"
                        },
                        {
                            "start": 1235,
                            "end": 1238,
                            "matchedPaperCorpusId": "201107180"
                        },
                        {
                            "start": 1238,
                            "end": 1241,
                            "matchedPaperCorpusId": "218487294"
                        },
                        {
                            "start": 1241,
                            "end": 1243,
                            "matchedPaperCorpusId": "265871679"
                        },
                        {
                            "start": 1427,
                            "end": 1429,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 1429,
                            "end": 1431,
                            "matchedPaperCorpusId": "248986690"
                        },
                        {
                            "start": 1431,
                            "end": 1433,
                            "matchedPaperCorpusId": "265871679"
                        },
                        {
                            "start": 1463,
                            "end": 1466,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 1466,
                            "end": 1468,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 1468,
                            "end": 1470,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 1470,
                            "end": 1472,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 1959,
                            "end": 1962,
                            "matchedPaperCorpusId": "265871679"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80126953125
                },
                {
                    "corpus_id": "270094801",
                    "title": "Aligning in a Compact Space: Contrastive Knowledge Distillation between Heterogeneous Architectures",
                    "text": "Knowledge distillation aims to transfer knowledge from a large-size teacher model T to a small-size student model S.This process often uses logits and features as carriers for knowledge transfer.\n\nLogits-based Distillation.Logits carry category distribution information that directly impacts classification probabilities.Logit-based approach has proven to be an effective strategy to circumvent the discrepancies in the spatial distribution of features among heterogeneous models, as recent research has demonstrated.Specifically, the output category distribution of the student model can be made to approximate that of the teacher model, thereby improving the performance of the student model.\n\nwhere p s and p t are the predicted logits of student model and teacher model, respectively.L KLD is the Kullback-Leibler divergence loss function.y is the one-hot groundtruth labels.\u03bb is the hyperparameter adjusts the weight of soft label p t .\n\nFeature-based Distillation.By minimizing the Euclidean distance between features, feature distillation offers a finer-grained supervision for the student model through the introduction of sample-level comparisons.Formally, the objective of feature distillation approaches can be written as:\n\nwhere F s and F t are the features extracted from the i-th layer of student model and teacher model.\u03d5(\u2022) is the projector that transform student feature to match the dimension of teacher feature.\u03bb KLD and \u03bb MSE balance the logit loss and feature loss.\n\nExisting feature distillation methods simply align feature dimensions through convolution or MLP modules, which is effective when both the teacher and student models belong to the same metaarchitecture.However, models with different meta-architectures exhibit significant differences in terms of model inputs, inductive biases, and spatial distributions, among others.Consequently, enforcing complete feature-level alignment between the student and teacher models may not be the optimal approach.",
                    "score": 0.5083162547704783,
                    "section_title": "Recap of knowledge distillation",
                    "char_start_offset": 8706,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 116,
                            "end": 195
                        },
                        {
                            "start": 197,
                            "end": 223
                        },
                        {
                            "start": 223,
                            "end": 321
                        },
                        {
                            "start": 321,
                            "end": 517
                        },
                        {
                            "start": 517,
                            "end": 694
                        },
                        {
                            "start": 696,
                            "end": 788
                        },
                        {
                            "start": 788,
                            "end": 843
                        },
                        {
                            "start": 843,
                            "end": 879
                        },
                        {
                            "start": 879,
                            "end": 941
                        },
                        {
                            "start": 943,
                            "end": 970
                        },
                        {
                            "start": 970,
                            "end": 1156
                        },
                        {
                            "start": 1156,
                            "end": 1233
                        },
                        {
                            "start": 1235,
                            "end": 1335
                        },
                        {
                            "start": 1335,
                            "end": 1430
                        },
                        {
                            "start": 1430,
                            "end": 1486
                        },
                        {
                            "start": 1488,
                            "end": 1690
                        },
                        {
                            "start": 1690,
                            "end": 1856
                        },
                        {
                            "start": 1856,
                            "end": 1984
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.779296875
                }
            ],
            "relevance_judgement": 0.80126953125,
            "relevance_judgment_input_expanded": "# Title: Aligning in a Compact Space: Contrastive Knowledge Distillation between Heterogeneous Architectures\n# Venue: arXiv.org\n# Authors: Hongjun Wu, Li Xiao, Xingkuo Zhang, Yining Miao\n## Abstract\nKnowledge distillation is commonly employed to compress neural networks, reducing the inference costs and memory footprint. In the scenario of homogenous architecture, feature-based methods have been widely validated for their effectiveness. However, in scenarios where the teacher and student models are of heterogeneous architectures, the inherent differences in feature representation significantly degrade the performance of these methods. Recent studies have highlighted that low-frequency components constitute the majority of image features. Motivated by this, we propose a Low-Frequency Components-based Contrastive Knowledge Distillation (LFCC) framework that significantly enhances the performance of feature-based distillation between heterogeneous architectures. Specifically, we designe a set of multi-scale low-pass filters to extract the low-frequency components of intermediate features from both the teacher and student models, aligning them in a compact space to overcome architectural disparities. Moreover, leveraging the intrinsic pairing characteristic of the teacher-student framework, we design an innovative sample-level contrastive learning framework that adeptly restructures the constraints of within-sample feature similarity and between-sample feature divergence into a contrastive learning task. This strategy enables the student model to capitalize on intra-sample feature congruence while simultaneously enhancing the discrimination of features among disparate samples. Consequently, our LFCC framework accurately captures the commonalities in feature representation across heterogeneous architectures. Extensive evaluations and empirical analyses across three architectures (CNNs, Transformers, and MLPs) demonstrate that LFCC achieves superior performance on the challenging benchmarks of ImageNet-1K and CIFAR-100. All codes will be publicly available.\n## Introduction\nKnowledge distillation has emerged as an extensively utilized strategy for model compression.It typically employs a teacher-student architecture, where the student model is guided to mimic the teacher's predictions [1,2,3,4] or feature representations [5,6,7,8], thereby enhancing the performance of compact models.Current methods predominantly focus on scenarios where both the teacher and student models share the same architectural lineage, often involving pairing a largerscale teacher model with a smaller-scale student, exemplified by transitions such as ResNet18 to ResNet34 [9] or ViT-S to ViT-G [10].\n\nHowever, the performance ceilings of models vary due to the differing scalability of the architectures.This limitation narrows the applicability of knowledge distillation homogeneous architectures.For instance, it is challenging for a student model from the MobileNet [11] family to find a teacher model within its own lineage that can match the capabilities of ViT-G [10].Knowledge distillation in heterogeneous architectures addresses this limitation by expanding the pool of potential teacherstudent pairings.While some research has ventured into exploring teacher-student combinations in heterogeneous architectures [12,13,14,4,15], the field is still in its early stages and holds substantial potential for further exploration and advancement.\n\nExisting knowledge distillation methods primarily employ logit [1,2,3,4] or intermediate feature maps [5,6,7,8] as the medium for knowledge transfer.The logit space serves as a task-specific shared domain, applicable to all knowledge distillation tasks.In contrast to logit-based approaches, feature-based methods transmit knowledge via aligning intermediate feature representations point-topoint.While these methods often excel in knowledge distillation with homogeneous architectures due to the intrinsic similarity of feature representations, they face serious setback in heterogeneous settings [4].The substantial differences in meta-architecture, input format, and spatial representation between heterogeneous models preclude the use of simple similarity metrics for aligning stage-wise feature representations.Moreover, mimicing the local details in feature representations may lead to detrimental guidance.\n\n## Recap of knowledge distillation\nKnowledge distillation aims to transfer knowledge from a large-size teacher model T to a small-size student model S.This process often uses logits and features as carriers for knowledge transfer.\n\nLogits-based Distillation.Logits carry category distribution information that directly impacts classification probabilities.Logit-based approach has proven to be an effective strategy to circumvent the discrepancies in the spatial distribution of features among heterogeneous models, as recent research has demonstrated.Specifically, the output category distribution of the student model can be made to approximate that of the teacher model, thereby improving the performance of the student model.\n\nwhere p s and p t are the predicted logits of student model and teacher model, respectively.L KLD is the Kullback-Leibler divergence loss function.y is the one-hot groundtruth labels.\u03bb is the hyperparameter adjusts the weight of soft label p t .\n\nFeature-based Distillation.By minimizing the Euclidean distance between features, feature distillation offers a finer-grained supervision for the student model through the introduction of sample-level comparisons.Formally, the objective of feature distillation approaches can be written as:\n\nwhere F s and F t are the features extracted from the i-th layer of student model and teacher model.\u03d5(\u2022) is the projector that transform student feature to match the dimension of teacher feature.\u03bb KLD and \u03bb MSE balance the logit loss and feature loss.\n\nExisting feature distillation methods simply align feature dimensions through convolution or MLP modules, which is effective when both the teacher and student models belong to the same metaarchitecture.However, models with different meta-architectures exhibit significant differences in terms of model inputs, inductive biases, and spatial distributions, among others.Consequently, enforcing complete feature-level alignment between the student and teacher models may not be the optimal approach.",
            "reference_string": "[270094801 | Wu et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Understanding and Improving Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 42,
            "citation_count": 133,
            "influential_citation_count": 11,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.03532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3431394",
                    "name": "Jiaxi Tang"
                },
                {
                    "authorId": "2934334",
                    "name": "Rakesh Shivanna"
                },
                {
                    "authorId": "48634137",
                    "name": "Zhe Zhao"
                },
                {
                    "authorId": "2116442426",
                    "name": "Dong Lin"
                },
                {
                    "authorId": "40077377",
                    "name": "Anima Singh"
                },
                {
                    "authorId": "2226805",
                    "name": "Ed H. Chi"
                },
                {
                    "authorId": "2116998330",
                    "name": "Sagar Jain"
                }
            ],
            "abstract": "Knowledge distillation is a model-agnostic technique to improve model quality while having a fixed capacity budget. It is a commonly used technique for model compression, where a higher capacity teacher model with better quality is used to train a more compact student model with better inference efficiency. Through distillation, one hopes to benefit from student's compactness, without sacrificing too much on model quality. Despite the large success of knowledge distillation, better understanding of how it benefits student model's training dynamics remains under-explored. In this paper, we dissect the effects of knowledge distillation into three main factors: (1) benefits inherited from label smoothing, (2) example re-weighting based on teacher's confidence on ground-truth, and (3) prior knowledge of optimal output (logit) layer geometry. Using extensive systematic analyses and empirical studies on synthetic and real-world datasets, we confirm that the aforementioned three factors play a major role in knowledge distillation. Furthermore, based on our findings, we propose a simple, yet effective technique to improve knowledge distillation empirically.",
            "corpus_id": 211068747,
            "sentences": [
                {
                    "corpus_id": "211068747",
                    "title": "Understanding and Improving Knowledge Distillation",
                    "text": "Knowledge distillation is a model-agnostic technique to improve model quality while having a fixed capacity budget. It is a commonly used technique for model compression, where a higher capacity teacher model with better quality is used to train a more compact student model with better inference efficiency. Through distillation, one hopes to benefit from student's compactness, without sacrificing too much on model quality. Despite the large success of knowledge distillation, better understanding of how it benefits student model's training dynamics remains under-explored. In this paper, we dissect the effects of knowledge distillation into three main factors: (1) benefits inherited from label smoothing, (2) example re-weighting based on teacher's confidence on ground-truth, and (3) prior knowledge of optimal output (logit) layer geometry. Using extensive systematic analyses and empirical studies on synthetic and real-world datasets, we confirm that the aforementioned three factors play a major role in knowledge distillation. Furthermore, based on our findings, we propose a simple, yet effective technique to improve knowledge distillation empirically.",
                    "score": 0.5402582780480588,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80078125
                }
            ],
            "relevance_judgement": 0.80078125,
            "relevance_judgment_input_expanded": "# Title: Understanding and Improving Knowledge Distillation\n# Venue: arXiv.org\n# Authors: Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H. Chi, Sagar Jain\n## Abstract\nKnowledge distillation is a model-agnostic technique to improve model quality while having a fixed capacity budget. It is a commonly used technique for model compression, where a higher capacity teacher model with better quality is used to train a more compact student model with better inference efficiency. Through distillation, one hopes to benefit from student's compactness, without sacrificing too much on model quality. Despite the large success of knowledge distillation, better understanding of how it benefits student model's training dynamics remains under-explored. In this paper, we dissect the effects of knowledge distillation into three main factors: (1) benefits inherited from label smoothing, (2) example re-weighting based on teacher's confidence on ground-truth, and (3) prior knowledge of optimal output (logit) layer geometry. Using extensive systematic analyses and empirical studies on synthetic and real-world datasets, we confirm that the aforementioned three factors play a major role in knowledge distillation. Furthermore, based on our findings, we propose a simple, yet effective technique to improve knowledge distillation empirically.\n",
            "reference_string": "[211068747 | Tang et al. | 2020 | Citations: 133]"
        },
        {
            "title": "Exploring Graph-based Knowledge: Multi-Level Feature Distillation via Channels Relational Graph",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.08547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301262030",
                    "name": "Zhiwei Wang"
                },
                {
                    "authorId": "2301263964",
                    "name": "Jun Huang"
                },
                {
                    "authorId": "2301265454",
                    "name": "Longhua Ma"
                },
                {
                    "authorId": "2301254673",
                    "name": "Chengyu Wu"
                },
                {
                    "authorId": "2301265433",
                    "name": "Hongyu Ma"
                }
            ],
            "abstract": "In visual tasks, large teacher models capture essential features and deep information, enhancing performance. However, distilling this information into smaller student models often leads to performance loss due to structural differences and capacity limitations. To tackle this, we propose a distillation framework based on graph knowledge, including a multi-level feature alignment strategy and an attention-guided mechanism to provide a targeted learning trajectory for the student model. We emphasize spectral embedding (SE) as a key technique in our distillation process, which merges the student's feature space with the relational knowledge and structural complexities similar to the teacher network. This method captures the teacher's understanding in a graph-based representation, enabling the student model to more accurately mimic the complex structural dependencies present in the teacher model. Compared to methods that focus only on specific distillation areas, our strategy not only considers key features within the teacher model but also endeavors to capture the relationships and interactions among feature sets, encoding these complex pieces of information into a graph structure to understand and utilize the dynamic relationships among these pieces of information from a global perspective. Experiments show that our method outperforms previous feature distillation methods on the CIFAR-100, MS-COCO, and Pascal VOC datasets, proving its efficiency and applicability.",
            "corpus_id": 269761365,
            "sentences": [
                {
                    "corpus_id": "269761365",
                    "title": "Exploring Graph-based Knowledge: Multi-Level Feature Distillation via Channels Relational Graph",
                    "text": "In the field of deep learning, the impressive success of large neural networks has come at the cost of increased computational complexity, which poses significant challenges for deployment in resourceconstrained environments.While these heavyweight models, often referred to as teacher networks, set state-of-the-art benchmarks on various tasks, their practical applicability is limited by their demanding requirements for memory, processing power, and energy.Knowledge Distillation (KD) [16] emerges as a promising solution to address this dichotomy by transferring the knowledge from a cumbersome model to a more compact and efficient student networks.\n\nThe quintessence of knowledge distillation lies in its ability to encapsulate the representational power of a larger model into a smaller one without incurring a substantial loss in performance.Pioneered by Hinton et al. [16], the process involves training a smaller model to mimic the behavior of the pre-trained larger model by softening the output logits [19], thus leveraging the rich information embedded in the output distributions of the teacher network.\n\nRecent advances in KD techniques have extended beyond the mere replication of output distributions.Contemporary works explore the distillation of intermediate representations [1,49,31], attention mechanisms [50,20], and even the inculcation of adversarial robustness from teacher to student [33,55,37].The underlying hypothesis is that the intermediate layers of a neural network embody a wealth of information about the data manifold that, when transferred effectively, can endow the student with nuanced understanding akin to its teacher.\n\nLarge and complex teacher models often capture a wealth of features and deep information, crucial for enhancing task performance [7].However, attempting to directly distill this rich information into smaller capacity student models often results in suboptimal performance reproduction due to the student model's capacity limitations.The student model may struggle to process the complex information present in the teacher model, leading to ineffective distillation.Moreover, simply mimicking all features of the teacher model overlooks the differences in relationships and structures between the information, particularly when there is a significant gap between the teacher and student models [12].",
                    "score": 0.5185738023716493,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 225
                        },
                        {
                            "start": 225,
                            "end": 460
                        },
                        {
                            "start": 460,
                            "end": 654
                        },
                        {
                            "start": 656,
                            "end": 850
                        },
                        {
                            "start": 850,
                            "end": 1117
                        },
                        {
                            "start": 1119,
                            "end": 1218
                        },
                        {
                            "start": 1218,
                            "end": 1421
                        },
                        {
                            "start": 1421,
                            "end": 1659
                        },
                        {
                            "start": 1661,
                            "end": 1794
                        },
                        {
                            "start": 1794,
                            "end": 1994
                        },
                        {
                            "start": 1994,
                            "end": 2126
                        },
                        {
                            "start": 2126,
                            "end": 2359
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1014,
                            "end": 1018,
                            "matchedPaperCorpusId": "260933721"
                        },
                        {
                            "start": 1294,
                            "end": 1297,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 1297,
                            "end": 1300,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 1300,
                            "end": 1303,
                            "matchedPaperCorpusId": "198185886"
                        },
                        {
                            "start": 1330,
                            "end": 1333,
                            "matchedPaperCorpusId": "125950115"
                        },
                        {
                            "start": 1414,
                            "end": 1417,
                            "matchedPaperCorpusId": "237194985"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80029296875
                }
            ],
            "relevance_judgement": 0.80029296875,
            "relevance_judgment_input_expanded": "# Title: Exploring Graph-based Knowledge: Multi-Level Feature Distillation via Channels Relational Graph\n# Venue: arXiv.org\n# Authors: Zhiwei Wang, Jun Huang, Longhua Ma, Chengyu Wu, Hongyu Ma\n## Abstract\nIn visual tasks, large teacher models capture essential features and deep information, enhancing performance. However, distilling this information into smaller student models often leads to performance loss due to structural differences and capacity limitations. To tackle this, we propose a distillation framework based on graph knowledge, including a multi-level feature alignment strategy and an attention-guided mechanism to provide a targeted learning trajectory for the student model. We emphasize spectral embedding (SE) as a key technique in our distillation process, which merges the student's feature space with the relational knowledge and structural complexities similar to the teacher network. This method captures the teacher's understanding in a graph-based representation, enabling the student model to more accurately mimic the complex structural dependencies present in the teacher model. Compared to methods that focus only on specific distillation areas, our strategy not only considers key features within the teacher model but also endeavors to capture the relationships and interactions among feature sets, encoding these complex pieces of information into a graph structure to understand and utilize the dynamic relationships among these pieces of information from a global perspective. Experiments show that our method outperforms previous feature distillation methods on the CIFAR-100, MS-COCO, and Pascal VOC datasets, proving its efficiency and applicability.\n## Introduction\nIn the field of deep learning, the impressive success of large neural networks has come at the cost of increased computational complexity, which poses significant challenges for deployment in resourceconstrained environments.While these heavyweight models, often referred to as teacher networks, set state-of-the-art benchmarks on various tasks, their practical applicability is limited by their demanding requirements for memory, processing power, and energy.Knowledge Distillation (KD) [16] emerges as a promising solution to address this dichotomy by transferring the knowledge from a cumbersome model to a more compact and efficient student networks.\n\nThe quintessence of knowledge distillation lies in its ability to encapsulate the representational power of a larger model into a smaller one without incurring a substantial loss in performance.Pioneered by Hinton et al. [16], the process involves training a smaller model to mimic the behavior of the pre-trained larger model by softening the output logits [19], thus leveraging the rich information embedded in the output distributions of the teacher network.\n\nRecent advances in KD techniques have extended beyond the mere replication of output distributions.Contemporary works explore the distillation of intermediate representations [1,49,31], attention mechanisms [50,20], and even the inculcation of adversarial robustness from teacher to student [33,55,37].The underlying hypothesis is that the intermediate layers of a neural network embody a wealth of information about the data manifold that, when transferred effectively, can endow the student with nuanced understanding akin to its teacher.\n\nLarge and complex teacher models often capture a wealth of features and deep information, crucial for enhancing task performance [7].However, attempting to directly distill this rich information into smaller capacity student models often results in suboptimal performance reproduction due to the student model's capacity limitations.The student model may struggle to process the complex information present in the teacher model, leading to ineffective distillation.Moreover, simply mimicking all features of the teacher model overlooks the differences in relationships and structures between the information, particularly when there is a significant gap between the teacher and student models [12].",
            "reference_string": "[269761365 | Wang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "ALP-KD: Attention-Based Layer Projection for Knowledge Distillation",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2020,
            "reference_count": 33,
            "citation_count": 122,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/17610/17417",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.14022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "5062230",
                    "name": "Peyman Passban"
                },
                {
                    "authorId": "2000863903",
                    "name": "Yimeng Wu"
                },
                {
                    "authorId": "1924511",
                    "name": "Mehdi Rezagholizadeh"
                },
                {
                    "authorId": "2115900360",
                    "name": "Qun Liu"
                }
            ],
            "abstract": "Knowledge distillation is considered as a training and compression strategy in which two neural networks, namely a teacher and a student, are coupled together during training. The teacher network is supposed to be a trustworthy predictor and the student tries to mimic its predictions. Usually, a student with a lighter architecture is selected so we can achieve compression and yet deliver high-quality results. In such a setting, distillation only happens for final predictions whereas the student could also benefit from teacher\u2019s supervision for internal components.\n\nMotivated by this, we studied the problem of distillation for intermediate layers. Since there might not be a one-to-one alignment between student and teacher layers, existing techniques skip some teacher layers and only distill from a subset of them. This shortcoming directly impacts quality, so we instead propose a combinatorial technique which relies on attention. Our model fuses teacher-side information and takes each layer\u2019s significance into consideration, then it performs distillation between combined teacher layers and those of the student. Using our technique, we distilled a 12-layer BERT (Devlin et al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE tasks (Wang et al. 2018). Experimental results show that our combinatorial approach is able to outperform other existing techniques.",
            "corpus_id": 229679667,
            "sentences": [
                {
                    "corpus_id": "229679667",
                    "title": "ALP-KD: Attention-Based Layer Projection for Knowledge Distillation",
                    "text": "To utilize intermediate layers' information (and other components in general), a family of models exists that defines a dedicated loss function to measure how much a student diverges from its teacher in terms of internal representations. In particular, if the goal is to distill from an n-layer teacher into an m-layer student, a subset of m (out of n) teacher layers is selected whose outputs are compared to those of student layers (see Equation 3 for more details). Figure 1 illustrates this concept.\n\nFigure 1: Student and teacher models have m and n layers, respectively. Each node is an intermediate layer and links are cross-model connections. In this example, every other layer of the teacher is skipped in order to match the size of the student. The output of nodes connected to each other are compared via a loss function (shown with \u2194) to ensure that the student model has similar internal representations as its teacher.\n\nAs the figure shows, each student layer is connected to a single, dedicated peer on the teacher side, e.g. the n-th teacher layer corresponds to the m-th student layer. Since outputs of these two layers are compared to each other, we hope that both models generate as similar outputs as possible at points n and m. With this simple technique, teacher's knowledge can be used to supervise student's intermediate layers.\n\nExperimental results show that intermediate layer matching could be quite effective, but in our study we realized that it may suffer from two shortcomings:\n\n\u2022 If n m, multiple layers in T have to be ignored for distillation but we know that those layers consist of precious information for which we spend expensive resources to learn. This issue is referred to as the skip problem in this paper.\n\n\u2022 Moreover, it seems the way teacher layers are kept/skipped is somewhat arbitrary as there is no particular strategy behind it. Before training, we lack enough knowledge to judge which subset of teacher layers contributes more to the distillation process, so there is a good chance of skipping significant layers if we pick them in an arbitrary fashion. Finding the best subset of layers to distill from requires an exhaustive search or an expert in the field to signify connections. We refer to this issue as the search problem.\n\nIn order to resolve the aforementioned issues we propose an alternative, which is the main contribution of this paper.",
                    "score": 0.5019413174440175,
                    "section_title": "Problem Definition",
                    "char_start_offset": 2648,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7919921875
                }
            ],
            "relevance_judgement": 0.7919921875,
            "relevance_judgment_input_expanded": "# Title: ALP-KD: Attention-Based Layer Projection for Knowledge Distillation\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Peyman Passban, Yimeng Wu, Mehdi Rezagholizadeh, Qun Liu\n## Abstract\nKnowledge distillation is considered as a training and compression strategy in which two neural networks, namely a teacher and a student, are coupled together during training. The teacher network is supposed to be a trustworthy predictor and the student tries to mimic its predictions. Usually, a student with a lighter architecture is selected so we can achieve compression and yet deliver high-quality results. In such a setting, distillation only happens for final predictions whereas the student could also benefit from teacher\u2019s supervision for internal components.\n\nMotivated by this, we studied the problem of distillation for intermediate layers. Since there might not be a one-to-one alignment between student and teacher layers, existing techniques skip some teacher layers and only distill from a subset of them. This shortcoming directly impacts quality, so we instead propose a combinatorial technique which relies on attention. Our model fuses teacher-side information and takes each layer\u2019s significance into consideration, then it performs distillation between combined teacher layers and those of the student. Using our technique, we distilled a 12-layer BERT (Devlin et al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE tasks (Wang et al. 2018). Experimental results show that our combinatorial approach is able to outperform other existing techniques.\n## Problem Definition\nTo utilize intermediate layers' information (and other components in general), a family of models exists that defines a dedicated loss function to measure how much a student diverges from its teacher in terms of internal representations. In particular, if the goal is to distill from an n-layer teacher into an m-layer student, a subset of m (out of n) teacher layers is selected whose outputs are compared to those of student layers (see Equation 3 for more details). Figure 1 illustrates this concept.\n\nFigure 1: Student and teacher models have m and n layers, respectively. Each node is an intermediate layer and links are cross-model connections. In this example, every other layer of the teacher is skipped in order to match the size of the student. The output of nodes connected to each other are compared via a loss function (shown with \u2194) to ensure that the student model has similar internal representations as its teacher.\n\nAs the figure shows, each student layer is connected to a single, dedicated peer on the teacher side, e.g. the n-th teacher layer corresponds to the m-th student layer. Since outputs of these two layers are compared to each other, we hope that both models generate as similar outputs as possible at points n and m. With this simple technique, teacher's knowledge can be used to supervise student's intermediate layers.\n\nExperimental results show that intermediate layer matching could be quite effective, but in our study we realized that it may suffer from two shortcomings:\n\n\u2022 If n m, multiple layers in T have to be ignored for distillation but we know that those layers consist of precious information for which we spend expensive resources to learn. This issue is referred to as the skip problem in this paper.\n\n\u2022 Moreover, it seems the way teacher layers are kept/skipped is somewhat arbitrary as there is no particular strategy behind it. Before training, we lack enough knowledge to judge which subset of teacher layers contributes more to the distillation process, so there is a good chance of skipping significant layers if we pick them in an arbitrary fashion. Finding the best subset of layers to distill from requires an exhaustive search or an expert in the field to signify connections. We refer to this issue as the search problem.\n\nIn order to resolve the aforementioned issues we propose an alternative, which is the main contribution of this paper.",
            "reference_string": "[229679667 | Passban et al. | 2020 | Citations: 122]"
        },
        {
            "title": "Knowledge Fusion of Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 62,
            "citation_count": 72,
            "influential_citation_count": 9,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.10491, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2217614543",
                    "name": "Fanqi Wan"
                },
                {
                    "authorId": "14799547",
                    "name": "Xinting Huang"
                },
                {
                    "authorId": "2266753374",
                    "name": "Deng Cai"
                },
                {
                    "authorId": "2258552983",
                    "name": "Xiaojun Quan"
                },
                {
                    "authorId": "2258552250",
                    "name": "Wei Bi"
                },
                {
                    "authorId": "2257446263",
                    "name": "Shuming Shi"
                }
            ],
            "abstract": "While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures--Llama-2, MPT, and OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the fusion of LLMs can improve the performance of the target model across a range of capabilities such as reasoning, commonsense, and code generation. Our code, model weights, and data are public at \\url{https://github.com/fanqiwan/FuseLLM}.",
            "corpus_id": 267061245,
            "sentences": [
                {
                    "corpus_id": "267061245",
                    "title": "Knowledge Fusion of Large Language Models",
                    "text": "In contrast, the approach proposed in this paper supports the fusion of multiple LLMs with diverse architectures by explicitly transferring their knowledge and capabilities to a target LLM. \n\nKnowledge Distillation Knowledge distillation (Hinton et al., 2015), initially proposed for model compression, involves training a student model under the guidance of one or more teacher models. In the NLP community, knowledge distillation has been widely applied to text classification tasks. These applications include training the student model to replicate the teacher's output distribution (Sanh et al., 2019;Turc et al., 2019), as well as features (Sun et al., 2019;Jiao et al., 2020) and relations (Wang et al., 2020) derived from intermediate layers of the teacher model. In the realm of text generation, the conventional approach focuses on minimizing the KL divergence between the student and teacher generation distributions. This is achieved by using the teacher's probability distributions at each time step as supervision (Khanuja et al., 2021;Gu et al., 2023;Agarwal et al., 2023) or by directly training on the teacher's generated texts (Peng et al., 2023;Xu et al., 2023). \n\nWhile our method shares a framework similar to multi-teacher knowledge distillation, there are two significant distinctions. First, in traditional knowledge distillation, the student models are typically constrained to be smaller in size than the teachers. In our scenario, however, there are no limitations on the size of the target model. Second, traditional knowledge distillation often results in the student models lagging behind the teachers in performance after distillation. In contrast, we anticipate that after the fusion, the target model will surpass any of the source models in performance.",
                    "score": 0.5964249404737969,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 7492,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 189
                        },
                        {
                            "start": 192,
                            "end": 386
                        },
                        {
                            "start": 387,
                            "end": 485
                        },
                        {
                            "start": 486,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 1181
                        },
                        {
                            "start": 1184,
                            "end": 1308
                        },
                        {
                            "start": 1309,
                            "end": 1440
                        },
                        {
                            "start": 1441,
                            "end": 1524
                        },
                        {
                            "start": 1525,
                            "end": 1666
                        },
                        {
                            "start": 1667,
                            "end": 1787
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 646,
                            "end": 664,
                            "matchedPaperCorpusId": "201670719"
                        },
                        {
                            "start": 664,
                            "end": 682,
                            "matchedPaperCorpusId": "202719327"
                        },
                        {
                            "start": 1028,
                            "end": 1050,
                            "matchedPaperCorpusId": "236477925"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78271484375
                }
            ],
            "relevance_judgement": 0.78271484375,
            "relevance_judgment_input_expanded": "# Title: Knowledge Fusion of Large Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, Shuming Shi\n## Abstract\nWhile training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures--Llama-2, MPT, and OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the fusion of LLMs can improve the performance of the target model across a range of capabilities such as reasoning, commonsense, and code generation. Our code, model weights, and data are public at \\url{https://github.com/fanqiwan/FuseLLM}.\n## RELATED WORK\nIn contrast, the approach proposed in this paper supports the fusion of multiple LLMs with diverse architectures by explicitly transferring their knowledge and capabilities to a target LLM. \n\nKnowledge Distillation Knowledge distillation (Hinton et al., 2015), initially proposed for model compression, involves training a student model under the guidance of one or more teacher models. In the NLP community, knowledge distillation has been widely applied to text classification tasks. These applications include training the student model to replicate the teacher's output distribution (Sanh et al., 2019;Turc et al., 2019), as well as features (Sun et al., 2019;Jiao et al., 2020) and relations (Wang et al., 2020) derived from intermediate layers of the teacher model. In the realm of text generation, the conventional approach focuses on minimizing the KL divergence between the student and teacher generation distributions. This is achieved by using the teacher's probability distributions at each time step as supervision (Khanuja et al., 2021;Gu et al., 2023;Agarwal et al., 2023) or by directly training on the teacher's generated texts (Peng et al., 2023;Xu et al., 2023). \n\nWhile our method shares a framework similar to multi-teacher knowledge distillation, there are two significant distinctions. First, in traditional knowledge distillation, the student models are typically constrained to be smaller in size than the teachers. In our scenario, however, there are no limitations on the size of the target model. Second, traditional knowledge distillation often results in the student models lagging behind the teachers in performance after distillation. In contrast, we anticipate that after the fusion, the target model will surpass any of the source models in performance.",
            "reference_string": "[267061245 | Wan et al. | 2024 | Citations: 72]"
        },
        {
            "title": "On Explaining Knowledge Distillation: Measuring and Visualising the Knowledge Transfer Process",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.13943, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "7827164",
                    "name": "Gereziher W. Adhane"
                },
                {
                    "authorId": "2178547",
                    "name": "Mohammad Mahdi Dehshibi"
                },
                {
                    "authorId": "2313639114",
                    "name": "Dennis Vetter"
                },
                {
                    "authorId": "2284862027",
                    "name": "David Masip"
                },
                {
                    "authorId": "2313639123",
                    "name": "Gemma Roig"
                }
            ],
            "abstract": "Knowledge distillation (KD) remains challenging due to the opaque nature of the knowledge transfer process from a Teacher to a Student, making it difficult to address certain issues related to KD. To address this, we proposed UniCAM, a novel gradient-based visual explanation method, which effectively interprets the knowledge learned during KD. Our experimental results demonstrate that with the guidance of the Teacher's knowledge, the Student model becomes more efficient, learning more relevant features while discarding those that are not relevant. We refer to the features learned with the Teacher's guidance as distilled features and the features irrelevant to the task and ignored by the Student as residual features. Distilled features focus on key aspects of the input, such as textures and parts of objects. In contrast, residual features demonstrate more diffused attention, often targeting irrelevant areas, including the backgrounds of the target objects. In addition, we proposed two novel metrics: the feature similarity score (FSS) and the relevance score (RS), which quantify the relevance of the distilled knowledge. Experiments on the CIFAR10, ASIRRA, and Plant Disease datasets demonstrate that UniCAM and the two metrics offer valuable insights to explain the KD process.",
            "corpus_id": 277652299,
            "sentences": [
                {
                    "corpus_id": "277652299",
                    "title": "On Explaining Knowledge Distillation: Measuring and Visualising the Knowledge Transfer Process",
                    "text": "The Student's performance often declines when there is a large architecture (capacity) gap between the Teacher and the Student [29,38]. The drop in the Student's performance may stem from either its own challenges in learning relevant features or the overwhelming knowledge of the Teacher. To investigate this issue, we employ two distillation strategies in our experiments using ResNet-101 as the Teacher and ResNet-18 as the Student, which have a significant capacity disparity. In the first approach, we conduct direct distillation from ResNet-101 to ResNet-18. The second approach introduces an intermediate \"Teacher assistant\" [37] to help bridge the capacity gap between ResNet-101 and ResNet-18. We use UniCAM and RS to analyse the KD process in these settings, with a focus on how well the smaller model manages to learn relevant features. \n\nUsing the proposed methods, we first examine the impact of a large capacity gap on the knowledge transfer between Teacher and Student. We use ResNet-101 as the Teacher and ResNet-18 as the Student and apply KD to train the Student model. Fig. 6 demonstrates that, in this setting, the Base Model captures more relevant features than the Student model. This suggests that a large capacity gap impedes knowledge transfer, as the Student model cannot effectively learn from the complex Teacher's knowledge. \n\nTo bridge the capacity gap, we use an intermediate Teacher assistant [29] to enable a more effective and focused knowledge transfer from ResNet-101 to ResNet-18 via ResNet-50. Figure 7 compares the saliency maps of the distilled features learned by two Students: ResNet-18 directly distilled from ResNet-101 (R18-R101) and ResNet-18 distilled from ResNet-101 through Teacher assistant ResNet-50 (R18-R50-R101). The saliency maps, visualised using UniCAM, reveal that the Teacher assistant helps learn more relevant features that highlight the object parts. In contrast, R18-R101 learns some irrelevant features and misses the salient features for the gt prediction. In fact,  We compare the relevance of features learned by the Student trained with the Teacher assistant and its equivalent Base model.",
                    "score": 0.49695696248746835,
                    "section_title": "Exploring the capacity gap impact",
                    "char_start_offset": 25588,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 289
                        },
                        {
                            "start": 290,
                            "end": 480
                        },
                        {
                            "start": 481,
                            "end": 564
                        },
                        {
                            "start": 565,
                            "end": 702
                        },
                        {
                            "start": 703,
                            "end": 847
                        },
                        {
                            "start": 850,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1087
                        },
                        {
                            "start": 1088,
                            "end": 1201
                        },
                        {
                            "start": 1202,
                            "end": 1353
                        },
                        {
                            "start": 1356,
                            "end": 1531
                        },
                        {
                            "start": 1532,
                            "end": 1766
                        },
                        {
                            "start": 1767,
                            "end": 1912
                        },
                        {
                            "start": 1913,
                            "end": 2021
                        },
                        {
                            "start": 2022,
                            "end": 2157
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 127,
                            "end": 131,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 131,
                            "end": 134,
                            "matchedPaperCorpusId": "235390933"
                        },
                        {
                            "start": 632,
                            "end": 636,
                            "matchedPaperCorpusId": "221802641"
                        },
                        {
                            "start": 1425,
                            "end": 1429,
                            "matchedPaperCorpusId": "212908749"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78271484375
                }
            ],
            "relevance_judgement": 0.78271484375,
            "relevance_judgment_input_expanded": "# Title: On Explaining Knowledge Distillation: Measuring and Visualising the Knowledge Transfer Process\n# Venue: arXiv.org\n# Authors: Gereziher W. Adhane, Mohammad Mahdi Dehshibi, Dennis Vetter, David Masip, Gemma Roig\n## Abstract\nKnowledge distillation (KD) remains challenging due to the opaque nature of the knowledge transfer process from a Teacher to a Student, making it difficult to address certain issues related to KD. To address this, we proposed UniCAM, a novel gradient-based visual explanation method, which effectively interprets the knowledge learned during KD. Our experimental results demonstrate that with the guidance of the Teacher's knowledge, the Student model becomes more efficient, learning more relevant features while discarding those that are not relevant. We refer to the features learned with the Teacher's guidance as distilled features and the features irrelevant to the task and ignored by the Student as residual features. Distilled features focus on key aspects of the input, such as textures and parts of objects. In contrast, residual features demonstrate more diffused attention, often targeting irrelevant areas, including the backgrounds of the target objects. In addition, we proposed two novel metrics: the feature similarity score (FSS) and the relevance score (RS), which quantify the relevance of the distilled knowledge. Experiments on the CIFAR10, ASIRRA, and Plant Disease datasets demonstrate that UniCAM and the two metrics offer valuable insights to explain the KD process.\n## Exploring the capacity gap impact\nThe Student's performance often declines when there is a large architecture (capacity) gap between the Teacher and the Student [29,38]. The drop in the Student's performance may stem from either its own challenges in learning relevant features or the overwhelming knowledge of the Teacher. To investigate this issue, we employ two distillation strategies in our experiments using ResNet-101 as the Teacher and ResNet-18 as the Student, which have a significant capacity disparity. In the first approach, we conduct direct distillation from ResNet-101 to ResNet-18. The second approach introduces an intermediate \"Teacher assistant\" [37] to help bridge the capacity gap between ResNet-101 and ResNet-18. We use UniCAM and RS to analyse the KD process in these settings, with a focus on how well the smaller model manages to learn relevant features. \n\nUsing the proposed methods, we first examine the impact of a large capacity gap on the knowledge transfer between Teacher and Student. We use ResNet-101 as the Teacher and ResNet-18 as the Student and apply KD to train the Student model. Fig. 6 demonstrates that, in this setting, the Base Model captures more relevant features than the Student model. This suggests that a large capacity gap impedes knowledge transfer, as the Student model cannot effectively learn from the complex Teacher's knowledge. \n\nTo bridge the capacity gap, we use an intermediate Teacher assistant [29] to enable a more effective and focused knowledge transfer from ResNet-101 to ResNet-18 via ResNet-50. Figure 7 compares the saliency maps of the distilled features learned by two Students: ResNet-18 directly distilled from ResNet-101 (R18-R101) and ResNet-18 distilled from ResNet-101 through Teacher assistant ResNet-50 (R18-R50-R101). The saliency maps, visualised using UniCAM, reveal that the Teacher assistant helps learn more relevant features that highlight the object parts. In contrast, R18-R101 learns some irrelevant features and misses the salient features for the gt prediction. In fact,  We compare the relevance of features learned by the Student trained with the Teacher assistant and its equivalent Base model.",
            "reference_string": "[277652299 | Adhane et al. | 2024 | Citations: 0]"
        },
        {
            "title": "TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.12342, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108494287",
                    "name": "Guopeng Li"
                },
                {
                    "authorId": "2280102525",
                    "name": "Qiang Wang"
                },
                {
                    "authorId": "2266750866",
                    "name": "Ke Yan"
                },
                {
                    "authorId": "2266754413",
                    "name": "Shouhong Ding"
                },
                {
                    "authorId": "2287700762",
                    "name": "Yuan Gao"
                },
                {
                    "authorId": "2286883466",
                    "name": "Gui-Song Xia"
                }
            ],
            "abstract": "Most knowledge distillation (KD) methodologies predominantly focus on teacher-student pairs with similar architectures, such as both being convolutional neural networks (CNNs). However, the potential and flexibility of KD can be greatly improved by expanding it to novel Cross-Architecture KD (CAKD), where the knowledge of homogeneous and heterogeneous teachers can be transferred flexibly to a given student. The primary challenge in CAKD lies in the substantial feature gaps between heterogeneous models, originating from the distinction of their inherent inductive biases and module functions. To this end, we introduce an assistant model as a bridge to facilitate smooth feature knowledge transfer between heterogeneous teachers and students. More importantly, within our proposed design principle, the assistant model combines the advantages of cross-architecture inductive biases and module functions by merging convolution and attention modules derived from both student and teacher module functions. Furthermore, we observe that heterogeneous features exhibit diverse spatial distributions in CAKD, hindering the effectiveness of conventional pixel-wise mean squared error (MSE) loss. Therefore, we leverage a spatial-agnostic InfoNCE loss to align features after spatial smoothing, thereby improving the feature alignments in CAKD. Our proposed method is evaluated across some homogeneous model pairs and arbitrary heterogeneous combinations of CNNs, ViTs, and MLPs, achieving state-of-the-art performance for distilled models with a maximum gain of 11.47% on CIFAR-100 and 3.67% on ImageNet-1K. Our code and models will be released.",
            "corpus_id": 273375639,
            "sentences": [
                {
                    "corpus_id": "273375639",
                    "title": "TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant",
                    "text": "Knowledge Distillation (KD) (Hinton et al., 2015;Romero et al., 2015) has been demonstrated as a powerful method to transfer knowledge from a pre-trained and cumbersome teacher model to a compact and efficient student model. Compared to the model trained from scratch, the performance of the student model distilled by appropriate teachers usually improves significantly. Commonly, knowledge transferred is derived from either the output logits (logits-based KD (Sun et al., 2024)) or the intermediate features (feature-based KD (Romero et al., 2015)) of the teacher model. Therefore, it is intuitive to understand different teachers have different knowledge (logits or features) determined by their unique architectures (Liu et al., 2021a). \n\nMost existing KD approaches focus on similar-architecture distillation (Romero et al., 2015;Tian et al., 2020;Liu et al., 2023) (called SAKD), i.e., optional teachers are restricted to a limited scope with structures similar to the student model. However, this homogeneous distillation presents two principal limitations: (1) Limited Potential: Compared to the broader range of arbitrary teachers (including homogeneous and heterogeneous ones), the restricted scope of teachers in SAKD may fail to include the optimal knowledge necessary to enhance the performance of certain students. For instance, as OFA (Hao et al., 2023) demonstrated, distilling knowledge from a heterogeneous ViT-Base to ResNet50 yields superior student performance compared to using a ResNet152 as the homogeneous teacher. (2) Limited Flexibility: The emergence of new models (Liu et al., 2022; Preprint  Tolstikhin et al., 2021) or the scarcity of perfectly tuned homogeneous teachers in domain-specific tasks (Ronneberger et al., 2015;Li et al., 2024) poses significant challenges in obtaining suitable homogeneous teachers, thereby impeding the applicability of SAKD. Thus, this paper expands KD to cross-architecture KD (CAKD), investigating methods to distill knowledge from both homogeneous and heterogeneous teachers to students (Hao et al., 2023).",
                    "score": 0.7032399378155897,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 224
                        },
                        {
                            "start": 225,
                            "end": 371
                        },
                        {
                            "start": 372,
                            "end": 573
                        },
                        {
                            "start": 574,
                            "end": 741
                        },
                        {
                            "start": 744,
                            "end": 990
                        },
                        {
                            "start": 991,
                            "end": 1329
                        },
                        {
                            "start": 1330,
                            "end": 1540
                        },
                        {
                            "start": 1541,
                            "end": 1888
                        },
                        {
                            "start": 1889,
                            "end": 2073
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 49,
                            "end": 69,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 462,
                            "end": 480,
                            "matchedPaperCorpusId": "268247468"
                        },
                        {
                            "start": 529,
                            "end": 550,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 815,
                            "end": 836,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 836,
                            "end": 854,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 1729,
                            "end": 1755,
                            "matchedPaperCorpusId": "3719281"
                        },
                        {
                            "start": 1755,
                            "end": 1771,
                            "matchedPaperCorpusId": "268554202"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76806640625
                }
            ],
            "relevance_judgement": 0.76806640625,
            "relevance_judgment_input_expanded": "# Title: TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant\n# Venue: arXiv.org\n# Authors: Guopeng Li, Qiang Wang, Ke Yan, Shouhong Ding, Yuan Gao, Gui-Song Xia\n## Abstract\nMost knowledge distillation (KD) methodologies predominantly focus on teacher-student pairs with similar architectures, such as both being convolutional neural networks (CNNs). However, the potential and flexibility of KD can be greatly improved by expanding it to novel Cross-Architecture KD (CAKD), where the knowledge of homogeneous and heterogeneous teachers can be transferred flexibly to a given student. The primary challenge in CAKD lies in the substantial feature gaps between heterogeneous models, originating from the distinction of their inherent inductive biases and module functions. To this end, we introduce an assistant model as a bridge to facilitate smooth feature knowledge transfer between heterogeneous teachers and students. More importantly, within our proposed design principle, the assistant model combines the advantages of cross-architecture inductive biases and module functions by merging convolution and attention modules derived from both student and teacher module functions. Furthermore, we observe that heterogeneous features exhibit diverse spatial distributions in CAKD, hindering the effectiveness of conventional pixel-wise mean squared error (MSE) loss. Therefore, we leverage a spatial-agnostic InfoNCE loss to align features after spatial smoothing, thereby improving the feature alignments in CAKD. Our proposed method is evaluated across some homogeneous model pairs and arbitrary heterogeneous combinations of CNNs, ViTs, and MLPs, achieving state-of-the-art performance for distilled models with a maximum gain of 11.47% on CIFAR-100 and 3.67% on ImageNet-1K. Our code and models will be released.\n## INTRODUCTION\nKnowledge Distillation (KD) (Hinton et al., 2015;Romero et al., 2015) has been demonstrated as a powerful method to transfer knowledge from a pre-trained and cumbersome teacher model to a compact and efficient student model. Compared to the model trained from scratch, the performance of the student model distilled by appropriate teachers usually improves significantly. Commonly, knowledge transferred is derived from either the output logits (logits-based KD (Sun et al., 2024)) or the intermediate features (feature-based KD (Romero et al., 2015)) of the teacher model. Therefore, it is intuitive to understand different teachers have different knowledge (logits or features) determined by their unique architectures (Liu et al., 2021a). \n\nMost existing KD approaches focus on similar-architecture distillation (Romero et al., 2015;Tian et al., 2020;Liu et al., 2023) (called SAKD), i.e., optional teachers are restricted to a limited scope with structures similar to the student model. However, this homogeneous distillation presents two principal limitations: (1) Limited Potential: Compared to the broader range of arbitrary teachers (including homogeneous and heterogeneous ones), the restricted scope of teachers in SAKD may fail to include the optimal knowledge necessary to enhance the performance of certain students. For instance, as OFA (Hao et al., 2023) demonstrated, distilling knowledge from a heterogeneous ViT-Base to ResNet50 yields superior student performance compared to using a ResNet152 as the homogeneous teacher. (2) Limited Flexibility: The emergence of new models (Liu et al., 2022; Preprint  Tolstikhin et al., 2021) or the scarcity of perfectly tuned homogeneous teachers in domain-specific tasks (Ronneberger et al., 2015;Li et al., 2024) poses significant challenges in obtaining suitable homogeneous teachers, thereby impeding the applicability of SAKD. Thus, this paper expands KD to cross-architecture KD (CAKD), investigating methods to distill knowledge from both homogeneous and heterogeneous teachers to students (Hao et al., 2023).",
            "reference_string": "[273375639 | Li et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Mamba base PKD for efficient knowledge compression",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 22,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348480395",
                    "name": "Jos\u00e9 Medina"
                },
                {
                    "authorId": "3221342",
                    "name": "Amnir Hadachi"
                },
                {
                    "authorId": "2256850325",
                    "name": "Paul Honeine"
                },
                {
                    "authorId": "2314690317",
                    "name": "Abdelaziz Bensrhair"
                }
            ],
            "abstract": "Deep neural networks (DNNs) have remarkably succeeded in various image processing tasks. However, their large size and computational complexity present significant challenges for deploying them in resource-constrained environments. This paper presents an innovative approach for integrating Mamba Architecture within a Progressive Knowledge Distillation (PKD) process to address the challenge of reducing model complexity while maintaining accuracy in image classification tasks. The proposed framework distills a large teacher model into progressively smaller student models, designed using Mamba blocks. Each student model is trained using Selective-State-Space Models (S-SSM) within the Mamba blocks, focusing on important input aspects while reducing computational complexity. The work's preliminary experiments use MNIST and CIFAR-10 as datasets to demonstrate the effectiveness of this approach. For MNIST, the teacher model achieves 98% accuracy. A set of seven student models as a group retained 63% of the teacher's FLOPs, approximating the teacher's performance with 98% accuracy. The weak student used only 1% of the teacher's FLOPs and maintained 72% accuracy. Similarly, for CIFAR-10, the students achieved 1% less accuracy compared to the teacher, with the small student retaining 5% of the teacher's FLOPs to achieve 50% accuracy. These results confirm the flexibility and scalability of Mamba Architecture, which can be integrated into PKD, succeeding in the process of finding students as weak learners. The framework provides a solution for deploying complex neural networks in real-time applications with a reduction in computational cost.",
            "corpus_id": 276776153,
            "sentences": [
                {
                    "corpus_id": "276776153",
                    "title": "Mamba base PKD for efficient knowledge compression",
                    "text": "Knowledge distillation has been extensively studied as a model compression technique that allows a smaller student model to learn from a larger teacher model [5,9] (see Figure 1). Traditional KD includes transferring the knowledge to a student model from a teacher model's output [2], intermediate layers [16], or only the relationships between different layers and data samples [15,21]. These techniques lead to computational savings while preserving high accuracy. However, one limitation of traditional KD is the performance drop when there is a large capacity gap between the teacher and student models because the student cannot effectively represent key features of the teacher. This limitation has been addressed by various methods [15,16,21,22] that modify distillation algorithms or employ multi-stage learning. \n\nTo overcome these challenges, hierarchical or layer-wise distillation approaches were proposed, where knowledge is progressively transferred across multiple layers of the teacher to the student. FitNets, introduced in [16], transfer intermediate representations from the teacher model to the student, improving the student's ability to learn fine-grained features progressively. Similarly, selfdistillation [22] trains a model by distilling knowledge into itself at different stages, a concept related to progressive distillation where multiple students incrementally improve performance [4]. \n\n. . .",
                    "score": 0.5648159647916858,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 5819,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 466
                        },
                        {
                            "start": 467,
                            "end": 684
                        },
                        {
                            "start": 685,
                            "end": 820
                        },
                        {
                            "start": 823,
                            "end": 1017
                        },
                        {
                            "start": 1018,
                            "end": 1201
                        },
                        {
                            "start": 1202,
                            "end": 1415
                        },
                        {
                            "start": 1418,
                            "end": 1423
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 158,
                            "end": 161,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 379,
                            "end": 383,
                            "matchedPaperCorpusId": "229679667"
                        },
                        {
                            "start": 383,
                            "end": 386,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 739,
                            "end": 743,
                            "matchedPaperCorpusId": "229679667"
                        },
                        {
                            "start": 746,
                            "end": 749,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 749,
                            "end": 752,
                            "matchedPaperCorpusId": "159041406"
                        },
                        {
                            "start": 1230,
                            "end": 1234,
                            "matchedPaperCorpusId": "159041406"
                        },
                        {
                            "start": 1411,
                            "end": 1414,
                            "matchedPaperCorpusId": "265067529"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.763671875
                }
            ],
            "relevance_judgement": 0.763671875,
            "relevance_judgment_input_expanded": "# Title: Mamba base PKD for efficient knowledge compression\n# Venue: arXiv.org\n# Authors: Jos\u00e9 Medina, Amnir Hadachi, Paul Honeine, Abdelaziz Bensrhair\n## Abstract\nDeep neural networks (DNNs) have remarkably succeeded in various image processing tasks. However, their large size and computational complexity present significant challenges for deploying them in resource-constrained environments. This paper presents an innovative approach for integrating Mamba Architecture within a Progressive Knowledge Distillation (PKD) process to address the challenge of reducing model complexity while maintaining accuracy in image classification tasks. The proposed framework distills a large teacher model into progressively smaller student models, designed using Mamba blocks. Each student model is trained using Selective-State-Space Models (S-SSM) within the Mamba blocks, focusing on important input aspects while reducing computational complexity. The work's preliminary experiments use MNIST and CIFAR-10 as datasets to demonstrate the effectiveness of this approach. For MNIST, the teacher model achieves 98% accuracy. A set of seven student models as a group retained 63% of the teacher's FLOPs, approximating the teacher's performance with 98% accuracy. The weak student used only 1% of the teacher's FLOPs and maintained 72% accuracy. Similarly, for CIFAR-10, the students achieved 1% less accuracy compared to the teacher, with the small student retaining 5% of the teacher's FLOPs to achieve 50% accuracy. These results confirm the flexibility and scalability of Mamba Architecture, which can be integrated into PKD, succeeding in the process of finding students as weak learners. The framework provides a solution for deploying complex neural networks in real-time applications with a reduction in computational cost.\n## Knowledge Distillation\nKnowledge distillation has been extensively studied as a model compression technique that allows a smaller student model to learn from a larger teacher model [5,9] (see Figure 1). Traditional KD includes transferring the knowledge to a student model from a teacher model's output [2], intermediate layers [16], or only the relationships between different layers and data samples [15,21]. These techniques lead to computational savings while preserving high accuracy. However, one limitation of traditional KD is the performance drop when there is a large capacity gap between the teacher and student models because the student cannot effectively represent key features of the teacher. This limitation has been addressed by various methods [15,16,21,22] that modify distillation algorithms or employ multi-stage learning. \n\nTo overcome these challenges, hierarchical or layer-wise distillation approaches were proposed, where knowledge is progressively transferred across multiple layers of the teacher to the student. FitNets, introduced in [16], transfer intermediate representations from the teacher model to the student, improving the student's ability to learn fine-grained features progressively. Similarly, selfdistillation [22] trains a model by distilling knowledge into itself at different stages, a concept related to progressive distillation where multiple students incrementally improve performance [4]. \n\n. . .",
            "reference_string": "[276776153 | Medina et al. | 2025 | Citations: 0]"
        },
        {
            "title": "CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 77,
            "citation_count": 29,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2203.06760",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.06760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145802952",
                    "name": "Yuan Gong"
                },
                {
                    "authorId": "40570741",
                    "name": "Sameer Khurana"
                },
                {
                    "authorId": "2110769738",
                    "name": "Andrew Rouditchenko"
                },
                {
                    "authorId": "145898106",
                    "name": "James R. Glass"
                }
            ],
            "abstract": "Audio classification is an active research area with a wide range of applications. Over the past decade, convolutional neural networks (CNNs) have been the de-facto standard building block for end-to-end audio classification models. Recently, neural networks based solely on self-attention mechanisms such as the Audio Spectrogram Transformer (AST) have been shown to outperform CNNs. In this paper, we find an intriguing interaction between the two very different models - CNN and AST models are good teachers for each other. When we use either of them as the teacher and train the other model as the student via knowledge distillation (KD), the performance of the student model noticeably improves, and in many cases, is better than the teacher model. In our experiments with this CNN/Transformer Cross-Model Knowledge Distillation (CMKD) method we achieve new state-of-the-art performance on FSD50K, AudioSet, and ESC-50.",
            "corpus_id": 247446679,
            "sentences": [
                {
                    "corpus_id": "247446679",
                    "title": "CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification",
                    "text": "We first explore the optimal teacher model for each student model. Specifically, we use the KD setting mentioned in Section 2 and test a set of (teacher, student) model pairs. Specifically, we consider the following models: EfficientNet B0, B2, B6 [21] and DenseNet-121 [22] (CNN models); and AST Tiny, Small, Base [11] (Transformer models). The reason for selecting these models are: 1) EfficientNet-B2 and AST-Base are the best CNN and Transformer models, and we want to see if KD can further improve their performance; 2) AST-Base is larger than EfficientNet-B2 in terms of the number of parameters. We include models with various sizes to clearly identify whether the improvement of KD is due to different model sizes or different model architecture; 3) We also include one different CNN architecture (DenseNet) to check if the cross-model KD framework generalizes to different CNN architectures. \n\nFor each (teacher, student) pair, we select the temperature \u03c4 \u2208 {1.0, 2.5} based on the performance on the validation set, all other knowledge distillation settings are identical for all (teacher, student) pairs. The mean average precisions (mAPs) of the student model of various (teacher,student) pairs on the FSD50K evaluation set are shown in Table 3. Key observations are as follows: \n\n1. CNNs and ASTs are good teachers for each other. While knowledge distillation improves the student model performance in almost all settings (denoted by \u2020 in Table 3), we find that models always prefer a different teacher. Specifically, all CNN models (including DenseNet) achieve the best performance when ASTs are the teacher; all AST models achieve the best performance when CNNs are the teacher. Note that even though DenseNet and EfficientNet models also have different architectures, knowledge distillation between these two models leads to limited improvement. This demonstrates that the inherent differences between CNNs and ASTs is crucial for the success of knowledge distillation. 2. For both directions, the student model matches or outperforms its teacher. Usually, in knowledge distillation, the student model gets closer to, but is still weaker than, its teacher model.",
                    "score": 0.5637114240368356,
                    "section_title": "Which model is a good teacher?",
                    "char_start_offset": 14718,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 66
                        },
                        {
                            "start": 67,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 900
                        },
                        {
                            "start": 903,
                            "end": 1115
                        },
                        {
                            "start": 1116,
                            "end": 1290
                        },
                        {
                            "start": 1293,
                            "end": 1343
                        },
                        {
                            "start": 1344,
                            "end": 1516
                        },
                        {
                            "start": 1517,
                            "end": 1693
                        },
                        {
                            "start": 1694,
                            "end": 1861
                        },
                        {
                            "start": 1862,
                            "end": 1985
                        },
                        {
                            "start": 1986,
                            "end": 2063
                        },
                        {
                            "start": 2064,
                            "end": 2178
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 248,
                            "end": 252,
                            "matchedPaperCorpusId": "167217261"
                        },
                        {
                            "start": 270,
                            "end": 274,
                            "matchedPaperCorpusId": "9433631"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76025390625
                }
            ],
            "relevance_judgement": 0.76025390625,
            "relevance_judgment_input_expanded": "# Title: CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification\n# Venue: arXiv.org\n# Authors: Yuan Gong, Sameer Khurana, Andrew Rouditchenko, James R. Glass\n## Abstract\nAudio classification is an active research area with a wide range of applications. Over the past decade, convolutional neural networks (CNNs) have been the de-facto standard building block for end-to-end audio classification models. Recently, neural networks based solely on self-attention mechanisms such as the Audio Spectrogram Transformer (AST) have been shown to outperform CNNs. In this paper, we find an intriguing interaction between the two very different models - CNN and AST models are good teachers for each other. When we use either of them as the teacher and train the other model as the student via knowledge distillation (KD), the performance of the student model noticeably improves, and in many cases, is better than the teacher model. In our experiments with this CNN/Transformer Cross-Model Knowledge Distillation (CMKD) method we achieve new state-of-the-art performance on FSD50K, AudioSet, and ESC-50.\n## Which model is a good teacher?\nWe first explore the optimal teacher model for each student model. Specifically, we use the KD setting mentioned in Section 2 and test a set of (teacher, student) model pairs. Specifically, we consider the following models: EfficientNet B0, B2, B6 [21] and DenseNet-121 [22] (CNN models); and AST Tiny, Small, Base [11] (Transformer models). The reason for selecting these models are: 1) EfficientNet-B2 and AST-Base are the best CNN and Transformer models, and we want to see if KD can further improve their performance; 2) AST-Base is larger than EfficientNet-B2 in terms of the number of parameters. We include models with various sizes to clearly identify whether the improvement of KD is due to different model sizes or different model architecture; 3) We also include one different CNN architecture (DenseNet) to check if the cross-model KD framework generalizes to different CNN architectures. \n\nFor each (teacher, student) pair, we select the temperature \u03c4 \u2208 {1.0, 2.5} based on the performance on the validation set, all other knowledge distillation settings are identical for all (teacher, student) pairs. The mean average precisions (mAPs) of the student model of various (teacher,student) pairs on the FSD50K evaluation set are shown in Table 3. Key observations are as follows: \n\n1. CNNs and ASTs are good teachers for each other. While knowledge distillation improves the student model performance in almost all settings (denoted by \u2020 in Table 3), we find that models always prefer a different teacher. Specifically, all CNN models (including DenseNet) achieve the best performance when ASTs are the teacher; all AST models achieve the best performance when CNNs are the teacher. Note that even though DenseNet and EfficientNet models also have different architectures, knowledge distillation between these two models leads to limited improvement. This demonstrates that the inherent differences between CNNs and ASTs is crucial for the success of knowledge distillation. 2. For both directions, the student model matches or outperforms its teacher. Usually, in knowledge distillation, the student model gets closer to, but is still weaker than, its teacher model.",
            "reference_string": "[247446679 | Gong et al. | 2022 | Citations: 29]"
        },
        {
            "title": "On-Device Language Models: A Comprehensive Review",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 178,
            "citation_count": 35,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.00088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316519813",
                    "name": "Jiajun Xu"
                },
                {
                    "authorId": "2294674012",
                    "name": "Zhiyuan Li"
                },
                {
                    "authorId": "2294845809",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2316514278",
                    "name": "Qun Wang"
                },
                {
                    "authorId": "2319809164",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2364055424",
                    "name": "Qi Cai"
                },
                {
                    "authorId": "2319410023",
                    "name": "Ziyuan Ling"
                }
            ],
            "abstract": "The advent of large language models (LLMs) revolutionized natural language processing applications, and running LLMs on edge devices has become increasingly attractive for reasons including reduced latency, data localization, and personalized user experiences. This comprehensive review examines the challenges of deploying computationally expensive LLMs on resource-constrained devices and explores innovative solutions across multiple domains. The paper investigates the development of on-device language models, their efficient architectures, including parameter sharing and modular designs, as well as state-of-the-art compression techniques like quantization, pruning, and knowledge distillation. Hardware acceleration strategies and collaborative edge-cloud deployment approaches are analyzed, highlighting the intricate balance between performance and resource utilization. Case studies of on-device language models from major mobile manufacturers demonstrate real-world applications and potential benefits. The review also addresses critical aspects such as adaptive learning, multi-modal capabilities, and personalization. By identifying key research directions and open challenges, this paper provides a roadmap for future advancements in on-device language models, emphasizing the need for interdisciplinary efforts to realize the full potential of ubiquitous, intelligent computing while ensuring responsible and ethical deployment. For a comprehensive review of research work and educational resources on on-device large language models (LLMs), please visit https://github.com/NexaAI/Awesome-LLMs-on-device. To download and run on-device LLMs, visit https://www.nexaai.com/models.",
            "corpus_id": 272368391,
            "sentences": [
                {
                    "corpus_id": "272368391",
                    "title": "On-Device Language Models: A Comprehensive Review",
                    "text": "Knowledge Distillation (KD) is a technique for transferring knowledge from a large, computationally intensive model (teacher) to a smaller, more efficient model (student). This method is crucial for condensing the capabilities of large language models (LLMs) into more manageable forms without significantly impacting performance. \n\n1. Black-box Knowledge Distillation: This approach involves the student model learning solely from the outputs of the teacher model, without access to its internal mechanics or parameters. It is particularly advantageous when the teacher model's details are proprietary or when the architectures of the teacher and student models differ markedly. For instance, Gu et al. (2023) demonstrated that black-box KD could effectively train models using only the output data from LLM APIs like ChatGPT. The student model trains to emulate the teacher's output distribution based on input-output pairs, a process that, while effective, limits learning to external behaviors without tapping into the teacher's deeper internal states. 2. White-box Knowledge Distillation: In contrast, White-box Knowledge Distillation allows the student model to access the internal states and workings of the teacher, facilitating a deeper and more precise learning process. This method enables the student to mimic not just the outputs but also the internal state distributions of the teacher, enhancing learning efficacy and depth. The increased access to the teacher's detailed workings helps guide the student's learning, resulting in more accurate and robust models. However, this technique requires a careful alignment of model architectures to ensure effective knowledge transfer and is generally more complex to implement.",
                    "score": 0.5097078083217186,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 38449,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 171
                        },
                        {
                            "start": 172,
                            "end": 330
                        },
                        {
                            "start": 333,
                            "end": 521
                        },
                        {
                            "start": 522,
                            "end": 679
                        },
                        {
                            "start": 680,
                            "end": 827
                        },
                        {
                            "start": 828,
                            "end": 1056
                        },
                        {
                            "start": 1057,
                            "end": 1280
                        },
                        {
                            "start": 1281,
                            "end": 1439
                        },
                        {
                            "start": 1440,
                            "end": 1577
                        },
                        {
                            "start": 1578,
                            "end": 1736
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76025390625
                }
            ],
            "relevance_judgement": 0.76025390625,
            "relevance_judgment_input_expanded": "# Title: On-Device Language Models: A Comprehensive Review\n# Venue: arXiv.org\n# Authors: Jiajun Xu, Zhiyuan Li, Wei Chen, Qun Wang, Xin Gao, Qi Cai, Ziyuan Ling\n## Abstract\nThe advent of large language models (LLMs) revolutionized natural language processing applications, and running LLMs on edge devices has become increasingly attractive for reasons including reduced latency, data localization, and personalized user experiences. This comprehensive review examines the challenges of deploying computationally expensive LLMs on resource-constrained devices and explores innovative solutions across multiple domains. The paper investigates the development of on-device language models, their efficient architectures, including parameter sharing and modular designs, as well as state-of-the-art compression techniques like quantization, pruning, and knowledge distillation. Hardware acceleration strategies and collaborative edge-cloud deployment approaches are analyzed, highlighting the intricate balance between performance and resource utilization. Case studies of on-device language models from major mobile manufacturers demonstrate real-world applications and potential benefits. The review also addresses critical aspects such as adaptive learning, multi-modal capabilities, and personalization. By identifying key research directions and open challenges, this paper provides a roadmap for future advancements in on-device language models, emphasizing the need for interdisciplinary efforts to realize the full potential of ubiquitous, intelligent computing while ensuring responsible and ethical deployment. For a comprehensive review of research work and educational resources on on-device large language models (LLMs), please visit https://github.com/NexaAI/Awesome-LLMs-on-device. To download and run on-device LLMs, visit https://www.nexaai.com/models.\n## Knowledge Distillation\nKnowledge Distillation (KD) is a technique for transferring knowledge from a large, computationally intensive model (teacher) to a smaller, more efficient model (student). This method is crucial for condensing the capabilities of large language models (LLMs) into more manageable forms without significantly impacting performance. \n\n1. Black-box Knowledge Distillation: This approach involves the student model learning solely from the outputs of the teacher model, without access to its internal mechanics or parameters. It is particularly advantageous when the teacher model's details are proprietary or when the architectures of the teacher and student models differ markedly. For instance, Gu et al. (2023) demonstrated that black-box KD could effectively train models using only the output data from LLM APIs like ChatGPT. The student model trains to emulate the teacher's output distribution based on input-output pairs, a process that, while effective, limits learning to external behaviors without tapping into the teacher's deeper internal states. 2. White-box Knowledge Distillation: In contrast, White-box Knowledge Distillation allows the student model to access the internal states and workings of the teacher, facilitating a deeper and more precise learning process. This method enables the student to mimic not just the outputs but also the internal state distributions of the teacher, enhancing learning efficacy and depth. The increased access to the teacher's detailed workings helps guide the student's learning, resulting in more accurate and robust models. However, this technique requires a careful alignment of model architectures to ensure effective knowledge transfer and is generally more complex to implement.",
            "reference_string": "[272368391 | Xu et al. | 2024 | Citations: 35]"
        },
        {
            "title": "Student-friendly Knowledge Distillation",
            "venue": "Knowledge-Based Systems",
            "year": 2023,
            "reference_count": 62,
            "citation_count": 19,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.10893",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.10893, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152117943",
                    "name": "Mengyang Yuan"
                },
                {
                    "authorId": "2217188742",
                    "name": "Bo Lang"
                },
                {
                    "authorId": "49972272",
                    "name": "Fengnan Quan"
                }
            ],
            "abstract": "In knowledge distillation, the knowledge from the teacher model is often too complex for the student model to thoroughly process. However, good teachers in real life always simplify complex material before teaching it to students. Inspired by this fact, we propose student-friendly knowledge distillation (SKD) to simplify teacher output into new knowledge representations, which makes the learning of the student model easier and more effective. SKD contains a softening processing and a learning simplifier. First, the softening processing uses the temperature hyperparameter to soften the output logits of the teacher model, which simplifies the output to some extent and makes it easier for the learning simplifier to process. The learning simplifier utilizes the attention mechanism to further simplify the knowledge of the teacher model and is jointly trained with the student model using the distillation loss, which means that the process of simplification is correlated with the training objective of the student model and ensures that the simplified new teacher knowledge representation is more suitable for the specific student model. Furthermore, since SKD does not change the form of the distillation loss, it can be easily combined with other distillation methods that are based on the logits or features of intermediate layers to enhance its effectiveness. Therefore, SKD has wide applicability. The experimental results on the CIFAR-100 and ImageNet datasets show that our method achieves state-of-the-art performance while maintaining high training efficiency.",
            "corpus_id": 258762557,
            "sentences": [
                {
                    "corpus_id": "258762557",
                    "title": "Student-friendly Knowledge Distillation",
                    "text": "The results on the CIFAR-100 dataset are shown in Table 3 and Table 4, where Table 3 shows the results where the teacher and student models had the same architectures, and Table 4 shows the results where the teacher and student models had different architectures. \n\nComparing the experimental results of KD with those of SKD, SKD performs significantly better when using the same distillation loss function as KD. The largest improvement occurred when the teacher model was ResNet32\u00d74 and the student model was ResNet8\u00d74. The improvement reached 3.28%. This indicates that SKD can improve the learning performance of the student model simply by modifying the logits of the teacher model. \n\nOf the six experiments where the teacher and student models had the same architectures, SKD achieved the best performance compared to all feature-based and logit-based methods in five experiments. It ranked third in only one experiment where the feature-based method OFD [13] and Furthermore, SKD outperformed other state-of-the-art knowledge distillation methods, such as ReviewKD [6] and DKD [53], in 8 and 9 out of 11 teacher-student model combination experiments, respectively, achieving the best performance. Only in one teacher-student model combination did SKD not achieve a top-two result. This suggests that SKD can achieve the best distillation effect while keeping the design and training simple. \n\nThe results on the ImageNet dataset are shown in Table 5. Our SKD continued to perform significantly better than the classical KD. Moreover, compared with other distillation methods, SKD achieved the first and second-best results in two experiments based on top-1 accuracy and the second and third-best results in two experiments based on top-5 accuracy. This suggests that the performance of SKD is superior to that of most of the current best methods and that it achieved the best performance among logit-based methods.",
                    "score": 0.5203958115346432,
                    "section_title": "Comparison with state-of-the-art methods",
                    "char_start_offset": 13230,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 263
                        },
                        {
                            "start": 266,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 521
                        },
                        {
                            "start": 522,
                            "end": 552
                        },
                        {
                            "start": 553,
                            "end": 687
                        },
                        {
                            "start": 690,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1203
                        },
                        {
                            "start": 1204,
                            "end": 1287
                        },
                        {
                            "start": 1288,
                            "end": 1397
                        },
                        {
                            "start": 1400,
                            "end": 1457
                        },
                        {
                            "start": 1458,
                            "end": 1530
                        },
                        {
                            "start": 1531,
                            "end": 1754
                        },
                        {
                            "start": 1755,
                            "end": 1921
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 961,
                            "end": 965,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 1072,
                            "end": 1075,
                            "matchedPaperCorpusId": "233296935"
                        },
                        {
                            "start": 1084,
                            "end": 1088,
                            "matchedPaperCorpusId": "247476179"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7509765625
                }
            ],
            "relevance_judgement": 0.7509765625,
            "relevance_judgment_input_expanded": "# Title: Student-friendly Knowledge Distillation\n# Venue: Knowledge-Based Systems\n# Authors: Mengyang Yuan, Bo Lang, Fengnan Quan\n## Abstract\nIn knowledge distillation, the knowledge from the teacher model is often too complex for the student model to thoroughly process. However, good teachers in real life always simplify complex material before teaching it to students. Inspired by this fact, we propose student-friendly knowledge distillation (SKD) to simplify teacher output into new knowledge representations, which makes the learning of the student model easier and more effective. SKD contains a softening processing and a learning simplifier. First, the softening processing uses the temperature hyperparameter to soften the output logits of the teacher model, which simplifies the output to some extent and makes it easier for the learning simplifier to process. The learning simplifier utilizes the attention mechanism to further simplify the knowledge of the teacher model and is jointly trained with the student model using the distillation loss, which means that the process of simplification is correlated with the training objective of the student model and ensures that the simplified new teacher knowledge representation is more suitable for the specific student model. Furthermore, since SKD does not change the form of the distillation loss, it can be easily combined with other distillation methods that are based on the logits or features of intermediate layers to enhance its effectiveness. Therefore, SKD has wide applicability. The experimental results on the CIFAR-100 and ImageNet datasets show that our method achieves state-of-the-art performance while maintaining high training efficiency.\n## Comparison with state-of-the-art methods\nThe results on the CIFAR-100 dataset are shown in Table 3 and Table 4, where Table 3 shows the results where the teacher and student models had the same architectures, and Table 4 shows the results where the teacher and student models had different architectures. \n\nComparing the experimental results of KD with those of SKD, SKD performs significantly better when using the same distillation loss function as KD. The largest improvement occurred when the teacher model was ResNet32\u00d74 and the student model was ResNet8\u00d74. The improvement reached 3.28%. This indicates that SKD can improve the learning performance of the student model simply by modifying the logits of the teacher model. \n\nOf the six experiments where the teacher and student models had the same architectures, SKD achieved the best performance compared to all feature-based and logit-based methods in five experiments. It ranked third in only one experiment where the feature-based method OFD [13] and Furthermore, SKD outperformed other state-of-the-art knowledge distillation methods, such as ReviewKD [6] and DKD [53], in 8 and 9 out of 11 teacher-student model combination experiments, respectively, achieving the best performance. Only in one teacher-student model combination did SKD not achieve a top-two result. This suggests that SKD can achieve the best distillation effect while keeping the design and training simple. \n\nThe results on the ImageNet dataset are shown in Table 5. Our SKD continued to perform significantly better than the classical KD. Moreover, compared with other distillation methods, SKD achieved the first and second-best results in two experiments based on top-1 accuracy and the second and third-best results in two experiments based on top-5 accuracy. This suggests that the performance of SKD is superior to that of most of the current best methods and that it achieved the best performance among logit-based methods.",
            "reference_string": "[258762557 | Yuan et al. | 2023 | Citations: 19]"
        },
        {
            "title": "General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings",
            "venue": "IEEE International Joint Conference on Neural Network",
            "year": 2021,
            "reference_count": 46,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2109.08449",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.08449, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "11220282",
                    "name": "Lukas Galke"
                },
                {
                    "authorId": "2127384933",
                    "name": "Isabelle Cuber"
                },
                {
                    "authorId": "2115007899",
                    "name": "Christophe Meyer"
                },
                {
                    "authorId": "2127389543",
                    "name": "Henrik Ferdinand Nolscher"
                },
                {
                    "authorId": "2127393042",
                    "name": "Angelina Sonderecker"
                },
                {
                    "authorId": "1753135",
                    "name": "A. Scherp"
                }
            ],
            "abstract": "Large pretrained language models (PreLMs) are rev-olutionizing natural language processing across all benchmarks. However, their sheer size is prohibitive for small laboratories or for deployment on mobile devices. Approaches like pruning and distillation reduce the model size but typically retain the same model architecture. In contrast, we explore distilling PreLMs into a different, more efficient architecture, Continual Multiplication of Words (CMOW), which embeds each word as a matrix and uses matrix multiplication to encode sequences. We extend the CMOW architecture and its CMOW/CBOW-Hybrid variant with a bidirectional component for more expressive power, per-token representations for a general (task-agnostic) distillation during pretraining, and a two-sequence encoding scheme that facilitates downstream tasks on sentence pairs, such as sentence similarity and natural language inference. Our matrix-based bidirectional CMOW/CBOW-Hybrid model is competitive to DistilBERT on question similarity and recognizing textual entailment, but uses only half of the number of parameters and is three times faster in terms of inference speed. We match or exceed the scores of ELMo for all tasks of the GLUE benchmark except for the sentiment analysis task SST-2 and the linguistic acceptability task CoLA. However, compared to previous cross-architecture distillation approaches, we demonstrate a doubling of the scores on detecting linguistic acceptability. This shows that matrix-based embeddings can be used to distill large PreLM into competitive models and motivates further research in this direction.",
            "corpus_id": 251135436,
            "sentences": [
                {
                    "corpus_id": "251135436",
                    "title": "General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings",
                    "text": "The problem of knowledge distillation [9] or model compression [10] refers to learning a smaller model g that imitates the behavior of a larger model f , such that desirably g(x) \u2248 f (x). The smaller model g is called the student and the larger model f is called the teacher.\n\nThe distillation is carried out by aligning the teacher's and student's output, e. g., via a loss term L(f (x), g(x)), which we call the teacher signal. There are more techniques to foster distillation such as using the teacher's weights as initialization for the student [11,12,13], but those are only applicable when teacher and student are of the same model architecture. When student and teacher models have a different architecture, we call that cross-architecture distillation.\n\nTo contextualize knowledge distillation with transfer learning, we adopt the distinction between general distillation and taskspecific distillation from Tang et al. [14].\n\n\u2022 General distillation: The distillation is carried out only in the (self-supervised) pretraining stage. After pretraining, the teacher is not needed anymore. The student can be fine-tuned to downstream tasks independently. \u2022 Task-specific distillation: In task-specific distillation, the teacher model can be consulted during fine-tuning for each of the downstream tasks. The respective task's training data are used for (supervised) distillation. Note that the model for task-specific distillation can still be initialized with a model obtained by general distillation. However, this also falls under task-specific distillation, because, after all, the teacher has to be consulted during fine-tuning. Distinguishing between general and task-specific distillation is important because this affects how the methods can be applied in practice. Imagine that we want to fine-tune a model for a downstream task on a mobile device. A model from general distillation would be able to learn new tasks on its own. With a task-specific distillation approach, the larger teacher model would need to be consulted on the mobile device. Thus, both approaches differ in how they can be applied in practice and should",
                    "score": 0.5279092918244465,
                    "section_title": "B. Knowledge Distillation",
                    "char_start_offset": 6663,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 38,
                            "end": 41,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 63,
                            "end": 67,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 549,
                            "end": 553,
                            "matchedPaperCorpusId": "203626972"
                        },
                        {
                            "start": 553,
                            "end": 556,
                            "matchedPaperCorpusId": "202719327"
                        },
                        {
                            "start": 556,
                            "end": 559,
                            "matchedPaperCorpusId": "215238853"
                        },
                        {
                            "start": 927,
                            "end": 931,
                            "matchedPaperCorpusId": "85543565"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74755859375
                }
            ],
            "relevance_judgement": 0.74755859375,
            "relevance_judgment_input_expanded": "# Title: General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings\n# Venue: IEEE International Joint Conference on Neural Network\n# Authors: Lukas Galke, Isabelle Cuber, Christophe Meyer, Henrik Ferdinand Nolscher, Angelina Sonderecker, A. Scherp\n## Abstract\nLarge pretrained language models (PreLMs) are rev-olutionizing natural language processing across all benchmarks. However, their sheer size is prohibitive for small laboratories or for deployment on mobile devices. Approaches like pruning and distillation reduce the model size but typically retain the same model architecture. In contrast, we explore distilling PreLMs into a different, more efficient architecture, Continual Multiplication of Words (CMOW), which embeds each word as a matrix and uses matrix multiplication to encode sequences. We extend the CMOW architecture and its CMOW/CBOW-Hybrid variant with a bidirectional component for more expressive power, per-token representations for a general (task-agnostic) distillation during pretraining, and a two-sequence encoding scheme that facilitates downstream tasks on sentence pairs, such as sentence similarity and natural language inference. Our matrix-based bidirectional CMOW/CBOW-Hybrid model is competitive to DistilBERT on question similarity and recognizing textual entailment, but uses only half of the number of parameters and is three times faster in terms of inference speed. We match or exceed the scores of ELMo for all tasks of the GLUE benchmark except for the sentiment analysis task SST-2 and the linguistic acceptability task CoLA. However, compared to previous cross-architecture distillation approaches, we demonstrate a doubling of the scores on detecting linguistic acceptability. This shows that matrix-based embeddings can be used to distill large PreLM into competitive models and motivates further research in this direction.\n## B. Knowledge Distillation\nThe problem of knowledge distillation [9] or model compression [10] refers to learning a smaller model g that imitates the behavior of a larger model f , such that desirably g(x) \u2248 f (x). The smaller model g is called the student and the larger model f is called the teacher.\n\nThe distillation is carried out by aligning the teacher's and student's output, e. g., via a loss term L(f (x), g(x)), which we call the teacher signal. There are more techniques to foster distillation such as using the teacher's weights as initialization for the student [11,12,13], but those are only applicable when teacher and student are of the same model architecture. When student and teacher models have a different architecture, we call that cross-architecture distillation.\n\nTo contextualize knowledge distillation with transfer learning, we adopt the distinction between general distillation and taskspecific distillation from Tang et al. [14].\n\n\u2022 General distillation: The distillation is carried out only in the (self-supervised) pretraining stage. After pretraining, the teacher is not needed anymore. The student can be fine-tuned to downstream tasks independently. \u2022 Task-specific distillation: In task-specific distillation, the teacher model can be consulted during fine-tuning for each of the downstream tasks. The respective task's training data are used for (supervised) distillation. Note that the model for task-specific distillation can still be initialized with a model obtained by general distillation. However, this also falls under task-specific distillation, because, after all, the teacher has to be consulted during fine-tuning. Distinguishing between general and task-specific distillation is important because this affects how the methods can be applied in practice. Imagine that we want to fine-tune a model for a downstream task on a mobile device. A model from general distillation would be able to learn new tasks on its own. With a task-specific distillation approach, the larger teacher model would need to be consulted on the mobile device. Thus, both approaches differ in how they can be applied in practice and should",
            "reference_string": "[251135436 | Galke et al. | 2021 | Citations: 2]"
        },
        {
            "title": "Tailoring Instructions to Student\u2019s Learning Levels Boosts Knowledge Distillation",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 50,
            "citation_count": 7,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.09651",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.09651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2149471533",
                    "name": "Yuxin Ren"
                },
                {
                    "authorId": "2069513824",
                    "name": "Zi-Qi Zhong"
                },
                {
                    "authorId": "2110332219",
                    "name": "Xingjian Shi"
                },
                {
                    "authorId": "2128816752",
                    "name": "Yi Zhu"
                },
                {
                    "authorId": "2117728946",
                    "name": "Chun Yuan"
                },
                {
                    "authorId": "1701799",
                    "name": "Mu Li"
                }
            ],
            "abstract": "It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student\u2019s generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher\u2019s learning process. By prioritizing samples that are likely to enhance the student\u2019s generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.",
            "corpus_id": 258715315,
            "sentences": [
                {
                    "corpus_id": "258715315",
                    "title": "Tailoring Instructions to Student\u2019s Learning Levels Boosts Knowledge Distillation",
                    "text": "In this paper, we focus on task-specific distillation given pre-trained language models. Under this setting, the teacher model is already pre-trained in an unsupervised manner and the student model is either derived from part of the teacher model or pre-trained in an unsupervised manner as well.\n\nVanilla distillation The typical approach to knowledge distillation is a two-stage process. It involves first fine-tuning a pre-trained teacher model to maximize its performance on a specific task. Once the teacher model has converged, a student model is trained to closely imitate the output of the teacher model on the training data. The optimization objective for the student model at each mini-batch is:\n\n(1)\n\nThe update of the student follows:\n\nThe limitation of vanilla distillation is that it does not allow teacher to adjust its behavior according to student's feedback, as the teacher's parameters are fixed during the distillation process.\n\nOnline distillation To achieve student-aware distillation, online distillation Zhu et al., 2018;Shi et al., 2020) is proposed which involves the simultaneous fine-tuning of both the student and teacher models in one-stage.\n\nIn addition to minimizing the cross-entropy loss with respect to the ground truth labels, the target distribution of the teacher model is constrained to be close to that of the student model through the minimization of the cross-entropy loss between the outputs of the teacher and student models: L t (\u03b8 t , \u03b8 s , z r ) = \u03b1L ce (y r , T (x r ; \u03b8 t )) +(1 \u2212 \u03b1)L ce (T (x r ; \u03b8 t ), S(x r ; \u03b8 s )).\n\n(\n\nThe training process involves iteratively updating the parameters of both models:\n\nThrough iterative update, the student model is able to learn from the learning curve of the teacher model (Shi et al., 2020), which improves its performance on the given task. However, online distillation focuses on transferring the knowledge of the teacher to the student on training set without explicitly considering how well the student model will perform on unseen test data. This might lead to the student model only memorizing the training examples without generalizing well to new ones (",
                    "score": 0.5277053576003501,
                    "section_title": "Revisiting Learning to Teach",
                    "char_start_offset": 6489,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 1028,
                            "end": 1045,
                            "matchedPaperCorpusId": "48352434"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7470703125
                }
            ],
            "relevance_judgement": 0.7470703125,
            "relevance_judgment_input_expanded": "# Title: Tailoring Instructions to Student\u2019s Learning Levels Boosts Knowledge Distillation\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yuxin Ren, Zi-Qi Zhong, Xingjian Shi, Yi Zhu, Chun Yuan, Mu Li\n## Abstract\nIt has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student\u2019s generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher\u2019s learning process. By prioritizing samples that are likely to enhance the student\u2019s generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.\n## Revisiting Learning to Teach\nIn this paper, we focus on task-specific distillation given pre-trained language models. Under this setting, the teacher model is already pre-trained in an unsupervised manner and the student model is either derived from part of the teacher model or pre-trained in an unsupervised manner as well.\n\nVanilla distillation The typical approach to knowledge distillation is a two-stage process. It involves first fine-tuning a pre-trained teacher model to maximize its performance on a specific task. Once the teacher model has converged, a student model is trained to closely imitate the output of the teacher model on the training data. The optimization objective for the student model at each mini-batch is:\n\n(1)\n\nThe update of the student follows:\n\nThe limitation of vanilla distillation is that it does not allow teacher to adjust its behavior according to student's feedback, as the teacher's parameters are fixed during the distillation process.\n\nOnline distillation To achieve student-aware distillation, online distillation Zhu et al., 2018;Shi et al., 2020) is proposed which involves the simultaneous fine-tuning of both the student and teacher models in one-stage.\n\nIn addition to minimizing the cross-entropy loss with respect to the ground truth labels, the target distribution of the teacher model is constrained to be close to that of the student model through the minimization of the cross-entropy loss between the outputs of the teacher and student models: L t (\u03b8 t , \u03b8 s , z r ) = \u03b1L ce (y r , T (x r ; \u03b8 t )) +(1 \u2212 \u03b1)L ce (T (x r ; \u03b8 t ), S(x r ; \u03b8 s )).\n\n(\n\nThe training process involves iteratively updating the parameters of both models:\n\nThrough iterative update, the student model is able to learn from the learning curve of the teacher model (Shi et al., 2020), which improves its performance on the given task. However, online distillation focuses on transferring the knowledge of the teacher to the student on training set without explicitly considering how well the student model will perform on unseen test data. This might lead to the student model only memorizing the training examples without generalizing well to new ones (",
            "reference_string": "[258715315 | Ren et al. | 2023 | Citations: 7]"
        },
        {
            "title": "InFiConD: Interactive No-code Fine-tuning with Concept-based Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 88,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.17838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "8669194",
                    "name": "Jinbin Huang"
                },
                {
                    "authorId": "2003575022",
                    "name": "Wenbin He"
                },
                {
                    "authorId": "52158581",
                    "name": "Liangke Gou"
                },
                {
                    "authorId": "2265644812",
                    "name": "Liu Ren"
                },
                {
                    "authorId": "2256515852",
                    "name": "C. Bryan"
                }
            ],
            "abstract": "The emergence of large-scale pre-trained models has heightened their application in various downstream tasks, yet deployment is a challenge in environments with limited computational resources. Knowledge distillation has emerged as a solution in such scenarios, whereby knowledge from large teacher models is transferred into smaller student' models, but this is a non-trivial process that traditionally requires technical expertise in AI/ML. To address these challenges, this paper presents InFiConD, a novel framework that leverages visual concepts to implement the knowledge distillation process and enable subsequent no-code fine-tuning of student models. We develop a novel knowledge distillation pipeline based on extracting text-aligned visual concepts from a concept corpus using multimodal models, and construct highly interpretable linear student models based on visual concepts that mimic a teacher model in a response-based manner. InFiConD's interface allows users to interactively fine-tune the student model by manipulating concept influences directly in the user interface. We validate InFiConD via a robust usage scenario and user study. Our findings indicate that InFiConD's human-in-the-loop and visualization-driven approach enables users to effectively create and analyze student models, understand how knowledge is transferred, and efficiently perform fine-tuning operations. We discuss how this work highlights the potential of interactive and visual methods in making knowledge distillation and subsequent no-code fine-tuning more accessible and adaptable to a wider range of users with domain-specific demands.",
            "corpus_id": 270737867,
            "sentences": [
                {
                    "corpus_id": "270737867",
                    "title": "InFiConD: Interactive No-code Fine-tuning with Concept-based Knowledge Distillation",
                    "text": "Our goal is to develop a framework that efficiently supports interpretable KD from parent models and subsequent (i.e., post-distillation) fine-tuning of student models, in a way optimizes human effort and accessibility (i.e., AI/ML non-experts can perform it).To help constrain our efforts, we reviewed prior interactive, no-code, and visualizationdriven KD design studies (see Section 2 papers), identifying four key design challenges for such a framework.\n\nExisting methods (e.g., [39,61,64,82]) often prioritize student model performance at the cost of introducing complex architectures.This increased complexity hinders the adoption of KD [23] and negatively impacts subsequent fine-tuning processes, such as identifying the causes of underperformance, devising improvement strategies, and executing them effectively [76,84].Conversely, \"simpler\" student models can increase interpretability but also widen the capacity gap between the student and teacher models, making it challenging to transfer the desired knowledge effectively [56].Therefore, striking an appropriate balance between high student effectiveness and suitable interpretability is crucial for successful knowledge distillation and fine-tuning processes.\n\n(C2) Ensuring Generalizability in Knowledge Distillation.While using more information from the teacher model can lead to better student model performance, it can also make the KD method heavily dependent on the specific teacher model architecture [34,70], hindering its generalizability.Model-agnostic methods have been suggested for making KD more widely applicable [33,68,89], but they come with their own challenges.For example, a model-agnostic strategy means that only the input and output of the teacher model (and any information derived from them) can be used to train the student model.This constraint can significantly limit the effectiveness of KD [23], especially when there is an additional requirement for interpretability.Developing a system that is generalizable across a variety of pretrained models (PTMs) while still maintaining high effectiveness is a non-trivial challenge, requiring careful consideration of the trade-offs between generalizability and performance in the context of interpretable knowledge distillation.\n\n(C3) Supporting Efficient Analysis of the Student Model.",
                    "score": 0.5395123450736194,
                    "section_title": "FORMALIZING DESIGN CONSTRAINTS FOR INFICOND 3.1 Design Challenges",
                    "char_start_offset": 14857,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 260
                        },
                        {
                            "start": 260,
                            "end": 457
                        },
                        {
                            "start": 459,
                            "end": 590
                        },
                        {
                            "start": 590,
                            "end": 829
                        },
                        {
                            "start": 829,
                            "end": 1041
                        },
                        {
                            "start": 1041,
                            "end": 1224
                        },
                        {
                            "start": 1226,
                            "end": 1283
                        },
                        {
                            "start": 1283,
                            "end": 1513
                        },
                        {
                            "start": 1513,
                            "end": 1645
                        },
                        {
                            "start": 1645,
                            "end": 1821
                        },
                        {
                            "start": 1821,
                            "end": 1963
                        },
                        {
                            "start": 1963,
                            "end": 2267
                        },
                        {
                            "start": 2269,
                            "end": 2325
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 483,
                            "end": 487,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 487,
                            "end": 490,
                            "matchedPaperCorpusId": "218487294"
                        },
                        {
                            "start": 643,
                            "end": 647,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 821,
                            "end": 825,
                            "matchedPaperCorpusId": "215745611"
                        },
                        {
                            "start": 825,
                            "end": 828,
                            "matchedPaperCorpusId": "251643827"
                        },
                        {
                            "start": 1473,
                            "end": 1477,
                            "matchedPaperCorpusId": "258888057"
                        },
                        {
                            "start": 1477,
                            "end": 1480,
                            "matchedPaperCorpusId": "221802641"
                        },
                        {
                            "start": 1593,
                            "end": 1597,
                            "matchedPaperCorpusId": "248986690"
                        },
                        {
                            "start": 1600,
                            "end": 1603,
                            "matchedPaperCorpusId": "48352434"
                        },
                        {
                            "start": 1885,
                            "end": 1889,
                            "matchedPaperCorpusId": "219559263"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.734375
                }
            ],
            "relevance_judgement": 0.734375,
            "relevance_judgment_input_expanded": "# Title: InFiConD: Interactive No-code Fine-tuning with Concept-based Knowledge Distillation\n# Venue: arXiv.org\n# Authors: Jinbin Huang, Wenbin He, Liangke Gou, Liu Ren, C. Bryan\n## Abstract\nThe emergence of large-scale pre-trained models has heightened their application in various downstream tasks, yet deployment is a challenge in environments with limited computational resources. Knowledge distillation has emerged as a solution in such scenarios, whereby knowledge from large teacher models is transferred into smaller student' models, but this is a non-trivial process that traditionally requires technical expertise in AI/ML. To address these challenges, this paper presents InFiConD, a novel framework that leverages visual concepts to implement the knowledge distillation process and enable subsequent no-code fine-tuning of student models. We develop a novel knowledge distillation pipeline based on extracting text-aligned visual concepts from a concept corpus using multimodal models, and construct highly interpretable linear student models based on visual concepts that mimic a teacher model in a response-based manner. InFiConD's interface allows users to interactively fine-tune the student model by manipulating concept influences directly in the user interface. We validate InFiConD via a robust usage scenario and user study. Our findings indicate that InFiConD's human-in-the-loop and visualization-driven approach enables users to effectively create and analyze student models, understand how knowledge is transferred, and efficiently perform fine-tuning operations. We discuss how this work highlights the potential of interactive and visual methods in making knowledge distillation and subsequent no-code fine-tuning more accessible and adaptable to a wider range of users with domain-specific demands.\n## FORMALIZING DESIGN CONSTRAINTS FOR INFICOND 3.1 Design Challenges\nOur goal is to develop a framework that efficiently supports interpretable KD from parent models and subsequent (i.e., post-distillation) fine-tuning of student models, in a way optimizes human effort and accessibility (i.e., AI/ML non-experts can perform it).To help constrain our efforts, we reviewed prior interactive, no-code, and visualizationdriven KD design studies (see Section 2 papers), identifying four key design challenges for such a framework.\n\nExisting methods (e.g., [39,61,64,82]) often prioritize student model performance at the cost of introducing complex architectures.This increased complexity hinders the adoption of KD [23] and negatively impacts subsequent fine-tuning processes, such as identifying the causes of underperformance, devising improvement strategies, and executing them effectively [76,84].Conversely, \"simpler\" student models can increase interpretability but also widen the capacity gap between the student and teacher models, making it challenging to transfer the desired knowledge effectively [56].Therefore, striking an appropriate balance between high student effectiveness and suitable interpretability is crucial for successful knowledge distillation and fine-tuning processes.\n\n(C2) Ensuring Generalizability in Knowledge Distillation.While using more information from the teacher model can lead to better student model performance, it can also make the KD method heavily dependent on the specific teacher model architecture [34,70], hindering its generalizability.Model-agnostic methods have been suggested for making KD more widely applicable [33,68,89], but they come with their own challenges.For example, a model-agnostic strategy means that only the input and output of the teacher model (and any information derived from them) can be used to train the student model.This constraint can significantly limit the effectiveness of KD [23], especially when there is an additional requirement for interpretability.Developing a system that is generalizable across a variety of pretrained models (PTMs) while still maintaining high effectiveness is a non-trivial challenge, requiring careful consideration of the trade-offs between generalizability and performance in the context of interpretable knowledge distillation.\n\n(C3) Supporting Efficient Analysis of the Student Model.",
            "reference_string": "[270737867 | Huang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 29,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.00448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2151789151",
                    "name": "Liyi Yao"
                },
                {
                    "authorId": "2282156422",
                    "name": "Shaobing Gao"
                }
            ],
            "abstract": "Due to the data imbalance and the diversity of defects, student-teacher networks (S-T) are favored in unsupervised anomaly detection, which explores the discrepancy in feature representation derived from the knowledge distillation process to recognize anomalies. However, vanilla S-T network is not stable. Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data. To address this problem, we propose a novel dual-student knowledge distillation (DSKD) architecture. Different from other S-T networks, we use two student networks a single pre-trained teacher network, where the students have the same scale but inverted structures. This framework can enhance the distillation effect to improve the consistency in recognition of normal data, and simultaneously introduce diversity for anomaly representation. To explore high-dimensional semantic information to capture anomaly clues, we employ two strategies. First, a pyramid matching mode is used to perform knowledge distillation on multi-scale feature maps in the intermediate layers of networks. Second, an interaction is facilitated between the two student networks through a deep feature embedding module, which is inspired by real-world group discussions. In terms of classification, we obtain pixel-wise anomaly segmentation maps by measuring the discrepancy between the output feature maps of the teacher and student networks, from which an anomaly score is computed for sample-wise determination. We evaluate DSKD on three benchmark datasets and probe the effects of internal modules through ablation experiments. The results demonstrate that DSKD can achieve exceptional performance on small models like ResNet18 and effectively improve vanilla S-T networks.",
            "corpus_id": 267364776,
            "sentences": [],
            "relevance_judgement": 0.734375,
            "relevance_judgment_input_expanded": "# Title: Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection\n# Venue: arXiv.org\n# Authors: Liyi Yao, Shaobing Gao\n## Abstract\nDue to the data imbalance and the diversity of defects, student-teacher networks (S-T) are favored in unsupervised anomaly detection, which explores the discrepancy in feature representation derived from the knowledge distillation process to recognize anomalies. However, vanilla S-T network is not stable. Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data. To address this problem, we propose a novel dual-student knowledge distillation (DSKD) architecture. Different from other S-T networks, we use two student networks a single pre-trained teacher network, where the students have the same scale but inverted structures. This framework can enhance the distillation effect to improve the consistency in recognition of normal data, and simultaneously introduce diversity for anomaly representation. To explore high-dimensional semantic information to capture anomaly clues, we employ two strategies. First, a pyramid matching mode is used to perform knowledge distillation on multi-scale feature maps in the intermediate layers of networks. Second, an interaction is facilitated between the two student networks through a deep feature embedding module, which is inspired by real-world group discussions. In terms of classification, we obtain pixel-wise anomaly segmentation maps by measuring the discrepancy between the output feature maps of the teacher and student networks, from which an anomaly score is computed for sample-wise determination. We evaluate DSKD on three benchmark datasets and probe the effects of internal modules through ablation experiments. The results demonstrate that DSKD can achieve exceptional performance on small models like ResNet18 and effectively improve vanilla S-T networks.\n",
            "reference_string": "[267364776 | Yao et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Decoupled Classifier Knowledge Distillation",
            "venue": "PLoS ONE",
            "year": 2025,
            "reference_count": 33,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1371/journal.pone.0314267",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11844843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2195337296",
                    "name": "Hairui Wang"
                },
                {
                    "authorId": "2346974967",
                    "name": "Mengjie Dong"
                },
                {
                    "authorId": "2181805784",
                    "name": "Guifu Zhu"
                },
                {
                    "authorId": "50024555",
                    "name": "Ya Li"
                }
            ],
            "abstract": "Mainstream knowledge distillation methods primarily include self-distillation, offline distillation, online distillation, output-based distillation, and feature-based distillation. While each approach has its respective advantages, they are typically employed independently. Simply combining two distillation methods often leads to redundant information. If the information conveyed by both methods is highly similar, this can result in wasted computational resources and increased complexity. To provide a new perspective on distillation research, we aim to explore a compromise solution that aligns complex features without conflicting with output alignment. In this work, we propose to decouple the classifier\u2019s output into two components: non-target classes learned by the student, and target classes obtained by both the teacher and the student. Finally, we introduce Decoupled Classifier Knowledge Distillation (DCKD), where on one hand, we fix the correct knowledge that the student has already acquired, which is crucial for merging the two methods; on the other hand, we encourage the student to further align its output with that of the teacher. Compared to using a single method, DCKD achieves superior results on both the CIFAR-100 and ImageNet datasets for image classification and object detection tasks, without reducing training efficiency. Moreover, it allows relational-based and feature-based distillation to operate more efficiently and flexibly. This work demonstrates the great potential of integrating distillation methods, and we hope it will inspire future research.",
            "corpus_id": 276533262,
            "sentences": [
                {
                    "corpus_id": "276533262",
                    "title": "Decoupled Classifier Knowledge Distillation",
                    "text": "However, because they focus on different aspects, the knowledge learned by the student model also varies. \n\nIn the case of heterogeneous architectures between teacher and student models [14], the features of these models reside in different latent feature spaces, making it challenging to ensure effective alignment of the learned features. Consequently, directly matching these unrelated features is not only unproductive but may also hinder the student model's performance. Furthermore, feature-based distillation methods focus more on local regions, and this localized attention may be insufficient for effectively transferring knowledge from the teacher model to the student model in knowledge distillation [15]. The knowledge embedded in the teacher model is often too complex for the student model to fully absorb and process [16]. Although teacher models are generally more complex and capable of capturing more knowledge, the main challenge lies in distilling this knowledge into a form that is accessible and beneficial for the student model [16]. \n\nWe have observed that existing feature-based and output-based knowledge distillation techniques are mostly applied independently. Although both methods have their respective advantages in improving model performance and reducing model complexity, there is still no effective method that can simultaneously integrate the strengths of both, as they focus on different aspects of the model. As pointed out by Jianping Gou et al [17], how to model these different types of knowledge in a unified and complementary framework remains an urgent challenge. Specifically, knowledge from different layers may have varying impacts on the training of the student model [18]. For example, output-based knowledge mainly comes from the model's final layer, while feature-based knowledge, guided by deeper layers, might face the risk of over-regularization. Meanwhile, recent studies have found that deep features are often linearized. Transforming features in certain directions can generate representations corresponding to the same category but with different samples. However, due to limited capacity, it is difficult for the student model to learn the distinctive features captured by the powerful teacher [19]. The lack of training data further exacerbates the performance gap between the student and teacher models. \n\nTherefore, the key step in knowledge distillation lies in how to effectively transfer the rich knowledge of the teacher model to the student model.",
                    "score": 0.6433251997015039,
                    "section_title": "Introduction",
                    "char_start_offset": 2228,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 105
                        },
                        {
                            "start": 108,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 475
                        },
                        {
                            "start": 476,
                            "end": 716
                        },
                        {
                            "start": 717,
                            "end": 837
                        },
                        {
                            "start": 838,
                            "end": 1056
                        },
                        {
                            "start": 1059,
                            "end": 1188
                        },
                        {
                            "start": 1189,
                            "end": 1446
                        },
                        {
                            "start": 1447,
                            "end": 1607
                        },
                        {
                            "start": 1608,
                            "end": 1721
                        },
                        {
                            "start": 1722,
                            "end": 1900
                        },
                        {
                            "start": 1901,
                            "end": 1978
                        },
                        {
                            "start": 1979,
                            "end": 2114
                        },
                        {
                            "start": 2115,
                            "end": 2259
                        },
                        {
                            "start": 2260,
                            "end": 2365
                        },
                        {
                            "start": 2368,
                            "end": 2515
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 186,
                            "end": 190,
                            "matchedPaperCorpusId": "265871679"
                        },
                        {
                            "start": 2254,
                            "end": 2258,
                            "matchedPaperCorpusId": "266998295"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72998046875
                },
                {
                    "corpus_id": "276533262",
                    "title": "Decoupled Classifier Knowledge Distillation",
                    "text": "Therefore, the key step in knowledge distillation lies in how to effectively transfer the rich knowledge of the teacher model to the student model. Based on this consideration, we propose a novel approach that decouples the classifier's output from the distillation process by fixing the knowledge acquired through feature alignment and further mining the knowledge from the teacher model on both correct and incorrect categories to supplement the student's learning. By simultaneously aligning the deep features and outputs of both the teacher and student models, we strive to minimize the performance gap between the two, and even surpass the teacher model in some experiments. \n\nTo achieve the above goals, we divide the structure into two core components: (1) retaining the target class knowledge already acquired by the student model, without fully covering all target classes; (2) utilizing both target and non-target class information to deeply align the teacher and student models. For better understanding, we refer to these two components as Stable Knowledge Distillation (SKD) and Polarized Knowledge Distillation (PKD), respectively. \n\nBased on the aforementioned challenges, we propose an optimized output distillation method for feature-based knowledge distillation, called Decoupled Classifier Knowledge Distillation (DCKD), as illustrated in Fig 1 . DCKD avoids the information redundancy caused by repetitive learning when using both feature-based and output-based distillation methods simultaneously. It also reinforces learned knowledge and excludes potential ambiguous or incorrect knowledge. The separate weighting of SKD and PKD helps determine the importance of each part. \n\nIn summary, our contributions are as follows: \n\n(1)We decouple the classifier's output into two parts, and split traditional KD into SKD and PKD. Through comparative analysis of these two components, we provide a suitable method to improve feature-based knowledge distillation by utilizing output-based distillation. (2)We reveal the limitations and even negative effects caused by redundant learning when combining output-based and feature-based distillation. PKD effectively avoids redundant learning of critical knowledge, while SKD further strengthens the knowledge based on PKD.",
                    "score": 0.4953939111034653,
                    "section_title": "Introduction",
                    "char_start_offset": 4596,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 147
                        },
                        {
                            "start": 148,
                            "end": 467
                        },
                        {
                            "start": 468,
                            "end": 679
                        },
                        {
                            "start": 682,
                            "end": 989
                        },
                        {
                            "start": 990,
                            "end": 1145
                        },
                        {
                            "start": 1148,
                            "end": 1365
                        },
                        {
                            "start": 1366,
                            "end": 1518
                        },
                        {
                            "start": 1519,
                            "end": 1612
                        },
                        {
                            "start": 1613,
                            "end": 1695
                        },
                        {
                            "start": 1698,
                            "end": 1743
                        },
                        {
                            "start": 1746,
                            "end": 1843
                        },
                        {
                            "start": 1844,
                            "end": 2014
                        },
                        {
                            "start": 2015,
                            "end": 2158
                        },
                        {
                            "start": 2159,
                            "end": 2281
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68212890625
                }
            ],
            "relevance_judgement": 0.72998046875,
            "relevance_judgment_input_expanded": "# Title: Decoupled Classifier Knowledge Distillation\n# Venue: PLoS ONE\n# Authors: Hairui Wang, Mengjie Dong, Guifu Zhu, Ya Li\n## Abstract\nMainstream knowledge distillation methods primarily include self-distillation, offline distillation, online distillation, output-based distillation, and feature-based distillation. While each approach has its respective advantages, they are typically employed independently. Simply combining two distillation methods often leads to redundant information. If the information conveyed by both methods is highly similar, this can result in wasted computational resources and increased complexity. To provide a new perspective on distillation research, we aim to explore a compromise solution that aligns complex features without conflicting with output alignment. In this work, we propose to decouple the classifier\u2019s output into two components: non-target classes learned by the student, and target classes obtained by both the teacher and the student. Finally, we introduce Decoupled Classifier Knowledge Distillation (DCKD), where on one hand, we fix the correct knowledge that the student has already acquired, which is crucial for merging the two methods; on the other hand, we encourage the student to further align its output with that of the teacher. Compared to using a single method, DCKD achieves superior results on both the CIFAR-100 and ImageNet datasets for image classification and object detection tasks, without reducing training efficiency. Moreover, it allows relational-based and feature-based distillation to operate more efficiently and flexibly. This work demonstrates the great potential of integrating distillation methods, and we hope it will inspire future research.\n## Introduction\nHowever, because they focus on different aspects, the knowledge learned by the student model also varies. \n\nIn the case of heterogeneous architectures between teacher and student models [14], the features of these models reside in different latent feature spaces, making it challenging to ensure effective alignment of the learned features. Consequently, directly matching these unrelated features is not only unproductive but may also hinder the student model's performance. Furthermore, feature-based distillation methods focus more on local regions, and this localized attention may be insufficient for effectively transferring knowledge from the teacher model to the student model in knowledge distillation [15]. The knowledge embedded in the teacher model is often too complex for the student model to fully absorb and process [16]. Although teacher models are generally more complex and capable of capturing more knowledge, the main challenge lies in distilling this knowledge into a form that is accessible and beneficial for the student model [16]. \n\nWe have observed that existing feature-based and output-based knowledge distillation techniques are mostly applied independently. Although both methods have their respective advantages in improving model performance and reducing model complexity, there is still no effective method that can simultaneously integrate the strengths of both, as they focus on different aspects of the model. As pointed out by Jianping Gou et al [17], how to model these different types of knowledge in a unified and complementary framework remains an urgent challenge. Specifically, knowledge from different layers may have varying impacts on the training of the student model [18]. For example, output-based knowledge mainly comes from the model's final layer, while feature-based knowledge, guided by deeper layers, might face the risk of over-regularization. Meanwhile, recent studies have found that deep features are often linearized. Transforming features in certain directions can generate representations corresponding to the same category but with different samples. However, due to limited capacity, it is difficult for the student model to learn the distinctive features captured by the powerful teacher [19]. The lack of training data further exacerbates the performance gap between the student and teacher models. \n\nTherefore, the key step in knowledge distillation lies in how to effectively transfer the rich knowledge of the teacher model to the student model.\n...\nTherefore, the key step in knowledge distillation lies in how to effectively transfer the rich knowledge of the teacher model to the student model. Based on this consideration, we propose a novel approach that decouples the classifier's output from the distillation process by fixing the knowledge acquired through feature alignment and further mining the knowledge from the teacher model on both correct and incorrect categories to supplement the student's learning. By simultaneously aligning the deep features and outputs of both the teacher and student models, we strive to minimize the performance gap between the two, and even surpass the teacher model in some experiments. \n\nTo achieve the above goals, we divide the structure into two core components: (1) retaining the target class knowledge already acquired by the student model, without fully covering all target classes; (2) utilizing both target and non-target class information to deeply align the teacher and student models. For better understanding, we refer to these two components as Stable Knowledge Distillation (SKD) and Polarized Knowledge Distillation (PKD), respectively. \n\nBased on the aforementioned challenges, we propose an optimized output distillation method for feature-based knowledge distillation, called Decoupled Classifier Knowledge Distillation (DCKD), as illustrated in Fig 1 . DCKD avoids the information redundancy caused by repetitive learning when using both feature-based and output-based distillation methods simultaneously. It also reinforces learned knowledge and excludes potential ambiguous or incorrect knowledge. The separate weighting of SKD and PKD helps determine the importance of each part. \n\nIn summary, our contributions are as follows: \n\n(1)We decouple the classifier's output into two parts, and split traditional KD into SKD and PKD. Through comparative analysis of these two components, we provide a suitable method to improve feature-based knowledge distillation by utilizing output-based distillation. (2)We reveal the limitations and even negative effects caused by redundant learning when combining output-based and feature-based distillation. PKD effectively avoids redundant learning of critical knowledge, while SKD further strengthens the knowledge based on PKD.",
            "reference_string": "[276533262 | Wang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport Alignment for Language Models with Different Tokenizers",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 35,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.16806, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2346978277",
                    "name": "Anh Duc Le"
                },
                {
                    "authorId": "2346974905",
                    "name": "Tu Vu"
                },
                {
                    "authorId": "2317277281",
                    "name": "Nam Le Hai"
                },
                {
                    "authorId": "2346977346",
                    "name": "Nguyen Thi Ngoc Diep"
                },
                {
                    "authorId": "15802036",
                    "name": "L. Van"
                },
                {
                    "authorId": "2346998678",
                    "name": "Trung Le"
                },
                {
                    "authorId": "2278832157",
                    "name": "T. Nguyen"
                }
            ],
            "abstract": "Large Language Models (LLMs) achieve state-of-the-art performance across various NLP tasks but face deployment challenges due to high computational costs and memory constraints. Knowledge distillation (KD) is a promising solution, transferring knowledge from large teacher models to smaller student models. However, existing KD methods often assume shared vocabularies and tokenizers, limiting their flexibility. While approaches like Universal Logit Distillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address vocabulary mismatches, they overlook the critical \\textbf{reasoning-aware distillation} aspect. To bridge this gap, we propose CoT2Align a universal KD framework that integrates Chain-of-Thought (CoT) augmentation and introduces Cross-CoT Alignment to enhance reasoning transfer. Additionally, we extend Optimal Transport beyond token-wise alignment to a sequence-level and layer-wise alignment approach that adapts to varying sequence lengths while preserving contextual integrity. Comprehensive experiments demonstrate that CoT2Align outperforms existing KD methods across different vocabulary settings, improving reasoning capabilities and robustness in domain-specific tasks.",
            "corpus_id": 276575418,
            "sentences": [
                {
                    "corpus_id": "276575418",
                    "title": "CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport Alignment for Language Models with Different Tokenizers",
                    "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing (NLP) tasks (Achiam et al., 2023;Touvron et al., 2023;Jiang et al., 2023Jiang et al., , 2024;;Guo et al., 2025). However, their deployment in real-world applications is often hindered by high computational costs, memory constraints, and latency issues. These limitations pose significant challenges to deploying LLMs efficiently on resource-constrained devices like mobile phones and IoT devices. Knowledge distillation (KD) (Hinton, 2015) has emerged as a promising solution to this challenge by transferring knowledge from a large teacher model to a more compact student model, thereby retaining essential performance while reducing computational overhead. Conventional KD approaches generally seek to align the output distributions of teacher and student models through methods like Kullback-Leibler (KL) divergence (Zhang et al., 2023;Hsieh et al., 2023;Ko et al., 2024). \n\nFor LLMs, knowledge distillation can be categorized into two main approaches: black-box KD and white-box KD. In black-box KD, the student model learns by mimicking the teacher model's outputs, as it has no access to the teacher's internal structure or variables (Fu et al., 2023;Kim and Rush, 2016). In contrast, white-box KD enables the student model to utilize the teacher model's architecture and variables while constructing regularization constraints during training. Theoretically, this approach facilitates more comprehensive knowledge transfer, leading to superior performance (Wen et al., 2023;Gu et al., 2024;Ko et al., 2024). However, a fundamental limitation of these approaches is that they assume both models share the same vocabulary and tokenizer, a requirement that is increasingly impractical given the diversity of architectures and tokenization schemes used in contemporary LLMs. \n\nSeveral recent studies have attempted to address this issue by enabling KD across models with different tokenizers.",
                    "score": 0.4990129736938408,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 374
                        },
                        {
                            "start": 375,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 780
                        },
                        {
                            "start": 781,
                            "end": 997
                        },
                        {
                            "start": 1000,
                            "end": 1108
                        },
                        {
                            "start": 1109,
                            "end": 1299
                        },
                        {
                            "start": 1300,
                            "end": 1472
                        },
                        {
                            "start": 1473,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1899
                        },
                        {
                            "start": 1902,
                            "end": 2017
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1262,
                            "end": 1279,
                            "matchedPaperCorpusId": "256390607"
                        },
                        {
                            "start": 1603,
                            "end": 1619,
                            "matchedPaperCorpusId": "258865989"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7275390625
                }
            ],
            "relevance_judgement": 0.7275390625,
            "relevance_judgment_input_expanded": "# Title: CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport Alignment for Language Models with Different Tokenizers\n# Venue: arXiv.org\n# Authors: Anh Duc Le, Tu Vu, Nam Le Hai, Nguyen Thi Ngoc Diep, L. Van, Trung Le, T. Nguyen\n## Abstract\nLarge Language Models (LLMs) achieve state-of-the-art performance across various NLP tasks but face deployment challenges due to high computational costs and memory constraints. Knowledge distillation (KD) is a promising solution, transferring knowledge from large teacher models to smaller student models. However, existing KD methods often assume shared vocabularies and tokenizers, limiting their flexibility. While approaches like Universal Logit Distillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address vocabulary mismatches, they overlook the critical \\textbf{reasoning-aware distillation} aspect. To bridge this gap, we propose CoT2Align a universal KD framework that integrates Chain-of-Thought (CoT) augmentation and introduces Cross-CoT Alignment to enhance reasoning transfer. Additionally, we extend Optimal Transport beyond token-wise alignment to a sequence-level and layer-wise alignment approach that adapts to varying sequence lengths while preserving contextual integrity. Comprehensive experiments demonstrate that CoT2Align outperforms existing KD methods across different vocabulary settings, improving reasoning capabilities and robustness in domain-specific tasks.\n## Introduction\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing (NLP) tasks (Achiam et al., 2023;Touvron et al., 2023;Jiang et al., 2023Jiang et al., , 2024;;Guo et al., 2025). However, their deployment in real-world applications is often hindered by high computational costs, memory constraints, and latency issues. These limitations pose significant challenges to deploying LLMs efficiently on resource-constrained devices like mobile phones and IoT devices. Knowledge distillation (KD) (Hinton, 2015) has emerged as a promising solution to this challenge by transferring knowledge from a large teacher model to a more compact student model, thereby retaining essential performance while reducing computational overhead. Conventional KD approaches generally seek to align the output distributions of teacher and student models through methods like Kullback-Leibler (KL) divergence (Zhang et al., 2023;Hsieh et al., 2023;Ko et al., 2024). \n\nFor LLMs, knowledge distillation can be categorized into two main approaches: black-box KD and white-box KD. In black-box KD, the student model learns by mimicking the teacher model's outputs, as it has no access to the teacher's internal structure or variables (Fu et al., 2023;Kim and Rush, 2016). In contrast, white-box KD enables the student model to utilize the teacher model's architecture and variables while constructing regularization constraints during training. Theoretically, this approach facilitates more comprehensive knowledge transfer, leading to superior performance (Wen et al., 2023;Gu et al., 2024;Ko et al., 2024). However, a fundamental limitation of these approaches is that they assume both models share the same vocabulary and tokenizer, a requirement that is increasingly impractical given the diversity of architectures and tokenization schemes used in contemporary LLMs. \n\nSeveral recent studies have attempted to address this issue by enabling KD across models with different tokenizers.",
            "reference_string": "[276575418 | Le et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Bring the Power of Diffusion Model to Defect Detection",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.13845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2317142609",
                    "name": "Xuyi Yu"
                }
            ],
            "abstract": "Due to the high complexity and technical requirements of industrial production processes, surface defects will inevitably appear, which seriously affects the quality of products. Although existing lightweight detection networks are highly efficient, they are susceptible to false or missed detection of non-salient defects due to the lack of semantic information. In contrast, the diffusion model can generate higher-order semantic representations in the denoising process. Therefore, the aim of this paper is to incorporate the higher-order modelling capability of the diffusion model into the detection model, so as to better assist in the classification and localization of difficult targets. First, the denoising diffusion probabilistic model (DDPM) is pre-trained to extract the features of denoising process to construct as a feature repository. In particular, to avoid the potential bottleneck of memory caused by the dataloader loading high-dimensional features, a residual convolutional variational auto-encoder (ResVAE) is designed to further compress the feature repository. The image is fed into both image backbone and feature repository for feature extraction and querying respectively. The queried latent features are reconstructed and filtered to obtain high-dimensional DDPM features. A dynamic cross-fusion method is proposed to fully refine the contextual features of DDPM to optimize the detection model. Finally, we employ knowledge distillation to migrate the higher-order modelling capabilities back into the lightweight baseline model without additional efficiency cost. Experiment results demonstrate that our method achieves competitive results on several industrial datasets.",
            "corpus_id": 271956980,
            "sentences": [
                {
                    "corpus_id": "271956980",
                    "title": "Bring the Power of Diffusion Model to Defect Detection",
                    "text": "Large-scale deep models have achieved remarkable success, but their computational complexity and massive storage requirements make real-time deployment a challenge, especially on resource-constrained devices such as video surveillance and self-driving cars. \n\nKnowledge distillation is a classical approach of model compression and acceleration that effectively learns small student models from large teacher models [18]. In knowledge distillation, small student models are usually supervised with the assistance of large teacher models, allowing the student models to mimic the teacher models for a competitive or even superior performance. Knowledge distillation is similar to the way humans learn, and the key issue in distillation is how to better transfer knowledge from large teacher models to small student models. Inspired by this, recent methods to knowledge distillation have been extended to teacher-student learning [19], mutual learning [20], self-learning [21] and so on. Most of the extensions to knowledge distillation focus on compressing deep neural networks. The generated lightweight student networks can be easily deployed in applications such as visual recognition, speech recognition and natural language processing (NLP). \n\nIn addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled [22]. We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student.",
                    "score": 0.552539954710405,
                    "section_title": "B. Knowledge Distillation",
                    "char_start_offset": 7189,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 257
                        },
                        {
                            "start": 260,
                            "end": 421
                        },
                        {
                            "start": 422,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 821
                        },
                        {
                            "start": 822,
                            "end": 985
                        },
                        {
                            "start": 986,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1245
                        },
                        {
                            "start": 1248,
                            "end": 1451
                        },
                        {
                            "start": 1452,
                            "end": 1564
                        },
                        {
                            "start": 1565,
                            "end": 1647
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 416,
                            "end": 420,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 928,
                            "end": 932,
                            "matchedPaperCorpusId": "259697005"
                        },
                        {
                            "start": 950,
                            "end": 954,
                            "matchedPaperCorpusId": "249145972"
                        },
                        {
                            "start": 970,
                            "end": 974,
                            "matchedPaperCorpusId": "251196646"
                        },
                        {
                            "start": 1446,
                            "end": 1450,
                            "matchedPaperCorpusId": "212908749"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7265625
                }
            ],
            "relevance_judgement": 0.7265625,
            "relevance_judgment_input_expanded": "# Title: Bring the Power of Diffusion Model to Defect Detection\n# Venue: arXiv.org\n# Authors: Xuyi Yu\n## Abstract\nDue to the high complexity and technical requirements of industrial production processes, surface defects will inevitably appear, which seriously affects the quality of products. Although existing lightweight detection networks are highly efficient, they are susceptible to false or missed detection of non-salient defects due to the lack of semantic information. In contrast, the diffusion model can generate higher-order semantic representations in the denoising process. Therefore, the aim of this paper is to incorporate the higher-order modelling capability of the diffusion model into the detection model, so as to better assist in the classification and localization of difficult targets. First, the denoising diffusion probabilistic model (DDPM) is pre-trained to extract the features of denoising process to construct as a feature repository. In particular, to avoid the potential bottleneck of memory caused by the dataloader loading high-dimensional features, a residual convolutional variational auto-encoder (ResVAE) is designed to further compress the feature repository. The image is fed into both image backbone and feature repository for feature extraction and querying respectively. The queried latent features are reconstructed and filtered to obtain high-dimensional DDPM features. A dynamic cross-fusion method is proposed to fully refine the contextual features of DDPM to optimize the detection model. Finally, we employ knowledge distillation to migrate the higher-order modelling capabilities back into the lightweight baseline model without additional efficiency cost. Experiment results demonstrate that our method achieves competitive results on several industrial datasets.\n## B. Knowledge Distillation\nLarge-scale deep models have achieved remarkable success, but their computational complexity and massive storage requirements make real-time deployment a challenge, especially on resource-constrained devices such as video surveillance and self-driving cars. \n\nKnowledge distillation is a classical approach of model compression and acceleration that effectively learns small student models from large teacher models [18]. In knowledge distillation, small student models are usually supervised with the assistance of large teacher models, allowing the student models to mimic the teacher models for a competitive or even superior performance. Knowledge distillation is similar to the way humans learn, and the key issue in distillation is how to better transfer knowledge from large teacher models to small student models. Inspired by this, recent methods to knowledge distillation have been extended to teacher-student learning [19], mutual learning [20], self-learning [21] and so on. Most of the extensions to knowledge distillation focus on compressing deep neural networks. The generated lightweight student networks can be easily deployed in applications such as visual recognition, speech recognition and natural language processing (NLP). \n\nIn addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled [22]. We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student.",
            "reference_string": "[271956980 | Yu | 2024 | Citations: 1]"
        },
        {
            "title": "Reinforced Multi-Teacher Selection for Knowledge Distillation",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2020,
            "reference_count": 27,
            "citation_count": 122,
            "influential_citation_count": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/17680/17487",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.06048, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "40247395",
                    "name": "Fei Yuan"
                },
                {
                    "authorId": "24962156",
                    "name": "Linjun Shou"
                },
                {
                    "authorId": "145525190",
                    "name": "J. Pei"
                },
                {
                    "authorId": "5617558",
                    "name": "Wutao Lin"
                },
                {
                    "authorId": "50175330",
                    "name": "Ming Gong"
                },
                {
                    "authorId": "1832664242",
                    "name": "Yan Fu"
                },
                {
                    "authorId": "71790825",
                    "name": "Daxin Jiang"
                }
            ],
            "abstract": "In natural language processing (NLP) tasks, slow inference speed and huge footprints in GPU usage remain the bottleneck of applying pre-trained deep models in production. As a popular method for model compression, knowledge distillation transfers knowledge from one or multiple large (teacher) models to a small (student) model. When multiple teacher models are available in distillation, the state-of-the-art methods assign a fixed weight to a teacher model in the whole distillation. Furthermore, most of the existing methods allocate an equal weight to every teacher model. In this paper, we observe that, due to the complexity of training examples and the differences in student model capability, learning differentially from teacher models can lead to better performance of student models distilled. We systematically develop a reinforced method to dynamically assign weights to teacher models for different training instances and optimize the performance of student model. Our extensive experimental results on several NLP tasks clearly verify the feasibility and effectiveness of our approach.",
            "corpus_id": 228376532,
            "sentences": [
                {
                    "corpus_id": "228376532",
                    "title": "Reinforced Multi-Teacher Selection for Knowledge Distillation",
                    "text": "weaknesses of various teacher models leaves a huge untouched space for better knowledge distillation. To make a multi-teacher approach work well, teacher models have to be diverse. Exploiting the diverse strengths of various teacher models can bring in huge advantages.\n\nIndividual teacher models may perform differently on various instances. Different models may vary in hypothesis space, optimization strategy, parameter initialization and many other factors, which result in different performance among different cases. Ideally, we want to assign different weights to different teacher models for different training instances according to their performance in individual cases.\n\nDifferentiating among teacher models is far from trivial. Surprisingly, a stronger teacher model may not necessarily lead to a better student model. As shown in Table 1 (Sun et al. (2019)), the RoBERTa-Base model performs better than the BERT-Base model on the MRPC and MNLI-mm tasks. However, the student model using three-layer transformer BERT distilled from the weaker teacher model performs better on the same tasks than the same student model distilled from the stronger teacher model. One possible reason is that the effectiveness of distillation may be bounded by the capability of the student model. A simple student model with fewer parameters may not be able to approximate a very complex teacher model, since the complex teacher model may capture finer-grained patterns in data and cause the student model to overfit in some parts of the data and under some other parts. To achieve good distillation, we have to choose teacher models matching capacities of student models.\n\nBased on the above insights, in this paper, we systematically study how to coordinate teacher models and student models in knowledge distillation. Specifically, we investigate how to assign appropriate weights to different teacher models on various training samples. To the best of our knowledge, we are the first to treat teacher models deferentially at instance level in knowledge distillation.\n\nWe formulate the teacher model selection problem under a reinforcement learning framework: the decision is made based on the characteristics of training examples and the outputs of teacher models, while the policy is learned towards maximizing the student performance as the return.\n\nTo verify the effectiveness of our approach in NLP, we conduct extensive experiments on several important tasks from the GLUE benchmark , including sentiment analysis, paraphrase similarity matching and natural language inference. Our experimental results clearly show that our reinforced",
                    "score": 0.6481762490587348,
                    "section_title": "Introduction",
                    "char_start_offset": 2006,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.708984375
                },
                {
                    "corpus_id": "228376532",
                    "title": "Reinforced Multi-Teacher Selection for Knowledge Distillation",
                    "text": "In natural language processing (NLP) tasks, slow inference speed and huge footprints in GPU usage remain the bottleneck of applying pre-trained deep models in production. As a popular method for model compression, knowledge distillation transfers knowledge from one or multiple large (teacher) models to a small (student) model. When multiple teacher models are available in distillation, the state-of-the-art methods assign a fixed weight to a teacher model in the whole distillation. Furthermore, most of the existing methods allocate an equal weight to every teacher model. In this paper, we observe that, due to the complexity of training examples and the differences in student model capability, learning differentially from teacher models can lead to better performance of student models distilled. We systematically develop a reinforced method to dynamically assign weights to teacher models for different training instances and optimize the performance of student model. Our extensive experimental results on several NLP tasks clearly verify the feasibility and effectiveness of our approach.",
                    "score": 0.5622328051024802,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64990234375
                }
            ],
            "relevance_judgement": 0.708984375,
            "relevance_judgment_input_expanded": "# Title: Reinforced Multi-Teacher Selection for Knowledge Distillation\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Fei Yuan, Linjun Shou, J. Pei, Wutao Lin, Ming Gong, Yan Fu, Daxin Jiang\n## Abstract\nIn natural language processing (NLP) tasks, slow inference speed and huge footprints in GPU usage remain the bottleneck of applying pre-trained deep models in production. As a popular method for model compression, knowledge distillation transfers knowledge from one or multiple large (teacher) models to a small (student) model. When multiple teacher models are available in distillation, the state-of-the-art methods assign a fixed weight to a teacher model in the whole distillation. Furthermore, most of the existing methods allocate an equal weight to every teacher model. In this paper, we observe that, due to the complexity of training examples and the differences in student model capability, learning differentially from teacher models can lead to better performance of student models distilled. We systematically develop a reinforced method to dynamically assign weights to teacher models for different training instances and optimize the performance of student model. Our extensive experimental results on several NLP tasks clearly verify the feasibility and effectiveness of our approach.\n## Introduction\nweaknesses of various teacher models leaves a huge untouched space for better knowledge distillation. To make a multi-teacher approach work well, teacher models have to be diverse. Exploiting the diverse strengths of various teacher models can bring in huge advantages.\n\nIndividual teacher models may perform differently on various instances. Different models may vary in hypothesis space, optimization strategy, parameter initialization and many other factors, which result in different performance among different cases. Ideally, we want to assign different weights to different teacher models for different training instances according to their performance in individual cases.\n\nDifferentiating among teacher models is far from trivial. Surprisingly, a stronger teacher model may not necessarily lead to a better student model. As shown in Table 1 (Sun et al. (2019)), the RoBERTa-Base model performs better than the BERT-Base model on the MRPC and MNLI-mm tasks. However, the student model using three-layer transformer BERT distilled from the weaker teacher model performs better on the same tasks than the same student model distilled from the stronger teacher model. One possible reason is that the effectiveness of distillation may be bounded by the capability of the student model. A simple student model with fewer parameters may not be able to approximate a very complex teacher model, since the complex teacher model may capture finer-grained patterns in data and cause the student model to overfit in some parts of the data and under some other parts. To achieve good distillation, we have to choose teacher models matching capacities of student models.\n\nBased on the above insights, in this paper, we systematically study how to coordinate teacher models and student models in knowledge distillation. Specifically, we investigate how to assign appropriate weights to different teacher models on various training samples. To the best of our knowledge, we are the first to treat teacher models deferentially at instance level in knowledge distillation.\n\nWe formulate the teacher model selection problem under a reinforcement learning framework: the decision is made based on the characteristics of training examples and the outputs of teacher models, while the policy is learned towards maximizing the student performance as the return.\n\nTo verify the effectiveness of our approach in NLP, we conduct extensive experiments on several important tasks from the GLUE benchmark , including sentiment analysis, paraphrase similarity matching and natural language inference. Our experimental results clearly show that our reinforced",
            "reference_string": "[228376532 | Yuan et al. | 2020 | Citations: 122]"
        },
        {
            "title": "PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 8,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.02886, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2288852433",
                    "name": "Rongzhi Zhang"
                },
                {
                    "authorId": "2266463492",
                    "name": "Jiaming Shen"
                },
                {
                    "authorId": "2239381730",
                    "name": "Tianqi Liu"
                },
                {
                    "authorId": "2266420540",
                    "name": "Haorui Wang"
                },
                {
                    "authorId": "2266819166",
                    "name": "Zhen Qin"
                },
                {
                    "authorId": "2304748093",
                    "name": "Feng Han"
                },
                {
                    "authorId": "2239559694",
                    "name": "Jialu Liu"
                },
                {
                    "authorId": "2282531735",
                    "name": "Simon Baumgartner"
                },
                {
                    "authorId": "1815447",
                    "name": "Michael Bendersky"
                },
                {
                    "authorId": "2305503379",
                    "name": "Chao Zhang"
                }
            ],
            "abstract": "Large Language Models (LLMs) have exhibited impressive capabilities in various tasks, yet their vast parameter sizes restrict their applicability in resource-constrained settings. Knowledge distillation (KD) offers a viable solution by transferring expertise from large teacher models to compact student models. However, traditional KD techniques face specific challenges when applied to LLMs, including restricted access to LLM outputs, significant teacher-student capacity gaps, and the inherited mis-calibration issue. In this work, we present PLaD, a novel preference-based LLM distillation framework. PLaD exploits the teacher-student capacity discrepancy to generate pseudo-preference pairs where teacher outputs are preferred over student outputs. Then, PLaD leverages a ranking loss to re-calibrate student's estimation of sequence likelihood, which steers the student's focus towards understanding the relative quality of outputs instead of simply imitating the teacher. PLaD bypasses the need for access to teacher LLM's internal states, tackles the student's expressivity limitations, and mitigates the student mis-calibration issue. Through extensive experiments on two sequence generation tasks and with various LLMs, we demonstrate the effectiveness of our proposed PLaD framework.",
            "corpus_id": 270257777,
            "sentences": [
                {
                    "corpus_id": "270257777",
                    "title": "PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs",
                    "text": "Large language models (LLMs) have shown remarkable abilities across a wide range of tasks (OpenAI, 2022;Anil et al., 2023).However, their huge parameter sizes and computational requirements pose significant challenges for practical deployment, especially in environments with limited resources.Knowledge distillation (KD) has emerged as a technique for addressing these challenges by transferring insights from a large, sophisticated teacher model to a compact student model with reduced memory footprints and inference costs.The seminal work (Hinton et al., 2015) proposes to train a student model to match the output class distribution of the teacher model.Kim and Rush (2016) further extends this idea to the sequence level and teaches the student to directly produce teachers' decoded sequences.Another line of work (Jiao et al., 2019;Wang et al., 2020) seeks to align the student model's intermediate-layer representations with the teacher's.All these approaches employ a teacher-forcing strategy, training the student to fully match the outputs or representations of the teacher model.\n\nApplying conventional KD methods to LLMs presents several significant challenges.First, those LLM teachers are typically only available through API calls.The absence of direct access to the full output logits or internal states of LLM teachers hinders the implementation of traditional distillation techniques.Second, the capacity gap between the student model and LLM teachers becomes significantly larger compared to the previous instances when a relatively smaller teacher model was employed.This disparity exacerbates the student model's limited ability to fully match the teacher LLM's output distribution.Third, as LLMs increase in size, they often encounter a mis-calibration issue (Zhao et al., 2023) where sequences that are highly likely according to the model don't necessarily exhibit high quality for target tasks.Consequently, when a student model is trained to mimic these outputs from the teacher LLM, arXiv:2406.02886v2[cs.CL] 6 Jun 2024 it inherits this mis-calibration, leading to suboptimal performance.",
                    "score": 0.5028715798374042,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 123,
                            "end": 294
                        },
                        {
                            "start": 294,
                            "end": 526
                        },
                        {
                            "start": 526,
                            "end": 659
                        },
                        {
                            "start": 659,
                            "end": 799
                        },
                        {
                            "start": 799,
                            "end": 947
                        },
                        {
                            "start": 947,
                            "end": 1091
                        },
                        {
                            "start": 1093,
                            "end": 1174
                        },
                        {
                            "start": 1174,
                            "end": 1247
                        },
                        {
                            "start": 1247,
                            "end": 1403
                        },
                        {
                            "start": 1403,
                            "end": 1588
                        },
                        {
                            "start": 1588,
                            "end": 1704
                        },
                        {
                            "start": 1704,
                            "end": 1920
                        },
                        {
                            "start": 1920,
                            "end": 2029
                        },
                        {
                            "start": 2029,
                            "end": 2116
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 839,
                            "end": 857,
                            "matchedPaperCorpusId": "211296536"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.708984375
                }
            ],
            "relevance_judgement": 0.708984375,
            "relevance_judgment_input_expanded": "# Title: PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Haorui Wang, Zhen Qin, Feng Han, Jialu Liu, Simon Baumgartner, Michael Bendersky, Chao Zhang\n## Abstract\nLarge Language Models (LLMs) have exhibited impressive capabilities in various tasks, yet their vast parameter sizes restrict their applicability in resource-constrained settings. Knowledge distillation (KD) offers a viable solution by transferring expertise from large teacher models to compact student models. However, traditional KD techniques face specific challenges when applied to LLMs, including restricted access to LLM outputs, significant teacher-student capacity gaps, and the inherited mis-calibration issue. In this work, we present PLaD, a novel preference-based LLM distillation framework. PLaD exploits the teacher-student capacity discrepancy to generate pseudo-preference pairs where teacher outputs are preferred over student outputs. Then, PLaD leverages a ranking loss to re-calibrate student's estimation of sequence likelihood, which steers the student's focus towards understanding the relative quality of outputs instead of simply imitating the teacher. PLaD bypasses the need for access to teacher LLM's internal states, tackles the student's expressivity limitations, and mitigates the student mis-calibration issue. Through extensive experiments on two sequence generation tasks and with various LLMs, we demonstrate the effectiveness of our proposed PLaD framework.\n## Introduction\nLarge language models (LLMs) have shown remarkable abilities across a wide range of tasks (OpenAI, 2022;Anil et al., 2023).However, their huge parameter sizes and computational requirements pose significant challenges for practical deployment, especially in environments with limited resources.Knowledge distillation (KD) has emerged as a technique for addressing these challenges by transferring insights from a large, sophisticated teacher model to a compact student model with reduced memory footprints and inference costs.The seminal work (Hinton et al., 2015) proposes to train a student model to match the output class distribution of the teacher model.Kim and Rush (2016) further extends this idea to the sequence level and teaches the student to directly produce teachers' decoded sequences.Another line of work (Jiao et al., 2019;Wang et al., 2020) seeks to align the student model's intermediate-layer representations with the teacher's.All these approaches employ a teacher-forcing strategy, training the student to fully match the outputs or representations of the teacher model.\n\nApplying conventional KD methods to LLMs presents several significant challenges.First, those LLM teachers are typically only available through API calls.The absence of direct access to the full output logits or internal states of LLM teachers hinders the implementation of traditional distillation techniques.Second, the capacity gap between the student model and LLM teachers becomes significantly larger compared to the previous instances when a relatively smaller teacher model was employed.This disparity exacerbates the student model's limited ability to fully match the teacher LLM's output distribution.Third, as LLMs increase in size, they often encounter a mis-calibration issue (Zhao et al., 2023) where sequences that are highly likely according to the model don't necessarily exhibit high quality for target tasks.Consequently, when a student model is trained to mimic these outputs from the teacher LLM, arXiv:2406.02886v2[cs.CL] 6 Jun 2024 it inherits this mis-calibration, leading to suboptimal performance.",
            "reference_string": "[270257777 | Zhang et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation on Language Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.14528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279961107",
                    "name": "Xiao Cui"
                },
                {
                    "authorId": "2336255303",
                    "name": "Mo Zhu"
                },
                {
                    "authorId": "2267903811",
                    "name": "Yulei Qin"
                },
                {
                    "authorId": "2336262172",
                    "name": "Liang Xie"
                },
                {
                    "authorId": "38272296",
                    "name": "Wen-gang Zhou"
                },
                {
                    "authorId": "2210048071",
                    "name": "Houqiang Li"
                }
            ],
            "abstract": "Knowledge distillation (KD) has become a prevalent technique for compressing large language models (LLMs). Existing KD methods are constrained by the need for identical tokenizers (i.e., vocabularies) between teacher and student models, limiting their versatility in handling LLMs of different architecture families. In this paper, we introduce the Multi-Level Optimal Transport (MultiLevelOT), a novel approach that advances the optimal transport for universal cross-tokenizer knowledge distillation. Our method aligns the logit distributions of the teacher and the student at both token and sequence levels using diverse cost matrices, eliminating the need for dimensional or token-by-token correspondence. At the token level, MultiLevelOT integrates both global and local information by jointly optimizing all tokens within a sequence to enhance robustness. At the sequence level, we efficiently capture complex distribution structures of logits via the Sinkhorn distance, which approximates the Wasserstein distance for divergence measures. Extensive experiments on tasks such as extractive QA, generative QA, and summarization demonstrate that the MultiLevelOT outperforms state-of-the-art cross-tokenizer KD methods under various settings. Our approach is robust to different student and teacher models across model families, architectures, and parameter sizes.",
            "corpus_id": 274859280,
            "sentences": [],
            "relevance_judgement": 0.708984375,
            "relevance_judgment_input_expanded": "# Title: Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation on Language Models\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Xiao Cui, Mo Zhu, Yulei Qin, Liang Xie, Wen-gang Zhou, Houqiang Li\n## Abstract\nKnowledge distillation (KD) has become a prevalent technique for compressing large language models (LLMs). Existing KD methods are constrained by the need for identical tokenizers (i.e., vocabularies) between teacher and student models, limiting their versatility in handling LLMs of different architecture families. In this paper, we introduce the Multi-Level Optimal Transport (MultiLevelOT), a novel approach that advances the optimal transport for universal cross-tokenizer knowledge distillation. Our method aligns the logit distributions of the teacher and the student at both token and sequence levels using diverse cost matrices, eliminating the need for dimensional or token-by-token correspondence. At the token level, MultiLevelOT integrates both global and local information by jointly optimizing all tokens within a sequence to enhance robustness. At the sequence level, we efficiently capture complex distribution structures of logits via the Sinkhorn distance, which approximates the Wasserstein distance for divergence measures. Extensive experiments on tasks such as extractive QA, generative QA, and summarization demonstrate that the MultiLevelOT outperforms state-of-the-art cross-tokenizer KD methods under various settings. Our approach is robust to different student and teacher models across model families, architectures, and parameter sizes.\n",
            "reference_string": "[274859280 | Cui et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Towards Oracle Knowledge Distillation with Neural Architecture Search",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2019,
            "reference_count": 47,
            "citation_count": 44,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/5866/5722",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.13019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2111623202",
                    "name": "Minsoo Kang"
                },
                {
                    "authorId": "8511875",
                    "name": "Jonghwan Mun"
                },
                {
                    "authorId": "40030651",
                    "name": "Bohyung Han"
                }
            ],
            "abstract": "We present a novel framework of knowledge distillation that is capable of learning powerful and efficient student models from ensemble teacher networks. Our approach addresses the inherent model capacity issue between teacher and student and aims to maximize benefit from teacher models during distillation by reducing their capacity gap. Specifically, we employ a neural architecture search technique to augment useful structures and operations, where the searched network is appropriate for knowledge distillation towards student models and free from sacrificing its performance by fixing the network capacity. We also introduce an oracle knowledge distillation loss to facilitate model search and distillation using an ensemble-based teacher model, where a student network is learned to imitate oracle performance of the teacher. We perform extensive experiments on the image classification datasets\u2014CIFAR-100 and TinyImageNet\u2014using various networks. We also show that searching for a new student model is effective in both accuracy and memory size and that the searched models often outperform their teacher models thanks to neural architecture search with oracle knowledge distillation.",
            "corpus_id": 208513309,
            "sentences": [
                {
                    "corpus_id": "208513309",
                    "title": "Towards Oracle Knowledge Distillation with Neural Architecture Search",
                    "text": "This is partly because a large gap in model capacity between student and teacher hinders learning process of KD as discussed in (Mirzadeh et al. 2019), and the simple objective function to fit the representations of the teacher given by model averaging is not effective to take full advantage of teacher models. In other words, the limited capacity in the student network becomes a bottleneck of KD, which implies that increasing capacity of student models would be beneficial to reduce the performance gap between teacher and student.\n\nWe propose an advanced framework for knowledge distillation from ensemble models, which aims to maximize accuracy and efficiency of student networks at the same time. Contrary to the existing methods assuming that a student model is fixed, we adapt its structure and size, and make it more appropriate for knowledge distillation by alleviating the model capacity gap issue. We also address how to effectively extract information from a teacher especially when the teacher is an ensemble of multiple models. To this end, students are made to learn the most accurate information from teacher, which is realized by transferring knowledge from the optimal ensemble combination for each example. We call this strategy Oracle knowledge Distillation (OD), which encourages student models to achieve oracle accuracy of ensemble teacher models. Since the inherent model capacity gap incurs critical challenges in KD and OD makes the capacity gap larger, we incorporate neural architecture search with oracle knowledge distillation; this strategy facilitates to identify the optimal student model with sufficient capacity, which is well-suited for distillation. In practice, our algorithm searches for a slightly larger model than the backbone student network for effective knowledge distillation, reduces the model capacity gap between student and teacher, and achieves competitive accuracy of the student model. The main contributions of our work are summarized as follows:\n\n\u2022 We propose a novel framework for knowledge distillation by incorporating neural architecture search. The proposed algorithm addresses capacity issue in KD and aims to identify the optimal structures and operations with adaptive model sizes.\n\n\u2022 Our algorithm introduces a novel oracle knowledge distillation loss, which is particularly useful for an ensemble teacher model. We claim that the student networks mimicking oracle predictions have a potential for achieving higher accuracy than the teacher especially when combined with neural architecture search.\n\n\u2022 We demonstrate outstanding performance of the proposed method in diverse settings. We also make a comprehensive analysis about knowledge distillation from ensemble teacher",
                    "score": 0.6176037929029825,
                    "section_title": "Introduction",
                    "char_start_offset": 2110,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70263671875
                }
            ],
            "relevance_judgement": 0.70263671875,
            "relevance_judgment_input_expanded": "# Title: Towards Oracle Knowledge Distillation with Neural Architecture Search\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Minsoo Kang, Jonghwan Mun, Bohyung Han\n## Abstract\nWe present a novel framework of knowledge distillation that is capable of learning powerful and efficient student models from ensemble teacher networks. Our approach addresses the inherent model capacity issue between teacher and student and aims to maximize benefit from teacher models during distillation by reducing their capacity gap. Specifically, we employ a neural architecture search technique to augment useful structures and operations, where the searched network is appropriate for knowledge distillation towards student models and free from sacrificing its performance by fixing the network capacity. We also introduce an oracle knowledge distillation loss to facilitate model search and distillation using an ensemble-based teacher model, where a student network is learned to imitate oracle performance of the teacher. We perform extensive experiments on the image classification datasets\u2014CIFAR-100 and TinyImageNet\u2014using various networks. We also show that searching for a new student model is effective in both accuracy and memory size and that the searched models often outperform their teacher models thanks to neural architecture search with oracle knowledge distillation.\n## Introduction\nThis is partly because a large gap in model capacity between student and teacher hinders learning process of KD as discussed in (Mirzadeh et al. 2019), and the simple objective function to fit the representations of the teacher given by model averaging is not effective to take full advantage of teacher models. In other words, the limited capacity in the student network becomes a bottleneck of KD, which implies that increasing capacity of student models would be beneficial to reduce the performance gap between teacher and student.\n\nWe propose an advanced framework for knowledge distillation from ensemble models, which aims to maximize accuracy and efficiency of student networks at the same time. Contrary to the existing methods assuming that a student model is fixed, we adapt its structure and size, and make it more appropriate for knowledge distillation by alleviating the model capacity gap issue. We also address how to effectively extract information from a teacher especially when the teacher is an ensemble of multiple models. To this end, students are made to learn the most accurate information from teacher, which is realized by transferring knowledge from the optimal ensemble combination for each example. We call this strategy Oracle knowledge Distillation (OD), which encourages student models to achieve oracle accuracy of ensemble teacher models. Since the inherent model capacity gap incurs critical challenges in KD and OD makes the capacity gap larger, we incorporate neural architecture search with oracle knowledge distillation; this strategy facilitates to identify the optimal student model with sufficient capacity, which is well-suited for distillation. In practice, our algorithm searches for a slightly larger model than the backbone student network for effective knowledge distillation, reduces the model capacity gap between student and teacher, and achieves competitive accuracy of the student model. The main contributions of our work are summarized as follows:\n\n\u2022 We propose a novel framework for knowledge distillation by incorporating neural architecture search. The proposed algorithm addresses capacity issue in KD and aims to identify the optimal structures and operations with adaptive model sizes.\n\n\u2022 Our algorithm introduces a novel oracle knowledge distillation loss, which is particularly useful for an ensemble teacher model. We claim that the student networks mimicking oracle predictions have a potential for achieving higher accuracy than the teacher especially when combined with neural architecture search.\n\n\u2022 We demonstrate outstanding performance of the proposed method in diverse settings. We also make a comprehensive analysis about knowledge distillation from ensemble teacher",
            "reference_string": "[208513309 | Kang et al. | 2019 | Citations: 44]"
        },
        {
            "title": "Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 76,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.11451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1606040932",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "2258962279",
                    "name": "Chenxin An"
                },
                {
                    "authorId": "2238157276",
                    "name": "Weizhu Chen"
                },
                {
                    "authorId": "2259869648",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "50462546",
                    "name": "Pengcheng He"
                }
            ],
            "abstract": "Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge (encompassing detection, editing, and merging), there remains an ambiguous understanding regarding their transferability across models with varying scales. In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we employ sensitivity-based techniques to extract and align knowledge-specific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales. Project website: https://maszhongming.github.io/ParaKnowTransfer.",
            "corpus_id": 264172668,
            "sentences": [
                {
                    "corpus_id": "264172668",
                    "title": "Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective",
                    "text": "The parametric knowledge transfer paradigm in this paper is fundamentally distinct from traditional distillation methods, characterized by the following differences: \n\n\u2022 Purpose and Focus: Rather than proposing better distillation methods, our study is focused on exploring the transferability of implicit knowledge embedded in static parameters. \n\nWhile prior research has explored the detectability and editability of parametric knowledge, its transferability remains less explored. Our experiments provide empirical evidence in the knowledge transfer scenario, where student models show improved performance after receiving task-specific knowledge from the teacher model, as shown in Tables 1 and 2. \u2022 Process and Efficiency: Our approach differs from standard knowledge distillation, which typically requires fine-tuning or the direct involvement of the teacher model in student model training -a computationally intensive process. In contrast, our parametric knowledge transfer involves extracting task-specific parameters from the vanilla teacher model and integrating them into the student model. This method, requiring only 32 inferences from the teacher model, offers a significant efficiency advantage, especially in the context of LLMs. \n\nDespite these differences, we provide the results of the distillation methods as a reference in the knowledge transfer scenario. Table 1 contains the results for LLaMA-1 13B to 7B and 30B to 7B, where KD refers to vanilla knowledge distillation (Hinton et al., 2015) and SeqKD is sequence-level knowledge distillation (Kim & Rush, 2016). All models are fine-tuned with LoRA, using identical training data and hyperparameters. In this paper, we attempt to explore the evidence that knowledge in static parameters can be transferred between different LLMs, and knowledge transfer is the scenario in which we find and provide empirical evidence. Our focus is not on proposing to find better methods in this scenario. \n\nA.5 TRADE-OFF DISCUSSION BETWEEN PERFORMANCE AND RUNNING COST Considering that users may have varying computational resources in practical application scenarios, we discuss the trade-offs between performance and running costs as follows: \n\nExperimental details: \n\n\u2022 We conduct comparisons for two knowledge transfers: LLaMA-1 13B to 7B and 30B to 7B.",
                    "score": 0.5415166298163354,
                    "section_title": "A.4 COMPARISON WITH DISTILLATION METHODS",
                    "char_start_offset": 30819,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 165
                        },
                        {
                            "start": 168,
                            "end": 346
                        },
                        {
                            "start": 349,
                            "end": 484
                        },
                        {
                            "start": 485,
                            "end": 935
                        },
                        {
                            "start": 936,
                            "end": 1103
                        },
                        {
                            "start": 1104,
                            "end": 1247
                        },
                        {
                            "start": 1250,
                            "end": 1378
                        },
                        {
                            "start": 1379,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1675
                        },
                        {
                            "start": 1676,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 1963
                        },
                        {
                            "start": 1966,
                            "end": 2203
                        },
                        {
                            "start": 2206,
                            "end": 2227
                        },
                        {
                            "start": 2230,
                            "end": 2316
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1568,
                            "end": 1586,
                            "matchedPaperCorpusId": "8451212"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69921875
                }
            ],
            "relevance_judgement": 0.69921875,
            "relevance_judgment_input_expanded": "# Title: Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective\n# Venue: International Conference on Learning Representations\n# Authors: Ming Zhong, Chenxin An, Weizhu Chen, Jiawei Han, Pengcheng He\n## Abstract\nLarge Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge (encompassing detection, editing, and merging), there remains an ambiguous understanding regarding their transferability across models with varying scales. In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we employ sensitivity-based techniques to extract and align knowledge-specific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales. Project website: https://maszhongming.github.io/ParaKnowTransfer.\n## A.4 COMPARISON WITH DISTILLATION METHODS\nThe parametric knowledge transfer paradigm in this paper is fundamentally distinct from traditional distillation methods, characterized by the following differences: \n\n\u2022 Purpose and Focus: Rather than proposing better distillation methods, our study is focused on exploring the transferability of implicit knowledge embedded in static parameters. \n\nWhile prior research has explored the detectability and editability of parametric knowledge, its transferability remains less explored. Our experiments provide empirical evidence in the knowledge transfer scenario, where student models show improved performance after receiving task-specific knowledge from the teacher model, as shown in Tables 1 and 2. \u2022 Process and Efficiency: Our approach differs from standard knowledge distillation, which typically requires fine-tuning or the direct involvement of the teacher model in student model training -a computationally intensive process. In contrast, our parametric knowledge transfer involves extracting task-specific parameters from the vanilla teacher model and integrating them into the student model. This method, requiring only 32 inferences from the teacher model, offers a significant efficiency advantage, especially in the context of LLMs. \n\nDespite these differences, we provide the results of the distillation methods as a reference in the knowledge transfer scenario. Table 1 contains the results for LLaMA-1 13B to 7B and 30B to 7B, where KD refers to vanilla knowledge distillation (Hinton et al., 2015) and SeqKD is sequence-level knowledge distillation (Kim & Rush, 2016). All models are fine-tuned with LoRA, using identical training data and hyperparameters. In this paper, we attempt to explore the evidence that knowledge in static parameters can be transferred between different LLMs, and knowledge transfer is the scenario in which we find and provide empirical evidence. Our focus is not on proposing to find better methods in this scenario. \n\nA.5 TRADE-OFF DISCUSSION BETWEEN PERFORMANCE AND RUNNING COST Considering that users may have varying computational resources in practical application scenarios, we discuss the trade-offs between performance and running costs as follows: \n\nExperimental details: \n\n\u2022 We conduct comparisons for two knowledge transfers: LLaMA-1 13B to 7B and 30B to 7B.",
            "reference_string": "[264172668 | Zhong et al. | 2023 | Citations: 12]"
        },
        {
            "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning",
            "venue": "IEEE Access",
            "year": 2023,
            "reference_count": 176,
            "citation_count": 43,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10433185.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.11932, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135267479",
                    "name": "Luigi Capogrosso"
                },
                {
                    "authorId": "1396330675",
                    "name": "Federico Cunico"
                },
                {
                    "authorId": "1780197",
                    "name": "D. Cheng"
                },
                {
                    "authorId": "2243336023",
                    "name": "Franco Fummi"
                },
                {
                    "authorId": "2238815087",
                    "name": "Marco Cristani"
                }
            ],
            "abstract": "The emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly, we will examine the three different workflows for implementing a TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly, we propose a taxonomy that covers the learning panorama under the TinyML lens, examining in detail the different families of model optimization and design, as well as the state-of-the-art learning techniques. Thirdly, this survey will present the distinct features of hardware devices and software tools that represent the current state-of-the-art for TinyML intelligent edge applications. Finally, we discuss the challenges and future directions.",
            "corpus_id": 262084420,
            "sentences": [
                {
                    "corpus_id": "262084420",
                    "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning",
                    "text": "This technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) [84]. This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies. \n\nThe extraction of knowledge from teachers and its utilization for training student networks can be classified into three categories: response-based, feature-based, and relationbased. Specifically, response-based knowledge distillation involves mimicking the final predictions of the teacher model by capturing the neural response in the last output layer [87]. Feature-based knowledge expands upon this approach by using both the outputs of the last layer and intermediate layers to train thinner networks [81]. Finally, relation-based knowledge takes a step further by exploring the relationships between different layers or data samples in addition to the outputs of specific layers in the teacher model [86]. \n\nThe distillation schemes are also crucial for the student learning process. Depending on the training strategy, the following three different categories are presented: offline distillation, online distillation, self-distillation. Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits [80]. On the other hand, online distillation is an end-to-end approach where both the teacher and student models are updated simultaneously, making it suitable when the teacher model is not significantly larger or higher performing [85]. Finally, self-distillation is a special case of online distillation where the teacher and student networks have the same architecture [79]. \n\nIn general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy [88]. For this reason, it is widely adopted in several fields where existing models are well-performing but unable to be deployed ''as they are'' in resource-constrained hardware.",
                    "score": 0.5393138291865188,
                    "section_title": "3) Knowledge Distillation",
                    "char_start_offset": 28218,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 252
                        },
                        {
                            "start": 253,
                            "end": 397
                        },
                        {
                            "start": 398,
                            "end": 500
                        },
                        {
                            "start": 503,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 1014
                        },
                        {
                            "start": 1015,
                            "end": 1214
                        },
                        {
                            "start": 1217,
                            "end": 1292
                        },
                        {
                            "start": 1293,
                            "end": 1446
                        },
                        {
                            "start": 1447,
                            "end": 1685
                        },
                        {
                            "start": 1686,
                            "end": 1917
                        },
                        {
                            "start": 1918,
                            "end": 2057
                        },
                        {
                            "start": 2060,
                            "end": 2188
                        },
                        {
                            "start": 2189,
                            "end": 2362
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 111,
                            "end": 115,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 858,
                            "end": 862,
                            "matchedPaperCorpusId": "232104927"
                        },
                        {
                            "start": 1009,
                            "end": 1013,
                            "matchedPaperCorpusId": "235613518"
                        },
                        {
                            "start": 1209,
                            "end": 1213,
                            "matchedPaperCorpusId": "239486869"
                        },
                        {
                            "start": 1680,
                            "end": 1684,
                            "matchedPaperCorpusId": "198179767"
                        },
                        {
                            "start": 1912,
                            "end": 1916,
                            "matchedPaperCorpusId": "224914013"
                        },
                        {
                            "start": 2052,
                            "end": 2056,
                            "matchedPaperCorpusId": "214727822"
                        },
                        {
                            "start": 2183,
                            "end": 2187,
                            "matchedPaperCorpusId": "255266316"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69580078125
                }
            ],
            "relevance_judgement": 0.69580078125,
            "relevance_judgment_input_expanded": "# Title: A Machine Learning-Oriented Survey on Tiny Machine Learning\n# Venue: IEEE Access\n# Authors: Luigi Capogrosso, Federico Cunico, D. Cheng, Franco Fummi, Marco Cristani\n## Abstract\nThe emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly, we will examine the three different workflows for implementing a TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly, we propose a taxonomy that covers the learning panorama under the TinyML lens, examining in detail the different families of model optimization and design, as well as the state-of-the-art learning techniques. Thirdly, this survey will present the distinct features of hardware devices and software tools that represent the current state-of-the-art for TinyML intelligent edge applications. Finally, we discuss the challenges and future directions.\n## 3) Knowledge Distillation\nThis technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) [84]. This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies. \n\nThe extraction of knowledge from teachers and its utilization for training student networks can be classified into three categories: response-based, feature-based, and relationbased. Specifically, response-based knowledge distillation involves mimicking the final predictions of the teacher model by capturing the neural response in the last output layer [87]. Feature-based knowledge expands upon this approach by using both the outputs of the last layer and intermediate layers to train thinner networks [81]. Finally, relation-based knowledge takes a step further by exploring the relationships between different layers or data samples in addition to the outputs of specific layers in the teacher model [86]. \n\nThe distillation schemes are also crucial for the student learning process. Depending on the training strategy, the following three different categories are presented: offline distillation, online distillation, self-distillation. Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits [80]. On the other hand, online distillation is an end-to-end approach where both the teacher and student models are updated simultaneously, making it suitable when the teacher model is not significantly larger or higher performing [85]. Finally, self-distillation is a special case of online distillation where the teacher and student networks have the same architecture [79]. \n\nIn general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy [88]. For this reason, it is widely adopted in several fields where existing models are well-performing but unable to be deployed ''as they are'' in resource-constrained hardware.",
            "reference_string": "[262084420 | Capogrosso et al. | 2023 | Citations: 43]"
        },
        {
            "title": "Knowledge Distillation in Image Classification: The Impact of Datasets",
            "venue": "De Computis",
            "year": 2024,
            "reference_count": 48,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2073-431X/13/8/184/pdf?version=1721890205",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/computers13080184?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/computers13080184, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293888061",
                    "name": "Ange-Gabriel Belinga"
                },
                {
                    "authorId": "2320520399",
                    "name": "St\u00e9phane C\u00e9dric Koum\u00e9tio T\u00e9kouabou"
                },
                {
                    "authorId": "144219703",
                    "name": "M. Haziti"
                },
                {
                    "authorId": "50729883",
                    "name": "M. Hassouni"
                }
            ],
            "abstract": "As the demand for efficient and lightweight models in image classification grows, knowledge distillation has emerged as a promising technique to transfer expertise from complex teacher models to simpler student models. However, the efficacy of knowledge distillation is intricately linked to the choice of datasets used during training. Datasets are pivotal in shaping a model\u2019s learning process, influencing its ability to generalize and discriminate between diverse patterns. While considerable research has independently explored knowledge distillation and image classification, a comprehensive understanding of how different datasets impact knowledge distillation remains a critical gap. This study systematically investigates the impact of diverse datasets on knowledge distillation in image classification. By varying dataset characteristics such as size, domain specificity, and inherent biases, we aim to unravel the nuanced relationship between datasets and the efficacy of knowledge transfer. Our experiments employ a range of datasets to comprehensively explore their impact on the performance gains achieved through knowledge distillation. This study contributes valuable guidance for researchers and practitioners seeking to optimize image classification models through kno-featured applications. By elucidating the intricate interplay between dataset characteristics and knowledge distillation outcomes, our findings empower the community to make informed decisions when selecting datasets, ultimately advancing the field toward more robust and efficient model development.",
            "corpus_id": 271441660,
            "sentences": [],
            "relevance_judgement": 0.6953125,
            "relevance_judgment_input_expanded": "# Title: Knowledge Distillation in Image Classification: The Impact of Datasets\n# Venue: De Computis\n# Authors: Ange-Gabriel Belinga, St\u00e9phane C\u00e9dric Koum\u00e9tio T\u00e9kouabou, M. Haziti, M. Hassouni\n## Abstract\nAs the demand for efficient and lightweight models in image classification grows, knowledge distillation has emerged as a promising technique to transfer expertise from complex teacher models to simpler student models. However, the efficacy of knowledge distillation is intricately linked to the choice of datasets used during training. Datasets are pivotal in shaping a model\u2019s learning process, influencing its ability to generalize and discriminate between diverse patterns. While considerable research has independently explored knowledge distillation and image classification, a comprehensive understanding of how different datasets impact knowledge distillation remains a critical gap. This study systematically investigates the impact of diverse datasets on knowledge distillation in image classification. By varying dataset characteristics such as size, domain specificity, and inherent biases, we aim to unravel the nuanced relationship between datasets and the efficacy of knowledge transfer. Our experiments employ a range of datasets to comprehensively explore their impact on the performance gains achieved through knowledge distillation. This study contributes valuable guidance for researchers and practitioners seeking to optimize image classification models through kno-featured applications. By elucidating the intricate interplay between dataset characteristics and knowledge distillation outcomes, our findings empower the community to make informed decisions when selecting datasets, ultimately advancing the field toward more robust and efficient model development.\n",
            "reference_string": "[271441660 | Belinga et al. | 2024 | Citations: 0]"
        },
        {
            "title": "DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images",
            "venue": "",
            "year": 2025,
            "reference_count": 46,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.09334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316207082",
                    "name": "Sadman Sakib Alif"
                },
                {
                    "authorId": "2316207342",
                    "name": "Nasim Anzum Promise"
                },
                {
                    "authorId": "2361268094",
                    "name": "Fiaz Al Abid"
                },
                {
                    "authorId": "9748590",
                    "name": "Aniqua Nusrat Zereen"
                }
            ],
            "abstract": "Lung cancer is a leading cause of cancer-related deaths globally, where early detection and accurate diagnosis are critical for improving survival rates. While deep learning, particularly convolutional neural networks (CNNs), has revolutionized medical image analysis by detecting subtle patterns indicative of early-stage lung cancer, its adoption faces challenges. These models are often computationally expensive and require significant resources, making them unsuitable for resource constrained environments. Additionally, their lack of transparency hinders trust and broader adoption in sensitive fields like healthcare. Knowledge distillation addresses these challenges by transferring knowledge from large, complex models (teachers) to smaller, lightweight models (students). We propose a knowledge distillation-based approach for lung cancer detection, incorporating explainable AI (XAI) techniques to enhance model transparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3, and VGG16, are evaluated as teacher models. We developed and trained a lightweight student model, Distilled Custom Student Network (DCSNet) using ResNet50 as the teacher. This approach not only ensures high diagnostic performance in resource-constrained settings but also addresses transparency concerns, facilitating the adoption of AI-driven diagnostic tools in healthcare.",
            "corpus_id": 278602421,
            "sentences": [
                {
                    "corpus_id": "278602421",
                    "title": "DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images",
                    "text": "KD is widely used in recent studies, where smaller student models are trained to mimic the behavior of larger teacher models. Hinton et al. [18], showed the student learns to mimic the output logits of the teacher models. Fukuda et al. [19] proposed an ensemble KD approach, where a smaller student network is trained using a set of teacher models, such as VGG or LSTM. The student model simultaneously learns from the outputs of multiple teachers. Yoon et al. [20] introduced a similarity-preserving knowledge distillation, where a small student network is trained to replicate the output of an extensive teacher network while maintaining important similarities between data. \n\nThese methods transfer knowledge from a large teacher to a smaller student model. However, if the student is significantly less complex it may fail to capture and replicate the teacher's intricate outputs. This issue is addressed in the multi-step KD method. Mirzadeh et al. [21] utilized the Teacher Assistant Knowledge Distillation (TAKD) approach. TAKD utilizes an intermediate model, called a Teacher Assistant (TA), to bridge the gap between a large teacher model and a small student model. Training a student using the TA gives better performance as the TA is closer in size to the student and makes the transfer of knowledge effective. Saleknia et al. [22] also employed midsize assistant networks that bridge the computational gap between the teacher and student models. They use three variants of EfficientNetV2 to build the teacher, teacher assistant, and student networks. Their approach improved the student model performance on the Standford-40 dataset from 94.75% to 96.30%. A disadvantage of this method is that computational complexity increases because of the multi-step distillation process However, transferring knowledge using only output logits has certain limitations, as it solely focuses on the output distribution while completely ignoring the intermediate representations learned at various layers. Featurebased distillation strategies address this limitation by transferring knowledge from intermediate layers. Remero et al. [23] used both the output logits and the intermediate representations of the teacher model to guide the training process of the student. Additionally, they introduced extra parameters to map the student's hidden layers to the prediction of the teacher's hidden layers.",
                    "score": 0.6123164647812745,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 6891,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 369
                        },
                        {
                            "start": 370,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 676
                        },
                        {
                            "start": 679,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 884
                        },
                        {
                            "start": 885,
                            "end": 937
                        },
                        {
                            "start": 938,
                            "end": 1029
                        },
                        {
                            "start": 1030,
                            "end": 1174
                        },
                        {
                            "start": 1175,
                            "end": 1321
                        },
                        {
                            "start": 1322,
                            "end": 1457
                        },
                        {
                            "start": 1458,
                            "end": 1562
                        },
                        {
                            "start": 1563,
                            "end": 1667
                        },
                        {
                            "start": 1668,
                            "end": 2003
                        },
                        {
                            "start": 2004,
                            "end": 2116
                        },
                        {
                            "start": 2117,
                            "end": 2267
                        },
                        {
                            "start": 2268,
                            "end": 2399
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 461,
                            "end": 465,
                            "matchedPaperCorpusId": "229376103"
                        },
                        {
                            "start": 954,
                            "end": 958,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 1338,
                            "end": 1342,
                            "matchedPaperCorpusId": "268714642"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69189453125
                }
            ],
            "relevance_judgement": 0.69189453125,
            "relevance_judgment_input_expanded": "# Title: DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images\n# Venue: \n# Authors: Sadman Sakib Alif, Nasim Anzum Promise, Fiaz Al Abid, Aniqua Nusrat Zereen\n## Abstract\nLung cancer is a leading cause of cancer-related deaths globally, where early detection and accurate diagnosis are critical for improving survival rates. While deep learning, particularly convolutional neural networks (CNNs), has revolutionized medical image analysis by detecting subtle patterns indicative of early-stage lung cancer, its adoption faces challenges. These models are often computationally expensive and require significant resources, making them unsuitable for resource constrained environments. Additionally, their lack of transparency hinders trust and broader adoption in sensitive fields like healthcare. Knowledge distillation addresses these challenges by transferring knowledge from large, complex models (teachers) to smaller, lightweight models (students). We propose a knowledge distillation-based approach for lung cancer detection, incorporating explainable AI (XAI) techniques to enhance model transparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3, and VGG16, are evaluated as teacher models. We developed and trained a lightweight student model, Distilled Custom Student Network (DCSNet) using ResNet50 as the teacher. This approach not only ensures high diagnostic performance in resource-constrained settings but also addresses transparency concerns, facilitating the adoption of AI-driven diagnostic tools in healthcare.\n## Knowledge Distillation\nKD is widely used in recent studies, where smaller student models are trained to mimic the behavior of larger teacher models. Hinton et al. [18], showed the student learns to mimic the output logits of the teacher models. Fukuda et al. [19] proposed an ensemble KD approach, where a smaller student network is trained using a set of teacher models, such as VGG or LSTM. The student model simultaneously learns from the outputs of multiple teachers. Yoon et al. [20] introduced a similarity-preserving knowledge distillation, where a small student network is trained to replicate the output of an extensive teacher network while maintaining important similarities between data. \n\nThese methods transfer knowledge from a large teacher to a smaller student model. However, if the student is significantly less complex it may fail to capture and replicate the teacher's intricate outputs. This issue is addressed in the multi-step KD method. Mirzadeh et al. [21] utilized the Teacher Assistant Knowledge Distillation (TAKD) approach. TAKD utilizes an intermediate model, called a Teacher Assistant (TA), to bridge the gap between a large teacher model and a small student model. Training a student using the TA gives better performance as the TA is closer in size to the student and makes the transfer of knowledge effective. Saleknia et al. [22] also employed midsize assistant networks that bridge the computational gap between the teacher and student models. They use three variants of EfficientNetV2 to build the teacher, teacher assistant, and student networks. Their approach improved the student model performance on the Standford-40 dataset from 94.75% to 96.30%. A disadvantage of this method is that computational complexity increases because of the multi-step distillation process However, transferring knowledge using only output logits has certain limitations, as it solely focuses on the output distribution while completely ignoring the intermediate representations learned at various layers. Featurebased distillation strategies address this limitation by transferring knowledge from intermediate layers. Remero et al. [23] used both the output logits and the intermediate representations of the teacher model to guide the training process of the student. Additionally, they introduced extra parameters to map the student's hidden layers to the prediction of the teacher's hidden layers.",
            "reference_string": "[278602421 | Alif et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Class-Incremental Continual Learning Into the eXtended DER-Verse",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2022,
            "reference_count": 94,
            "citation_count": 141,
            "influential_citation_count": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/34/4359286/09891836.pdf",
                "status": "HYBRID",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.00766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51096265",
                    "name": "Matteo Boschini"
                },
                {
                    "authorId": "2123319338",
                    "name": "Lorenzo Bonicelli"
                },
                {
                    "authorId": "1429191945",
                    "name": "Pietro Buzzega"
                },
                {
                    "authorId": "51119730",
                    "name": "Angelo Porrello"
                },
                {
                    "authorId": "2175529",
                    "name": "S. Calderara"
                }
            ],
            "abstract": "The staple of human intelligence is the capability of acquiring knowledge in a continuous fashion. In stark contrast, Deep Networks forget catastrophically and, for this reason, the sub-field of Class-Incremental Continual Learning fosters methods that learn a sequence of tasks incrementally, blending sequentially-gained knowledge into a comprehensive prediction. This work aims at assessing and overcoming the pitfalls of our previous proposal Dark Experience Replay (DER), a simple and effective approach that combines rehearsal and Knowledge Distillation. Inspired by the way our minds constantly rewrite past recollections and set expectations for the future, we endow our model with the abilities to i) revise its replay memory to welcome novel information regarding past data ii) pave the way for learning yet unseen classes. We show that the application of these strategies leads to remarkable improvements; indeed, the resulting method \u2013 termed eXtended-DER (X-DER) \u2013 outperforms the state of the art on both standard benchmarks (such as CIFAR-100 and miniImageNet) and a novel one here introduced. To gain a better understanding, we further provide extensive ablation studies that corroborate and extend the findings of our previous research (e.g., the value of Knowledge Distillation and flatter minima in continual learning setups). We make our results fully reproducible; the codebase is available at https://github.com/aimagelab/mammoth.",
            "corpus_id": 245650327,
            "sentences": [
                {
                    "corpus_id": "245650327",
                    "title": "Class-Incremental Continual Learning Into the eXtended DER-Verse",
                    "text": "Knowledge Distillation (KD) [36] is a training methodology that allows transferring the knowledge of a teacher model into a separate student model. While Hinton et al. originally proposed to distillate large teachers -possibly ensembles -into smaller students, further studies revealed additional interesting properties about this technique. In particular, Furlanello et al. [37] show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student. More recently, other works [38], [39] explore an interesting variation of self-distillation that distills knowledge from the deeper layers of the network to its shallower ones to accelerate convergence and attain higher accuracy. \n\nKnowledge Distillation and Continual Learning. Distillation can be used to hinder catastrophic forgetting by appointing a previous snapshot of the model as teacher and distilling from it while new tasks are learned. Learning Without Forgetting [7] uses teacher responses to new exemplars to constrain the evolution of the student. Several other works combine distillation with rehearsal: iCaRL [11] distills the responses of the model at the previous task boundary, learning latent representations to be used in a nearest meanof-exemplars classifier; EtEIL [40], LUCIR [31] and BiC [41] focus on contrasting the prediction bias that comes from incremental classification; IL2M [30] stores additional statistics to facilitate distillation and compensate bias.",
                    "score": 0.5446771396853975,
                    "section_title": "Self-Distillation",
                    "char_start_offset": 8104,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 147
                        },
                        {
                            "start": 148,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 547
                        },
                        {
                            "start": 548,
                            "end": 777
                        },
                        {
                            "start": 780,
                            "end": 826
                        },
                        {
                            "start": 827,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1110
                        },
                        {
                            "start": 1111,
                            "end": 1538
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 28,
                            "end": 32,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 375,
                            "end": 379,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 575,
                            "end": 579,
                            "matchedPaperCorpusId": "159041406"
                        },
                        {
                            "start": 1024,
                            "end": 1027,
                            "matchedPaperCorpusId": "4853851"
                        },
                        {
                            "start": 1174,
                            "end": 1178,
                            "matchedPaperCorpusId": "206596260"
                        },
                        {
                            "start": 1349,
                            "end": 1353,
                            "matchedPaperCorpusId": "195453293"
                        },
                        {
                            "start": 1362,
                            "end": 1366,
                            "matchedPaperCorpusId": "173187918"
                        },
                        {
                            "start": 1457,
                            "end": 1461,
                            "matchedPaperCorpusId": "204923710"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69140625
                }
            ],
            "relevance_judgement": 0.69140625,
            "relevance_judgment_input_expanded": "# Title: Class-Incremental Continual Learning Into the eXtended DER-Verse\n# Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence\n# Authors: Matteo Boschini, Lorenzo Bonicelli, Pietro Buzzega, Angelo Porrello, S. Calderara\n## Abstract\nThe staple of human intelligence is the capability of acquiring knowledge in a continuous fashion. In stark contrast, Deep Networks forget catastrophically and, for this reason, the sub-field of Class-Incremental Continual Learning fosters methods that learn a sequence of tasks incrementally, blending sequentially-gained knowledge into a comprehensive prediction. This work aims at assessing and overcoming the pitfalls of our previous proposal Dark Experience Replay (DER), a simple and effective approach that combines rehearsal and Knowledge Distillation. Inspired by the way our minds constantly rewrite past recollections and set expectations for the future, we endow our model with the abilities to i) revise its replay memory to welcome novel information regarding past data ii) pave the way for learning yet unseen classes. We show that the application of these strategies leads to remarkable improvements; indeed, the resulting method \u2013 termed eXtended-DER (X-DER) \u2013 outperforms the state of the art on both standard benchmarks (such as CIFAR-100 and miniImageNet) and a novel one here introduced. To gain a better understanding, we further provide extensive ablation studies that corroborate and extend the findings of our previous research (e.g., the value of Knowledge Distillation and flatter minima in continual learning setups). We make our results fully reproducible; the codebase is available at https://github.com/aimagelab/mammoth.\n## Self-Distillation\nKnowledge Distillation (KD) [36] is a training methodology that allows transferring the knowledge of a teacher model into a separate student model. While Hinton et al. originally proposed to distillate large teachers -possibly ensembles -into smaller students, further studies revealed additional interesting properties about this technique. In particular, Furlanello et al. [37] show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student. More recently, other works [38], [39] explore an interesting variation of self-distillation that distills knowledge from the deeper layers of the network to its shallower ones to accelerate convergence and attain higher accuracy. \n\nKnowledge Distillation and Continual Learning. Distillation can be used to hinder catastrophic forgetting by appointing a previous snapshot of the model as teacher and distilling from it while new tasks are learned. Learning Without Forgetting [7] uses teacher responses to new exemplars to constrain the evolution of the student. Several other works combine distillation with rehearsal: iCaRL [11] distills the responses of the model at the previous task boundary, learning latent representations to be used in a nearest meanof-exemplars classifier; EtEIL [40], LUCIR [31] and BiC [41] focus on contrasting the prediction bias that comes from incremental classification; IL2M [30] stores additional statistics to facilitate distillation and compensate bias.",
            "reference_string": "[245650327 | Boschini et al. | 2022 | Citations: 141]"
        },
        {
            "title": "Review of Recent Distillation Studies",
            "venue": "MATEC Web of Conferences",
            "year": 2023,
            "reference_count": 25,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.matec-conferences.org/articles/matecconf/pdf/2023/09/matecconf_amme2023_01034.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1051/matecconf/202338201034?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1051/matecconf/202338201034, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2147417425",
                    "name": "Min Gao"
                }
            ],
            "abstract": "Knowledge distillation has gained a lot of interest in recent years because it allows for compressing a large deep neural network (teacher DNN) into a smaller DNN (student DNN), while maintaining its accuracy. Recent improvements have been made to knowledge distillation. One such improvement is the teaching assistant distillation method. This method involves introducing an intermediate \"teaching assistant\" model between the teacher and student. The teaching assistant is first trained to mimic the teacher, and then the student is trained to mimic the teaching assistant. This multi-step process can improve student performance. Another improvement to knowledge distillation is curriculum distillation. This method involves gradually training the student by exposing it to increasingly difficult concepts over time, similar to curriculum learning in humans. This process can help the student learn in a more stable and consistent manner. Finally, there is the mask distillation method. Here, the student is trained to specifically mimic the attention mechanisms learned by the teacher, not just the overall output of the teacher DNN. These improvements help to enhance the knowledge distillation process and enable the creation of more efficient DNNs.",
            "corpus_id": 259378480,
            "sentences": [
                {
                    "corpus_id": "259378480",
                    "title": "Review of Recent Distillation Studies",
                    "text": "The use of Simple Knowledge Distillation (SimKD) allows the compression of powerful teacher models into lightweight student models without sacrificing performance. This is achieved by reusing the discriminative classifier of the teacher model for student inference and training the student encoder by feature alignment with a single loss. A projector was also developed to help match the student encoder with the teacher classifier, making the technique applicable to a variety of teacher and student architectures. Experiments show that with the addition of the projector, the technique achieves state-of-the-art results at the cost of a reduced compression ratio. Vanilla KD aligns the logit or class projections of the two models, but the performance gap between the original teacher model and the refined student model is still large. Various methods have been proposed to overcome this problem, but they require detailed knowledge representation and optimized hyperparameters. The SimKD technique is proposed, which trains the student model by feature alignment in the previous layer of the classifier and directly replicates the teacher classifier for student inference. On a standard benchmark dataset, SimKD outperforms all state-of-the-art methods compared with various combinations of teacher-student architectures. The concept of knowledge distillation (KD), a technique for compressing knowledge from a powerful teacher model into a smaller student model, is discussed here. Transferred knowledge, usually in the form of soft targets, is thought to capture the relationships between different categories and serve as effective regularization during student training. Feature distillation is a common solution to prevent performance degradation in teacherto-student compression, which utilizes more information from the middle layer of the teacher model. One of the proposed methods, SimKD, is related to hypothetical transfer learning (HTL) and aims to close the performance gap between teachers and students on the same dataset. Here, by analyzing the limitations of the existing method and focusing on its improvement, a new method, logit distillation, is proposed, and the knowledge distillation loss function is re-represented as a weighted sum of two components, target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD).",
                    "score": 0.5165954708734831,
                    "section_title": "Simple knowledge distillation",
                    "char_start_offset": 36280,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 515
                        },
                        {
                            "start": 516,
                            "end": 665
                        },
                        {
                            "start": 666,
                            "end": 838
                        },
                        {
                            "start": 839,
                            "end": 981
                        },
                        {
                            "start": 982,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1325
                        },
                        {
                            "start": 1326,
                            "end": 1486
                        },
                        {
                            "start": 1487,
                            "end": 1678
                        },
                        {
                            "start": 1679,
                            "end": 1865
                        },
                        {
                            "start": 1866,
                            "end": 2041
                        },
                        {
                            "start": 2042,
                            "end": 2374
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68310546875
                }
            ],
            "relevance_judgement": 0.68310546875,
            "relevance_judgment_input_expanded": "# Title: Review of Recent Distillation Studies\n# Venue: MATEC Web of Conferences\n# Authors: Min Gao\n## Abstract\nKnowledge distillation has gained a lot of interest in recent years because it allows for compressing a large deep neural network (teacher DNN) into a smaller DNN (student DNN), while maintaining its accuracy. Recent improvements have been made to knowledge distillation. One such improvement is the teaching assistant distillation method. This method involves introducing an intermediate \"teaching assistant\" model between the teacher and student. The teaching assistant is first trained to mimic the teacher, and then the student is trained to mimic the teaching assistant. This multi-step process can improve student performance. Another improvement to knowledge distillation is curriculum distillation. This method involves gradually training the student by exposing it to increasingly difficult concepts over time, similar to curriculum learning in humans. This process can help the student learn in a more stable and consistent manner. Finally, there is the mask distillation method. Here, the student is trained to specifically mimic the attention mechanisms learned by the teacher, not just the overall output of the teacher DNN. These improvements help to enhance the knowledge distillation process and enable the creation of more efficient DNNs.\n## Simple knowledge distillation\nThe use of Simple Knowledge Distillation (SimKD) allows the compression of powerful teacher models into lightweight student models without sacrificing performance. This is achieved by reusing the discriminative classifier of the teacher model for student inference and training the student encoder by feature alignment with a single loss. A projector was also developed to help match the student encoder with the teacher classifier, making the technique applicable to a variety of teacher and student architectures. Experiments show that with the addition of the projector, the technique achieves state-of-the-art results at the cost of a reduced compression ratio. Vanilla KD aligns the logit or class projections of the two models, but the performance gap between the original teacher model and the refined student model is still large. Various methods have been proposed to overcome this problem, but they require detailed knowledge representation and optimized hyperparameters. The SimKD technique is proposed, which trains the student model by feature alignment in the previous layer of the classifier and directly replicates the teacher classifier for student inference. On a standard benchmark dataset, SimKD outperforms all state-of-the-art methods compared with various combinations of teacher-student architectures. The concept of knowledge distillation (KD), a technique for compressing knowledge from a powerful teacher model into a smaller student model, is discussed here. Transferred knowledge, usually in the form of soft targets, is thought to capture the relationships between different categories and serve as effective regularization during student training. Feature distillation is a common solution to prevent performance degradation in teacherto-student compression, which utilizes more information from the middle layer of the teacher model. One of the proposed methods, SimKD, is related to hypothetical transfer learning (HTL) and aims to close the performance gap between teachers and students on the same dataset. Here, by analyzing the limitations of the existing method and focusing on its improvement, a new method, logit distillation, is proposed, and the knowledge distillation loss function is re-represented as a weighted sum of two components, target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD).",
            "reference_string": "[259378480 | Gao | 2023 | Citations: 0]"
        },
        {
            "title": "Efficient Point Cloud Classification via Offline Distillation Framework and Negative-Weight Self-Distillation Technique",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.02020, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2114172292",
                    "name": "Qiang Zheng"
                },
                {
                    "authorId": "2302950444",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "2303058368",
                    "name": "Jian Sun"
                }
            ],
            "abstract": "The rapid advancement in point cloud processing technologies has significantly increased the demand for efficient and compact models that achieve high-accuracy classification. Knowledge distillation has emerged as a potent model compression technique. However, traditional KD often requires extensive computational resources for forward inference of large teacher models, thereby reducing training efficiency for student models and increasing resource demands. To address these challenges, we introduce an innovative offline recording strategy that avoids the simultaneous loading of both teacher and student models, thereby reducing hardware demands. This approach feeds a multitude of augmented samples into the teacher model, recording both the data augmentation parameters and the corresponding logit outputs. By applying shape-level augmentation operations such as random scaling and translation, while excluding point-level operations like random jittering, the size of the records is significantly reduced. Additionally, to mitigate the issue of small student model over-imitating the teacher model's outputs and converging to suboptimal solutions, we incorporate a negative-weight self-distillation strategy. Experimental results demonstrate that the proposed distillation strategy enables the student model to achieve performance comparable to state-of-the-art models while maintaining lower parameter count. This approach strikes an optimal balance between performance and complexity. This study highlights the potential of our method to optimize knowledge distillation for point cloud classification tasks, particularly in resource-constrained environments, providing a novel solution for efficient point cloud analysis.",
            "corpus_id": 272367233,
            "sentences": [
                {
                    "corpus_id": "272367233",
                    "title": "Efficient Point Cloud Classification via Offline Distillation Framework and Negative-Weight Self-Distillation Technique",
                    "text": "This section investigates the impact of distillation weights on the classification performance of the student model. The experimental design follows conventional settings in knowledge distillation, where the teacher-student distillation weight \u03b1 is set to a high value without self-distillation. This ensures that the weight is comparable to the classification loss weight, providing strong guidance for the student model. Once the teacher-student distillation parameters are established, the model further introduces a self-distillation with a negative weight \u03b2 of low magnitude to prevent interference with the training of the student model. This section explores the effects of varying weights on model performance through a series of experiments. The results are presented in Tab. 5 and 6, corresponding to the accuracy of the student model under different \u03b1 and \u03b2 values, respectively. As shown in Tab. 5, without self-distillation, the accuracy varies with different \u03b1 values, indicating that the strength of guidance has a direct impact on the knowledge transfer from the teacher model and the learning of the student model. The table demonstrates that an appropriately high \u03b1 is crucial for effective knowledge distillation and student model guidance. \n\nTab. 6 illustrates that when the teacher model provides strong guidance, the model's performance is generally better with a negative self-distillation weight \u03b2 than with a positive \u03b2. This confirms that negative-weight selfdistillation serves a regularizing role, enhancing the model's ability to generalize from training data and preventing rapid convergence to suboptimal solutions under the teacher model's guidance.",
                    "score": 0.5095375913109867,
                    "section_title": "Effects of Distillation Weights on Model Performance:",
                    "char_start_offset": 18652,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 422
                        },
                        {
                            "start": 423,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 750
                        },
                        {
                            "start": 751,
                            "end": 890
                        },
                        {
                            "start": 891,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1259
                        },
                        {
                            "start": 1262,
                            "end": 1445
                        },
                        {
                            "start": 1446,
                            "end": 1681
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68115234375
                }
            ],
            "relevance_judgement": 0.68115234375,
            "relevance_judgment_input_expanded": "# Title: Efficient Point Cloud Classification via Offline Distillation Framework and Negative-Weight Self-Distillation Technique\n# Venue: arXiv.org\n# Authors: Qiang Zheng, Chao Zhang, Jian Sun\n## Abstract\nThe rapid advancement in point cloud processing technologies has significantly increased the demand for efficient and compact models that achieve high-accuracy classification. Knowledge distillation has emerged as a potent model compression technique. However, traditional KD often requires extensive computational resources for forward inference of large teacher models, thereby reducing training efficiency for student models and increasing resource demands. To address these challenges, we introduce an innovative offline recording strategy that avoids the simultaneous loading of both teacher and student models, thereby reducing hardware demands. This approach feeds a multitude of augmented samples into the teacher model, recording both the data augmentation parameters and the corresponding logit outputs. By applying shape-level augmentation operations such as random scaling and translation, while excluding point-level operations like random jittering, the size of the records is significantly reduced. Additionally, to mitigate the issue of small student model over-imitating the teacher model's outputs and converging to suboptimal solutions, we incorporate a negative-weight self-distillation strategy. Experimental results demonstrate that the proposed distillation strategy enables the student model to achieve performance comparable to state-of-the-art models while maintaining lower parameter count. This approach strikes an optimal balance between performance and complexity. This study highlights the potential of our method to optimize knowledge distillation for point cloud classification tasks, particularly in resource-constrained environments, providing a novel solution for efficient point cloud analysis.\n## Effects of Distillation Weights on Model Performance:\nThis section investigates the impact of distillation weights on the classification performance of the student model. The experimental design follows conventional settings in knowledge distillation, where the teacher-student distillation weight \u03b1 is set to a high value without self-distillation. This ensures that the weight is comparable to the classification loss weight, providing strong guidance for the student model. Once the teacher-student distillation parameters are established, the model further introduces a self-distillation with a negative weight \u03b2 of low magnitude to prevent interference with the training of the student model. This section explores the effects of varying weights on model performance through a series of experiments. The results are presented in Tab. 5 and 6, corresponding to the accuracy of the student model under different \u03b1 and \u03b2 values, respectively. As shown in Tab. 5, without self-distillation, the accuracy varies with different \u03b1 values, indicating that the strength of guidance has a direct impact on the knowledge transfer from the teacher model and the learning of the student model. The table demonstrates that an appropriately high \u03b1 is crucial for effective knowledge distillation and student model guidance. \n\nTab. 6 illustrates that when the teacher model provides strong guidance, the model's performance is generally better with a negative self-distillation weight \u03b2 than with a positive \u03b2. This confirms that negative-weight selfdistillation serves a regularizing role, enhancing the model's ability to generalize from training data and preventing rapid convergence to suboptimal solutions under the teacher model's guidance.",
            "reference_string": "[272367233 | Zheng et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Deep Collaborative Learning for Randomly Wired Neural Networks",
            "venue": "Electronics",
            "year": 2021,
            "reference_count": 32,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2079-9292/10/14/1669/pdf?version=1626176884",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/ELECTRONICS10141669?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/ELECTRONICS10141669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2416266",
                    "name": "Ehab Essa"
                },
                {
                    "authorId": "2168049",
                    "name": "Xianghua Xie"
                }
            ],
            "abstract": "A deep collaborative learning approach is introduced in which a chain of randomly wired neural networks is trained simultaneously to improve the overall generalization and form a strong ensemble model. The proposed method takes advantage of functional-preserving transfer learning and knowledge distillation to produce an ensemble model. Knowledge distillation is an effective learning scheme for improving the performance of small neural networks by using the knowledge learned by teacher networks. Most of the previous methods learn from one or more teachers but not in a collaborative way. In this paper, we created a chain of randomly wired neural networks based on a random graph algorithm and collaboratively trained the models using functional-preserving transfer learning, so that the small network in the chain could learn from the largest one simultaneously. The training method applies knowledge distillation between randomly wired models, where each model is considered as a teacher to the next model in the chain. The decision of multiple chains of models can be combined to produce a robust ensemble model. The proposed method is evaluated on CIFAR-10, CIFAR-100, and TinyImageNet. The experimental results show that the collaborative training significantly improved the generalization of each model, which allowed for obtaining a small model that can mimic the performance of a large model and produce a more robust ensemble approach.",
            "corpus_id": 237845150,
            "sentences": [
                {
                    "corpus_id": "237845150",
                    "title": "Deep Collaborative Learning for Randomly Wired Neural Networks",
                    "text": "The implicit ensemble is usually seen as a regularization method to reduce overfitting. Moreover, it can be used along with an explicit ensemble approach. \n\nKnowledge distillation refers to training a smaller model (i.e., a student) to mimic the performance of a large model or an ensemble (i.e., a teacher). The student model is trained with an additional loss function to prompt the model to be identical to the teacher model. Various distillation methods have been introduced to examine different types of loss functions [21,22], different forms of teacher model [23,24], and the best way to train the student model [25,26]. For example, in [27], the authors introduced an approach (called AvgMKD) to distill knowledge from multiple teachers. \n\nThey integrated softened outputs of each teacher equally and imposed constraints on the intermediate layers of the student models using the relative dissimilarity learned from the teacher networks. However, by treating each teacher equally, the differences between teacher models could be lost. In [14], authors proposed an adaptive multi-teacher knowledge distillation method (named AMTML-KD) that extended the previous method by adding an adaptive weight for each teacher model and transferring the intermediatelevel knowledge from hidden layers of the teacher models to the student models. \n\nAnother distillation variant is co-distillation [12,13] where the teacher and student had the same network architecture and were trained in parallel using distillation loss before any model converged. It has shown improvement in the speed of model training and its accuracy. Zhang's method [12] can be seen as co-distillation of models that have different architectures. Our proposed method can be seen as the co-distillation of randomly generated models, but the distillation method is using transfer learning instead of an extra loss function. \n\nKnowledge transfer is another student-teacher paradigm, where the knowledge is transferred by passing the parameters of each layer of a trained teacher model to the student model as initialization before beginning training the student model. The knowledge is transferred from a smaller model to a larger model with function preserving transformations to accelerate the training of the student model. The expansion of the student network can be achieved by increasing its depth, width, or kernel size.",
                    "score": 0.5071290696158306,
                    "section_title": "Related Work",
                    "char_start_offset": 9045,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 87
                        },
                        {
                            "start": 88,
                            "end": 154
                        },
                        {
                            "start": 157,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 428
                        },
                        {
                            "start": 429,
                            "end": 627
                        },
                        {
                            "start": 628,
                            "end": 745
                        },
                        {
                            "start": 748,
                            "end": 945
                        },
                        {
                            "start": 946,
                            "end": 1042
                        },
                        {
                            "start": 1043,
                            "end": 1340
                        },
                        {
                            "start": 1343,
                            "end": 1543
                        },
                        {
                            "start": 1544,
                            "end": 1617
                        },
                        {
                            "start": 1618,
                            "end": 1713
                        },
                        {
                            "start": 1714,
                            "end": 1888
                        },
                        {
                            "start": 1891,
                            "end": 2132
                        },
                        {
                            "start": 2133,
                            "end": 2290
                        },
                        {
                            "start": 2291,
                            "end": 2391
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 644,
                            "end": 648,
                            "matchedPaperCorpusId": "26021416"
                        },
                        {
                            "start": 1046,
                            "end": 1050,
                            "matchedPaperCorpusId": "224818016"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68017578125
                }
            ],
            "relevance_judgement": 0.68017578125,
            "relevance_judgment_input_expanded": "# Title: Deep Collaborative Learning for Randomly Wired Neural Networks\n# Venue: Electronics\n# Authors: Ehab Essa, Xianghua Xie\n## Abstract\nA deep collaborative learning approach is introduced in which a chain of randomly wired neural networks is trained simultaneously to improve the overall generalization and form a strong ensemble model. The proposed method takes advantage of functional-preserving transfer learning and knowledge distillation to produce an ensemble model. Knowledge distillation is an effective learning scheme for improving the performance of small neural networks by using the knowledge learned by teacher networks. Most of the previous methods learn from one or more teachers but not in a collaborative way. In this paper, we created a chain of randomly wired neural networks based on a random graph algorithm and collaboratively trained the models using functional-preserving transfer learning, so that the small network in the chain could learn from the largest one simultaneously. The training method applies knowledge distillation between randomly wired models, where each model is considered as a teacher to the next model in the chain. The decision of multiple chains of models can be combined to produce a robust ensemble model. The proposed method is evaluated on CIFAR-10, CIFAR-100, and TinyImageNet. The experimental results show that the collaborative training significantly improved the generalization of each model, which allowed for obtaining a small model that can mimic the performance of a large model and produce a more robust ensemble approach.\n## Related Work\nThe implicit ensemble is usually seen as a regularization method to reduce overfitting. Moreover, it can be used along with an explicit ensemble approach. \n\nKnowledge distillation refers to training a smaller model (i.e., a student) to mimic the performance of a large model or an ensemble (i.e., a teacher). The student model is trained with an additional loss function to prompt the model to be identical to the teacher model. Various distillation methods have been introduced to examine different types of loss functions [21,22], different forms of teacher model [23,24], and the best way to train the student model [25,26]. For example, in [27], the authors introduced an approach (called AvgMKD) to distill knowledge from multiple teachers. \n\nThey integrated softened outputs of each teacher equally and imposed constraints on the intermediate layers of the student models using the relative dissimilarity learned from the teacher networks. However, by treating each teacher equally, the differences between teacher models could be lost. In [14], authors proposed an adaptive multi-teacher knowledge distillation method (named AMTML-KD) that extended the previous method by adding an adaptive weight for each teacher model and transferring the intermediatelevel knowledge from hidden layers of the teacher models to the student models. \n\nAnother distillation variant is co-distillation [12,13] where the teacher and student had the same network architecture and were trained in parallel using distillation loss before any model converged. It has shown improvement in the speed of model training and its accuracy. Zhang's method [12] can be seen as co-distillation of models that have different architectures. Our proposed method can be seen as the co-distillation of randomly generated models, but the distillation method is using transfer learning instead of an extra loss function. \n\nKnowledge transfer is another student-teacher paradigm, where the knowledge is transferred by passing the parameters of each layer of a trained teacher model to the student model as initialization before beginning training the student model. The knowledge is transferred from a smaller model to a larger model with function preserving transformations to accelerate the training of the student model. The expansion of the student network can be achieved by increasing its depth, width, or kernel size.",
            "reference_string": "[237845150 | Essa et al. | 2021 | Citations: 1]"
        },
        {
            "title": "Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 69,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.12954",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.12954, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146248526",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2116210564",
                    "name": "Yuxuan Li"
                },
                {
                    "authorId": "2284827556",
                    "name": "Penghai Zhao"
                },
                {
                    "authorId": "2067622132",
                    "name": "Renjie Song"
                },
                {
                    "authorId": "2144439048",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2146236917",
                    "name": "Jian Yang"
                }
            ],
            "abstract": "Diffusion models have recently achieved astonishing performance in generating high-fidelity photo-realistic images. Given their huge success, it is still unclear whether synthetic images are applicable for knowledge distillation when real images are unavailable. In this paper, we extensively study whether and how synthetic images produced from state-of-the-art diffusion models can be used for knowledge distillation without access to real images, and obtain three key conclusions: (1) synthetic data from diffusion models can easily lead to state-of-the-art performance among existing synthesis-based distillation methods, (2) low-fidelity synthetic images are better teaching materials, and (3) relatively weak classifiers are better teachers. Code is available at https://github.com/zhengli97/DM-KD.",
            "corpus_id": 258832674,
            "sentences": [
                {
                    "corpus_id": "258832674",
                    "title": "Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?",
                    "text": "Experiment setup. To validate if relatively weak classifiers are better teachers, we carefully select multiple teacher-student model pairs with different architectures, including ResNet [19], VGG [54], and ShuffleNetV2 (SNV2) [39]. The experiments are conducted on 200K synthetic datasets and the students are trained by 100 epochs. \n\nResults. When training on real datasets, it is common to use a relatively large teacher model to train the student model, such as distilling ResNet34 to ResNet18. In general, smaller teacher models often fail to achieve satisfactory distillation performance compared to larger teacher models. However, when working with synthetic datasets, we observe the opposite phenomenon: relatively weak teacher models can actually achieve better distillation performance than strong ones, as shown in Fig. 3 and Fig. 4. Interestingly, we found that as the capacity of the teacher model increases, a significant drop in performance is observed. Specifically, when training ResNet34 on the synthetic dataset, using ResNet18 as the teacher model leads to a 3% improvement in performance compared to using ResNet50 as the teacher model. These results highlight the importance of carefully selecting a teacher model when performing knowledge distillation and suggest that using a smaller, weaker teacher model may be preferable when working with synthetic datasets.",
                    "score": 0.5868778853283315,
                    "section_title": "Relatively weak classifiers are better teachers.",
                    "char_start_offset": 22185,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 17
                        },
                        {
                            "start": 18,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 332
                        },
                        {
                            "start": 335,
                            "end": 343
                        },
                        {
                            "start": 344,
                            "end": 497
                        },
                        {
                            "start": 498,
                            "end": 627
                        },
                        {
                            "start": 628,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1156
                        },
                        {
                            "start": 1157,
                            "end": 1384
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 186,
                            "end": 190,
                            "matchedPaperCorpusId": "206594692"
                        },
                        {
                            "start": 226,
                            "end": 230,
                            "matchedPaperCorpusId": "51880435"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67724609375
                }
            ],
            "relevance_judgement": 0.67724609375,
            "relevance_judgment_input_expanded": "# Title: Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?\n# Venue: arXiv.org\n# Authors: Zheng Li, Yuxuan Li, Penghai Zhao, Renjie Song, Xiang Li, Jian Yang\n## Abstract\nDiffusion models have recently achieved astonishing performance in generating high-fidelity photo-realistic images. Given their huge success, it is still unclear whether synthetic images are applicable for knowledge distillation when real images are unavailable. In this paper, we extensively study whether and how synthetic images produced from state-of-the-art diffusion models can be used for knowledge distillation without access to real images, and obtain three key conclusions: (1) synthetic data from diffusion models can easily lead to state-of-the-art performance among existing synthesis-based distillation methods, (2) low-fidelity synthetic images are better teaching materials, and (3) relatively weak classifiers are better teachers. Code is available at https://github.com/zhengli97/DM-KD.\n## Relatively weak classifiers are better teachers.\nExperiment setup. To validate if relatively weak classifiers are better teachers, we carefully select multiple teacher-student model pairs with different architectures, including ResNet [19], VGG [54], and ShuffleNetV2 (SNV2) [39]. The experiments are conducted on 200K synthetic datasets and the students are trained by 100 epochs. \n\nResults. When training on real datasets, it is common to use a relatively large teacher model to train the student model, such as distilling ResNet34 to ResNet18. In general, smaller teacher models often fail to achieve satisfactory distillation performance compared to larger teacher models. However, when working with synthetic datasets, we observe the opposite phenomenon: relatively weak teacher models can actually achieve better distillation performance than strong ones, as shown in Fig. 3 and Fig. 4. Interestingly, we found that as the capacity of the teacher model increases, a significant drop in performance is observed. Specifically, when training ResNet34 on the synthetic dataset, using ResNet18 as the teacher model leads to a 3% improvement in performance compared to using ResNet50 as the teacher model. These results highlight the importance of carefully selecting a teacher model when performing knowledge distillation and suggest that using a smaller, weaker teacher model may be preferable when working with synthetic datasets.",
            "reference_string": "[258832674 | Li et al. | 2023 | Citations: 20]"
        },
        {
            "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 78,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.17653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2258553172",
                    "name": "Karsten Roth"
                },
                {
                    "authorId": "2261738641",
                    "name": "Lukas Thede"
                },
                {
                    "authorId": "2064172797",
                    "name": "A. S. Koepke"
                },
                {
                    "authorId": "1689108",
                    "name": "O. Vinyals"
                },
                {
                    "authorId": "2066734400",
                    "name": "Olivier J. H'enaff"
                },
                {
                    "authorId": "1854487018",
                    "name": "Zeynep Akata"
                }
            ],
            "abstract": "Training deep networks requires various design decisions regarding for instance their architecture, data augmentation, or optimization. In this work, we find these training variations to result in networks learning unique feature sets from the data. Using public model libraries comprising thousands of models trained on canonical datasets like ImageNet, we observe that for arbitrary pairings of pretrained models, one model extracts significant data context unavailable in the other -- independent of overall performance. Given any arbitrary pairing of pretrained models and no external rankings (such as separate test sets, e.g. due to data privacy), we investigate if it is possible to transfer such\"complementary\"knowledge from one model to another without performance degradation -- a task made particularly difficult as additional knowledge can be contained in stronger, equiperformant or weaker models. Yet facilitating robust transfer in scenarios agnostic to pretrained model pairings would unlock auxiliary gains and knowledge fusion from any model repository without restrictions on model and problem specifics - including from weaker, lower-performance models. This work therefore provides an initial, in-depth exploration on the viability of such general-purpose knowledge transfer. Across large-scale experiments, we first reveal the shortcomings of standard knowledge distillation techniques, and then propose a much more general extension through data partitioning for successful transfer between nearly all pretrained models, which we show can also be done unsupervised. Finally, we assess both the scalability and impact of fundamental model properties on successful model-agnostic knowledge transfer.",
            "corpus_id": 264490946,
            "sentences": [
                {
                    "corpus_id": "264490946",
                    "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
                    "text": "While better than normal distillation, it is often too strong a constraint when the teachers have niche areas of expertise or are overall stronger. We thus propose to constrain distillation at the data level by partitioning it into two sets -one with samples where transfer from a teacher is desired, and one where we wish to retain the student behavior. This introduces significantly fewer constraints on the model weights to learn from arbitrary teacher context, while reducing forgetting by retaining initial performance on samples where the teacher has limited positive (even detrimental) impact. Moreover, our data partitioning can be achieved without any supervision. \n\nDoing so, we see significant increases in the success rate (non-zero gains of the student) for all teacher-student pairings -from 32.5% with normal distillation to 92.5% with data partitioning. Our data-level regularization is the only setting which allows for consistently positive transfer from weaker teachers, while retaining the transfer performance of normal distillation for much stronger teachers and even outperforming normal distillation for equiperformant ones. In addition, it allows for the transfer of specialized knowledge ( \u00a75.1) and requires no pairing-specific hyperparameters. Unlike ensembling methods (Lakshminarayanan et al., 2017;Gontijo-Lopes et al., 2022;Sinha et al., 2021;Dietterich, 2000), our approach maintains original inference costs and handles high performance differences. Finally, we study architectural properties and their impact on the transfer process ( \u00a75.1) beyond the transfer method, and look into scalability to knowledge transfer from multiple models, where we find that simple sequential transfer can perform favorably when leveraging our transfer method, achieving clear improvements over transfer from just the single best teacher model. Overall, our contributions can be summarized as: (1) We discover the consistent existence of complementary knowledge between arbitrary models pretrained on the same dataset -even if model families or performances differ. (2) We conduct extensive, exploratory studies to investigate the possibility of guaranteed model-and performance-independent transfer of the complementary knowledge without performance degradation.",
                    "score": 0.5317584710695773,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 4027,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 147
                        },
                        {
                            "start": 148,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 600
                        },
                        {
                            "start": 601,
                            "end": 673
                        },
                        {
                            "start": 676,
                            "end": 869
                        },
                        {
                            "start": 870,
                            "end": 1148
                        },
                        {
                            "start": 1149,
                            "end": 1271
                        },
                        {
                            "start": 1272,
                            "end": 1483
                        },
                        {
                            "start": 1484,
                            "end": 1862
                        },
                        {
                            "start": 1863,
                            "end": 2083
                        },
                        {
                            "start": 2084,
                            "end": 2281
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1298,
                            "end": 1329,
                            "matchedPaperCorpusId": "6294674"
                        },
                        {
                            "start": 1375,
                            "end": 1392,
                            "matchedPaperCorpusId": "10765854"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67626953125
                }
            ],
            "relevance_judgement": 0.67626953125,
            "relevance_judgment_input_expanded": "# Title: Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model\n# Venue: International Conference on Learning Representations\n# Authors: Karsten Roth, Lukas Thede, A. S. Koepke, O. Vinyals, Olivier J. H'enaff, Zeynep Akata\n## Abstract\nTraining deep networks requires various design decisions regarding for instance their architecture, data augmentation, or optimization. In this work, we find these training variations to result in networks learning unique feature sets from the data. Using public model libraries comprising thousands of models trained on canonical datasets like ImageNet, we observe that for arbitrary pairings of pretrained models, one model extracts significant data context unavailable in the other -- independent of overall performance. Given any arbitrary pairing of pretrained models and no external rankings (such as separate test sets, e.g. due to data privacy), we investigate if it is possible to transfer such\"complementary\"knowledge from one model to another without performance degradation -- a task made particularly difficult as additional knowledge can be contained in stronger, equiperformant or weaker models. Yet facilitating robust transfer in scenarios agnostic to pretrained model pairings would unlock auxiliary gains and knowledge fusion from any model repository without restrictions on model and problem specifics - including from weaker, lower-performance models. This work therefore provides an initial, in-depth exploration on the viability of such general-purpose knowledge transfer. Across large-scale experiments, we first reveal the shortcomings of standard knowledge distillation techniques, and then propose a much more general extension through data partitioning for successful transfer between nearly all pretrained models, which we show can also be done unsupervised. Finally, we assess both the scalability and impact of fundamental model properties on successful model-agnostic knowledge transfer.\n## INTRODUCTION\nWhile better than normal distillation, it is often too strong a constraint when the teachers have niche areas of expertise or are overall stronger. We thus propose to constrain distillation at the data level by partitioning it into two sets -one with samples where transfer from a teacher is desired, and one where we wish to retain the student behavior. This introduces significantly fewer constraints on the model weights to learn from arbitrary teacher context, while reducing forgetting by retaining initial performance on samples where the teacher has limited positive (even detrimental) impact. Moreover, our data partitioning can be achieved without any supervision. \n\nDoing so, we see significant increases in the success rate (non-zero gains of the student) for all teacher-student pairings -from 32.5% with normal distillation to 92.5% with data partitioning. Our data-level regularization is the only setting which allows for consistently positive transfer from weaker teachers, while retaining the transfer performance of normal distillation for much stronger teachers and even outperforming normal distillation for equiperformant ones. In addition, it allows for the transfer of specialized knowledge ( \u00a75.1) and requires no pairing-specific hyperparameters. Unlike ensembling methods (Lakshminarayanan et al., 2017;Gontijo-Lopes et al., 2022;Sinha et al., 2021;Dietterich, 2000), our approach maintains original inference costs and handles high performance differences. Finally, we study architectural properties and their impact on the transfer process ( \u00a75.1) beyond the transfer method, and look into scalability to knowledge transfer from multiple models, where we find that simple sequential transfer can perform favorably when leveraging our transfer method, achieving clear improvements over transfer from just the single best teacher model. Overall, our contributions can be summarized as: (1) We discover the consistent existence of complementary knowledge between arbitrary models pretrained on the same dataset -even if model families or performances differ. (2) We conduct extensive, exploratory studies to investigate the possibility of guaranteed model-and performance-independent transfer of the complementary knowledge without performance degradation.",
            "reference_string": "[264490946 | Roth et al. | 2023 | Citations: 13]"
        },
        {
            "title": "A Survey on Recent Teacher-student Learning Studies",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 23,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.04615",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.04615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2147417425",
                    "name": "Min Gao"
                }
            ],
            "abstract": "Knowledge distillation is a method of transferring the knowledge from a complex deep neural network (DNN) to a smaller and faster DNN, while preserving its accuracy. Recent variants of knowledge distillation include teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation, which aim to improve the performance of knowledge distillation by introducing additional components or by changing the learning process. Teaching assistant distillation involves an intermediate model called the teaching assistant, while curriculum distillation follows a curriculum similar to human education. Mask distillation focuses on transferring the attention mechanism learned by the teacher, and decoupling distillation decouples the distillation loss from the task loss. Overall, these variants of knowledge distillation have shown promising results in improving the performance of knowledge distillation.",
            "corpus_id": 258048760,
            "sentences": [
                {
                    "corpus_id": "258048760",
                    "title": "A Survey on Recent Teacher-student Learning Studies",
                    "text": "The use of Simple Knowledge Distillation (SimKD) allows the compression of powerful teacher models into lightweight student models without sacrificing performance. This is achieved by reusing the discriminative classifier of the teacher model for student inference and training the student encoder by feature alignment with a single loss. A projector was also developed to help match the student encoder with the teacher classifier, making the technique applicable to a variety of teacher and student architectures. Experiments show that with the addition of the projector, the technique achieves state-of-the-art results at the cost of a reduced compression ratio. Vanilla KD aligns the logit or class projections of the two models, but the performance gap between the original teacher model and the refined student model is still large. Various methods have been proposed to overcome this problem, but they require detailed knowledge representation and optimized hyperparameters. The SimKD technique is proposed, which trains the student model by feature alignment in the previous layer of the classifier and directly replicates the teacher classifier for student inference. On a standard benchmark dataset, SimKD outperforms all state-of-the-art methods compared with various combinations of teacher-student architectures. The concept of knowledge distillation (KD), a technique for compressing knowledge from a powerful teacher model into a smaller student model, is discussed here. Transferred knowledge, usually in the form of soft targets, is thought to capture the relationships between different categories and serve as effective regularization during student training. Feature distillation is a common solution to prevent performance degradation in teacher-to-student compression, which utilizes more information from the middle layer of the teacher model. One of the proposed methods, SimKD, is related to hypothetical transfer learning (HTL) and aims to close the performance gap between teachers and students on the same dataset. Here, by analyzing the limitations of the existing method and focusing on its improvement, a new method, logit distillation, is proposed, and the knowledge distillation loss function is re-represented as a weighted sum of two components, target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD).",
                    "score": 0.5055771230057073,
                    "section_title": "SIMPLE KNOWLEDGE DISTILLATION",
                    "char_start_offset": 36338,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 515
                        },
                        {
                            "start": 516,
                            "end": 665
                        },
                        {
                            "start": 666,
                            "end": 838
                        },
                        {
                            "start": 839,
                            "end": 981
                        },
                        {
                            "start": 982,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1325
                        },
                        {
                            "start": 1326,
                            "end": 1486
                        },
                        {
                            "start": 1487,
                            "end": 1678
                        },
                        {
                            "start": 1679,
                            "end": 1866
                        },
                        {
                            "start": 1867,
                            "end": 2042
                        },
                        {
                            "start": 2043,
                            "end": 2375
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67529296875
                }
            ],
            "relevance_judgement": 0.67529296875,
            "relevance_judgment_input_expanded": "# Title: A Survey on Recent Teacher-student Learning Studies\n# Venue: arXiv.org\n# Authors: Min Gao\n## Abstract\nKnowledge distillation is a method of transferring the knowledge from a complex deep neural network (DNN) to a smaller and faster DNN, while preserving its accuracy. Recent variants of knowledge distillation include teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation, which aim to improve the performance of knowledge distillation by introducing additional components or by changing the learning process. Teaching assistant distillation involves an intermediate model called the teaching assistant, while curriculum distillation follows a curriculum similar to human education. Mask distillation focuses on transferring the attention mechanism learned by the teacher, and decoupling distillation decouples the distillation loss from the task loss. Overall, these variants of knowledge distillation have shown promising results in improving the performance of knowledge distillation.\n## SIMPLE KNOWLEDGE DISTILLATION\nThe use of Simple Knowledge Distillation (SimKD) allows the compression of powerful teacher models into lightweight student models without sacrificing performance. This is achieved by reusing the discriminative classifier of the teacher model for student inference and training the student encoder by feature alignment with a single loss. A projector was also developed to help match the student encoder with the teacher classifier, making the technique applicable to a variety of teacher and student architectures. Experiments show that with the addition of the projector, the technique achieves state-of-the-art results at the cost of a reduced compression ratio. Vanilla KD aligns the logit or class projections of the two models, but the performance gap between the original teacher model and the refined student model is still large. Various methods have been proposed to overcome this problem, but they require detailed knowledge representation and optimized hyperparameters. The SimKD technique is proposed, which trains the student model by feature alignment in the previous layer of the classifier and directly replicates the teacher classifier for student inference. On a standard benchmark dataset, SimKD outperforms all state-of-the-art methods compared with various combinations of teacher-student architectures. The concept of knowledge distillation (KD), a technique for compressing knowledge from a powerful teacher model into a smaller student model, is discussed here. Transferred knowledge, usually in the form of soft targets, is thought to capture the relationships between different categories and serve as effective regularization during student training. Feature distillation is a common solution to prevent performance degradation in teacher-to-student compression, which utilizes more information from the middle layer of the teacher model. One of the proposed methods, SimKD, is related to hypothetical transfer learning (HTL) and aims to close the performance gap between teachers and students on the same dataset. Here, by analyzing the limitations of the existing method and focusing on its improvement, a new method, logit distillation, is proposed, and the knowledge distillation loss function is re-represented as a weighted sum of two components, target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD).",
            "reference_string": "[258048760 | Gao | 2023 | Citations: 3]"
        },
        {
            "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07168, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2330192141",
                    "name": "Ra'ul de la Fuente"
                },
                {
                    "authorId": "2047941908",
                    "name": "Luciano Radrig\u00e1n"
                },
                {
                    "authorId": "2330183718",
                    "name": "Anibal S Morales"
                }
            ],
            "abstract": "Mining machinery operating in variable environments faces high wear and unpredictable stress, challenging Predictive Maintenance (PdM). This paper introduces the Edge Sensor Network for Predictive Maintenance (ESN-PdM), a hierarchical inference framework across edge devices, gateways, and cloud services for real-time condition monitoring. The system dynamically adjusts inference locations--on-device, on-gateway, or on-cloud--based on trade-offs among accuracy, latency, and battery life, leveraging Tiny Machine Learning (TinyML) techniques for model optimization on resource-constrained devices. Performance evaluations showed that on-sensor and on-gateway inference modes achieved over 90\\% classification accuracy, while cloud-based inference reached 99\\%. On-sensor inference reduced power consumption by approximately 44\\%, enabling up to 104 hours of operation. Latency was lowest for on-device inference (3.33 ms), increasing when offloading to the gateway (146.67 ms) or cloud (641.71 ms). The ESN-PdM framework provides a scalable, adaptive solution for reliable anomaly detection and PdM, crucial for maintaining machinery uptime in remote environments. By balancing accuracy, latency, and energy consumption, this approach advances PdM frameworks for industrial applications.",
            "corpus_id": 273963558,
            "sentences": [
                {
                    "corpus_id": "273963558",
                    "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network",
                    "text": "Knowledge distillation transfers knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student) [33]. The process takes into account two main factors: the type of knowledge and the distillation scheme. Knowledge types include response-based, where the student mimics the teacher's final predictions; feature-based, where the student learns from the teacher's intermediate layers; and relation-based, where the student learns from the relationships between layers or samples [21]. Distillation schemes include offline distillation, where the teacher is trained first and then guides the student; online distillation, where both models train simultaneously; and self-distillation, where the teacher and student share the same architecture [21], [34]. This method creates lightweight models that can maintain or even surpass the performance of larger models. Figure 1.c illustrates the knowledge distillation process, showing the transfer of knowledge from a teacher model to a student model.",
                    "score": 0.5512643357140857,
                    "section_title": "3) Knowledge Distillation",
                    "char_start_offset": 11001,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 132
                        },
                        {
                            "start": 133,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 510
                        },
                        {
                            "start": 511,
                            "end": 779
                        },
                        {
                            "start": 780,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1020
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 127,
                            "end": 131,
                            "matchedPaperCorpusId": "264109973"
                        },
                        {
                            "start": 505,
                            "end": 509,
                            "matchedPaperCorpusId": "248507674"
                        },
                        {
                            "start": 768,
                            "end": 772,
                            "matchedPaperCorpusId": "248507674"
                        },
                        {
                            "start": 774,
                            "end": 778,
                            "matchedPaperCorpusId": "219559263"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66796875
                }
            ],
            "relevance_judgement": 0.66796875,
            "relevance_judgment_input_expanded": "# Title: Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network\n# Venue: arXiv.org\n# Authors: Ra'ul de la Fuente, Luciano Radrig\u00e1n, Anibal S Morales\n## Abstract\nMining machinery operating in variable environments faces high wear and unpredictable stress, challenging Predictive Maintenance (PdM). This paper introduces the Edge Sensor Network for Predictive Maintenance (ESN-PdM), a hierarchical inference framework across edge devices, gateways, and cloud services for real-time condition monitoring. The system dynamically adjusts inference locations--on-device, on-gateway, or on-cloud--based on trade-offs among accuracy, latency, and battery life, leveraging Tiny Machine Learning (TinyML) techniques for model optimization on resource-constrained devices. Performance evaluations showed that on-sensor and on-gateway inference modes achieved over 90\\% classification accuracy, while cloud-based inference reached 99\\%. On-sensor inference reduced power consumption by approximately 44\\%, enabling up to 104 hours of operation. Latency was lowest for on-device inference (3.33 ms), increasing when offloading to the gateway (146.67 ms) or cloud (641.71 ms). The ESN-PdM framework provides a scalable, adaptive solution for reliable anomaly detection and PdM, crucial for maintaining machinery uptime in remote environments. By balancing accuracy, latency, and energy consumption, this approach advances PdM frameworks for industrial applications.\n## 3) Knowledge Distillation\nKnowledge distillation transfers knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student) [33]. The process takes into account two main factors: the type of knowledge and the distillation scheme. Knowledge types include response-based, where the student mimics the teacher's final predictions; feature-based, where the student learns from the teacher's intermediate layers; and relation-based, where the student learns from the relationships between layers or samples [21]. Distillation schemes include offline distillation, where the teacher is trained first and then guides the student; online distillation, where both models train simultaneously; and self-distillation, where the teacher and student share the same architecture [21], [34]. This method creates lightweight models that can maintain or even surpass the performance of larger models. Figure 1.c illustrates the knowledge distillation process, showing the transfer of knowledge from a teacher model to a student model.",
            "reference_string": "[273963558 | Fuente et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Learning from Stochastic Teacher Representations Using Student-Guided Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 38,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.14307, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2057319769",
                    "name": "Muhammad Haseeb Aslam"
                },
                {
                    "authorId": "2356582101",
                    "name": "Clara Martinez"
                },
                {
                    "authorId": "3048367",
                    "name": "Marco Pedersoli"
                },
                {
                    "authorId": "2263129961",
                    "name": "A. Koerich"
                },
                {
                    "authorId": "2269469250",
                    "name": "Ali Etemad"
                },
                {
                    "authorId": "2256991724",
                    "name": "Eric Granger"
                }
            ],
            "abstract": "Advances in self-distillation have shown that when knowledge is distilled from a teacher to a student using the same deep learning (DL) architecture, the student performance can surpass the teacher particularly when the network is overparameterized and the teacher is trained with early stopping. Alternatively, ensemble learning also improves performance, although training, storing, and deploying multiple models becomes impractical as the number of models grows. Even distilling an ensemble to a single student model or weight averaging methods first requires training of multiple teacher models and does not fully leverage the inherent stochasticity for generating and distilling diversity in DL models. These constraints are particularly prohibitive in resource-constrained or latency-sensitive applications such as wearable devices. This paper proposes to train only one model and generate multiple diverse teacher representations using distillation-time dropout. However, generating these representations stochastically leads to noisy representations that are misaligned with the learned task. To overcome this problem, a novel stochastic self-distillation (SSD) training strategy is introduced for filtering and weighting teacher representation to distill from task-relevant representations only, using student-guided knowledge distillation (SGKD). The student representation at each distillation step is used as authority to guide the distillation process. Experimental results on real-world affective computing, wearable/biosignal datasets from the UCR Archive, the HAR dataset, and image classification datasets show that the proposed SSD method can outperform state-of-the-art methods without increasing the model size at both training and testing time, and incurs negligible computational complexity compared to state-of-the-art ensemble learning and weight averaging methods.",
            "corpus_id": 277954831,
            "sentences": [],
            "relevance_judgement": 0.6591796875,
            "relevance_judgment_input_expanded": "# Title: Learning from Stochastic Teacher Representations Using Student-Guided Knowledge Distillation\n# Venue: arXiv.org\n# Authors: Muhammad Haseeb Aslam, Clara Martinez, Marco Pedersoli, A. Koerich, Ali Etemad, Eric Granger\n## Abstract\nAdvances in self-distillation have shown that when knowledge is distilled from a teacher to a student using the same deep learning (DL) architecture, the student performance can surpass the teacher particularly when the network is overparameterized and the teacher is trained with early stopping. Alternatively, ensemble learning also improves performance, although training, storing, and deploying multiple models becomes impractical as the number of models grows. Even distilling an ensemble to a single student model or weight averaging methods first requires training of multiple teacher models and does not fully leverage the inherent stochasticity for generating and distilling diversity in DL models. These constraints are particularly prohibitive in resource-constrained or latency-sensitive applications such as wearable devices. This paper proposes to train only one model and generate multiple diverse teacher representations using distillation-time dropout. However, generating these representations stochastically leads to noisy representations that are misaligned with the learned task. To overcome this problem, a novel stochastic self-distillation (SSD) training strategy is introduced for filtering and weighting teacher representation to distill from task-relevant representations only, using student-guided knowledge distillation (SGKD). The student representation at each distillation step is used as authority to guide the distillation process. Experimental results on real-world affective computing, wearable/biosignal datasets from the UCR Archive, the HAR dataset, and image classification datasets show that the proposed SSD method can outperform state-of-the-art methods without increasing the model size at both training and testing time, and incurs negligible computational complexity compared to state-of-the-art ensemble learning and weight averaging methods.\n",
            "reference_string": "[277954831 | Aslam et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Distilling Knowledge via Intermediate Classifiers",
            "venue": "",
            "year": 2021,
            "reference_count": 36,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.00497, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2051713799",
                    "name": "Aryan Asadian"
                },
                {
                    "authorId": "1403711294",
                    "name": "Amirali Salehi-Abari"
                }
            ],
            "abstract": "The crux of knowledge distillation is to effectively train a resource-limited student model with the guide of a pre-trained larger teacher model. However, when there is a large difference between the model complexities of teacher and student (i.e., capacity gap), knowledge distillation loses its strength in transferring knowledge from the teacher to the student, thus training a weaker student. To mitigate the impact of the capacity gap, we introduce knowledge distillation via intermediate heads. By extending the intermediate layers of the teacher (at various depths) with classifier heads, we cheaply acquire a cohort of heterogeneous pre-trained teachers. The intermediate classifier heads can all together be efficiently learned while freezing the backbone of the pre-trained teacher. The cohort of teachers (including the original teacher) co-teach the student simultaneously. Our experiments on various teacher-student pairs and datasets have demonstrated that the proposed approach outperforms the canonical knowledge distillation approach and its extensions.",
            "corpus_id": 235262724,
            "sentences": [
                {
                    "corpus_id": "235262724",
                    "title": "Distilling Knowledge via Intermediate Classifiers",
                    "text": "There has been growing interest in model compression techniques for compressing the larger neural networks to smaller ones. Model pruning, as an optimization framework, intends to reduce the unnecessary structure and parameters of a large model to gain an efficient smaller network [3]. Model quantization compresses the neural network by reducing the precision of its parameters at the expense of accuracy [29]. Designing efficient deep neural architectures [22] intends to satisfy the specified accuracy requirement given resource limitations. Knowledge distillation [10], that can be combined with others, aims to preserve the accuracy of a large trained teacher model in a smaller student model. Knowledge Distillation. Knowledge distillation is a training framework in which a huge pre-trained teacher model transfers its knowledge to a small student network [4,10] to preserve teacher's accuracy in a resource-limited student model. The student trained under this framework is expected to have higher accuracy compared to if it was trained by the conventional training framework. The student owes this success to the teacher's output class probabilities for each input data. The relative class probabilities tell the student how the teacher generalizes and views the similarities between classes for an input sample. Many different extensions to knowledge distillation are proposed to leverage intermediate representations [16,36,20], shared representations [33,31], representation grafting [18], adversarial learning [9,27,28], and contrastive learning [24]. Some other variants have improved the robustness of the student by injecting noise in the teacher's outputs [17], aggregating multiple teachers' knowledge by voting mechanisms [32], deploying teacher assistants [14], or allowing peers to teach each other [38]. Our work is closely related to those extensions of knowledge distillations, which exploit the knowledge of intermediate representations (e.g., [16,36,20,33]). We attempt to address their practical challenges when (1) the dimension mismatch between student's and teacher's intermediate layers is large, (2) the choices of intermediate/shared layers are not obvious for the teacher and student models. \n\nKnowledge Distillation via Intermediate Classifiers.",
                    "score": 0.5215421155736869,
                    "section_title": "Related Work",
                    "char_start_offset": 3016,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 124,
                            "end": 286
                        },
                        {
                            "start": 287,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 545
                        },
                        {
                            "start": 546,
                            "end": 699
                        },
                        {
                            "start": 700,
                            "end": 723
                        },
                        {
                            "start": 724,
                            "end": 938
                        },
                        {
                            "start": 939,
                            "end": 1085
                        },
                        {
                            "start": 1086,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1322
                        },
                        {
                            "start": 1323,
                            "end": 1565
                        },
                        {
                            "start": 1566,
                            "end": 1826
                        },
                        {
                            "start": 1827,
                            "end": 1985
                        },
                        {
                            "start": 1986,
                            "end": 2226
                        },
                        {
                            "start": 2229,
                            "end": 2281
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 282,
                            "end": 285,
                            "matchedPaperCorpusId": "212628335"
                        },
                        {
                            "start": 569,
                            "end": 573,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 864,
                            "end": 867,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 867,
                            "end": 870,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1429,
                            "end": 1433,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 1433,
                            "end": 1436,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 1436,
                            "end": 1439,
                            "matchedPaperCorpusId": "201670719"
                        },
                        {
                            "start": 1464,
                            "end": 1468,
                            "matchedPaperCorpusId": "28101867"
                        },
                        {
                            "start": 1468,
                            "end": 1471,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 1524,
                            "end": 1527,
                            "matchedPaperCorpusId": "21679091"
                        },
                        {
                            "start": 1527,
                            "end": 1530,
                            "matchedPaperCorpusId": "53976534"
                        },
                        {
                            "start": 1530,
                            "end": 1533,
                            "matchedPaperCorpusId": "19182852"
                        },
                        {
                            "start": 1560,
                            "end": 1564,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 1742,
                            "end": 1746,
                            "matchedPaperCorpusId": "26021416"
                        },
                        {
                            "start": 1777,
                            "end": 1781,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 1821,
                            "end": 1825,
                            "matchedPaperCorpusId": "26071966"
                        },
                        {
                            "start": 1970,
                            "end": 1974,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 1974,
                            "end": 1977,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 1977,
                            "end": 1980,
                            "matchedPaperCorpusId": "201670719"
                        },
                        {
                            "start": 1980,
                            "end": 1983,
                            "matchedPaperCorpusId": "28101867"
                        },
                        {
                            "start": 2040,
                            "end": 2043,
                            "matchedPaperCorpusId": "9794990"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65771484375
                }
            ],
            "relevance_judgement": 0.65771484375,
            "relevance_judgment_input_expanded": "# Title: Distilling Knowledge via Intermediate Classifiers\n# Venue: \n# Authors: Aryan Asadian, Amirali Salehi-Abari\n## Abstract\nThe crux of knowledge distillation is to effectively train a resource-limited student model with the guide of a pre-trained larger teacher model. However, when there is a large difference between the model complexities of teacher and student (i.e., capacity gap), knowledge distillation loses its strength in transferring knowledge from the teacher to the student, thus training a weaker student. To mitigate the impact of the capacity gap, we introduce knowledge distillation via intermediate heads. By extending the intermediate layers of the teacher (at various depths) with classifier heads, we cheaply acquire a cohort of heterogeneous pre-trained teachers. The intermediate classifier heads can all together be efficiently learned while freezing the backbone of the pre-trained teacher. The cohort of teachers (including the original teacher) co-teach the student simultaneously. Our experiments on various teacher-student pairs and datasets have demonstrated that the proposed approach outperforms the canonical knowledge distillation approach and its extensions.\n## Related Work\nThere has been growing interest in model compression techniques for compressing the larger neural networks to smaller ones. Model pruning, as an optimization framework, intends to reduce the unnecessary structure and parameters of a large model to gain an efficient smaller network [3]. Model quantization compresses the neural network by reducing the precision of its parameters at the expense of accuracy [29]. Designing efficient deep neural architectures [22] intends to satisfy the specified accuracy requirement given resource limitations. Knowledge distillation [10], that can be combined with others, aims to preserve the accuracy of a large trained teacher model in a smaller student model. Knowledge Distillation. Knowledge distillation is a training framework in which a huge pre-trained teacher model transfers its knowledge to a small student network [4,10] to preserve teacher's accuracy in a resource-limited student model. The student trained under this framework is expected to have higher accuracy compared to if it was trained by the conventional training framework. The student owes this success to the teacher's output class probabilities for each input data. The relative class probabilities tell the student how the teacher generalizes and views the similarities between classes for an input sample. Many different extensions to knowledge distillation are proposed to leverage intermediate representations [16,36,20], shared representations [33,31], representation grafting [18], adversarial learning [9,27,28], and contrastive learning [24]. Some other variants have improved the robustness of the student by injecting noise in the teacher's outputs [17], aggregating multiple teachers' knowledge by voting mechanisms [32], deploying teacher assistants [14], or allowing peers to teach each other [38]. Our work is closely related to those extensions of knowledge distillations, which exploit the knowledge of intermediate representations (e.g., [16,36,20,33]). We attempt to address their practical challenges when (1) the dimension mismatch between student's and teacher's intermediate layers is large, (2) the choices of intermediate/shared layers are not obvious for the teacher and student models. \n\nKnowledge Distillation via Intermediate Classifiers.",
            "reference_string": "[235262724 | Asadian et al. | 2021 | Citations: 1]"
        },
        {
            "title": "Knowledge Distillation Using Frontier Open-source LLMs: Generalizability and the Role of Synthetic Data",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.18588, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2304553832",
                    "name": "Anup Shirgaonkar"
                },
                {
                    "authorId": "2304553870",
                    "name": "Nikhil Pandey"
                },
                {
                    "authorId": "2216373",
                    "name": "Nazmiye Ceren Abay"
                },
                {
                    "authorId": "2294358285",
                    "name": "Tolga Aktas"
                },
                {
                    "authorId": "2257349985",
                    "name": "Vijay Aski"
                }
            ],
            "abstract": "Leading open-source large language models (LLMs) such as Llama-3.1-Instruct-405B are extremely capable at generating text, answering questions, and solving a variety of natural language understanding tasks. However, they incur higher inference cost and latency compared to smaller LLMs. Knowledge distillation provides a way to use outputs from these large, capable teacher models to train smaller student models which can be used for inference at lower cost and latency, while retaining comparable accuracy. We investigate the efficacy of distillation using the Llama-3.1-405B-Instruct teacher and the smaller Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct student models. Contributions of this work include (a) We evaluate the generalizability of distillation with the above Llama-3.1 teacher-student pairs across different tasks and datasets (b) We show that using synthetic data during distillation significantly improves the accuracy of 8B and 70B models, and when used with reasoning chains, even matches or surpasses the zero-shot accuracy of 405B model on some datasets (c) We empirically show that distillation enables 8B and 70B models to internalize 405B's reasoning ability by using only standard fine-tuning (without customizing any loss function). This allows cost and latency-efficient student model inference. (d) We show pitfalls in evaluation of distillation, and present task-specific evaluation, including both human and LLM-grading, and ground-truth based traditional accuracy benchmarks. This methodical study brings out the fundamental importance of synthetic data quality in knowledge distillation, and of combining multiple, task-specific ways of accuracy and quality evaluation in assessing the effectiveness of distillation.",
            "corpus_id": 273549440,
            "sentences": [
                {
                    "corpus_id": "273549440",
                    "title": "Knowledge Distillation Using Frontier Open-source LLMs: Generalizability and the Role of Synthetic Data",
                    "text": "Large Language Models (LLMs) have gradually become more capable of showing remarkable abilities to understand and generate language, and being able to solve a variety of natural language processing (NLP) tasks such as questionanswering and summarization. The largest and most capable LLMs, such as Llama-3.1-405B-Instruct, are setting new standards in AI performance. With its massive 405 billion parameters, the model not only holds immense pre-trained knowledge, but according to Meta's benchmark evaluations (Meta, 2024), also performs competitively with leading closed-source models. Beyond the direct use of the model for inference and text generation, which is associated with significant computational demands, the 405B model can also be utilized for knowledge distillation. \n\nKnowledge distillation is a promising direction to alleviate inference cost and latency without sacrificing accuracy (Hinton et al., 2015;Papamakarios, 2015). In knowledge distillation, the knowledge from a large, complex model (the \"teacher\" model) is transferred to a smaller, simpler model (the \"student\" model). The objective is to develop a student model that closely approximates the performance of the teacher model while being smaller, cheaper and faster for inference. In knowledge distillation, knowledge is transferred from a teacher model to a student model using one Our methodical study highlights the critical role of high-quality synthetic data in knowledge distillation and the necessity of accurate evaluation processes. Through extensive experiments on various well-known benchmarking datasets, we demonstrate that enhanced task-specific synthetic data helps the student LLM learn the teacher's reasoning abilities. Rest of the paper is organized as follows. Section 2 describes prior related work on similar methods. Section 3 describes our distillation methodology that uses task-specific prompts, metrics, and synthetic data generation procedure. Section 4 covers the results for all the tasks and datasets showing generalizability of distillation using the Llama-3.1 teacher-student pairs. In Section 5, we present the conclusions and future work, while Section 6 discusses the limitations of our work. All the prompts used in our experimentation are provided in the Appendix.",
                    "score": 0.5011596449536888,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 254
                        },
                        {
                            "start": 255,
                            "end": 367
                        },
                        {
                            "start": 368,
                            "end": 587
                        },
                        {
                            "start": 588,
                            "end": 781
                        },
                        {
                            "start": 784,
                            "end": 942
                        },
                        {
                            "start": 943,
                            "end": 1099
                        },
                        {
                            "start": 1100,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1522
                        },
                        {
                            "start": 1523,
                            "end": 1718
                        },
                        {
                            "start": 1719,
                            "end": 1761
                        },
                        {
                            "start": 1762,
                            "end": 1820
                        },
                        {
                            "start": 1821,
                            "end": 1952
                        },
                        {
                            "start": 1953,
                            "end": 2096
                        },
                        {
                            "start": 2097,
                            "end": 2209
                        },
                        {
                            "start": 2210,
                            "end": 2283
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65478515625
                }
            ],
            "relevance_judgement": 0.65478515625,
            "relevance_judgment_input_expanded": "# Title: Knowledge Distillation Using Frontier Open-source LLMs: Generalizability and the Role of Synthetic Data\n# Venue: arXiv.org\n# Authors: Anup Shirgaonkar, Nikhil Pandey, Nazmiye Ceren Abay, Tolga Aktas, Vijay Aski\n## Abstract\nLeading open-source large language models (LLMs) such as Llama-3.1-Instruct-405B are extremely capable at generating text, answering questions, and solving a variety of natural language understanding tasks. However, they incur higher inference cost and latency compared to smaller LLMs. Knowledge distillation provides a way to use outputs from these large, capable teacher models to train smaller student models which can be used for inference at lower cost and latency, while retaining comparable accuracy. We investigate the efficacy of distillation using the Llama-3.1-405B-Instruct teacher and the smaller Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct student models. Contributions of this work include (a) We evaluate the generalizability of distillation with the above Llama-3.1 teacher-student pairs across different tasks and datasets (b) We show that using synthetic data during distillation significantly improves the accuracy of 8B and 70B models, and when used with reasoning chains, even matches or surpasses the zero-shot accuracy of 405B model on some datasets (c) We empirically show that distillation enables 8B and 70B models to internalize 405B's reasoning ability by using only standard fine-tuning (without customizing any loss function). This allows cost and latency-efficient student model inference. (d) We show pitfalls in evaluation of distillation, and present task-specific evaluation, including both human and LLM-grading, and ground-truth based traditional accuracy benchmarks. This methodical study brings out the fundamental importance of synthetic data quality in knowledge distillation, and of combining multiple, task-specific ways of accuracy and quality evaluation in assessing the effectiveness of distillation.\n## Introduction\nLarge Language Models (LLMs) have gradually become more capable of showing remarkable abilities to understand and generate language, and being able to solve a variety of natural language processing (NLP) tasks such as questionanswering and summarization. The largest and most capable LLMs, such as Llama-3.1-405B-Instruct, are setting new standards in AI performance. With its massive 405 billion parameters, the model not only holds immense pre-trained knowledge, but according to Meta's benchmark evaluations (Meta, 2024), also performs competitively with leading closed-source models. Beyond the direct use of the model for inference and text generation, which is associated with significant computational demands, the 405B model can also be utilized for knowledge distillation. \n\nKnowledge distillation is a promising direction to alleviate inference cost and latency without sacrificing accuracy (Hinton et al., 2015;Papamakarios, 2015). In knowledge distillation, the knowledge from a large, complex model (the \"teacher\" model) is transferred to a smaller, simpler model (the \"student\" model). The objective is to develop a student model that closely approximates the performance of the teacher model while being smaller, cheaper and faster for inference. In knowledge distillation, knowledge is transferred from a teacher model to a student model using one Our methodical study highlights the critical role of high-quality synthetic data in knowledge distillation and the necessity of accurate evaluation processes. Through extensive experiments on various well-known benchmarking datasets, we demonstrate that enhanced task-specific synthetic data helps the student LLM learn the teacher's reasoning abilities. Rest of the paper is organized as follows. Section 2 describes prior related work on similar methods. Section 3 describes our distillation methodology that uses task-specific prompts, metrics, and synthetic data generation procedure. Section 4 covers the results for all the tasks and datasets showing generalizability of distillation using the Llama-3.1 teacher-student pairs. In Section 5, we present the conclusions and future work, while Section 6 discusses the limitations of our work. All the prompts used in our experimentation are provided in the Appendix.",
            "reference_string": "[273549440 | Shirgaonkar et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Dynamic Knowledge Distillation for Pre-trained Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2021,
            "reference_count": 43,
            "citation_count": 49,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.emnlp-main.31.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.11295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49192881",
                    "name": "Lei Li"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "1906099",
                    "name": "Shuhuai Ren"
                },
                {
                    "authorId": "50492525",
                    "name": "Peng Li"
                },
                {
                    "authorId": "2108485135",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "11774802",
                    "name": "Xu Sun"
                }
            ],
            "abstract": "Knowledge distillation (KD) has been proved effective for compressing large-scale pre-trained language models. However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset. In this paper, we explore whether a dynamic knowledge distillation that empowers the student to adjust the learning procedure according to its competency, regarding the student performance and learning efficiency. We explore the dynamical adjustments on three aspects: teacher model adoption, data selection, and KD objective adaptation. Experimental results show that (1) proper selection of teacher model can boost the performance of student model; (2) conducting KD with 10% informative instances achieves comparable performance while greatly accelerates the training; (3) the student performance can be boosted by adjusting the supervision contribution of different alignment objective. We find dynamic knowledge distillation is promising and provide discussions on potential future directions towards more efficient KD methods.",
            "corpus_id": 237605152,
            "sentences": [
                {
                    "corpus_id": "237605152",
                    "title": "Dynamic Knowledge Distillation for Pre-trained Language Models",
                    "text": "Specifically, we are curious about whether learning from a bigger PLM with better performance can lead to a better distilled student model. We conduct probing experiments to distill a 6-layer student BERT model from BERT BASE with 12 layers, and BERT LARGE with 24 layers, respectively. We conducts the experiment on two datasets, RTE (Bentivogli et al., 2009) and CoLA (Warstadt et al., 2019), where two teacher models exhibit clear performance gap, and a sentiment classification benchmark IMDB (Maas et al., 2011). Detailed experimental setup can be found in Appendix A. As shown in Table 1, we surprisingly find that while the BERT LARGE teacher clearly outperforms the small BERT BASE teacher model, the student model distilled by the BERT BASE teacher achieves better performance on all three datasets. This phenomenon is counter-intuitive as a larger teacher is supposed to provide better supervision signal for the student model. We think that there are two possible factors regarding the size of teacher model that leading to the deteriorated performance:\n\n(1) The predicted logits of the teacher model become less soft as the teacher model becomes larger and more confident about its prediction (Guo et al.,  The teacher model is BERT BASE with 12 layers. 2017; Desai and Durrett, 2020), which decreases the effect of knowledge transfer via the soft targets. We find that a smaller \u03c4 also leads to a decreased performance of the student model, indicating the the less-softened teacher prediction will decrease the student performance. 2 (2) The capacity gap between the teacher and student model increases as the teacher becomes larger. The competency of the student model can not match that of the large teacher model, which weakens the performance of KD.\n\nTo explore the combined influence of these factors, we distill student models with different layers and plot the performance gain compared to directly training the student model without distillation in Figure 2. It can be found that by decreasing the student size, the better supervision from teacher model boosts the performance, while the two counteractive factors dominate as the gap becomes much larger, decreasing the performance gain. We notice that this phenomenon is also observed by Mirzadeh et al. (2020) in computer vision tasks using convolutional networks, showing that it is a widespread issue and",
                    "score": 0.5829030763902456,
                    "section_title": "Bigger Teacher Not Always Raises Better Student",
                    "char_start_offset": 6839,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 497,
                            "end": 516,
                            "matchedPaperCorpusId": "1428702"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65234375
                }
            ],
            "relevance_judgement": 0.65234375,
            "relevance_judgment_input_expanded": "# Title: Dynamic Knowledge Distillation for Pre-trained Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Lei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou, Xu Sun\n## Abstract\nKnowledge distillation (KD) has been proved effective for compressing large-scale pre-trained language models. However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset. In this paper, we explore whether a dynamic knowledge distillation that empowers the student to adjust the learning procedure according to its competency, regarding the student performance and learning efficiency. We explore the dynamical adjustments on three aspects: teacher model adoption, data selection, and KD objective adaptation. Experimental results show that (1) proper selection of teacher model can boost the performance of student model; (2) conducting KD with 10% informative instances achieves comparable performance while greatly accelerates the training; (3) the student performance can be boosted by adjusting the supervision contribution of different alignment objective. We find dynamic knowledge distillation is promising and provide discussions on potential future directions towards more efficient KD methods.\n## Bigger Teacher Not Always Raises Better Student\nSpecifically, we are curious about whether learning from a bigger PLM with better performance can lead to a better distilled student model. We conduct probing experiments to distill a 6-layer student BERT model from BERT BASE with 12 layers, and BERT LARGE with 24 layers, respectively. We conducts the experiment on two datasets, RTE (Bentivogli et al., 2009) and CoLA (Warstadt et al., 2019), where two teacher models exhibit clear performance gap, and a sentiment classification benchmark IMDB (Maas et al., 2011). Detailed experimental setup can be found in Appendix A. As shown in Table 1, we surprisingly find that while the BERT LARGE teacher clearly outperforms the small BERT BASE teacher model, the student model distilled by the BERT BASE teacher achieves better performance on all three datasets. This phenomenon is counter-intuitive as a larger teacher is supposed to provide better supervision signal for the student model. We think that there are two possible factors regarding the size of teacher model that leading to the deteriorated performance:\n\n(1) The predicted logits of the teacher model become less soft as the teacher model becomes larger and more confident about its prediction (Guo et al.,  The teacher model is BERT BASE with 12 layers. 2017; Desai and Durrett, 2020), which decreases the effect of knowledge transfer via the soft targets. We find that a smaller \u03c4 also leads to a decreased performance of the student model, indicating the the less-softened teacher prediction will decrease the student performance. 2 (2) The capacity gap between the teacher and student model increases as the teacher becomes larger. The competency of the student model can not match that of the large teacher model, which weakens the performance of KD.\n\nTo explore the combined influence of these factors, we distill student models with different layers and plot the performance gain compared to directly training the student model without distillation in Figure 2. It can be found that by decreasing the student size, the better supervision from teacher model boosts the performance, while the two counteractive factors dominate as the gap becomes much larger, decreasing the performance gain. We notice that this phenomenon is also observed by Mirzadeh et al. (2020) in computer vision tasks using convolutional networks, showing that it is a widespread issue and",
            "reference_string": "[237605152 | Li et al. | 2021 | Citations: 49]"
        },
        {
            "title": "Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2019,
            "reference_count": 41,
            "citation_count": 225,
            "influential_citation_count": 32,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.08962, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1388156275",
                    "name": "Iulia Turc"
                },
                {
                    "authorId": "1744179",
                    "name": "Ming-Wei Chang"
                },
                {
                    "authorId": "2544107",
                    "name": "Kenton Lee"
                },
                {
                    "authorId": "3259253",
                    "name": "Kristina Toutanova"
                }
            ],
            "abstract": "Recent developments in NLP have been accompanied by large, expensive models. Knowledge distillation is the standard method to realize these gains in applications with limited resources: a compact student is trained to recover the outputs of a powerful teacher. While most prior work investigates student architectures and transfer techniques, we focus on an often-neglected aspect---student initialization. We argue that a random starting point hinders students from fully leveraging the teacher expertise, even in the presence of a large transfer set. We observe that applying language model pre-training to students unlocks their generalization potential, surprisingly even for very compact networks. We conduct experiments on 4 NLP tasks and 24 sizes of Transformer-based students; for sentiment classification on the Amazon Book Reviews dataset, pre-training boosts size reduction and TPU speed-up from 3.1x/1.25x to 31x/16x. Extensive ablation studies dissect the interaction between pre-training and distillation, revealing a compound effect even when they are applied on the same unlabeled dataset.",
            "corpus_id": 201666324,
            "sentences": [
                {
                    "corpus_id": "201666324",
                    "title": "Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation",
                    "text": "While recent work has distilled pre-trained teacher models (Chia et al., 2018;Tang et al., 2019;Hu et al., 2018;Yang et al., 2019a), or learned representations from unlabeled text for compact models without using a teacher (Gururangan et al., 2019), the interactions between pre-training and distillation for compact students have yet to be studied thoroughly. The main research question in this work is whether unsupervised representation learning for the student is still important in the presence of a strong-and usually also pre-trained-teacher. Towards answering this question, we perform controlled experiments on 4 NLP tasks, including ones requiring inferences for text pairs (natural language language inference and textual entailment) and single texts (sentiment classification). For each experiment, we train 24 Transformer-based student models with sizes ranging from 4m to 110m parameters. \n\nOur experiments show that the student initialization strategy in distillation for compact models is crucial, even when the teacher models are already pre-trained. When the amount of in-domain unlabeled task-relevant data for transfer is limited (less than 2 million examples in our experiments), pre-training+fine-tuning alone outperforms distillation with a randomly initialized student. We find that the best strategy across all settings is a sequence of three standard training operations: MLM pre-training, distillation, and optional fine-tuning. In this work, we will refer to this method as Pretrained Distillation (PD) (Figure 1). PD not only outperforms all of its constituent operations applied in isolation, but also displays robustness to various aspects of the data: small and slightly out-ofdomain transfer sets have less negative impact on student performance than in standard distillation. \n\nMore interestingly, we observe that PD still ranks highest even when the same unlabeled dataset is used for pre-training and distillation. This indicates that their compound effect cannot be explained solely by their ability to take advantage of complementary resources, and that their different training objectives contribute to the overall success.",
                    "score": 0.49801230661791246,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1679,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 360
                        },
                        {
                            "start": 361,
                            "end": 789
                        },
                        {
                            "start": 790,
                            "end": 902
                        },
                        {
                            "start": 905,
                            "end": 1067
                        },
                        {
                            "start": 1068,
                            "end": 1293
                        },
                        {
                            "start": 1294,
                            "end": 1455
                        },
                        {
                            "start": 1456,
                            "end": 1542
                        },
                        {
                            "start": 1543,
                            "end": 1809
                        },
                        {
                            "start": 1812,
                            "end": 1950
                        },
                        {
                            "start": 1951,
                            "end": 2162
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65234375
                }
            ],
            "relevance_judgement": 0.65234375,
            "relevance_judgment_input_expanded": "# Title: Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation\n# Venue: arXiv.org\n# Authors: Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova\n## Abstract\nRecent developments in NLP have been accompanied by large, expensive models. Knowledge distillation is the standard method to realize these gains in applications with limited resources: a compact student is trained to recover the outputs of a powerful teacher. While most prior work investigates student architectures and transfer techniques, we focus on an often-neglected aspect---student initialization. We argue that a random starting point hinders students from fully leveraging the teacher expertise, even in the presence of a large transfer set. We observe that applying language model pre-training to students unlocks their generalization potential, surprisingly even for very compact networks. We conduct experiments on 4 NLP tasks and 24 sizes of Transformer-based students; for sentiment classification on the Amazon Book Reviews dataset, pre-training boosts size reduction and TPU speed-up from 3.1x/1.25x to 31x/16x. Extensive ablation studies dissect the interaction between pre-training and distillation, revealing a compound effect even when they are applied on the same unlabeled dataset.\n## INTRODUCTION\nWhile recent work has distilled pre-trained teacher models (Chia et al., 2018;Tang et al., 2019;Hu et al., 2018;Yang et al., 2019a), or learned representations from unlabeled text for compact models without using a teacher (Gururangan et al., 2019), the interactions between pre-training and distillation for compact students have yet to be studied thoroughly. The main research question in this work is whether unsupervised representation learning for the student is still important in the presence of a strong-and usually also pre-trained-teacher. Towards answering this question, we perform controlled experiments on 4 NLP tasks, including ones requiring inferences for text pairs (natural language language inference and textual entailment) and single texts (sentiment classification). For each experiment, we train 24 Transformer-based student models with sizes ranging from 4m to 110m parameters. \n\nOur experiments show that the student initialization strategy in distillation for compact models is crucial, even when the teacher models are already pre-trained. When the amount of in-domain unlabeled task-relevant data for transfer is limited (less than 2 million examples in our experiments), pre-training+fine-tuning alone outperforms distillation with a randomly initialized student. We find that the best strategy across all settings is a sequence of three standard training operations: MLM pre-training, distillation, and optional fine-tuning. In this work, we will refer to this method as Pretrained Distillation (PD) (Figure 1). PD not only outperforms all of its constituent operations applied in isolation, but also displays robustness to various aspects of the data: small and slightly out-ofdomain transfer sets have less negative impact on student performance than in standard distillation. \n\nMore interestingly, we observe that PD still ranks highest even when the same unlabeled dataset is used for pre-training and distillation. This indicates that their compound effect cannot be explained solely by their ability to take advantage of complementary resources, and that their different training objectives contribute to the overall success.",
            "reference_string": "[201666324 | Turc et al. | 2019 | Citations: 225]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "249192332",
            "title": "A Closer Look at Self-supervised Lightweight Vision Transformers",
            "text": "We further conduct additional experiments with various models as teachers and compared their performance on various downstream tasks (see Tab. A9). The configurations of the student model (ViT-Tiny) and teacher models are presented in Tab. A7. The results indicate that an appropriately sized teacher model provides the most improvement gains in distillation, which is a common finding in the area of knowledge distillation (Cho & Hariharan, 2019;Jin et al., 2019;Mirzadeh et al., 2020). To further investigate the impact of teacher size, we conducted CKA-based layer representation analyses of these teachers, as shown in Fig. A7. It can be seen that a teacher that is too small (MAE-Small) also suffers from degraded representation on higher layers and can not provide sufficient knowledge, while a teacher that is too large (MAE-Large) would result in a mismatch of capacity with the tiny student model, considering it has over 50 times more parameters than ViT-Tiny with different depths and attention head numbers, which leads to a little distinct learned pattern compared to the reference tiny model, and may not be suitable for the student.",
            "score": 0.7099576131164311,
            "section_title": "C.4. Distilling with Larger Teachers",
            "char_start_offset": 34492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 1147
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 447,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 447,
                    "end": 464,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 464,
                    "end": 486,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.499755859375
        },
        {
            "corpus_id": "273375639",
            "title": "TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant",
            "text": "Knowledge Distillation (KD) (Hinton et al., 2015;Romero et al., 2015) has been demonstrated as a powerful method to transfer knowledge from a pre-trained and cumbersome teacher model to a compact and efficient student model. Compared to the model trained from scratch, the performance of the student model distilled by appropriate teachers usually improves significantly. Commonly, knowledge transferred is derived from either the output logits (logits-based KD (Sun et al., 2024)) or the intermediate features (feature-based KD (Romero et al., 2015)) of the teacher model. Therefore, it is intuitive to understand different teachers have different knowledge (logits or features) determined by their unique architectures (Liu et al., 2021a). \n\nMost existing KD approaches focus on similar-architecture distillation (Romero et al., 2015;Tian et al., 2020;Liu et al., 2023) (called SAKD), i.e., optional teachers are restricted to a limited scope with structures similar to the student model. However, this homogeneous distillation presents two principal limitations: (1) Limited Potential: Compared to the broader range of arbitrary teachers (including homogeneous and heterogeneous ones), the restricted scope of teachers in SAKD may fail to include the optimal knowledge necessary to enhance the performance of certain students. For instance, as OFA (Hao et al., 2023) demonstrated, distilling knowledge from a heterogeneous ViT-Base to ResNet50 yields superior student performance compared to using a ResNet152 as the homogeneous teacher. (2) Limited Flexibility: The emergence of new models (Liu et al., 2022; Preprint  Tolstikhin et al., 2021) or the scarcity of perfectly tuned homogeneous teachers in domain-specific tasks (Ronneberger et al., 2015;Li et al., 2024) poses significant challenges in obtaining suitable homogeneous teachers, thereby impeding the applicability of SAKD. Thus, this paper expands KD to cross-architecture KD (CAKD), investigating methods to distill knowledge from both homogeneous and heterogeneous teachers to students (Hao et al., 2023).",
            "score": 0.7032399378155897,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2073
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 69,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 462,
                    "end": 480,
                    "matchedPaperCorpusId": "268247468"
                },
                {
                    "start": 529,
                    "end": 550,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 815,
                    "end": 836,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 836,
                    "end": 854,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1729,
                    "end": 1755,
                    "matchedPaperCorpusId": "3719281"
                },
                {
                    "start": 1755,
                    "end": 1771,
                    "matchedPaperCorpusId": "268554202"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76806640625
        },
        {
            "corpus_id": "272753230",
            "title": "Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights",
            "text": "Our research demonstrates an effective and straightforward approach for generating rationales from a large model to enhance a smaller model. However, a significant performance gap between the teacher model (the large model) and the student model (the small model) still exists. As illustrated in table 6, the flan-t5-3b, which serves as the teacher model, significantly outperforms the smaller t5-small model across all four datasets we tested. \n\nThis limitation underscores the inherent challenge in knowledge distillation and model scaling. While our method effectively transfers knowledge from a large to a small model, the reduced capacity of the smaller model limits its ability to fully replicate the performance of its larger counterpart. This performance discrepancy highlights the trade-offs involved in model downsizing, where gains in efficiency and deployability often come at the cost of reduced accuracy and overall capability. \n\nAddressing this gap remains a key area for future research. Efforts could focus on developing more advanced distillation techniques or optimizing small models to better capture and utilize the knowledge transferred from larger models. Our findings lay the groundwork for such exploration, offering insights into the dynamics of knowledge transfer between models of varying sizes and complexities.",
            "score": 0.6887177270358885,
            "section_title": "Limitation",
            "char_start_offset": 23879,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 941
                },
                {
                    "start": 944,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1340
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "264941356",
            "title": "Incorporating Natural Language Processing into Virtual Assistants: An Intelligent Assessment Strategy for Enhancing Language Comprehension",
            "text": "Given the imperative of modest model sizes for lowlatency applications, a direct distillation from extensively www.ijacsa.thesai.org large teacher models to considerably smaller student models might hinder the effective transfer of the teacher's expertise. As a remedy, a two-stage teacher assistance setup was devised for the distillation process, as illustrated in Fig. 1. This strategy aimed to strike a balance between knowledge transfer and model size reduction. In the initial stage, the immense teacher model was compressed into an intermediate-sized model. Subsequently, the final student model was trained using this intermediate model as a guide. This approach ensured that the transfer of knowledge from the teacher to the student was well-optimized, despite the significant size reduction. Drawing inspiration from the teacher's pretraining methodology, a distillation process was initiated from a randomly initialized student model. Convergence in training signaled a seamless transition to the deployment of the Stage 2 teacher model, thus continuing the distillation process. Importantly, the data employed for distillation in both stages remained consistent with the data utilized for teacher pretraining in their respective stages. Within the intermediate student/teacher pairing, a balanced blend of categorical crossentropy (MLM loss) and soft cross-entropy was applied, with equal weighting. Remarkably, experimentation indicated no substantial benefits from incorporating the attention and hidden layer outputs of the teacher model. Transitioning to the final student model, a dual-stage process was undertaken. First, the intermediate model underwent further pretraining, exclusively using Stage 2 data and without teacher involvement. Subsequently, a distillation procedure was executed to seamlessly transfer knowledge to the compact final student model. During this distillation phase, techniques mirrored those employed in the initial distillation, with the addition of hidden-layer output matching. In essence, the approach mirrors the core principles of the process outlined in the source paper. The process ensures effective knowledge transfer while mitigating the challenges arising from substantial model size reductions.",
            "score": 0.6871667486292476,
            "section_title": "D. Distillation",
            "char_start_offset": 34882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2252
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58544921875
        },
        {
            "corpus_id": "228376532",
            "title": "Reinforced Multi-Teacher Selection for Knowledge Distillation",
            "text": "weaknesses of various teacher models leaves a huge untouched space for better knowledge distillation. To make a multi-teacher approach work well, teacher models have to be diverse. Exploiting the diverse strengths of various teacher models can bring in huge advantages.\n\nIndividual teacher models may perform differently on various instances. Different models may vary in hypothesis space, optimization strategy, parameter initialization and many other factors, which result in different performance among different cases. Ideally, we want to assign different weights to different teacher models for different training instances according to their performance in individual cases.\n\nDifferentiating among teacher models is far from trivial. Surprisingly, a stronger teacher model may not necessarily lead to a better student model. As shown in Table 1 (Sun et al. (2019)), the RoBERTa-Base model performs better than the BERT-Base model on the MRPC and MNLI-mm tasks. However, the student model using three-layer transformer BERT distilled from the weaker teacher model performs better on the same tasks than the same student model distilled from the stronger teacher model. One possible reason is that the effectiveness of distillation may be bounded by the capability of the student model. A simple student model with fewer parameters may not be able to approximate a very complex teacher model, since the complex teacher model may capture finer-grained patterns in data and cause the student model to overfit in some parts of the data and under some other parts. To achieve good distillation, we have to choose teacher models matching capacities of student models.\n\nBased on the above insights, in this paper, we systematically study how to coordinate teacher models and student models in knowledge distillation. Specifically, we investigate how to assign appropriate weights to different teacher models on various training samples. To the best of our knowledge, we are the first to treat teacher models deferentially at instance level in knowledge distillation.\n\nWe formulate the teacher model selection problem under a reinforcement learning framework: the decision is made based on the characteristics of training examples and the outputs of teacher models, while the policy is learned towards maximizing the student performance as the return.\n\nTo verify the effectiveness of our approach in NLP, we conduct extensive experiments on several important tasks from the GLUE benchmark , including sentiment analysis, paraphrase similarity matching and natural language inference. Our experimental results clearly show that our reinforced",
            "score": 0.6481762490587348,
            "section_title": "Introduction",
            "char_start_offset": 2006,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.708984375
        },
        {
            "corpus_id": "252873105",
            "title": "You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models",
            "text": "The purpose of this experiment is to visualize how the model's performance scales with the size of the pre-training dataset. Also, we check the behavior of the teacher-student knowledge distillation with the change of data size used to train a teacher and a student in a monolingual setting. \n\nWe train a monolingual model on German Wikipedia data with five sizes (in millions of characters): 10, 20, 30, 50, and 100. Subsequently, we designate 10, 50, and 100 million character models as teachers and distill their knowledge into students on the same size or smaller corpus. 7 s presented in figure 4, the teacher performance Shared script Diverse script HL -2.9 -2.5 HL Balanced -9.2 -12 Ours -6.1 -5.2  can be nearly matched by a student trained on a considerably smaller corpus. For the teacher trained on the largest split, the student performance rises steadily with the increase of distillation detest from 10 to 30 million characters and drops after that point. The performance of the student trained on 100 million characters is noticeably low. It is a sign of over-fitting, as in our setting, distillation set is always a subset of the teacher's training set. Also, in the case of teachers trained on smaller corpora, distillation on the dataset of the same size (as the teacher training set) leads to a drop in performance. Therefore, we claim that the distillation is beneficial when the teacher's training set is larger than the student's one.",
            "score": 0.6479346929162738,
            "section_title": "C Teacher-Student Method in the Monolingual World",
            "char_start_offset": 18211,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1456
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360107421875
        },
        {
            "corpus_id": "276533262",
            "title": "Decoupled Classifier Knowledge Distillation",
            "text": "However, because they focus on different aspects, the knowledge learned by the student model also varies. \n\nIn the case of heterogeneous architectures between teacher and student models [14], the features of these models reside in different latent feature spaces, making it challenging to ensure effective alignment of the learned features. Consequently, directly matching these unrelated features is not only unproductive but may also hinder the student model's performance. Furthermore, feature-based distillation methods focus more on local regions, and this localized attention may be insufficient for effectively transferring knowledge from the teacher model to the student model in knowledge distillation [15]. The knowledge embedded in the teacher model is often too complex for the student model to fully absorb and process [16]. Although teacher models are generally more complex and capable of capturing more knowledge, the main challenge lies in distilling this knowledge into a form that is accessible and beneficial for the student model [16]. \n\nWe have observed that existing feature-based and output-based knowledge distillation techniques are mostly applied independently. Although both methods have their respective advantages in improving model performance and reducing model complexity, there is still no effective method that can simultaneously integrate the strengths of both, as they focus on different aspects of the model. As pointed out by Jianping Gou et al [17], how to model these different types of knowledge in a unified and complementary framework remains an urgent challenge. Specifically, knowledge from different layers may have varying impacts on the training of the student model [18]. For example, output-based knowledge mainly comes from the model's final layer, while feature-based knowledge, guided by deeper layers, might face the risk of over-regularization. Meanwhile, recent studies have found that deep features are often linearized. Transforming features in certain directions can generate representations corresponding to the same category but with different samples. However, due to limited capacity, it is difficult for the student model to learn the distinctive features captured by the powerful teacher [19]. The lack of training data further exacerbates the performance gap between the student and teacher models. \n\nTherefore, the key step in knowledge distillation lies in how to effectively transfer the rich knowledge of the teacher model to the student model.",
            "score": 0.6433251997015039,
            "section_title": "Introduction",
            "char_start_offset": 2228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 108,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2259
                },
                {
                    "start": 2260,
                    "end": 2365
                },
                {
                    "start": 2368,
                    "end": 2515
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "265871679"
                },
                {
                    "start": 2254,
                    "end": 2258,
                    "matchedPaperCorpusId": "266998295"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72998046875
        },
        {
            "corpus_id": "253018808",
            "title": "Similarity of Neural Architectures Using Adversarial Attack Transferability",
            "text": "Knowledge distillation (KD) [47] is a training method for transferring rich knowledge of a well-trained teacher network. Intuitively, KD performance affects a lot by choice of the teacher network; however, the relationship between similarity and KD performance has not yet been explored enough, especially for ViT. This subsection investigates how the similarity between teacher and student networks contributes to the distillation performance. There are several studies showing two contradictory conclusions; Jin et al . [54] and Mirzadeh et al . [73] showed that a similar teacher leads to better KD performance; Touvron et al . [100] reports that distillation from a substantially different teacher is beneficial for ViT. \n\nWe train 25 ViT-Ti models with different teacher networks from 69 models that we used by the hard distillation strategy [47]. Experimental details are described in Appendix. Fig. 11a illustrates the relationship between the teacherstudent similarity and the distillation performance. Fig. 11a tends to show a not significant negative correlation between teacher-student similarity and distillation performance (\u22120.32 Pearson correlation coefficient with 0.12 p-value). However, if we only focus on when the teacher and student networks are based on the same architecture (i.e., Transformer), we can observe a strong positive correlation (Fig. 11b) -0.70 Pearson correlation coefficient with 0.078 p-value. In this case, our observation is aligned with [54,73]: a teacher similar to the student improves distillation performance. However, when the teacher and student networks are based on different architectures (e.g., CNN), then we can observe a stronger negative correlation (Fig. 11c) with \u22120.51 Pearson correlation coefficient and 0.030 p-value. In this case, a more dissimilar teacher leads  to better distillation performance. We also test other factors that can affect distillation performance in Appendix; We observe that distillation performance is not correlated to teacher accuracy in our experiments. \n\nWhy do we observe contradictory results for Transformer teachers (Fig. 11b) and other teachers (Fig. 11c)?",
            "score": 0.6180209414366747,
            "section_title": "Model Diversity and Knowledge Distillation",
            "char_start_offset": 27153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2040
                },
                {
                    "start": 2043,
                    "end": 2149
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 522,
                    "end": 526,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 548,
                    "end": 552,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 631,
                    "end": 636,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1479,
                    "end": 1483,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 1483,
                    "end": 1486,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6494140625
        },
        {
            "corpus_id": "208513309",
            "title": "Towards Oracle Knowledge Distillation with Neural Architecture Search",
            "text": "This is partly because a large gap in model capacity between student and teacher hinders learning process of KD as discussed in (Mirzadeh et al. 2019), and the simple objective function to fit the representations of the teacher given by model averaging is not effective to take full advantage of teacher models. In other words, the limited capacity in the student network becomes a bottleneck of KD, which implies that increasing capacity of student models would be beneficial to reduce the performance gap between teacher and student.\n\nWe propose an advanced framework for knowledge distillation from ensemble models, which aims to maximize accuracy and efficiency of student networks at the same time. Contrary to the existing methods assuming that a student model is fixed, we adapt its structure and size, and make it more appropriate for knowledge distillation by alleviating the model capacity gap issue. We also address how to effectively extract information from a teacher especially when the teacher is an ensemble of multiple models. To this end, students are made to learn the most accurate information from teacher, which is realized by transferring knowledge from the optimal ensemble combination for each example. We call this strategy Oracle knowledge Distillation (OD), which encourages student models to achieve oracle accuracy of ensemble teacher models. Since the inherent model capacity gap incurs critical challenges in KD and OD makes the capacity gap larger, we incorporate neural architecture search with oracle knowledge distillation; this strategy facilitates to identify the optimal student model with sufficient capacity, which is well-suited for distillation. In practice, our algorithm searches for a slightly larger model than the backbone student network for effective knowledge distillation, reduces the model capacity gap between student and teacher, and achieves competitive accuracy of the student model. The main contributions of our work are summarized as follows:\n\n\u2022 We propose a novel framework for knowledge distillation by incorporating neural architecture search. The proposed algorithm addresses capacity issue in KD and aims to identify the optimal structures and operations with adaptive model sizes.\n\n\u2022 Our algorithm introduces a novel oracle knowledge distillation loss, which is particularly useful for an ensemble teacher model. We claim that the student networks mimicking oracle predictions have a potential for achieving higher accuracy than the teacher especially when combined with neural architecture search.\n\n\u2022 We demonstrate outstanding performance of the proposed method in diverse settings. We also make a comprehensive analysis about knowledge distillation from ensemble teacher",
            "score": 0.6176037929029825,
            "section_title": "Introduction",
            "char_start_offset": 2110,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70263671875
        },
        {
            "corpus_id": "278602421",
            "title": "DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images",
            "text": "KD is widely used in recent studies, where smaller student models are trained to mimic the behavior of larger teacher models. Hinton et al. [18], showed the student learns to mimic the output logits of the teacher models. Fukuda et al. [19] proposed an ensemble KD approach, where a smaller student network is trained using a set of teacher models, such as VGG or LSTM. The student model simultaneously learns from the outputs of multiple teachers. Yoon et al. [20] introduced a similarity-preserving knowledge distillation, where a small student network is trained to replicate the output of an extensive teacher network while maintaining important similarities between data. \n\nThese methods transfer knowledge from a large teacher to a smaller student model. However, if the student is significantly less complex it may fail to capture and replicate the teacher's intricate outputs. This issue is addressed in the multi-step KD method. Mirzadeh et al. [21] utilized the Teacher Assistant Knowledge Distillation (TAKD) approach. TAKD utilizes an intermediate model, called a Teacher Assistant (TA), to bridge the gap between a large teacher model and a small student model. Training a student using the TA gives better performance as the TA is closer in size to the student and makes the transfer of knowledge effective. Saleknia et al. [22] also employed midsize assistant networks that bridge the computational gap between the teacher and student models. They use three variants of EfficientNetV2 to build the teacher, teacher assistant, and student networks. Their approach improved the student model performance on the Standford-40 dataset from 94.75% to 96.30%. A disadvantage of this method is that computational complexity increases because of the multi-step distillation process However, transferring knowledge using only output logits has certain limitations, as it solely focuses on the output distribution while completely ignoring the intermediate representations learned at various layers. Featurebased distillation strategies address this limitation by transferring knowledge from intermediate layers. Remero et al. [23] used both the output logits and the intermediate representations of the teacher model to guide the training process of the student. Additionally, they introduced extra parameters to map the student's hidden layers to the prediction of the teacher's hidden layers.",
            "score": 0.6123164647812745,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2267
                },
                {
                    "start": 2268,
                    "end": 2399
                }
            ],
            "ref_mentions": [
                {
                    "start": 461,
                    "end": 465,
                    "matchedPaperCorpusId": "229376103"
                },
                {
                    "start": 954,
                    "end": 958,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1338,
                    "end": 1342,
                    "matchedPaperCorpusId": "268714642"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69189453125
        },
        {
            "corpus_id": "271974282",
            "title": "LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation",
            "text": "teacher. Interestingly, the smaller Qwen-1.5-0.5B student model shows marginal improvement with either the Qwen-1.5-4B or Qwen-1.5-7B teacher. This suggests that an excessively large capacity gap between the teacher and student models can hinder effective knowledge transfer. Utilizing a \"middle teacher\" with an intermediate capacity could bridge this gap, facilitating smoother knowledge transfer and boosting the student model's learning efficiency. \n\nTable 9: Comparison between the strong and weak teachers within the distillation. We set the strong teacher as the LLM is Qwen-1.5-7B and the weak teacher as the LLM is Qwen-1.5-4B. The model configuration is E4T2.",
            "score": 0.6046986857633506,
            "section_title": "IMPACT OF PREFERENCE DISTILLATION",
            "char_start_offset": 31825,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 669
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14892578125
        },
        {
            "corpus_id": "264405606",
            "title": "DistillCSE: Distilled Contrastive Learning for Sentence Embeddings",
            "text": "Knowledge distillation (Hinton et al., 2015) involves training a compact model, often referred to as a student model, to mimic the behavior and knowledge of a larger, more complex model known as the teacher model. It has been successfully applied to various tasks, such as language modeling (Zhuang et al., 2021), text classification (Heinzerling and Strube, 2018;Chia et al., 2019), named entity recognition (Zhou et al., 2021), machine translation (Tan et al., 2019), language generation (Melas-Kyriazi et al., 2019). \n\nTeacher model knowledge guide students in multiple ways during the distillation. Its predictions or soft targets, attention weights, or hidden representations, could all be used to guide the training of the student model. Consequently, the student is provided with stronger training signals from the teacher and achieves even superior performance. For example, Zhuang et al. (2021) directly mimics the output logits on vocabulary while Jiao et al. (2020) utilizes both hidden representations and attention matrix. \n\nDifferent from those studies, we employ knowledge distillation as an element of our self-training framework. Therefore, we focus on the most fundamental and general form of distillation which only minimize the cross entropy of prediction logits distribution between teacher and students. We use the homogeneous structure model for both the student and teacher model for distillation. Our research mainly focuses on the output logit distribution from the teacher instead of a special distillation framework. Therefore, our method is generic for more advanced distillation technologies.",
            "score": 0.5975021484790498,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 25385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1622
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 44,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 291,
                    "end": 312,
                    "matchedPaperCorpusId": "236164690"
                },
                {
                    "start": 334,
                    "end": 364,
                    "matchedPaperCorpusId": "21697629"
                },
                {
                    "start": 490,
                    "end": 518,
                    "matchedPaperCorpusId": "208050764"
                },
                {
                    "start": 883,
                    "end": 903,
                    "matchedPaperCorpusId": "236164690"
                },
                {
                    "start": 958,
                    "end": 976,
                    "matchedPaperCorpusId": "202719327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53857421875
        },
        {
            "corpus_id": "267061245",
            "title": "Knowledge Fusion of Large Language Models",
            "text": "In contrast, the approach proposed in this paper supports the fusion of multiple LLMs with diverse architectures by explicitly transferring their knowledge and capabilities to a target LLM. \n\nKnowledge Distillation Knowledge distillation (Hinton et al., 2015), initially proposed for model compression, involves training a student model under the guidance of one or more teacher models. In the NLP community, knowledge distillation has been widely applied to text classification tasks. These applications include training the student model to replicate the teacher's output distribution (Sanh et al., 2019;Turc et al., 2019), as well as features (Sun et al., 2019;Jiao et al., 2020) and relations (Wang et al., 2020) derived from intermediate layers of the teacher model. In the realm of text generation, the conventional approach focuses on minimizing the KL divergence between the student and teacher generation distributions. This is achieved by using the teacher's probability distributions at each time step as supervision (Khanuja et al., 2021;Gu et al., 2023;Agarwal et al., 2023) or by directly training on the teacher's generated texts (Peng et al., 2023;Xu et al., 2023). \n\nWhile our method shares a framework similar to multi-teacher knowledge distillation, there are two significant distinctions. First, in traditional knowledge distillation, the student models are typically constrained to be smaller in size than the teachers. In our scenario, however, there are no limitations on the size of the target model. Second, traditional knowledge distillation often results in the student models lagging behind the teachers in performance after distillation. In contrast, we anticipate that after the fusion, the target model will surpass any of the source models in performance.",
            "score": 0.5964249404737969,
            "section_title": "RELATED WORK",
            "char_start_offset": 7492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 192,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1787
                }
            ],
            "ref_mentions": [
                {
                    "start": 646,
                    "end": 664,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 664,
                    "end": 682,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1028,
                    "end": 1050,
                    "matchedPaperCorpusId": "236477925"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "273969804",
            "title": "Quantifying Knowledge Distillation Using Partial Information Decomposition",
            "text": "With the growing interest in knowledge distillation, our work provides critical insights into the explainability of knowledge distillation. We propose using U ni(Y : T \\S) to quantify the knowledge available in a teacher model for distillation w.r.t. a student and a downstream task. This, in turn, leads to the definition of the amount of knowledge that has been distilled to a student as Red(Y : T, S). We show that knowledge distillation frameworks which use mutual information between the teacher and the student representations to achieve knowledge distillation have a fundamental problem: These frameworks force the student to mimic the teacher regardless of the usefulness of the teacher's information to perform the task at hand. In contrast, through many examples we demonstrate that the proposed metrics can correctly characterize the amounts of knowledge available for distillation and the already transferred knowledge. Moreover, we show the advantage of the proposed metric by implementing a new distillation framework -Redundant Information Distillation (RID) -and comparing its performance with the existing technique VID [3]. While VID and RID perform similarly when the teacher is well-trained for the downstream task, VID performance degrades largely when the teacher is not trained but RID performs close to a student model trained without distillation. Limitations and future work: While the RID framework uses an alternative definition for redundant information, computation of exact Red(Y : T, S) during training can be computationally prohibitive due to the optimization over \u2206 P . Moreover, characterizing the extent to which the assumption in Section 4 holds is not explored in this work. Extending the mathematical formulation in Section 4 to analyze other knowledge distillation frameworks is an interesting path for future research. Other potential research directions include: (i) distilling from an ensemble of teachers [49] in a way that the adverse effects of corrupted teachers are mitigated; (ii) dataset distillation [50]; or (iii) distillation for model reconstruction from counterfactual explanations [51]. Incorporating fundamentally different definitions for PID components, such as [41] which provides explicit formulae, as regularizers can also be interesting.",
            "score": 0.5895649193122656,
            "section_title": "Conclusion",
            "char_start_offset": 23193,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2301
                }
            ],
            "ref_mentions": [
                {
                    "start": 1137,
                    "end": 1140,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1950,
                    "end": 1954,
                    "matchedPaperCorpusId": "141465546"
                },
                {
                    "start": 2052,
                    "end": 2056,
                    "matchedPaperCorpusId": "207930130"
                },
                {
                    "start": 2138,
                    "end": 2142,
                    "matchedPaperCorpusId": "273850321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5693359375
        },
        {
            "corpus_id": "264490946",
            "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
            "text": "data privacy, separate test servers, ...), we explore if transfer of complementary knowledge between any teacher and student is possible without performance degradation. Achieving such transfer through any possible model pair unlocks any freely available or self-generated model collection as an auxiliary resource for gains in canonical and problem-specific pretraining. It also avoids the need for model-specific transfer that require expert knowledge, and reduces the reliance on external evaluation measures for model selection. More importantly however, it also enables improvements of larger models by knowledge transfer from weaker, lower-resource models, without the explicit need for additional data & supervision, or sacrifices in e.g. speed, fairness, interpretability or ease-of-use. \n\nWe investigate the limits of knowledge distillation (Hinton et al., 2015;Tian et al., 2020) for this task, which in contrast to data-free approaches (e.g. Wortsman et al. (2022a)), operates independently of model choices. However, standard knowledge distillation frameworks assume information to be distilled to an untrained student. In contrast, we only wish to transfer knowledge not available in an already trained student model, which may even outperform its teacher. This crucially entails a successful trade-off between knowledge gain and retention. Indeed, for knowledge transfer between arbitrary pretrained models, common distillation ( \u00a75.1) exhibits strong model/hyperparameter dependence and performance drops for the majority of student models, particularly for weaker/equiperformant teachers. This can be attributed to catastrophic forgetting (Kirkpatrick et al., 2016;Zenke et al., 2017) outweighing the benefits of complementary knowledge transfer from the teacher. \n\nFor a favorable trade-off between forgetting and knowledge gain, we treat the transfer process as a continual learning problem, where a model is continuously presented with new context for data already seen. To encourage retention, we first study weight interpolation (Stojanovski et al., 2022;Wortsman et al., 2022b). While better than normal distillation, it is often too strong a constraint when the teachers have niche areas of expertise or are overall stronger.",
            "score": 0.5886317011932884,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2248
                }
            ],
            "ref_mentions": [
                {
                    "start": 871,
                    "end": 889,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 953,
                    "end": 976,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 2050,
                    "end": 2076,
                    "matchedPaperCorpusId": "253383803"
                },
                {
                    "start": 2076,
                    "end": 2099,
                    "matchedPaperCorpusId": "237420687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.388671875
        },
        {
            "corpus_id": "258832674",
            "title": "Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?",
            "text": "Experiment setup. To validate if relatively weak classifiers are better teachers, we carefully select multiple teacher-student model pairs with different architectures, including ResNet [19], VGG [54], and ShuffleNetV2 (SNV2) [39]. The experiments are conducted on 200K synthetic datasets and the students are trained by 100 epochs. \n\nResults. When training on real datasets, it is common to use a relatively large teacher model to train the student model, such as distilling ResNet34 to ResNet18. In general, smaller teacher models often fail to achieve satisfactory distillation performance compared to larger teacher models. However, when working with synthetic datasets, we observe the opposite phenomenon: relatively weak teacher models can actually achieve better distillation performance than strong ones, as shown in Fig. 3 and Fig. 4. Interestingly, we found that as the capacity of the teacher model increases, a significant drop in performance is observed. Specifically, when training ResNet34 on the synthetic dataset, using ResNet18 as the teacher model leads to a 3% improvement in performance compared to using ResNet50 as the teacher model. These results highlight the importance of carefully selecting a teacher model when performing knowledge distillation and suggest that using a smaller, weaker teacher model may be preferable when working with synthetic datasets.",
            "score": 0.5868778853283315,
            "section_title": "Relatively weak classifiers are better teachers.",
            "char_start_offset": 22185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1384
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "51880435"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67724609375
        },
        {
            "corpus_id": "246200092",
            "title": "Explainable CNN With Fuzzy Tree Regularization for Respiratory Sound Analysis",
            "text": "The development of deep learning has brought about complex models with huge overhead. The application of these complex models in production requires a lot of inference time. Knowledge distillation based on teacher-student framework as an effective model compression method tries to achieve a trade-off between model accuracy and inference efficiency [28]. To improve the applicability of the knowledge distillation framework and the accuracy of the model, some researchers have proposed to improve the framework of distillation from a single teacher to multiple teachers. For example, multiple pre-trained teacher models are directly assigned fixed weights to integrate their predictions [29]. Some researchers have used different teacher models to learn different types of inputs and then used the weighted average to teach student models [30]. In addition, to accelerate the training of the student model in the word embeddings task, multiple teacher models were used to train a student model by combining their logit values such that the student no longer needs the teachers during decoding [31]. However, these approaches only treat teacher models equally, without taking into account the differences between them. In order to solve the conflicts and competition among all teachers, Shangchen et al. formulated ensemble knowledge distillation as a multi-objective optimization problem and assigned dynamic weights to each teacher model [32]. You et al. proposed to use a voting mechanism to unify multiple relative dissimilarity information, which can be transferred into the student network [33]. These existing methods use multiple teacher models to learn category-fixed training samples. In this paper, we use multiple teachers models to learn local data distributed across multiple hospitals with flexible categories.",
            "score": 0.5861767474659125,
            "section_title": "B. Knowledge Distillation with Multiple Teachers",
            "char_start_offset": 9962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1825
                }
            ],
            "ref_mentions": [
                {
                    "start": 688,
                    "end": 692,
                    "matchedPaperCorpusId": "18195425"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "145841463"
                },
                {
                    "start": 1094,
                    "end": 1098,
                    "matchedPaperCorpusId": "173990395"
                },
                {
                    "start": 1440,
                    "end": 1444,
                    "matchedPaperCorpusId": "227276362"
                },
                {
                    "start": 1596,
                    "end": 1600,
                    "matchedPaperCorpusId": "26021416"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.435546875
        },
        {
            "corpus_id": "237605152",
            "title": "Dynamic Knowledge Distillation for Pre-trained Language Models",
            "text": "Specifically, we are curious about whether learning from a bigger PLM with better performance can lead to a better distilled student model. We conduct probing experiments to distill a 6-layer student BERT model from BERT BASE with 12 layers, and BERT LARGE with 24 layers, respectively. We conducts the experiment on two datasets, RTE (Bentivogli et al., 2009) and CoLA (Warstadt et al., 2019), where two teacher models exhibit clear performance gap, and a sentiment classification benchmark IMDB (Maas et al., 2011). Detailed experimental setup can be found in Appendix A. As shown in Table 1, we surprisingly find that while the BERT LARGE teacher clearly outperforms the small BERT BASE teacher model, the student model distilled by the BERT BASE teacher achieves better performance on all three datasets. This phenomenon is counter-intuitive as a larger teacher is supposed to provide better supervision signal for the student model. We think that there are two possible factors regarding the size of teacher model that leading to the deteriorated performance:\n\n(1) The predicted logits of the teacher model become less soft as the teacher model becomes larger and more confident about its prediction (Guo et al.,  The teacher model is BERT BASE with 12 layers. 2017; Desai and Durrett, 2020), which decreases the effect of knowledge transfer via the soft targets. We find that a smaller \u03c4 also leads to a decreased performance of the student model, indicating the the less-softened teacher prediction will decrease the student performance. 2 (2) The capacity gap between the teacher and student model increases as the teacher becomes larger. The competency of the student model can not match that of the large teacher model, which weakens the performance of KD.\n\nTo explore the combined influence of these factors, we distill student models with different layers and plot the performance gain compared to directly training the student model without distillation in Figure 2. It can be found that by decreasing the student size, the better supervision from teacher model boosts the performance, while the two counteractive factors dominate as the gap becomes much larger, decreasing the performance gain. We notice that this phenomenon is also observed by Mirzadeh et al. (2020) in computer vision tasks using convolutional networks, showing that it is a widespread issue and",
            "score": 0.5829030763902456,
            "section_title": "Bigger Teacher Not Always Raises Better Student",
            "char_start_offset": 6839,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 497,
                    "end": 516,
                    "matchedPaperCorpusId": "1428702"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65234375
        },
        {
            "corpus_id": "248524841",
            "title": "Knowledge Distillation of Russian Language Models with Reduction of Vocabulary",
            "text": "We distilled the teacher model into 3-layer student model distil-tiny30 with 30k subwords in vocabulary. We tried different combinations of loss functions and alignment strategies. Combinations are summarized in Figure 4. In our experiments, we use MLM loss in summation with KL or MSE, or both of them. To compute KL loss teacher and student presoftmax outputs should be aligned: 1. with the match strategy by sequence and vocabulary (KL-match); 2. with the reduce-match strategy, where reduction was made by sequence dimension and match-by vocabulary (KL-reduce-match). MSE loss for hidden states distillation was applied with match and reduce strategies. To match hidden sizes projection layers were used (see details in Appendix B.2). \n\nTo apply reduce strategy, we passed two inputs to the student: 1. a student-tokenized input for MLM loss; 2. teacher inputs tokenized by student for MSE and KL. Student representations corresponding to the student-tokenized input were not aligned and were not used to compute MSE or KL divergence. We initialized student models' embeddings by re-using teacher embeddings (see details in Appendix B.1). Other training details could be found in Appendix B. \n\nAblation To check whether match and reduce strategies are effective for distilling knowledge from the teacher, we pre-trained distil-tiny30 using only MLM loss and without any distillation losses. We also performed pre-training without KL divergence loss term to evaluate its contribution to KL-match & MLM & MSE combination. \n\nTo evaluate effect of reduced vocabulary on the distillation quality, we compared distil-tiny* models with reduced vocabulary to distil-base and distil-small with the same vocabulary as the teacher. \n\nTo determine how further vocabulary size reduction affects the distillation quality, we also distilled teacher into distil-tiny models with 5k, 10k and 20k vocabulary sizes (results are in Appendix C).",
            "score": 0.5828852883648742,
            "section_title": "Distillation with reduced vocabulary",
            "char_start_offset": 14677,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1724
                },
                {
                    "start": 1727,
                    "end": 1928
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4833984375
        },
        {
            "corpus_id": "267617086",
            "title": "Cooperative Knowledge Distillation: A Learner Agnostic Approach",
            "text": "Knowledge distillation is a simple and elegant approach that allows one machine (the teacher) to instruct another machine (the student). Typically, the teacher model is more complex than the student model, and knowledge distillation compresses models for efficiency (Hinton 2015), though more recent work explores improving performance as well (Xie et al. 2020). However, existing knowledge distillation has its limitations. First, offline knowledge distillation, that is, a trained teacher teaching an untrained student, assumes that all of the teacher's knowledge is good and should be learned by the student even if the teacher performs worse than the student. Second, it is unidirectional and singular; one teacher informs one student, and students do not inform teachers. \n\nIn this work, we extend knowledge distillation to novel settings by creating what we call cooperative distillation. This is useful in domains where there are multiple learners, each of which can be considered a semi-expert deficient in one or more particular aspect(s) of a task, and can help overcome each other's limitations. This setting is not covered by existing distillation work. Consider our FashionMNIST dataset experiment. Here, we create ten classifiers (one for each class) trained with one class being undersampled by 95% to induce a conceptual deficiency. A model might understand the majority of clothes it sees, but since it hasn't seen many, say, ankle boots, it struggles to classify them correctly and will rely on other models to teach it this concept. This will require targeted and multidirectional transfer: this model needs to be taught only about ankle boots and can be a teacher for other classes. \n\nIn the tradition of knowledge distillation simplicity, we propose a learner agnostic, counterfactual-based cooperative approach. Consider an instance x which model i can predict correctly, but model j cannot. We say that model i is a qualified teacher to model j for the specific instance x. Our method will have model i teach model j about x by generating a new type of quintessential counterfactual x \u2032 which can be added to j's training set.",
            "score": 0.5825641197460143,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2149
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 279,
                    "matchedPaperCorpusId": "262849415"
                },
                {
                    "start": 344,
                    "end": 361,
                    "matchedPaperCorpusId": "207853355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.483642578125
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "To improve the efficacy of knowledge transfer, the relationships between the model complexity and existing distillation schemes or other novel distillation schemes (Sun et al. 2021) should be further investigated. \n\nCurrently, most KD methods focus on new types of knowledge or distillation loss functions, leaving the design of the teacher-student architectures poorly investigated (Nowak and Corso 2018;Crowley et al. 2018;Kang et al. 2020;Liu et al. 2019i;Ashok et al. 2018;Liu et al. 2019a). In fact, apart from the knowledge and distillation algorithms, the relationship between the structures of the teacher and the student also significantly influences the performance of knowledge distillation. For example, on one hand, some recent works find that the student model can learn little from some teacher models due to the model capacity gap between the teacher model and the student model (Zhang et al. 2019b;Kang et al. 2020); On the other hand, from some early theoretical analysis on the capacity of neural networks, shallow networks are capable of learning the same representation as deep neural networks (Ba and Caruana 2014). Therefore, the design of an effective student model or construction of a proper teacher model are still challenging problems in knowledge distillation. \n\nDespite a huge number of the knowledge distillation methods and applications, the understanding of knowledge distillation including theoretical explanations and empiri-cal evaluations remains insufficient (Lopez-Paz et al. 2016;Phuong and Lampert 2019a;Cho and Hariharan 2019). For example, distillation can be viewed as a form of learning with privileged information (Lopez-Paz et al. 2016). The assumption of linear teacher and student models enables the study of the theoretical explanations of characteristics of the student learning via distillation (Phuong and Lampert 2019a). Furthermore, some empirical evaluations and analysis on the efficacy of knowledge distillation were performed by Cho and Hariharan (2019). However, a deep understanding of generalizability of knowledge distillation, especially how to measure the quality of knowledge or the quality of the teacher-student architecture, is still very difficult to attain.",
            "score": 0.5816238077180814,
            "section_title": "Challenges",
            "char_start_offset": 82695,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 216,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1289
                },
                {
                    "start": 1292,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 405,
                    "end": 425,
                    "matchedPaperCorpusId": "23316647"
                },
                {
                    "start": 425,
                    "end": 442,
                    "matchedPaperCorpusId": "208513309"
                },
                {
                    "start": 442,
                    "end": 459,
                    "matchedPaperCorpusId": "208175624"
                },
                {
                    "start": 459,
                    "end": 477,
                    "matchedPaperCorpusId": "13352766"
                },
                {
                    "start": 477,
                    "end": 493,
                    "matchedPaperCorpusId": "69629714"
                },
                {
                    "start": 895,
                    "end": 915,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 915,
                    "end": 932,
                    "matchedPaperCorpusId": "208513309"
                },
                {
                    "start": 1497,
                    "end": 1520,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 1520,
                    "end": 1545,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 1545,
                    "end": 1568,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1660,
                    "end": 1683,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 1847,
                    "end": 1873,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 1988,
                    "end": 2012,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45458984375
        },
        {
            "corpus_id": "264146954",
            "title": "Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories",
            "text": "Currently, there exist some methods in the field of Knowledge Distillation (KD) that explore the factors of what makes experts conducive to students learning or how to transfer more valuable knowledge from highly proficient experts. Huang et al. [41] has empirically pointed out that simply enhancing an expert's capabilities or employing a stronger expert can, paradoxically, result in a decrease in student performance. In some cases, this decline can even be more pronounced than if students were to begin training from scratch with vanilla KD. Consequently, they have proposed a loose matching approach to replace the traditional Kullback-Leibler (KL) divergence. \n\nSimilarly, Yuan et al. [42] argues that simplifying the expert model's outputs can offer greater advantages to the student's learning process. Meanwhile, Shao et al. [43] proposes the principle that \"experts should instruct on what ought to be taught' and introduces a data-based knowledge distillation method. All of the methods mentioned above underscore the intimate relationship between experts and students, necessitating a thorough exploration of how to enhance the abilities of both experts and students consistently. Nonetheless, there remains a vacancy in this relationship under the context of Dataset Distillation.",
            "score": 0.5808029971327355,
            "section_title": "B. Relationship between Expert and Student",
            "char_start_offset": 7729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1295
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 250,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 836,
                    "end": 840,
                    "matchedPaperCorpusId": "255341297"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20166015625
        },
        {
            "corpus_id": "240419662",
            "title": "Rethinking the Knowledge Distillation From the Perspective of Model Calibration",
            "text": "Knowledge distillation (Hinton, Vinyals, and Dean 2015) is widely used to compress the deep neural networks, which employs the soft probabilities of a lager teacher network to guide the training of smaller student network (Cho and Hariharan 2019). The applications of knowledge distillation seem to be evident, such as model compression, transfer learning, domain adaption, incremental learning. Many follow-ups work endeavor to improve the performance of knowledge distillation, using different strategies. Despite sustainable efforts have been made, many studies (Mirzadeh et al. 2020) suggest that: more accurate teacher models do not necessarily make for better teachers. The small model does not imitate the large model very well, when the performance gap between the large and small models is large. Researchers attributed the phenomena to the mismatch of abilities between the teacher and student. What characterizes this phenomenon? Are the teacher models over-confident? In our study, we aim to answer these questions.",
            "score": 0.5794282303959282,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1027
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 246,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 565,
                    "end": 587,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5810546875
        },
        {
            "corpus_id": "231855304",
            "title": "NewsBERT: Distilling Pre-trained Language Model for Intelligent News Application",
            "text": "In this section, we conduct experiments to validate the advantage of our proposed teacher-student  joint learning and distillation framework over conventional methods that learn teacher and student models successively (Hinton et al., 2015). We first compare the performance of the student models under our framework and their variants learned in a disjoint manner. The results are shown in Fig. 4. We find that our proposed joint learning and distillation framework can consistently improve the performance of student models with different capacities. This is because in our approach the student model can learn from the useful experience evoked by the learning process of the teacher model, and the teacher model is also aware of the student's learning status. However, in the disjoint learning framework, student can only learn from the results of a passive teacher. Thus, learning teacher and student models successively may not be optimal for distilling a high-quality student model.\n\nWe also explore the influence of the teacherstudent joint learning and distillation framework on the teacher model. We compare the performance of the original UniLM-8 model and its variants that serve as the teacher model for distilling different student models. The results are shown in Fig. 5. We find a very interesting phenomenon that the performance of some teacher models is better than the original UniLM-8 model that does not participate in UniLM-8 (w/o student) UniLM-8 (NewsBERT-4 student) UniLM-8 (NewsBERT-2 student) UniLM-8 (NewsBERT-1 student) (b) News recommendation. Figure 5: Influence of the teacher-student joint learning and distillation framework on the teacher model. the joint learning and distillation framework. This may be because the teacher model may also benefit from the useful knowledge encoded by the student model. These results show that our teacher-student joint learning and distillation framework can help learn the teacher and student models reciprocally, which may improve both of their performance.",
            "score": 0.5770832620143016,
            "section_title": "Effectiveness of Teacher-Student Joint Learning and Distillation Framework",
            "char_start_offset": 24182,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.455078125
        },
        {
            "corpus_id": "273969804",
            "title": "Quantifying Knowledge Distillation Using Partial Information Decomposition",
            "text": "Modern-day machine learning requires large amounts of compute for both training and inference. Knowledge distillation [1,2] can be used to compress a complex machine learning model (the teacher) by distilling it into a relatively simpler model (the student). The term \"distillation\" in this context means obtaining some assistance from the teacher while training the student so that the student performs much better than when trained alone (see Figure 1). In its earliest forms, knowledge distillation involved the student trying to match the output logits of the teacher [1]. More advanced methods focus on distilling multiple intermediate representations of the teacher to the corresponding layers of the student [2][3][4][5]. We also refer the reader to [6,7] for surveys. \n\nInformation theory has been instrumental in both designing [3,4] and explaining [8,9] knowledge distillation techniques. However, less attention has been given to characterizing the fundamental limits of the process from an information-theoretical perspective. Our goal is to bridge this gap by first introducing new measures to quantify the \"transferred knowledge\" and \"knowledge to distill\" for a teacher and a student model given a target downstream task. We bring in an emerging body of work called Partial Information Decomposition (PID) [10][11][12] to explain knowledge distillation. We define the knowledge to distill using the PID measure of \"unique\" information about the task that is available only with the teacher but not the student. As it follows, the transferred knowledge is succinctly quantified by the measure of \"redundant\" information that is common between the teacher and student. \n\nWe propose a multi-level optimization that maximizes redundant information (transferred knowledge) as a regularizer for more effective distillation. While PID has been explored in a few avenues of machine learning, Figure 1: Knowledge Distillation: The teacher (a complex model) assists the student (usually a substantially simpler model) during their training. The learned student can perform much better than an independently trained student without distillation with a similar training setup (i.e., hyperparameters and data). The teacher may or may not have been trained for the same task as the student.",
            "score": 0.575383947140079,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 123,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 715,
                    "end": 718,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 718,
                    "end": 721,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 721,
                    "end": 724,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 724,
                    "end": 727,
                    "matchedPaperCorpusId": "252693152"
                },
                {
                    "start": 757,
                    "end": 760,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 837,
                    "end": 840,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 840,
                    "end": 842,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 858,
                    "end": 861,
                    "matchedPaperCorpusId": "251643827"
                },
                {
                    "start": 861,
                    "end": 863,
                    "matchedPaperCorpusId": "252846591"
                },
                {
                    "start": 1325,
                    "end": 1329,
                    "matchedPaperCorpusId": "1031742"
                },
                {
                    "start": 1329,
                    "end": 1333,
                    "matchedPaperCorpusId": "10901107"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5419921875
        },
        {
            "corpus_id": "271720097",
            "title": "Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations",
            "text": "They found a lack of available small-scale distilling datasets, and available datasets need to provide more diversity to produce a high-quality model. Moreover, student evaluation focuses too narrowly on one model family, and the evaluation process needs to be more precise. To improve dataset diversity, a new distilling dataset is proposed. Furthermore, trained models on this dataset are thoroughly evaluated using a comprehensive testing methodology. \n\nChallenges. There are significant challenges that impact distillation quality. In order to achieve more students with the teacher's abilities, the reduction of the distribution mismatch is actively investigated [61] [66]. The difficulty in reduction could stem from the lack of publicly available diverse distilling datasets [65]. This results in either manually creating or augmenting existing datasets. However, the diversity and detail of current datasets could also be a challenge in distilling capable students [65]. Also, reaching specific quality aspects for the student is challenging. The authors of [66] found that it is challenging distilling a student that has similar distribution and expressive power to the teacher. One possible reason investigated [78], which criticized that the student can only be as good as the used teacher. That means that the student learns hallucinations, bias, and toxicity behavior already present in the teacher. If only Black-Box Knowledge Distillation is available, the risk of only imitating the teacher's style without actually learning the behavior is present [78]. \n\nPractical Constraints. In many knowledge distillation strategies, a student-teacher setup is deployed. This results in two models loaded into memory during white-box and local black-box distillation, which requires a memory-potent environment [61] [66] [67]. Running two models is costly because computing and memory are expensive. These costs also apply to APIs, where memory costs are factored into API usage costs. Therefore, it has to be considered if fine-tuning is needed given the resource requirements and if it is preferred over simple optimization techniques like quantization. \n\nIn Conclusion. \n\n\u2022 Knowledge distillation is a technique for fine-tuning and compressing models, with the goal of transferring knowledge from larger models to smaller ones.",
            "score": 0.5740992765204045,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 30741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2160
                },
                {
                    "start": 2163,
                    "end": 2177
                },
                {
                    "start": 2180,
                    "end": 2335
                }
            ],
            "ref_mentions": [
                {
                    "start": 668,
                    "end": 672,
                    "matchedPaperCorpusId": "259164722"
                },
                {
                    "start": 782,
                    "end": 786,
                    "matchedPaperCorpusId": "258352678"
                },
                {
                    "start": 973,
                    "end": 977,
                    "matchedPaperCorpusId": "258352678"
                },
                {
                    "start": 1221,
                    "end": 1225,
                    "matchedPaperCorpusId": "258887629"
                },
                {
                    "start": 1565,
                    "end": 1569,
                    "matchedPaperCorpusId": "258887629"
                },
                {
                    "start": 1816,
                    "end": 1820,
                    "matchedPaperCorpusId": "259164722"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.351318359375
        },
        {
            "corpus_id": "267782419",
            "title": "Wisdom of Committee: Distilling from Foundation Model to Specialized Application Model",
            "text": "Thus, we propose an interactive distillation process, allowing the student to model each teacher's expertise and ask tailored questions to each teacher. With no constraints on the teachers, DIVERSEDISTILL is adaptable to various teacher combinations. \n\nEstablishing a teacher committee brings additional distillation costs. Fortunately, the distillation cost from adding a complementary teacher is significantly lower, considering foundation models are highly resource-intensive. Further, with DIVERSEDISTILL, each teacher's importance for the current data is dynamically weighted based on the student's modeling of the teacher's expertise. We imagine such weighting mechanics may enable the student to select only one teacher for every training data when distillation cost becomes the major constraint. \n\nIn this work, we argue that forming a teacher committee realizes effective knowledge transfer from foundation models to specialized application models. To accommodate a teacher committee without constraints on teacher choices, we design the distillation process as a question-and-answer process with additional parameters for the student to model each teacher's expertise on the task. Our contributions are summarized as follows. \n\n\u2022 We hypothesize that forming a teacher committee of both foundation models and complementary models would lead to a smoother knowledge transfer from foundation model to specialized application model. \u2022 In Section 3.2, we introduce DIVERSEDISTILL, which learns a distillation module, including a Question Augmenter and an Answer Augmenter, along with the student model. We describe the distillation process in Section 3.1, and how we design and train the distillation module in Section 3.2 and Section 3.3 \u2022 We present our evaluation in Section 4. In Section 4.1, we describe how to enable distillation in recommendation tasks using both the language model and recommendation system as teachers and how to enable distillation in vision tasks using both the vision-language model and vision model as teachers. We demonstrate that leveraging complementary teachers benefits the distillation quality. Also, DIVERSEDISTILL significantly increases distillation performance when dealing with a diverse teacher committee, leading to the best student compared to various baselines across different teacher choices. \n\n2 Related Work",
            "score": 0.5727095813273257,
            "section_title": "Introduction",
            "char_start_offset": 5069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1235
                },
                {
                    "start": 1238,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2344
                },
                {
                    "start": 2347,
                    "end": 2361
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2890625
        },
        {
            "corpus_id": "278165498",
            "title": "Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom",
            "text": "We describe the experimental design considered to study the impact of vocabulary (same and different) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the distilled model. We also analyze the impact of untrained/SFT teacher/student model on the final distilled model. Fig. 1 shows the schematic representation of our experimental setup. Depending on choice of SFT training, teacher model and distillation algorithms, there are 4 parameters of interest here: \n\n\u2022 Teacher -Two variants of the teacher model, base model and SFT model, arising out of SFT training (depicted as a 1/0 switch) in Fig. 1. \n\n\u2022 Student -Similarly, student model is also considered with two variants -base model and SFT model (refer to Fig. 1). These two addresses RQ1. \n\n\u2022 Vocabulary -To study impact of vocabulary, we consider two cases where teacher and student (i) both having same vocabulary (ii) both having different vocabulary. We fix student model to be from the Llama family -TinyLlama. Hence, with respect to the student SLM TinyLlama, choosing the teacher model as (i) LLM Llama results in same vocabulary (ii) LLM Mistral results in different vocabulary. This addresses RQ2. \n\n\u2022 KD algorithm -To analyze if insights on vocabulary is invariant to choice of KD algorithm, we consider two KD algorithms -Vanilla KD and DSKD. \n\nFig. 2 shows a schematic representation of 16 combinations of experiments arising out of the design described above. \n\nThe notations used to depict the various combinations are summarized in Table 1. We report performance using 14 metrics (refer Section 1.3) for each of the 16 combinations of distillation experiments.",
            "score": 0.5708167909916335,
            "section_title": "Experimental Setup",
            "char_start_offset": 6884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 466
                },
                {
                    "start": 469,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1316
                },
                {
                    "start": 1319,
                    "end": 1435
                },
                {
                    "start": 1438,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1638
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.398681640625
        },
        {
            "corpus_id": "4110009",
            "title": "Born Again Neural Networks",
            "text": "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction.",
            "score": 0.5682836396440566,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55615234375
        },
        {
            "corpus_id": "263620565",
            "title": "Can a student Large Language Model perform as well as it's teacher?",
            "text": "While knowledge distillation is a powerful tool, it's not without its limitations and challenges. Here are a few to consider: \n\n1. Performance Gap: The performance of the distilled student model, although better than a similarly-sized model trained from scratch, typically doesn't reach the performance level of the larger teacher model. There is usually a trade-off between model size and accuracy. \n\n2. Dependence on a Good Teacher Model: The effectiveness of knowledge distillation heavily depends on the quality of the teacher model. The student model can only be as good as the teacher model allows. If the teacher model isn't well-trained or doesn't perform well, the student model is unlikely to perform well either. \n\n3. Hyperparameter Sensitivity: The process of knowledge distillation requires careful tuning of several hyperparameters, such as the temperature parameter and the weighting between the original loss and the distillation loss. Finding the right settings can be tricky and might require a lot of experimentation. \n\n4. Computational Overhead: Although the student model is smaller and more efficient, the distillation process itself requires the teacher model to generate predictions for the data, which could be computationally expensive, especially for large models and datasets. \n\n5. Opaque Process: The process of knowledge distillation is somewhat opaque and difficult to interpret. It's not always clear why a certain student model learns effectively from a teacher model, or why certain hyperparameters work better than others.",
            "score": 0.5678563413109863,
            "section_title": "Limitations",
            "char_start_offset": 17792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 125
                },
                {
                    "start": 128,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1557
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "267617160",
            "title": "A Survey on Transformer Compression",
            "text": "These models work together to decompose and solve complex problems effectively. Fu et al. [120] demonstrates that specializing the small student model's abilities for the specific reasoning task, by transferring the knowledge in larger teacher model's generic directions, yields promising results. Jiang et al. [7] identifies a shortcoming in the aforementioned approaches, as they lack the ability to incorporate feedback for identifying challenging instructions where the student model's performance falls short. To address this limitation, they propose a three-stage adversarial loop of distillation that incorporates feedback and addresses these challenging instructions. Other KDs. Xu et al. [121] compresses the BERT model by progressive module replacing. The original model is first divided into multiple modules, and more compact substitutes are created for these modules. Then, they randomly substitute the original modules with their corresponding substitutes and train the compact modules without any additional loss functions. \n\nJha et al. [122] discovers that fitting both the student and teacher models, each with numerous parameters, into GPU memory with traditional knowledge distillation is impractical. To overcome this limitation, they propose a teacher-free task-agnostic distillation method which uses a truncated version (student) of the large model (teacher) for initialization [123] and then continuing the pre-training of student, without employing any distillation loss.",
            "score": 0.5677078201688809,
            "section_title": "KD for Transformer-Based Large Language Models",
            "char_start_offset": 32573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1496
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58154296875
        },
        {
            "corpus_id": "257912519",
            "title": "MiniRBT: A Two-stage Distilled Small Chinese Pre-trained Model",
            "text": "The traditional knowledge distillation method transfers knowledge directly from the teacher to the student. However, when there is a significant difference in the structures of the teacher and student models, this approach may result in a performance gap. To address this issue, we proposed using a twostage distillation approach during the pre-training stage, which builds on the concept of Teacher Assistant Knowledge Distillation (Mirzadeh et al., 2020). \n\nAs depicted in Figure 1, this method involves distilling knowledge from the teacher (RoBERTa) to the teacher assistant (RBT6), and then from the teacher assistant to the student (MiniRBT). The intermediate step of the teacher assistant helps to reduce the size gap between the teacher and the student model, subsequently improving the student models' performance in downstream tasks. \n\nTo apply knowledge distillation (KD) with hidden layer distillation and prediction layer distillation, we employed TextBrewer (Yang et al., 2020), a PyTorch-based model distillation toolkit designed for natural language processing. We distill the knowledge from the output of the hidden layer. The objective is \n\nwhere the matrices H i s \u2208 R l\u00d7d and H i t \u2208 R l\u00d7d represent the hidden representation of the i -th student's hidden layer and the i-th teacher's hidden layer respectively. The W h \u2208 R d \u00d7d is a linear transformation that matches the hidden state of the student network and the hidden state of the teacher network. Apart from mimicking the hidden layer behavior of the teacher, we also trained the student model by employing the cross-entropy loss with the teacher's soft target probability \n\nwhere z S and z T are the logits vectors predicted by the student and teacher respectively, and p = softmax(z/t) is the scaled probability with temperature t and logits z. Finally, we combine the hidden layer distillation with the prediction layer distillation: \n\n(3)",
            "score": 0.566592614718824,
            "section_title": "Two-stage Distillation",
            "char_start_offset": 4988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1649
                },
                {
                    "start": 1652,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1913
                },
                {
                    "start": 1916,
                    "end": 1919
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 456,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 972,
                    "end": 991,
                    "matchedPaperCorpusId": "211572557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51904296875
        },
        {
            "corpus_id": "276741883",
            "title": "Scientific Reasoning: Assessment of Multimodal Generative LLMs",
            "text": "While we observed that learning from teachers' outputs leads to lower performance compared to learning from human-curated solutions, the precise impact remains to be measured, since we were not able to derive accuracy for the student models. If the teacher's performance is estimated at 80% of human performance, would models trained on the teacher's outputs achieve 80% of the performance of those trained on golden data, or could the student's ability to learn from noisy data mitigate this effect? One could also run experiments with different Knowledge Distillation techniques like the ones introduced in Background (2) or (Gou et al., 2021). \n\nAnother direction for future work could be trying out other adapter architectures, starting from larger student models (which would require more computational resources but probably provide better results), and learning with a less strict loss function. \n\nWe could additionally define a multi-head setup for finetuning, with part of the model being responsible for explanation generation and another one for answer prediction.",
            "score": 0.5661849507181089,
            "section_title": "Future work",
            "char_start_offset": 19865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 646
                },
                {
                    "start": 649,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 1075
                }
            ],
            "ref_mentions": [
                {
                    "start": 627,
                    "end": 645,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4619140625
        },
        {
            "corpus_id": "278310499",
            "title": "Improving Group Fairness in Knowledge Distillation via Laplace Approximation of Early Exits",
            "text": "Knowledge Distillation (KD) Hinton et al. [2015] offers a sample-efficient approach for training a student model by leveraging a pretrained teacher model. The training loss for the student is a combination of the standard cross-entropy loss calculated on the ground-truth labels and a knowledge distillation (KD) loss that encourages the student's predictions to align with the \"soft targets\" provided by the teacher. Let D = (x i , y i ) N i=1 represent the training dataset, L student = (1 \u2212 \u03b1)L CE (f s (x i ), y i ) + \u03b1L KD (f s (x i ; T ), f t (x i ; T )), with T being the temperature parameter usually kept high during training. This process effectively transfers the teacher's generalization capabilities to the student. A significant advantage of KD is the flexibility to choose a student model with significantly fewer parameters than the often computationally intensive teacher, thereby facilitating easier deployment. \n\nOne issue identified in the original knowledge distillation paper is the occurrence of bias during the training process. Since the student model learns from the teacher's output probabilities, any biases present in the teacher's predictions can be inherited-or even amplified-by the student. This becomes particularly problematic in settings where the teacher model has overfit to spurious correlations in the data or exhibits imbalanced performance across subgroups. As a result, the distillation process may unintentionally reinforce unfair or misleading patterns, highlighting the need for strategies that account for such biases during student training. \n\nReweighting losses for spuriously correlated groups has been shown to improve group fairness metrics. However, this approach typically requires prior knowledge of the group during training and lacks adaptivity based on the student model's current performance on challenging groups. To address these limitations, Tiwari et al. Tiwari et al. [2024] observed that student models often exhibit overconfidence on difficult instances in their early layers.",
            "score": 0.5659706212482747,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 2042
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5888671875
        },
        {
            "corpus_id": "260815602",
            "title": "Learning Lightweight Object Detectors via Multi-Teacher Progressive Distillation",
            "text": "In this section, we provide more details about distillation with heterogeneous teachers (Section 11). We investigate the heterogeneous cases where the backbones or input resolutions are different between the teachers and student. \n\nOverall performance: Again, Tables 11 and 12 show that MTPD is consistently effective with respect to all the teachers and their combinations, e.g., the box AP improves from 33.3% to 37.0%, and the mask AP improves from 30.5% to 33.7%. \n\nTwo key findings in heterogeneous distillation: Compared with the homogeneous case, we find that the capacity gap between models is a more important factor, and to bridge this gap a proper teacher order plays a more critical role. Details are explained as follows. \n\nThe student-teacher capacity gap is more pronounced in heterogeneous distillation. Among the four teachers, Teacher I shares exactly the same neck and head structure with the student, and has a similar but larger backbone; Teacher V has the same head with the student as well, but has different backbone and neck; Teacher III has similar backbone and neck, but has a different head; and Teacher IV is the most powerful one with completely different architecture. Table 12 (rows 3-6) summarizes the distillation results with single teachers. First, directly distilling from the strongest teacher (Teacher IV) does not yield the largest improvement. Second, a relatively less powerful but more similar teacher (Teacher I) leads to the best distillation performance, improving the APs by 2%, although Teachers V, III, and IV are all stronger than Teacher I. One possible reason is that Teacher I has the same neck and head as Student III as well as a similar but deeper backbone, so the capacity gap between Student III and Teacher I is the smallest. Finally, we find that Teacher III is a strong but not particularly helpful teacher, achieving the worst distillation results. One possible reason is that Teacher III has a very different head from Student III, while not as accurate as Teacher IV, making it unable to provide enough guidance to Student III. These observations suggest that a smaller capacity gap between the student and the teacher may facilities knowledge transfer. \n\nThe sequential order of the teachers plays a more critical role in the heterogeneous setting.",
            "score": 0.5650958436930728,
            "section_title": "C. Ablation on Distillation with Heterogeneous Teachers",
            "char_start_offset": 36909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2217
                },
                {
                    "start": 2220,
                    "end": 2313
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20654296875
        },
        {
            "corpus_id": "267750227",
            "title": "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs",
            "text": "General results in Tab. 2 show a consistent pattern regarding the model size and the gain achieved with the ULD loss, especially for challenging tasks such as generative QA. To understand the impact of student size on distillation capability, we performed an ablation study over the Pythia family. We hold the training dataset size fixed at 100% and compare the performance of models from 160m, 410m to 1b parameters and report results in Fig. 3. We observe that incorporating ULD loss consistently enhances student models across various tasks. The enhancements are particularly noticeable for smaller models on simpler tasks, while ULD loss requires larger models for effectively distilling teacher logits on harder tasks. This is especially evident in tasks requiring reasoning, such as FairytaleQA. While using logits teacher improves training, deep reasoning tasks still require appropriate model sizes to process complex relationships taught by teachers. Generally, we observe a significant increase in capacity transfer from teacher to student models through the use of ULD, enabling student models to match models twice bigger trained with the teacher-generated text method. For example, Pythia 410m with ULD loss matches the performance of the Pythia 1b distilled with teacher-generated text on QED and DIALOGSum.",
            "score": 0.5650908985266192,
            "section_title": "Student Size Ablation Study",
            "char_start_offset": 21808,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4091796875
        },
        {
            "corpus_id": "276776153",
            "title": "Mamba base PKD for efficient knowledge compression",
            "text": "Knowledge distillation has been extensively studied as a model compression technique that allows a smaller student model to learn from a larger teacher model [5,9] (see Figure 1). Traditional KD includes transferring the knowledge to a student model from a teacher model's output [2], intermediate layers [16], or only the relationships between different layers and data samples [15,21]. These techniques lead to computational savings while preserving high accuracy. However, one limitation of traditional KD is the performance drop when there is a large capacity gap between the teacher and student models because the student cannot effectively represent key features of the teacher. This limitation has been addressed by various methods [15,16,21,22] that modify distillation algorithms or employ multi-stage learning. \n\nTo overcome these challenges, hierarchical or layer-wise distillation approaches were proposed, where knowledge is progressively transferred across multiple layers of the teacher to the student. FitNets, introduced in [16], transfer intermediate representations from the teacher model to the student, improving the student's ability to learn fine-grained features progressively. Similarly, selfdistillation [22] trains a model by distilling knowledge into itself at different stages, a concept related to progressive distillation where multiple students incrementally improve performance [4]. \n\n. . .",
            "score": 0.5648159647916858,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5819,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1415
                },
                {
                    "start": 1418,
                    "end": 1423
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 161,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "229679667"
                },
                {
                    "start": 383,
                    "end": 386,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "229679667"
                },
                {
                    "start": 746,
                    "end": 749,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 749,
                    "end": 752,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1230,
                    "end": 1234,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1411,
                    "end": 1414,
                    "matchedPaperCorpusId": "265067529"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.763671875
        },
        {
            "corpus_id": "249712279",
            "title": "Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems",
            "text": "Low-latency applications require models of relatively small sizes. However, distilling from large pretrained models into much smaller models directly can hinder the student from fully taking advantage of the teacher's knowledge [7]. Therefore, we distill the pretrained teacher models in two phases with a teacher assistant setup [26,44]. The distillation workflow is depicted in Figure 1. First, we distill an intermediate sized model from the large teacher model. We then use this distilled model as a teacher for the final student. \n\nWhen distilling the intermediate model, we followed a similar approach to the pretraining of the teacher. A randomly initialized student model was distilled from the Stage 1 teacher model. Once training converged, we switched the teacher model to the Stage 2 teacher and resumed the distillation process. For both of these stages, the distillation data is the same that was used for teacher pretraining for its respective stage. As for our distillation techniques, we explored different components described in [17]. Our final run for the intermediate student/teacher used the sum of categorical cross-entropy (MLM loss) and soft cross-entropy weighted equally, because we did not observe any gain from utilizing the attention and hidden layer outputs of the teacher. \n\nFor our final student, we first pretrained the intermediate model further without teacher involvement on Stage 2 data only. Next, we distilled it into the final, small student. The distillation techniques in this phase were similar to the first distillation phase, with an additional usage of hidden-layer output matching as in [17].",
            "score": 0.5642554227404508,
            "section_title": "Distillation",
            "char_start_offset": 11231,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1640
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 231,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1635,
                    "end": 1639,
                    "matchedPaperCorpusId": "202719327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.444580078125
        },
        {
            "corpus_id": "258377699",
            "title": "Blend and Match: Distilling Semantic Search Models with Different Inductive Biases and Model Architectures",
            "text": "This limits the number of models that can be deployed in parallel. To our knowledge, large scale commercial systems deploy only one of these two model types. On the other hand, because the two approaches formulate the semantic matching problem differently, they have different inductive biases and thus predict a different set of (relevant) items. \n\nIn order to capture the most relevant products from the output of both Bi-encoders and PECOS, we propose a distillation mechanism called blending that uses the output of one model to teach the other model. Most distillation approaches focus on simplifying the student architecture and even multi-teacher distillation usually assumes similar inductive biases of the teachers and students [17,20,35]. We show, through offline as well as crowd-sourced evaluations, that blended model is able to substantially improve over the recall of the Bi-encoder model by 11% to 17% with a negligible loss in precision. Our experiments show that, for our problem setting, the existing knowledge distillation approaches provide suboptimal rankings. We also compare our blended model with strong Rank Fusion methods and show that our blended model provides comparable ranking performance, while only a single model needed for deployment. Rank Fusion methods on the other hand, would require deploying multiple models and hence is not a practical option for us. \n\nIn summary, the main contributions of this paper constitute: \n\n\u2022 A blending architecture to combine two different formulations of semantic matching into a single model, \u2022 Extensive experiments and analyses to show that our method is able to deliver the best of both worlds.",
            "score": 0.5637191586397324,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 347
                },
                {
                    "start": 350,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1669
                }
            ],
            "ref_mentions": [
                {
                    "start": 744,
                    "end": 747,
                    "matchedPaperCorpusId": "233237266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51171875
        },
        {
            "corpus_id": "247446679",
            "title": "CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification",
            "text": "We first explore the optimal teacher model for each student model. Specifically, we use the KD setting mentioned in Section 2 and test a set of (teacher, student) model pairs. Specifically, we consider the following models: EfficientNet B0, B2, B6 [21] and DenseNet-121 [22] (CNN models); and AST Tiny, Small, Base [11] (Transformer models). The reason for selecting these models are: 1) EfficientNet-B2 and AST-Base are the best CNN and Transformer models, and we want to see if KD can further improve their performance; 2) AST-Base is larger than EfficientNet-B2 in terms of the number of parameters. We include models with various sizes to clearly identify whether the improvement of KD is due to different model sizes or different model architecture; 3) We also include one different CNN architecture (DenseNet) to check if the cross-model KD framework generalizes to different CNN architectures. \n\nFor each (teacher, student) pair, we select the temperature \u03c4 \u2208 {1.0, 2.5} based on the performance on the validation set, all other knowledge distillation settings are identical for all (teacher, student) pairs. The mean average precisions (mAPs) of the student model of various (teacher,student) pairs on the FSD50K evaluation set are shown in Table 3. Key observations are as follows: \n\n1. CNNs and ASTs are good teachers for each other. While knowledge distillation improves the student model performance in almost all settings (denoted by \u2020 in Table 3), we find that models always prefer a different teacher. Specifically, all CNN models (including DenseNet) achieve the best performance when ASTs are the teacher; all AST models achieve the best performance when CNNs are the teacher. Note that even though DenseNet and EfficientNet models also have different architectures, knowledge distillation between these two models leads to limited improvement. This demonstrates that the inherent differences between CNNs and ASTs is crucial for the success of knowledge distillation. 2. For both directions, the student model matches or outperforms its teacher. Usually, in knowledge distillation, the student model gets closer to, but is still weaker than, its teacher model.",
            "score": 0.5637114240368356,
            "section_title": "Which model is a good teacher?",
            "char_start_offset": 14718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 252,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 270,
                    "end": 274,
                    "matchedPaperCorpusId": "9433631"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76025390625
        },
        {
            "corpus_id": "272593221",
            "title": "What is the Role of Small Models in the LLM Era: A Survey",
            "text": "Scaling models to larger sizes is a straightforward method to enhance performance, but it often proves too computationally expensive for widespread deployment to numerous users. To mitigate this challenge, Knowledge Distillation (KD) (Hinton, 2015;Gou et al., 2021;Zhu et al., 2023;Xu et al., 2024a) offers an effective solution. In KD, a smaller student model is trained to replicate the behavior of a larger teacher model. Typically, this process involves the larger model generating a dataset with pseudo labels, which the smaller model then uses for training. \n\nWhite-box distillation involves using internal states of the teacher model which provides transparency in the training process of the student model. This approach leverages the output distributions, and intermediate features from the teacher LLMs, collectively referred to as feature knowledge (Liang et al., 2023;Gu et al., 2024;Liu et al., 2023b). It enables the development of costeffective yet powerful models, exemplified by Dis-tilBERT (Sanh, 2019) and QuantizedGPT (Yao et al., 2022). \n\nIn contrast, black-box knowledge distillation typically involves generating a distillation dataset through the teacher LLM, which is then used for fine-tuning the student model. For instance, Chain-of-Thought distillation (Wei et al., 2022b) extracts LLM rationales to provide additional supervision, thereby enhancing the reasoning capabilities of smaller models (Li et al., 2022;Hsieh et al., 2023;Shridhar et al., 2023;Magister et al., 2023;Li et al., 2023a;Fu et al., 2023;Tian et al., 2024). Additionally, Instruction Following Distillation aims to improve the zero-shot performance of LLMs by fine-tuning them with a set of instruction-like prompt-response pairs (Jiang et al., 2023c;Li et al., 2024b).",
            "score": 0.5636638960702371,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 37344,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1057
                },
                {
                    "start": 1060,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1768
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 248,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 248,
                    "end": 265,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 860,
                    "end": 880,
                    "matchedPaperCorpusId": "252693152"
                },
                {
                    "start": 1038,
                    "end": 1056,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 1282,
                    "end": 1301,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1441,
                    "end": 1460,
                    "matchedPaperCorpusId": "258461606"
                },
                {
                    "start": 1460,
                    "end": 1482,
                    "matchedPaperCorpusId": "258762841"
                },
                {
                    "start": 1482,
                    "end": 1504,
                    "matchedPaperCorpusId": "254823156"
                },
                {
                    "start": 1504,
                    "end": 1521,
                    "matchedPaperCorpusId": "259251773"
                },
                {
                    "start": 1521,
                    "end": 1537,
                    "matchedPaperCorpusId": "256390607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53125
        },
        {
            "corpus_id": "255942245",
            "title": "Dataset Distillation: A Comprehensive Review",
            "text": "Knowledge distillation (KD) [37], [38], [39], [40] aims to transfer knowledge from a large teacher network to a smaller student network, such that the student network can preserve the performance of the teacher with reduced computational overhead. The seminal work by Hinton et al. [37] leads the student to mimic the outputs of the teacher, which can represent knowledge acquired by the teacher network. Afterward, improvements of KD have focused on four aspects: representations of knowledge, teacherstudent architectures, distillation algorithms, and distillation schemes. First, knowledge can be represented by model response/output [37], [41], features [38], [42], [43], and relation [44], [45], [46]. Second, teacher-student architectures refer to the network architectures of teacher and student models, which determines the quality of knowledge acquisition and distillation from teacher to student [40]. Third, distillation algorithms determine the ways of knowledge transfer. A simple and typical way is to match the knowledge captured by the teacher and student models directly [37], [38]. Beyond that, many different algorithms are proposed to handle more complex settings, such as adversarial distillation [47], attention-based distillation [39], and data-free distillation [48], [49]. Finally, distillation schemes control training configurations of teacher and student, and there are offline- [37], [38], online- [50], and self-distillation [51]. As for application, KD is widely used in ensemble learning [52] and model compression [38], [53], [54]. \n\nThe concept of DD is inspired by KD [18]. Specifically, DD aims at a lightweight dataset, while KD aims at a lightweight model. In this view, DD and KD are only conceptually related but technically orthogonal. It is worth noting that, similar to DD, recent data-free KD methods [48], [49], [55] are also concerned with the generation of synthetic training samples since original training datasets are unavailable. Their differences are two-fold.",
            "score": 0.5635083604811632,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1564
                },
                {
                    "start": 1567,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 50,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 643,
                    "end": 647,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 664,
                    "end": 668,
                    "matchedPaperCorpusId": "221559239"
                },
                {
                    "start": 670,
                    "end": 674,
                    "matchedPaperCorpusId": "227232038"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 695,
                    "end": 699,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "198160865"
                },
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1218,
                    "end": 1222,
                    "matchedPaperCorpusId": "53976534"
                },
                {
                    "start": 1292,
                    "end": 1296,
                    "matchedPaperCorpusId": "261514306"
                },
                {
                    "start": 1427,
                    "end": 1431,
                    "matchedPaperCorpusId": "24982157"
                },
                {
                    "start": 1455,
                    "end": 1459,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "7350432"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1851,
                    "end": 1855,
                    "matchedPaperCorpusId": "261514306"
                },
                {
                    "start": 1857,
                    "end": 1861,
                    "matchedPaperCorpusId": "159041346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6455078125
        },
        {
            "corpus_id": "228376532",
            "title": "Reinforced Multi-Teacher Selection for Knowledge Distillation",
            "text": "In natural language processing (NLP) tasks, slow inference speed and huge footprints in GPU usage remain the bottleneck of applying pre-trained deep models in production. As a popular method for model compression, knowledge distillation transfers knowledge from one or multiple large (teacher) models to a small (student) model. When multiple teacher models are available in distillation, the state-of-the-art methods assign a fixed weight to a teacher model in the whole distillation. Furthermore, most of the existing methods allocate an equal weight to every teacher model. In this paper, we observe that, due to the complexity of training examples and the differences in student model capability, learning differentially from teacher models can lead to better performance of student models distilled. We systematically develop a reinforced method to dynamically assign weights to teacher models for different training instances and optimize the performance of student model. Our extensive experimental results on several NLP tasks clearly verify the feasibility and effectiveness of our approach.",
            "score": 0.5622328051024802,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64990234375
        },
        {
            "corpus_id": "271399352",
            "title": "A continual learning framework to train robust image recognition models by adversarial training and knowledge distillation",
            "text": "After examining the performance of the teacher model, in the second phase, we adopted this model to perform knowledge transfer from teacher to student with three multi-teacher strategies to evaluate the proposed approach. The goal here is to assess the performance of transferring knowledge from pretrained teacher models to a small student model (a lightweight model) by training with a small amount of data. We thus randomly sampled 20% of the amount of data from the CIFAR-100 dataset to train the student model. \n\nThe strategies include MTKD, SMTKD and SMTKDS (SMTKD with the student as one of the teachers). For all three methods, the experimental setup was the same as described in the previous section. For the baseline model MTKD, the multiple teachers represented the divided sections, and their outputs (knowledge) were averaged as the overall result (You et al., 2017). For the second method, SMTKD, additional parameter configuration was needed: Temperature was 2, the loss weights \u03b1 was 0.9 and \u03b2 was 0.3 in the total loss Equation (6). These parameters were chosen based on a preliminary test, with details listed in Table 8. \n\nA series of experiments was conducted to investigate the performance of the three multi-teacher methods mentioned and the effect of different numbers of teachers used in the distillation process. Five sets of trials were performed for each method, including using one classifier (ensemble or the deepest classifier) and multiple classifiers (with 2, 3,  4, or 5 classifiers) as teachers. Table 9 lists the results, showing the accuracy of each classifier in detail, and Figure 7 also illustrates the results. The loss converging behaviour of all three multi-teaches was the same as the loss curve shown in Figure 6. The results demonstrate that all three methods have obtained better results, and among them, the proposed approach performed the best. To sum up, more teachers used in the distillation process can obtain higher performance. The effect is especially obvious for the proposed approach, in which the best result (71.33%) is obtained with five teachers by the proposed SMTKDS method.",
            "score": 0.5614039867220941,
            "section_title": "Evaluation of different multi-teacher methods with self distillation",
            "char_start_offset": 52020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 515
                },
                {
                    "start": 518,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 861,
                    "end": 879,
                    "matchedPaperCorpusId": "26021416"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.476806640625
        },
        {
            "corpus_id": "266174074",
            "title": "Dynamic Corrective Self-Distillation for Better Fine-Tuning of Pretrained Models",
            "text": "This intriguing outcome stems from the fact that distillation inherently involves simplifying the student's model based on the teacher's knowledge. The crucial point here is that it doesn't need an exact match between the student and the teacher. In fact, striving for a perfect match would hinder the student's ability to learn and outperform the teacher. These research works (Stanton et al., 2021;Furlanello et al., 2018) study the significance of not merely mimicking the teacher but rather distilling and imparting vital knowledge in a manner that amplifies the student's performance. Weighing discordant samples. Introducing sample weighting has been demonstrated to enhance the KD process (Lu et al., 2021). More specifically, DCS assigns increased weights to the discordant samples, amplifying their impact in the subsequent stages of the student model training. This technique steers the student model toward challenging samples by leveraging the teacher's soft labels while concurrently adhering to the guidance provided by the hard labels (i.e., the ground truth labels).",
            "score": 0.5610081287076373,
            "section_title": "Dynamic Corrective Self-Distillation",
            "char_start_offset": 4338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1082
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 424,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 696,
                    "end": 713,
                    "matchedPaperCorpusId": "244119770"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328125
        },
        {
            "corpus_id": "263620565",
            "title": "Can a student Large Language Model perform as well as it's teacher?",
            "text": "Capacity: There's also a limit to how much a smaller student model can learn from a large teacher model. If the student model's capacity is too small, it may not effectively learn the teacher's knowledge. \n\n7. Overfitting Risk: If the teacher model has overfit to the training data, it's possible that the student model might learn these overfit predictions, leading to poor generalization to new data. \n\nDespite these limitations, knowledge distillation can still be a very useful technique, especially when dealing with constraints on computational resources or when deploying models in real-world applications where efficiency is key. \n\n7 Future Work \n\nThere are several directions that future work on knowledge distillation could take to further improve this technique and its application in various fields: \n\n\u2022 Improved Understanding of Distillation Dynamics: Further research is needed to understand the dynamics of knowledge transfer during distillation. For example, understanding which aspects of the teacher's knowledge are most effectively transferred and why could help optimize the process. \n\n\u2022 Automated Hyperparameter Tuning: Given the sensitivity of the distillation process to hyperparameters like the temperature and the weighting between the original loss and distillation loss, developing methods for automatic or more efficient hyperparameter tuning could be beneficial. \n\n\u2022 Advanced Distillation Techniques: Exploring advanced distillation techniques beyond the standard approach could lead to better results. This could involve novel loss functions, training methods, or types of teacher-student relationships. \n\n\u2022 Multi-Teacher Distillation: The idea of distilling knowledge from multiple teacher models into a single student model is an interesting area for exploration. This could potentially combine the strengths of various models into a single efficient student model. \n\n\u2022 Domain-Specific Adaptations: Adapting and optimizing knowledge distillation techniques for specific domains or tasks could also be a valuable avenue for future work. Different tasks might benefit from different distillation strategies. \n\n\u2022 Privacy and Security in Distillation: As distillation involves transferring knowledge from a teacher model, there could be concerns about privacy and security, especially when the teacher model has been trained on sensitive data. Future work could look at how to ensure that distillation does not leak sensitive information.",
            "score": 0.5606240415003465,
            "section_title": "Student Model",
            "char_start_offset": 19367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 204
                },
                {
                    "start": 207,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1103
                },
                {
                    "start": 1106,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1633
                },
                {
                    "start": 1636,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1897
                },
                {
                    "start": 1900,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2137
                },
                {
                    "start": 2140,
                    "end": 2371
                },
                {
                    "start": 2372,
                    "end": 2466
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88134765625
        },
        {
            "corpus_id": "265295530",
            "title": "Multi-teacher Distillation for Multilingual Spelling Correction",
            "text": "These approaches have proven highly successful for seq2seq problems in general (Kim and Rush, 2016;Liang et al., 2022). Distillation approaches vary in the degree to which they presuppose access to the teacher model during student model training (Gou et al., 2021). At one extreme, the teacher and student models are trained together (i.e., co-distillation; Chung et al. 2020). At the other extreme, the teacher is simply used to generate output labels for the training data, based on which the student is trained (e.g., Sequence-level Knowledge Distillation (Seq-KD); Kim and Rush 2016). In our multi-teacher distillation, we aim to decouple the teacher and student training regimes in order to train the best model for each language, and so we use Seq-KD. There are existing studies about multiple teacher distillation. For example, You et al. combined multiple teacher networks by averaging the softened output targets and selecting layers in student and teacher networks (You et al., 2017). Yuan et al. selected soft labels from a collec- tion of teacher models, based on the reward signal from performance of distilled student model (Yuan et al., 2021). Both studies use multiple teachers to generate variant candidates and distill knowledge to build a robust and accurate student. In this paper, we apply multi-teachers for multilingual problem, where each teacher specializes in one language and they work together to guide the learning of a multilingual student.",
            "score": 0.5603049181345986,
            "section_title": "Related Work",
            "char_start_offset": 4386,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1470
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 118,
                    "matchedPaperCorpusId": "247184813"
                },
                {
                    "start": 246,
                    "end": 264,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 358,
                    "end": 376,
                    "matchedPaperCorpusId": "209319166"
                },
                {
                    "start": 975,
                    "end": 993,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1138,
                    "end": 1157,
                    "matchedPaperCorpusId": "228376532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.297119140625
        },
        {
            "corpus_id": "246016232",
            "title": "Ensemble Transformer for Efficient and Accurate Ranking Tasks: an Application to Question Answering Systems",
            "text": "Using the heterogeneous teacher models shown in Table 2, we discuss how AS2 performance varies when using different combinations of teachers for knowledge distillation. The first method, KD Sum , simply combines loss values from multiple teachers to train a single transformer model, similarly to the task-specific distillation stage with multiple teachers in Yang et al. (2020). In the second method, KD RR , we switch teacher models for each training batch in a round-robin style; i.e., the student transformer model will be trained with the first teacher model in the first batch, with the second teacher model in the second batch, and so forth. \n\nTable 3 compares performance of the multipleteacher knowledge distillation strategies described above to that of our proposed method; we also evaluate the effect of using one teacher per head, rather than a single teacher (ELECTRA LARGE ), on CERBERUS. For ELECTRA BASE , we found that KD Sum method slightly outperforms KD RR ; this result highlights the importance of leveraging multiple teachers for knowledge distillation in the same mini-batch. For CERBERUS, we found that using multiple heterogeneous teachers (specifically, one per ranking head) is crucial in achieving the best performance; without it, CERBERUS B 11 3H 1 achieves the same performance of ELECTRA BASE despite having more parameters. Besides these two trends, the results of rows 13 and 14 in  emphasize the importance of heterogeneity in the set of teacher models. As a result, CERBERUS B 11 3H 1 performs the best and achieves the comparable performance with some of the teacher (LARGE) models, while saving between 45% and 63% of model parameters. From the aforementioned three trends, we can confirm that the improved AS2 performance was achieved thanks to the multiple ranking heads in CERBERUS, the use of multiple teachers, and heterogeneity in teacher model families; on the other hand, the slightly increased parameters compared to ELECTRA BASE did not contribute to performance uplift.",
            "score": 0.5594536417532858,
            "section_title": "Are Multiple Ranking Heads and Heterogeneous Teachers Necessary?",
            "char_start_offset": 21303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 360,
                    "end": 378,
                    "matchedPaperCorpusId": "215238853"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339599609375
        },
        {
            "corpus_id": "266977086",
            "title": "Graph Relation Distillation for Efficient Biomedical Instance Segmentation",
            "text": "We conduct an ablation study on reduced-size models to evaluate the effectiveness of distillation. The models are created by reducing the number of channels in each layer of the networks. Specifically, we generate student networks with width reductions of approximately 1  20 , 1 15 , 1 10 , and 1 5 compared to the original width. The experiments utilize ResUet network pairs on the CVPPP dataset. The results in Tab. VI demonstrate that our knowledge distillation method improves the performance of all student networks, even when they have very few parameters. However, it is important to note that the effectiveness of knowledge distillation depends on the initial performance gap between the teacher and student networks. When this gap is minimal, achieving significant improvements becomes challenging.",
            "score": 0.5593012494729468,
            "section_title": "4) Student networks with different widths:",
            "char_start_offset": 38367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 808
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.509765625
        },
        {
            "corpus_id": "225069306",
            "title": "Pre-trained Summarization Distillation",
            "text": "Other approaches are possible as well. Several works suggest that subsets of trained teacher models can be extracted directly (Sanh et al., 2019;Xu et al., 2020;Fan et al., 2019). We therefore propose a \"shrink and fine-tune\" (\"SFT\") approach that extracts a student model from the maximally spaced layers of a fine-tuned teacher. Since transformer layers are stacked using residual connections, we hypothesize that removing full layers has a minimal impact on summarization performance. This shrunken student model is then used to re-run the original fine-tuning procedure without modification. \n\nWe test all three methods on the CNN and XSUM datasets. On CNN, SFT outperforms the more expensive methods. For both BART and Pegasus, SFT produces distilled models that are 75% faster than their teacher with minimal loss in performance. On the more abstractive XSUM task, KD and PL can generate significant improvements over SFT. For BART, we use KD 1 to match teacher performance. For Pegasus, no technique matches teacher performance, but PL comes closest. As shown in Figure 1, we manage to find an approach that generates the best available model at its computational budget for each task and teacher model. In the BART case, we generate many such models of various sizes. \n\nThe paper is organized as follows: Section 2 discusses related work in further detail. Section 3 describes the specifics of our implementation of the three families of techniques. Section 5 describes summarization speed and quality for various teachers, datasets, student sizes, and distillation methods. Sections 6.2 and 6.3 describe extensions of pseudo-labeling and knowledge distillation which can further improve performance on the XSUM task.",
            "score": 0.5586608592861533,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1275
                },
                {
                    "start": 1278,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1725
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59375
        },
        {
            "corpus_id": "270703310",
            "title": "Self-Regulated Data-Free Knowledge Amalgamation for Text Classification",
            "text": "Recent NLP advancements have yielded numerous pre-trained models, often achieving state-of-the-art performance across various tasks.These models are publicly available to promote reproducibility and further research.To facilitate knowledge transfer from pre-trained teacher models, Hinton et al. (2015) pioneered Knowledge Distillation (KD), utilizing soft target labels to train light-weight student models effectively.Subsequently, diverse KD Figure 1: Given a set of pre-trained teacher models (Teacher Models 1 & 2), each with distinct expertise, the goal is to train a student model capable of amalgamating their knowledge, mastering prediction across all specialized classes of the teachers.\n\napproaches have been successfully applied in different domains.Traditionally, KD relies on using original training data to guide the student model's learning from a task-specific teacher model.However, this approach has limitations, often involving learning from a single teacher model (Sanh et al., 2019;Liu et al., 2020) or a task-specific ensemble of teachers (Fukuda et al., 2017;Tian et al., 2019).\n\nUnlike traditional KD, where teachers focus on the same task, knowledge amalgamation (KA) techniques (Luo et al., 2019;Shen et al., 2019) enable learning in a student network by integrating knowledge from multiple teachers with diverse expertise.These methods enhance the student model's classification abilities across a wider range of labels.While KA techniques are well-established in Computer Vision, their exploration in NLP literature is limited.Li et al. (2022) utilized Monte-Carlo Dropout to estimate model uncertainty for merging knowledge from different pre-trained teacher models.However, these techniques often require access to unlabeled data from the original training set used by the pre-trained models (Luo et al., 2019;Shen et al., 2019;Li et al., 2021;Vongkulbhisal et al., 2019) to train a versatile student model.Unfortunately, the original training data and annotations are often unavailable due to various issues.Moreover, the diverse expertise of teacher models may lead to uncertain states and probabilities when handling input sequences outside their domains.These challenges hinder the application of KA methods in broader domains.",
            "score": 0.5580684410398495,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 697
                },
                {
                    "start": 699,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 892
                },
                {
                    "start": 892,
                    "end": 1102
                },
                {
                    "start": 1104,
                    "end": 1350
                },
                {
                    "start": 1350,
                    "end": 1448
                },
                {
                    "start": 1448,
                    "end": 1556
                },
                {
                    "start": 1556,
                    "end": 1696
                },
                {
                    "start": 1696,
                    "end": 1938
                },
                {
                    "start": 1938,
                    "end": 2040
                },
                {
                    "start": 2040,
                    "end": 2189
                },
                {
                    "start": 2189,
                    "end": 2262
                }
            ],
            "ref_mentions": [
                {
                    "start": 1004,
                    "end": 1021,
                    "matchedPaperCorpusId": "214802887"
                },
                {
                    "start": 1062,
                    "end": 1083,
                    "matchedPaperCorpusId": "30258763"
                },
                {
                    "start": 1223,
                    "end": 1241,
                    "matchedPaperCorpusId": "53231387"
                },
                {
                    "start": 1556,
                    "end": 1572,
                    "matchedPaperCorpusId": "252815916"
                },
                {
                    "start": 1841,
                    "end": 1859,
                    "matchedPaperCorpusId": "53231387"
                },
                {
                    "start": 1875,
                    "end": 1902,
                    "matchedPaperCorpusId": "119185541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40185546875
        },
        {
            "corpus_id": "270094801",
            "title": "Aligning in a Compact Space: Contrastive Knowledge Distillation between Heterogeneous Architectures",
            "text": "Knowledge distillation has emerged as an extensively utilized strategy for model compression.It typically employs a teacher-student architecture, where the student model is guided to mimic the teacher's predictions [1,2,3,4] or feature representations [5,6,7,8], thereby enhancing the performance of compact models.Current methods predominantly focus on scenarios where both the teacher and student models share the same architectural lineage, often involving pairing a largerscale teacher model with a smaller-scale student, exemplified by transitions such as ResNet18 to ResNet34 [9] or ViT-S to ViT-G [10].\n\nHowever, the performance ceilings of models vary due to the differing scalability of the architectures.This limitation narrows the applicability of knowledge distillation homogeneous architectures.For instance, it is challenging for a student model from the MobileNet [11] family to find a teacher model within its own lineage that can match the capabilities of ViT-G [10].Knowledge distillation in heterogeneous architectures addresses this limitation by expanding the pool of potential teacherstudent pairings.While some research has ventured into exploring teacher-student combinations in heterogeneous architectures [12,13,14,4,15], the field is still in its early stages and holds substantial potential for further exploration and advancement.\n\nExisting knowledge distillation methods primarily employ logit [1,2,3,4] or intermediate feature maps [5,6,7,8] as the medium for knowledge transfer.The logit space serves as a task-specific shared domain, applicable to all knowledge distillation tasks.In contrast to logit-based approaches, feature-based methods transmit knowledge via aligning intermediate feature representations point-topoint.While these methods often excel in knowledge distillation with homogeneous architectures due to the intrinsic similarity of feature representations, they face serious setback in heterogeneous settings [4].The substantial differences in meta-architecture, input format, and spatial representation between heterogeneous models preclude the use of simple similarity metrics for aligning stage-wise feature representations.Moreover, mimicing the local details in feature representations may lead to detrimental guidance.",
            "score": 0.557591667147001,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 609
                },
                {
                    "start": 611,
                    "end": 714
                },
                {
                    "start": 714,
                    "end": 808
                },
                {
                    "start": 808,
                    "end": 984
                },
                {
                    "start": 984,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1359
                },
                {
                    "start": 1361,
                    "end": 1510
                },
                {
                    "start": 1510,
                    "end": 1614
                },
                {
                    "start": 1614,
                    "end": 1758
                },
                {
                    "start": 1758,
                    "end": 1963
                },
                {
                    "start": 1963,
                    "end": 2177
                },
                {
                    "start": 2177,
                    "end": 2274
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 220,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 220,
                    "end": 222,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 222,
                    "end": 224,
                    "matchedPaperCorpusId": "265871679"
                },
                {
                    "start": 252,
                    "end": 255,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 255,
                    "end": 257,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 257,
                    "end": 259,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 259,
                    "end": 261,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 582,
                    "end": 585,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 604,
                    "end": 608,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 979,
                    "end": 983,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 1231,
                    "end": 1235,
                    "matchedPaperCorpusId": "195584436"
                },
                {
                    "start": 1235,
                    "end": 1238,
                    "matchedPaperCorpusId": "201107180"
                },
                {
                    "start": 1238,
                    "end": 1241,
                    "matchedPaperCorpusId": "218487294"
                },
                {
                    "start": 1241,
                    "end": 1243,
                    "matchedPaperCorpusId": "265871679"
                },
                {
                    "start": 1427,
                    "end": 1429,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1429,
                    "end": 1431,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 1431,
                    "end": 1433,
                    "matchedPaperCorpusId": "265871679"
                },
                {
                    "start": 1463,
                    "end": 1466,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1466,
                    "end": 1468,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1468,
                    "end": 1470,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1470,
                    "end": 1472,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1959,
                    "end": 1962,
                    "matchedPaperCorpusId": "265871679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80126953125
        },
        {
            "corpus_id": "253499102",
            "title": "PILE: Pairwise Iterative Logits Ensemble for Multi-Teacher Labeled Distillation",
            "text": "We conduct several comparison experiments to verify our proposed method. The models in the offline comparison experiments include: \n\n\u2022 Base: We use an ERNIE-based ranking model as our base model, which is finetuned with a pairwise loss using human-labeled querydocument pairs without any guidelines from teachers; \n\n\u2022 single-KD: In this setting we add knowledge distillation loss when training the base model using the teacher that performs best on the test set;  \u2022 AE-KD: Instead of using the single teacher, this variant uses an ensemble of 3 teachers with averaged weight to perform knowledge distillation; \n\n\u2022 PILE-KD: When performing knowledge distillation, PILE-KD uses human-annotated labels with the help of the PILE algorithm to conduct a dynamic weighting process for the ensemble of 3 teachers. \n\nThe results of each model are shown in Table 2 with the improvement compared to the base model. We also report the performance of the teachers used in knowledge distillation. As we expected, all the distilled models consistently outperform finetuned base model thanks to teacher models' guidance and regularization. And besides, using an ensemble of teachers gains further promotion than the single teacher distillation. After ensembling multiple teachers by averaging distillation logits, the PNR reaches 3.16, exceeding the base by 1.61%. This shows that the remission from biased distillation by the cooperation of multiple teachers improves students in semantic matching. Moreover, by applying the PILE algorithm, we can see that the student can beat the base model by a large margin w.r.t PNR, where the value is improved to 3.18 by 2.25% improvement. It shows the effectiveness of dynamic reduction of biased teachers' weight in the ensemble process.",
            "score": 0.5572418772208939,
            "section_title": "Offline Experimental Results",
            "char_start_offset": 16999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 130
                },
                {
                    "start": 133,
                    "end": 313
                },
                {
                    "start": 316,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1764
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1917724609375
        },
        {
            "corpus_id": "268536644",
            "title": "Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models",
            "text": "Table 2 shows the performance of various students when trained to emulate the teacher debiased decisions, where students are either purely distilled ( \u00a74.1) or trained to achieve error-correction ( \u00a74.2).The table shows that:\n\n1) For some tasks (e.g.comparative assessment on SummEval) the teacher's abilities can be adequately learned by a smaller student through standard knowledge distillation.The resulting student can achieve performance considerably better than the biased teacher and low permutation sensitivity, all while being considerably more computationally efficient.\n\n2) For complex tasks (e.g.RACE++) the stu-dent is not powerful enough to alone capture the abilities of the teacher.However, in such cases, error correction students can effectively leverage a single-biased teacher decision to predict the estimated general debiased distributions.These student systems are more robust to changes in permutations, although are not fully permutation invariant.Note that error correction consistently yields better performance than copying the biased teacher's decision, illustrating that the students can capture useful information of the underlying teacher's prediction space.\n\n3) Although the size and ability of student can be an important factor when applying the framework (e.g.RACE++), for some tasks the required model complexity can saturate early and a further increase in size/ability does not impact downstream performance.",
            "score": 0.5571614086238956,
            "section_title": "Debiased Student Performance",
            "char_start_offset": 19628,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 225
                },
                {
                    "start": 227,
                    "end": 250
                },
                {
                    "start": 250,
                    "end": 397
                },
                {
                    "start": 397,
                    "end": 580
                },
                {
                    "start": 582,
                    "end": 608
                },
                {
                    "start": 608,
                    "end": 698
                },
                {
                    "start": 698,
                    "end": 862
                },
                {
                    "start": 862,
                    "end": 973
                },
                {
                    "start": 973,
                    "end": 1190
                },
                {
                    "start": 1192,
                    "end": 1296
                },
                {
                    "start": 1296,
                    "end": 1447
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4404296875
        },
        {
            "corpus_id": "268820185",
            "title": "A Comprehensive Review of Knowledge Distillation in Computer Vision",
            "text": "A knowledge distillation approach called online distillation teaches a smaller student model to mimic the actions of a larger teacher model in real-time.The teacher and student models are jointly trained on a smaller dataset during an online training procedure [59].Updates to the student model's parameters are frequently required throughout the online distillation process in response to input from the teacher model.This can be done using a variety of techniques, including knowledge distillation loss [59], attention transfer [60], or feature imitation [61].The objective is to improve the student model's performance on the given task while simultaneously teaching the student model to behave similarly to the teacher model.There are various advantages to online distillation over offline distillation.First, the input from the teacher model can help direct the learning process, which can improve the convergence speed of the student model.Second, it can assist the student model in learning from cases that would otherwise be impossible to capture in a huge dataset.Finally, because the teacher model may provide instruction even when the training data is minimal, it can reduce the amount of labelled training data required for training.While Online Knowledge Distillation (OKD) enhances models by leveraging differences between students and teachers, key issues remain unexplored, such as the impact of a large gap on student performance.In order to overcome these challenges, The authors in this paper [62] implement Switchable Online Knowledge Distillation (SwitOKD), which adjusts the distillation gap during training rather than taking into account accuracy differences during testing.\n\nUsing a switching strategy, SwitchOKD [62] switches between expert and learning modes: the teacher pauses while the student continues to learn, and the teacher resumes when the student has finished learning.As a result, an adaptive switching threshold determines when to switch modes, optimizing the distillation gap and enhancing student performance.Teachers also benefit from this method, since their performance is comparable to those obtained by other online methods.The Knowledge Distillation (KD) online system aims to train multiple student models collaboratively, enabling them to distill knowledge from each other's experiences.Although current methods focus on class probabilities, they often overlook valuable feature representational information.",
            "score": 0.5567219641283885,
            "section_title": "Online distillation",
            "char_start_offset": 20830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 153,
                    "end": 266
                },
                {
                    "start": 266,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 562
                },
                {
                    "start": 562,
                    "end": 729
                },
                {
                    "start": 729,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1073
                },
                {
                    "start": 1073,
                    "end": 1245
                },
                {
                    "start": 1245,
                    "end": 1447
                },
                {
                    "start": 1447,
                    "end": 1698
                },
                {
                    "start": 1700,
                    "end": 1907
                },
                {
                    "start": 1907,
                    "end": 2051
                },
                {
                    "start": 2051,
                    "end": 2171
                },
                {
                    "start": 2171,
                    "end": 2337
                },
                {
                    "start": 2337,
                    "end": 2458
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "252198825"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "252198825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.495361328125
        },
        {
            "corpus_id": "270878760",
            "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
            "text": "The feature-based knowledge distillation methods [46,122] extract knowledge from the embedding space, transformer layers, and prediction layers, allowing the student model to learn various aspects of the teacher model comprehensively.For instance, Sun et al. [122] proposed a patient knowledge distillation (PKD) method aimed at compressing a large-scale teacher model into an equally effective lightweight student model.They proposed two distinct distillation strategies: 1) PKD-Last: The student model learns from the last  layers of the teacher model, based on the assumption that the top layers contain the most informative knowledge.2) PKD-Skip: The student learns from every -layer of the teacher, suggesting that the lower layers also contain essential information that should be gradually transferred during distillation.Experiments conducted on seven datasets across four tasks-sentiment classification, paraphrase similarity matching, natural language inference, and machine reading comprehension-showed that the PKD method outperformed standard knowledge distillation methods.It achieved superior performance and better generalization, significantly enhancing training efficiency and reducing storage requirements while maintaining accuracy comparable to the original large-scale model.MetaDistill [168] offers a simple and efficient alternative to traditional KD methods by keeping the teacher model fixed during training.Within the meta-learning framework, teacher networks enhance knowledge transfer to student networks by distilling feedback on student performance.Additionally, a pilot update mechanism is introduced to improve the alignment between internal learners and meta-learners, focusing on enhancing internal learners' performance.Extensive experiments have validated the effectiveness and versatility of this method across text and image classification tasks.Furthermore, experiments on the GLUE benchmark have shown that MetaDistill significantly outperforms traditional knowledge distillation, achieving state-of-the-art performance compression.AD-KD [155] addresses two key limitations of existing knowledge distillation methods.First, student models often merely mimic the teacher's behavior without developing their own reasoning capabilities.Second, these methods typically focus on transferring knowledge specific to complex models while neglecting data-specific knowledge.",
            "score": 0.5561276207017527,
            "section_title": "Hint-based KD.",
            "char_start_offset": 16483,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 421
                },
                {
                    "start": 421,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 829
                },
                {
                    "start": 829,
                    "end": 1087
                },
                {
                    "start": 1087,
                    "end": 1297
                },
                {
                    "start": 1297,
                    "end": 1434
                },
                {
                    "start": 1434,
                    "end": 1580
                },
                {
                    "start": 1580,
                    "end": 1756
                },
                {
                    "start": 1756,
                    "end": 1885
                },
                {
                    "start": 1885,
                    "end": 2073
                },
                {
                    "start": 2073,
                    "end": 2158
                },
                {
                    "start": 2158,
                    "end": 2274
                },
                {
                    "start": 2274,
                    "end": 2406
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 53,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 53,
                    "end": 57,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 259,
                    "end": 264,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 1309,
                    "end": 1314,
                    "matchedPaperCorpusId": "237250417"
                },
                {
                    "start": 2079,
                    "end": 2084,
                    "matchedPaperCorpusId": "258740796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64892578125
        },
        {
            "corpus_id": "228376532",
            "title": "Reinforced Multi-Teacher Selection for Knowledge Distillation",
            "text": "Deep pre-trained models, such as BERT (Devlin et al. 2018), XLNet (Yang et al. 2019), RoBERTa ) and ALBERT (Lan et al. 2019), have proved effective on many NLP tasks by establishing record-breaking state-of-the-art results. However, due to huge amounts of model parameters, typically at the magnitude of hundreds of millions or even billions, the bottleneck for applying those pre-trained models in production is the slow inference speed and the huge footprints in using GPUs. To save the computation cost and speed up the inference process, knowledge distillation (KD) (Hinton, Vinyals, and Dean 2015) as an effective approach to compress large models into smaller ones stands out and becomes the de facto best choice among other alternatives, such as pruning (Han, Mao, and Dally 2015; and quantization (Gong et al. 2014).\n\nThe knowledge distillation approach is based on a teacherstudent learning paradigm, where a teacher model, which is often large, is taken and the output of the teacher model Table 1: RoBERTa-Base performs better than BERT-Base. However, the student model distilled from BERT-Base, the weaker model, performs better than the same student model distilled from RoBERTa-Base, the stronger teacher model. is integrated as a soft target in the loss function to train a student model, which is often small. The teacher-student learning paradigm demonstrates excellent performance on various NLP tasks (Kim and Rush 2016;Tang et al. 2019). Knowledge distillation methods start from a single teacher. Some recent methods employ multiple teachers and show great promises in further boosting student model performance effectively.\n\nMost of the existing methods using multiple teachers simply assign an equal weight to all teacher models during the whole distillation process. The uniform distribution of weights among all teachers keeps the coordination, management and implementation of the multi-teacher framework simple. At the same time, the indifference to strengths and weaknesses of various teacher models leaves a huge untouched space for better knowledge distillation. To make a multi-teacher approach work well, teacher models have to be diverse. Exploiting the diverse strengths of various teacher models can bring in huge advantages.\n\nIndividual teacher models may perform differently on various instances. Different models may vary in",
            "score": 0.5558946632243897,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1439,
                    "end": 1456,
                    "matchedPaperCorpusId": "85543565"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.439208984375
        },
        {
            "corpus_id": "272827138",
            "title": "On Importance of Pruning and Distillation for Efficient Low Resource NLP",
            "text": "Knowledge distillation [4], [5] is a compression technique, where a larger model (teacher) is used to train a smaller model (student). The student model tries to replicate the behavior of the teacher model. The goal is to distill the knowledge of the teacher into the student, allowing the student to achieve performance similar to the teacher while being more computationally efficient. \n\n[20] explores using knowledge distillation to improve performance in machine translation for low-resource languages. Instead of a single teacher, the authors propose using an ensemble of multilingual models as teachers. This ensemble approach potentially captures a wider range of knowledge beneficial for the student model. Their research finds that the student model trained with knowledge distillation can even outperform the teacher models and the original ensemble. \n\nIn the last 3 experiments, we apply Block Movement Pruning followed by Knowledge Distillation. In this case, the teacher model is our base model, marathi-topic-all-doc-v2 and the student model is the pruned model. [14] is a technique that aims to optimize the computational efficiency of training models by utilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point precision (fp32 or float32) to represent and process variables. By reducing the precision of certain variables to lower numerical formats like 16-bit floating point (fp16 or float16), we can speed up the computations.",
            "score": 0.5557169544050932,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 13662,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 387
                },
                {
                    "start": 390,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1507
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56884765625
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "Different teacher architectures can provide their own useful knowledge for a student network. The multiple teacher networks can be individually and integrally used for distillation during the period of training a student network. In a typical teacher-student framework, the teacher usually has a large model or an ensemble of large models. To transfer knowledge from multiple teachers, the simplest way is to use the averaged response from all teachers as the supervision signal (Hinton et al. 2015) Yuan et al. 2021). A generic framework for multi-teacher distillation is shown in Fig. 11. Multiple teacher networks have turned out to be effective for training student model usually using logits and feature representation as the knowledge. In addition to the averaged logits from all teachers, You et al. (2017) further incorporated features from the intermediate layers in order to encourage the dissimilarity among different training samples. To utilize both logits and intermediate features, Chen et al. (2019b) used two teacher networks, in which one teacher transfers response-based knowledge to the student and the other teacher transfers feature-based knowledge to the student. Fukuda et al. (2017) randomly selected one teacher from the pool of teacher networks at each iteration. To transfer featurebased knowledge from multiple teachers, additional teacher branches are added to the student networks to mimic the intermediate features of teachers (Park and Kwak 2020;Asif et al. 2020). Born again networks address multiple teachers in a step-by-step manner, i.e., the student at the t step is used as the teacher of the student at the t + 1 step (Furlanello et al. 2018),  and similar ideas can be found in Yang et al. (2019a). To efficiently perform knowledge transfer and explore the power of multiple teachers, several alternative methods have been proposed to simulate multiple teachers by adding different types of noise to a given teacher (Sau and Balasubramanian 2016) or by using stochastic blocks and skip connections (Lee et al. 2019c). Using multiple teacher models with feature ensembles, knowledge amalgamation is designed in (Shen et al. 2019a;Luo et al. 2019;Shen et al. 2019b;Luo et al. 2020).",
            "score": 0.5554720663236808,
            "section_title": "Multi-teacher Distillation",
            "char_start_offset": 38164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 500,
                    "end": 517,
                    "matchedPaperCorpusId": "228376532"
                },
                {
                    "start": 796,
                    "end": 813,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1459,
                    "end": 1479,
                    "matchedPaperCorpusId": "220378802"
                },
                {
                    "start": 1479,
                    "end": 1496,
                    "matchedPaperCorpusId": "202660953"
                },
                {
                    "start": 1719,
                    "end": 1738,
                    "matchedPaperCorpusId": "21668571"
                },
                {
                    "start": 2039,
                    "end": 2057,
                    "matchedPaperCorpusId": "210952293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.384765625
        },
        {
            "corpus_id": "214468602",
            "title": "Role-Wise Data Augmentation for Knowledge Distillation",
            "text": "Knowledge Distillation (KD) is a common method for transferring the ``knowledge'' learned by one machine learning model (the \\textit{teacher}) into another model (the \\textit{student}), where typically, the teacher has a greater capacity (e.g., more parameters or higher bit-widths). To our knowledge, existing methods overlook the fact that although the student absorbs extra knowledge from the teacher, both models share the same input data -- and this data is the only medium by which the teacher's knowledge can be demonstrated. Due to the difference in model capacities, the student may not benefit fully from the same data points on which the teacher is trained. On the other hand, a human teacher may demonstrate a piece of knowledge with individualized examples adapted to a particular student, for instance, in terms of her cultural background and interests. Inspired by this behavior, we design data augmentation agents with distinct roles to facilitate knowledge distillation. Our data augmentation agents generate distinct training data for the teacher and student, respectively. We find empirically that specially tailored data points enable the teacher's knowledge to be demonstrated more effectively to the student. We compare our approach with existing KD methods on training popular neural architectures and demonstrate that role-wise data augmentation improves the effectiveness of KD over strong prior approaches. The code for reproducing our results can be found at this https URL",
            "score": 0.5544938881814186,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5107421875
        },
        {
            "corpus_id": "248848394",
            "title": "Bearing Faulty Prediction Method Based on Federated Transfer Learning and Knowledge Distillation",
            "text": "After completing the knowledge distillation process, the three established teacher models and the student model are performed on the testing set of the target datasets of case study II. As shown in Figure 20 and Table 10 in case study II, the student model outperforms the three teacher models in terms of both the average prediction accuracy and the average KL-divergence loss with a smaller parameter size.",
            "score": 0.5539379346511862,
            "section_title": "Model Evaluation",
            "char_start_offset": 51229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 408
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35009765625
        },
        {
            "corpus_id": "231648215",
            "title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation",
            "text": "Effects of data size. We compare over varied numbers of trained samples: 20, 40, 60, 80 for each class to examine  Table 2: Comparisons with the teacher and student fine-tuning on all datasets. P. and S. mean Pearson and Spearman correlation respectively. The bold and underlined numbers represent the best and 2nd best results respectively.  model performance w.r.t. different domain data size. As in Figure 2, L2A consistently improves the performance of the knowledge distill model and even outperform large teacher model by leveraging information from resource-rich source domain. And the improvement is more obvious when the amount of data is smaller. This shows L2A can effectively help BERT knowledge distillation for data-scarce domains.\n\nEffect of different source domains. We further proceed to examine the impact of similarity between the source and target domains. We take \"Home\", \"Electronics\" as source data respectively and take \"Watches\" as target data. To keep fair comparisons, we both choose 5000 instances for source data to help the same data-scarce target. We find that using \"Electronics\" instead of \"Home\" as source domain achieves better results by 0.5% which shows transferring between similar domains leads to better performance. Effect of different sample size. We also conduct experiments to show how the number of generated augmented sentences per original sentence affects performance. We find  that augmenting target domain data yields large performance boosts. By augmenting more source domain data has slightly better performance. We suspect that, if the source domain is relatively large, more source domain data does help the target but may not be very significant.\n\nEffect of different student models. We use the first 4 layers of the teacher model as student initialization and find that L2A is also effective, as it improves 10% in terms of ACC over the base student model on the text classification task. This shows the L2A method is generally helpful for the student model with different initialization setups.\n\nEffects of distillation objective. To investigate the effects of distillation objectives, we compare the L2A method with its variants: the L2A without the attention layer (w/o L att ), the intermediate hidden layer (w/o L hidden ), and dark knowledge distillation (w/o L dark )",
            "score": 0.553775813755288,
            "section_title": "Ablation Study",
            "char_start_offset": 24978,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.432373046875
        },
        {
            "corpus_id": "271956980",
            "title": "Bring the Power of Diffusion Model to Defect Detection",
            "text": "Large-scale deep models have achieved remarkable success, but their computational complexity and massive storage requirements make real-time deployment a challenge, especially on resource-constrained devices such as video surveillance and self-driving cars. \n\nKnowledge distillation is a classical approach of model compression and acceleration that effectively learns small student models from large teacher models [18]. In knowledge distillation, small student models are usually supervised with the assistance of large teacher models, allowing the student models to mimic the teacher models for a competitive or even superior performance. Knowledge distillation is similar to the way humans learn, and the key issue in distillation is how to better transfer knowledge from large teacher models to small student models. Inspired by this, recent methods to knowledge distillation have been extended to teacher-student learning [19], mutual learning [20], self-learning [21] and so on. Most of the extensions to knowledge distillation focus on compressing deep neural networks. The generated lightweight student networks can be easily deployed in applications such as visual recognition, speech recognition and natural language processing (NLP). \n\nIn addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled [22]. We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student.",
            "score": 0.552539954710405,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 7189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 260,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1647
                }
            ],
            "ref_mentions": [
                {
                    "start": 416,
                    "end": 420,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "259697005"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "249145972"
                },
                {
                    "start": 970,
                    "end": 974,
                    "matchedPaperCorpusId": "251196646"
                },
                {
                    "start": 1446,
                    "end": 1450,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7265625
        },
        {
            "corpus_id": "260704230",
            "title": "Teacher-Student Architecture for Knowledge Distillation: A Survey",
            "text": "Teacher-Student architectures were first proposed in KD, which aim to obtain lightweight student networks with comparable performance to deep teacher networks. Different from the existing KD surveys [12,13,14] primarily focusing on the objective of knowledge compression, this survey provides a comprehensive review of Teacher-Student architectures for multiple distillation objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. Moreover, this survey not only introduces various knowledge representations and optimization objectives associated with each specific representation, but also provides a detailed overview of representative learning algorithms and distillation schemes under the framework of Teacher-Student architectures. The recent applications of Teacher-Student architectures with KD are summarized based on various network purposes: classification, recognition, generation, ranking, and regression. Lastly, this survey investigates the promising research directions of knowledge learning on Teacher-Student architecture design, knowledge quality, and theoretical studies of regression-based learning, respectively. Through this comprehensive survey, industry practitioners and the academic community can gain valuable insights and guidelines for effectively designing, learning, and applying Teacher-Student architectures on various distillation objectives.",
            "score": 0.5524411156166894,
            "section_title": "CONCLUSION",
            "char_start_offset": 74015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1434
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 209,
                    "matchedPaperCorpusId": "220632998"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.471435546875
        },
        {
            "corpus_id": "273963558",
            "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network",
            "text": "Knowledge distillation transfers knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student) [33]. The process takes into account two main factors: the type of knowledge and the distillation scheme. Knowledge types include response-based, where the student mimics the teacher's final predictions; feature-based, where the student learns from the teacher's intermediate layers; and relation-based, where the student learns from the relationships between layers or samples [21]. Distillation schemes include offline distillation, where the teacher is trained first and then guides the student; online distillation, where both models train simultaneously; and self-distillation, where the teacher and student share the same architecture [21], [34]. This method creates lightweight models that can maintain or even surpass the performance of larger models. Figure 1.c illustrates the knowledge distillation process, showing the transfer of knowledge from a teacher model to a student model.",
            "score": 0.5512643357140857,
            "section_title": "3) Knowledge Distillation",
            "char_start_offset": 11001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1020
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 131,
                    "matchedPaperCorpusId": "264109973"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "248507674"
                },
                {
                    "start": 768,
                    "end": 772,
                    "matchedPaperCorpusId": "248507674"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66796875
        },
        {
            "corpus_id": "237277964",
            "title": "Improving Object Detection by Label Assignment Distillation",
            "text": "To understand how the relative gap between the model sizes can affect the student's performance, we compare the results of two model pairs, including (PAA-R18, PAA-R50) and (PAA-R18, PAA-R101) using LAD and CoLAD. Table 6 reports the experiments. We see that the impact of teachers with different capacity and pretrained weights is negligible and in the typical noise \u00b10.1AP. This contradicts with the paradox [34] that a student's performance degrades when the gap with its teacher is large. This proves that the discrepancy between teacher and student may not be a weakness of LAD and CoLAD. However, from a different viewpoint, this is also a drawback because a better teacher can't improve the student further. Hence, we should also combine LAD with other distillation techniques in order to take the full advantages of the teacher.",
            "score": 0.5510074607982017,
            "section_title": "Impact of Teacher-Student's relative model sizes",
            "char_start_offset": 22176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 836
                }
            ],
            "ref_mentions": [
                {
                    "start": 410,
                    "end": 414,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2462158203125
        },
        {
            "corpus_id": "213158848",
            "title": "Look One and More: Distilling Hybrid Order Relational Knowledge for Cross-Resolution Image Recognition",
            "text": "With the improvement of model performance, the number of model parameters is also increasing. It often needs to occupy huge memory resources and consume lots of time, leading to the issue of model compression. Knowledge distillation (Hinton, Vinyals, and Dean 2015;Romero et al. 2015;Liu et al. 2019;Park et al. 2019;Xue et al. 2019) provides a feasible way to address that issue. Simply speaking, its idea is using a large parameter model (teacher) to supervise the learning of a small parameter model (student). In practice, the student model learns the output behaviors of the teacher model that has been pre-trained on some target dataset. Although the student model can't reach the accuracy of the teacher model, it still is very powerful and yet efficient. There are two key factors of knowledge distillation: what knowledge has been learned and how to effectively transfer knowledge from teacher model to student model. \n\nTraditional knowledge distillation tends to the direct transmission of instance information, and the teacher and student often input in pairs, losing the structural information Figure 2: The framework of our approach. By focusing on the structural relational knowledge in various orders, our approach step-wisely transfers knowledge from a complex teacher to a light teacher with an assistant by cross-structure distillation and and cross-resolution distillation. about original data. Some recent approaches (Yu et al. 2019;Liu et al. 2019) attempt to pay more attention to the structural relationship of the output rather than the output itself, so when training student model, they try to mimic the same relationship structures as teachers. These approaches can retain higher-order attribute information in transfer learning. Intuitively, more effective information can be obtained by focusing on higher-order relational knowledge, which inspires the original intention of our new distillation approach. \n\n3 Proposed Approach",
            "score": 0.5500919029742506,
            "section_title": "Knowledge Distillation and Transfer",
            "char_start_offset": 6716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1934
                },
                {
                    "start": 1937,
                    "end": 1956
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 284,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 284,
                    "end": 300,
                    "matchedPaperCorpusId": "198185886"
                },
                {
                    "start": 300,
                    "end": 317,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 317,
                    "end": 333,
                    "matchedPaperCorpusId": "198905931"
                },
                {
                    "start": 1437,
                    "end": 1453,
                    "matchedPaperCorpusId": "102351826"
                },
                {
                    "start": 1453,
                    "end": 1469,
                    "matchedPaperCorpusId": "198185886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.480224609375
        },
        {
            "corpus_id": "246294774",
            "title": "Dynamic Rectification Knowledge Distillation",
            "text": "The primary challenge with conventional knowledge distillation is training a cumbersome model to create resource-and time-intensive teacher models. To circumvent this constraint, research in the area of Teacher Free Knowledge Distillation has seen some interest [24]. The concept of this framework was motivated by two further novel experiments conducted by the authors. The first experiment established that a poorly trained teacher model may enhance the performance of a student model, which they termed Defective Knowledge Distillation (De-KD). The second experiment established that a student model can distill knowledge back to a teacher model in order to enhance its performance, which they termed Reversed Knowledge Distillation (Re-KD). By examining the results of these two experiments, the researchers demonstrated that a student model may improve its performance without the assistance of a teacher model completely, which they termed Teacher Free Knowledge Distillation (Tf-KD) [24]. \n\nThe first Tf-KD approach is referred to as \"Self-Training Knowledge Distillation\" or Tf-KD self . As the authors explain in their study, a student may teach a teacher, and a poorly trained teacher can benefit a student as well. The authors devised this strategy in response to the absence of a large teacher model. They trained a model with the configuration of student S over a certain number of epochs using cross-entropy loss to build a pre-trained self-teacher model that was then utilized as the weak teacher S t in order to distill knowledge to the student S using soft target transfer. In their experiment, the authors employed Hinton's [7] knowledge distillation setup, including the loss function, described in section (III-A), but they substituted the cumbersome teacher T with the weak teacher S t . This means the output scores O t = S t (x) instead of coming from T . Meaning now O \u03c4 t = \u03c3(S t (x), \u03c4 ) , that is the KL divergence term learns from the weak teacher.",
            "score": 0.5494464672089951,
            "section_title": "B. Teacher Free knowledge Distillation",
            "char_start_offset": 11002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 995
                },
                {
                    "start": 998,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1976
                }
            ],
            "ref_mentions": [
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 990,
                    "end": 994,
                    "matchedPaperCorpusId": "219962714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.486572265625
        },
        {
            "corpus_id": "257513218",
            "title": "Model Compression for Deep Neural Networks: A Survey",
            "text": "As shown in Figure 5, knowledge distillation is a teacher-student architecture [106][107][108].The teacher network is a complex pre-trained network, and the student network is a simple small network.The teacher network provides the student network with prior knowledge so that the student network achieves similar performance to that of the teacher network.Deploying deep models in mobile devices is challenging due to the limited processing power and memory of these devices.To address these issues, Bucilu\u0203 et al. [109] first proposed model compression to transfer information from a large model to train a small model without significant accuracy degradation.Henceforth, the training of small models by large models was called knowledge distillation [108,110,111].Chen et al. [112] posited that feature embedding from deep neural networks could convey complementary information and, thus, proposed a novel knowledge-distilling strategy to improve its performance.The main idea of knowledge distillation was that the student model imitated the teacher model to achieve competitive, or even superior, performance.The key focus was how to transfer knowledge from a large teacher model to a small student model.\n\nIn the process of knowledge distillation, knowledge types, distillation strategies, and teacher-student architectures have played key roles in the student learning process.The activations, neurons, and features of the middle layer were available as knowledge to guide the learning of the student model [113][114][115][116][117]. The relationship between different activations, neurons, and features contained the rich knowledge learned by the teacher model [118][119][120][121][122]. As shown in Figure 6, three methods of knowledge distillation were introduced.These three distillation methods are described in detail in the following sections.",
            "score": 0.5484730385070178,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 26253,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 767
                },
                {
                    "start": 767,
                    "end": 966
                },
                {
                    "start": 966,
                    "end": 1114
                },
                {
                    "start": 1114,
                    "end": 1210
                },
                {
                    "start": 1212,
                    "end": 1384
                },
                {
                    "start": 1384,
                    "end": 1774
                },
                {
                    "start": 1774,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 89,
                    "matchedPaperCorpusId": "236159551"
                },
                {
                    "start": 516,
                    "end": 521,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 758,
                    "end": 762,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 762,
                    "end": 766,
                    "matchedPaperCorpusId": "16550689"
                },
                {
                    "start": 779,
                    "end": 784,
                    "matchedPaperCorpusId": "52927917"
                },
                {
                    "start": 1524,
                    "end": 1529,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1529,
                    "end": 1534,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1534,
                    "end": 1539,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1669,
                    "end": 1674,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1674,
                    "end": 1679,
                    "matchedPaperCorpusId": "195847947"
                },
                {
                    "start": 1679,
                    "end": 1684,
                    "matchedPaperCorpusId": "198185886"
                },
                {
                    "start": 1684,
                    "end": 1689,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1689,
                    "end": 1694,
                    "matchedPaperCorpusId": "102351826"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5546875
        },
        {
            "corpus_id": "269137377",
            "title": "Federated Distillation: A Survey",
            "text": "These methods focus on transferring knowledge from the middle layers of the teacher model to guide the training of the student model, helping the student model learn crucial representations and features at an intermediate level.\n\n3) Parameter Knowledge -In this category, partially trained parameters or network modules of the teacher model are directly used as knowledge during distillation training.This method is often combined with other distillation approaches to enhance knowledge transfer [63].Specifically, PESF-KD proposes an efficient approach to transferring knowledge from the teacher network to the student network by updating specific parameters of the pre-trained teacher model [64].FSKD compresses the pre-trained teacher model to obtain a student model and adjusts the feature dimensions using 1 \u00d7 1 convolutions between layers, allowing the student model to achieve performance comparable to traditional fine-tuned distillation methods with a small number of samples [65].Similarly, IAKD [66] and NGFSKD [67] assist distillation by replacing modules of the teacher model and the student model.The distillation loss for individual module replacement is expressed as:\n\nwhere L CE represents the calculation of cross-entropy, and M T o and M S o are the outputs of the corresponding modules in the teacher and student networks.Recently, SAKD proposes merging teacher and student networks of the same style system into a multi-path network.\n\nDuring training, a different teacher network module is dynamically selected to replace the corresponding student network module for each sample, resulting in a student model with superior performance [68].These methods leverage parameter knowledge from the teacher model to guide the training of the student model, either by updating specific parameters or by replacing modules.This enables the student model to learn from the partially trained teacher model, improving its performance and reducing its complexity.",
            "score": 0.5484483635556114,
            "section_title": "Soft targets Teacher model Loss",
            "char_start_offset": 14697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 230,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 501
                },
                {
                    "start": 501,
                    "end": 698
                },
                {
                    "start": 698,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1111
                },
                {
                    "start": 1111,
                    "end": 1183
                },
                {
                    "start": 1185,
                    "end": 1342
                },
                {
                    "start": 1342,
                    "end": 1454
                },
                {
                    "start": 1456,
                    "end": 1661
                },
                {
                    "start": 1661,
                    "end": 1834
                },
                {
                    "start": 1834,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "216035835"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "249209818"
                },
                {
                    "start": 1006,
                    "end": 1010,
                    "matchedPaperCorpusId": "234805083"
                },
                {
                    "start": 1022,
                    "end": 1026,
                    "matchedPaperCorpusId": "228063796"
                },
                {
                    "start": 1656,
                    "end": 1660,
                    "matchedPaperCorpusId": "248503415"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.473876953125
        },
        {
            "corpus_id": "232478420",
            "title": "Sampling and Filtering of Neural Machine Translation Distillation Data",
            "text": "The general methodology for knowledge distillation in the form of teacher-student has been proposed by Hinton et al. (2015). For the MT task specifically, Tan et al. (2019) focus on vastly reducing the number of parameters, while retaining the performance of a multi-lingual teacher. Wei et al. ( 2019) and Gordon and Duh (2020) use distillation during training to further improve the model performance. \n\nThe work of Kim and Rush (2016) shows that taking either the top sentence with respect to the teacher decoder score or BLEU (Papineni et al., 2002) improves the performance. Germann et al. (2020) presented student models that distil knowledge from a larger teacher model with a negligible loss in performance. They manipulate the queried data based on target sentence quality, such as by re-1 github.com/zouharvi/reference-mt-distill arXiv:2104.00664v1 [cs.CL] 1 Apr 2021 moving sentences that are not correctly recognized by a language identifier. For the parallel part of the data, they extract the best BLEU scoring sentence out of 8 hypotheses. Freitag et al. (2017) experiment with pruning sentences that are below some TER (Snover et al., 2006) threshold (lower is better). They further document the effect of using an ensemble of teachers and also reducing the student model size.",
            "score": 0.5479956654824543,
            "section_title": "Related work",
            "char_start_offset": 1987,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1293
                }
            ],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 553,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 1135,
                    "end": 1156,
                    "matchedPaperCorpusId": "263887736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.268310546875
        },
        {
            "corpus_id": "256460933",
            "title": "Calibrating Student Models for Emotion-related Tasks",
            "text": "Knowledge Distillation: Knowledge distillation (KD) is an efficient method broadly used for transferring knowledge from a teacher network to a student network. In the knowledge distillation setting, a student model is trained to obtain the knowledge of a deeper or more complex teacher model and can therefore estimate the capacity of the powerful teacher model by incorporating the extra knowledge. KD was first introduced as an approach to compress large networks into smaller networks (Ba and Caruana, 2014;Bucilu\u01ce et al., 2006) for computational efficiency. The advances of KD, however, go beyond model compression. Zhang and Sabuncu (2020) empirically explained the reason behind the enhanced performance of self-distillation and proposed a framework that employs instance-specific regularization for teacher predictions. Phuong and Lampert (2019) examined the impact of distillation on student models by analyzing linear and deep linear classifiers. Unlike previous works, we are interested in analyzing the impact of knowledge distillation on the calibration of the models. Thus, we examine the calibration of large-scale pre-trained models through knowledge distillation. We further analyze the impact of dataset shift on calibration for all these settings. We evaluate the predictive uncertainty on both in-domain and out-of-domain test sets from known and unknown distributions on emotion-related datasets. \n\nMixup: Mixup (Zhang et al., 2018) was first proposed to improve the generalization of deep neural networks in computer vision. Since then, many studies have explored mixup in natural language processing tasks (Guo et al., 2019;Guo, 2020;Chen et al., 2020;Yin et al., 2021;Kong et al., 2020;Liang et al., 2021). Liang et al. (2021) proposed a data-agnostic distillation framework that leverages mixup to confer the student model with better generalization ability. Kong et al. (2020) examined BERT calibration using mixup by generating augmented samples based on a cosine distance of extracted features.",
            "score": 0.5477463553363483,
            "section_title": "Related Work",
            "char_start_offset": 7664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 488,
                    "end": 510,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 510,
                    "end": 531,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1432,
                    "end": 1452,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 1646,
                    "end": 1656,
                    "matchedPaperCorpusId": "212928508"
                },
                {
                    "start": 1656,
                    "end": 1674,
                    "matchedPaperCorpusId": "216553182"
                },
                {
                    "start": 1674,
                    "end": 1691,
                    "matchedPaperCorpusId": "236477688"
                },
                {
                    "start": 1709,
                    "end": 1728,
                    "matchedPaperCorpusId": "226226888"
                },
                {
                    "start": 1730,
                    "end": 1749,
                    "matchedPaperCorpusId": "226226888"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52587890625
        },
        {
            "corpus_id": "268060109",
            "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems",
            "text": "Knowledge distillation involves training a smaller, more compact model (the student) using the knowledge gained by a larger, more complex model (the teacher). The primary aim is to transfer the essence of the teacher model's knowledge to the student, ensuring that the student achieves comparable performance with reduced computational overhead.",
            "score": 0.5477125001978624,
            "section_title": "Concept of distillation",
            "char_start_offset": 11680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 345
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45263671875
        },
        {
            "corpus_id": "243861089",
            "title": "A Survey on Green Deep Learning",
            "text": "Park et al., 2021). For example,  designed a sequence of intermediate targets to impose curriculum-based constraint on the optimization path of the student model for improved KD.  and Park et al. (2021) proposed to jointly update the teacher model and the student model with task specific objectives during KD.\n\nDiscussion Knowledge distillation is a widely-used approach to get a smaller but more competitive model. However, although the idea of KD is easy to implement, it also has several limitations. First, the performance of KD is very sensitive to the size gap between the teacher model and the student model. The discrepancy between the expressive power of the models would make it hard to teach the student model. Second, it relies on training data and may not be suitable for few-shot or zeroshot settings. In addition, recent studies Stanton et al., 2021) have revealed that while knowledge distillation can effectively improve student generalization, there was still a large discrepancy between the predictive distributions of student and teacher models. It means that there is still a long way to distill full knowledge in a teacher model to a student model.",
            "score": 0.5474223182721767,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 92227,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5732421875
        },
        {
            "corpus_id": "273811396",
            "title": "Decoupling Dark Knowledge via Block-Wise Logit Distillation for Feature-Level Alignment",
            "text": "Another failure scenario for traditional knowledge distillation arises when there is a significant disparity in capacity between the student and teacher, particularly when \u03b1 st is minimal. Leveraging their insights and analytical framework, let us introduce an intermediate stepping-stone model, donated as f n , between the student and teacher: \n\nand subsequently, this stepping-stone model assimilates knowledge from the teacher: \n\nHere, \u03b1 sn , \u03f5 sn , \u03b1 nt , and \u03f5 nt are defined to elucidate the learning dynamics within this tripartite framework. It is pertinent to note that the incorporation of an intermediary simplifies the learning process, whether it is the student learning from \n\nAdditionally, under the assumptions made by [16], \u03f5 nt +\u03f5 sn \u2264 \u03f5 st , we have, \n\nThese relationships corroborate that the upper bound of error with a stepping-stone model is smaller than in vanilla knowledge distillation. In scenarios characterized by significant performance disparities between the student and the teacher, the integration of a stepping-stone model intermediary effectively partitions the modest \u03b1 st into two more tractable components, \u03b1 sn and \u03b1 nt . This strategic division substantially enhances the efficiency of the knowledge distillation process, optimizing the learning trajectory for the student. \n\nMoreover, the principles underlying Block-KD's branch structure are reminiscent of self-distillation, and its benefits can be understood through a multi-view hypothesis [2]. Introducing L cross , which averages votes during the training process, often leads to more effective teaching labels. This approach aligns with constructing an output ensemble [51]. \n\nDuring training, it can yield performance that surpasses even the initial teacher model, thereby elevating the overall framework's upper bound. These observations generally align with empirical experiments [51].",
            "score": 0.5469581359724011,
            "section_title": "IV. THEORETICAL ANALYSIS",
            "char_start_offset": 18905,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 345
                },
                {
                    "start": 348,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 1669,
                    "end": 1673,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 1883,
                    "end": 1887,
                    "matchedPaperCorpusId": "215745611"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.246337890625
        },
        {
            "corpus_id": "271039429",
            "title": "AMD: Automatic Multi-step Distillation of Large-scale Vision Models",
            "text": "In this work, we adopt the structural pruning method [54,84] due to its known advantages in knowledge distillation [86].It gradually prunes the network with the least important parameters according to their important scores, which are obtained by introducing learnable mask variables during teacher's inference stage.The resulting teacher-assistant candidates are represented as {M i | i = 1 to m}.More details are provided in the supplementary material.Joint Optimization.The intuitive approach to finding the best teacher-assistant involves maximizing the performance of each individual candidate and subsequently employing NPSD to measure the optimality of each candidate.However, the computational overhead remains significant when assessing the performance of all candidates, given the large number of candidates.To further mitigate computational complexity, we explore parameter sharing among teacher-assistant candidates across different scales, leveraging the incremental property inherent in candidates obtained from structural pruning.Essentially, the incremental property posits that for two candidates M i and M j , if S i < S j , then the parameters of M i form a subset of M j .This ensures that a larger candidate can be transformed into a smaller one by continuously pruning less significant parameters, facilitating parameter sharing across the candidates.\n\nWith parameter sharing, the total number of parameters to be optimized becomes equivalent to that of the largest candidate.Subsequently, we devise a joint optimization framework that simultaneously optimizes distillations from the teacher to all candidates in a single run.In doing so, we effectively reduce the memory footprints of all candidates to that of a distinguished one.The computational costs are also significantly diminished through the parameter-sharing optimization.More importantly, we propose a natural resolution to the issue highlighted in [6], wherein the researchers discovered that a reduced capacity disparity between the student and teacher models can facilitate more effective knowledge transfer.Unlike existing approaches that necessitate the initialization and teaching of a separate student model from scratch, both in homogeneous distillation (e.g., similar architectures with varying numbers of layers) and heterogeneous distillation (e.g., distinct network designs), our method is orthogonal and introduces Joint Optimization, enabling the direct inheritance of parameters by student models from their teacher counterparts within the same, singular architectural design.",
            "score": 0.5465514513358103,
            "section_title": "Automatic Multi-step Distillation",
            "char_start_offset": 17720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 120,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 398
                },
                {
                    "start": 398,
                    "end": 454
                },
                {
                    "start": 454,
                    "end": 473
                },
                {
                    "start": 473,
                    "end": 675
                },
                {
                    "start": 675,
                    "end": 818
                },
                {
                    "start": 818,
                    "end": 1045
                },
                {
                    "start": 1045,
                    "end": 1192
                },
                {
                    "start": 1192,
                    "end": 1373
                },
                {
                    "start": 1375,
                    "end": 1498
                },
                {
                    "start": 1498,
                    "end": 1648
                },
                {
                    "start": 1648,
                    "end": 1754
                },
                {
                    "start": 1754,
                    "end": 1855
                },
                {
                    "start": 1855,
                    "end": 2095
                },
                {
                    "start": 2095,
                    "end": 2575
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 57,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 57,
                    "end": 60,
                    "matchedPaperCorpusId": "258740796"
                },
                {
                    "start": 115,
                    "end": 119,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 1933,
                    "end": 1936,
                    "matchedPaperCorpusId": "260815602"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402099609375
        },
        {
            "corpus_id": "267374660",
            "title": "Unleashing the Potential of Knowledge Distillation for IoT Traffic Classification",
            "text": "We repeated the same experimental setup as before, using the same student model, training dataset, and hyperparameters across all experiments. Moreover, upon conducting a more in-depth analysis of the CIC-IDS2017 dataset, we discovered that the results were strikingly similar to those of the previous dataset, highlighting a consistent pattern in our findings(see Fig. 12). Looking at the results in Fig. 11 and Fig. 12, we can see that increasing the number of layers in the teacher model did not necessarily result in a significant improvement in performance. While the seven-layer teacher model had slightly better performance than the three-layer teacher model in terms of accuracy, F1 score, and recall, the difference was not significant. One interesting finding is that the training time and model   size increased significantly as the number of layers in the teacher model increased. The seven-layer teacher model had the highest training time and model size, which indicates that increasing the model capacity beyond a certain point may not be worth the additional computational cost. Additionally, the results suggest that the knowledge distillation approach was effective in transferring knowledge from the teacher model to the student model, as the student model achieved a high level of performance despite having a much smaller capacity than the teacher models.",
            "score": 0.5456362141861578,
            "section_title": "C. ANALYZING THE TEACHER CAPACITY",
            "char_start_offset": 43766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1376
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.583984375
        },
        {
            "corpus_id": "274777116",
            "title": "Neural Collapse Inspired Knowledge Distillation",
            "text": "Distillation from Bigger Models. In principle, effective knowledge distillation should lead to GREAT TEACHERS PRODUCING OUTSTANDING STUDENTS, meaning that a superior teacher should guide the student to better distillation. However, in practice, such ideal case is not always achieved. We do evaluation using ResNet and Swin models of varying scales, as shown in Table 4. One can observe that existing methods do not consistently guarantee steady improvements in student performance as the teacher model's size increases. In contrast, our approach effectively addresses this issue, likely because better models establish a refined NC structure, which facilitates the student's consistent enhancement. \n\nDoes N C impact KD? Yes! We evaluate the contribution of each N C property to the distillation process through ablation study, as shown in Table 5. The results show that removing any NC property would reduce the student prediction accuracy, with N C 2 having the most significant impact. This underscores the critical role of each module in our framework, especially the importance of preserving the teacher's ETF structure for effective knowledge transfer. Additionally, when combined with standard KD, our method further improves the distillation performance. \n\nDoes N C 3 -classifier trade performance for efficiency? No! We conduct ablation study on the N C 3 classifier, with results presented in Figure 5. Notably, the N C 3 classifier either outperforms or matches the standard classifier's results. Additionally, as shown in the right table, the training time is significantly reduced, suggesting that this design effectively balances performance and efficiency. \n\nCase Study While we are the first to explicitly integrate NC into the KD framework, we recognize that some existing methods have implicitly leveraged N C to enhance distillation, albeit without explicit acknowledgment. Here, we investigate the role of N C in the effective distillation results of two representative methods, CRD and SimKD. \n\nCase 1: CRD uses contrastive learning at the instance level to align teacher and student features, implicitly encouraging feature convergence toward class centroids (Khosla et al. 2020). This is reflected in the significant reduction of N C 1 in CRD compared to KD (see Table 6), indicating its implicit use of N C 1 .",
            "score": 0.545325242957215,
            "section_title": "Ablation Study",
            "char_start_offset": 20668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1672
                },
                {
                    "start": 1675,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2014
                },
                {
                    "start": 2017,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2335
                }
            ],
            "ref_mentions": [
                {
                    "start": 2182,
                    "end": 2202,
                    "matchedPaperCorpusId": "216080787"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58642578125
        },
        {
            "corpus_id": "245650327",
            "title": "Class-Incremental Continual Learning Into the eXtended DER-Verse",
            "text": "Knowledge Distillation (KD) [36] is a training methodology that allows transferring the knowledge of a teacher model into a separate student model. While Hinton et al. originally proposed to distillate large teachers -possibly ensembles -into smaller students, further studies revealed additional interesting properties about this technique. In particular, Furlanello et al. [37] show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student. More recently, other works [38], [39] explore an interesting variation of self-distillation that distills knowledge from the deeper layers of the network to its shallower ones to accelerate convergence and attain higher accuracy. \n\nKnowledge Distillation and Continual Learning. Distillation can be used to hinder catastrophic forgetting by appointing a previous snapshot of the model as teacher and distilling from it while new tasks are learned. Learning Without Forgetting [7] uses teacher responses to new exemplars to constrain the evolution of the student. Several other works combine distillation with rehearsal: iCaRL [11] distills the responses of the model at the previous task boundary, learning latent representations to be used in a nearest meanof-exemplars classifier; EtEIL [40], LUCIR [31] and BiC [41] focus on contrasting the prediction bias that comes from incremental classification; IL2M [30] stores additional statistics to facilitate distillation and compensate bias.",
            "score": 0.5446771396853975,
            "section_title": "Self-Distillation",
            "char_start_offset": 8104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1538
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 375,
                    "end": 379,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1024,
                    "end": 1027,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 1174,
                    "end": 1178,
                    "matchedPaperCorpusId": "206596260"
                },
                {
                    "start": 1349,
                    "end": 1353,
                    "matchedPaperCorpusId": "195453293"
                },
                {
                    "start": 1362,
                    "end": 1366,
                    "matchedPaperCorpusId": "173187918"
                },
                {
                    "start": 1457,
                    "end": 1461,
                    "matchedPaperCorpusId": "204923710"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69140625
        },
        {
            "corpus_id": "266335531",
            "title": "Let All be Whitened: Multi-teacher Distillation for Efficient Visual Retrieval",
            "text": "Table 3 summarizes the performance and complexity of original teacher models and distillation models using varying teacher combinations. The first group reports the performance of whitened teacher models. As shown in the table, single-teacher distillation could significantly reduce the model complexity during the inference, but their performance is only comparable to the corresponding teacher models. By contrast, multi-teacher distillation methods including double-teacher and triple-teacher distillation not only reduce the model complexity but also achieve further performance gain. We attribute the performance gain to the complementarity of multiple teacher models and the effectiveness of our proposed multi-teacher distillation framework. Besides, the triple-teacher distillation almost surpassing the double-teacher distillation demonstrates the incremental advantage of fusing more teacher models. It is possible to improve even further by introducing more teacher models. But we can also imagine a performance saturation with endless addition of teachers, due to the student model's limited capacity, less complementarity between teachers, etc. Using more teacher models also means larger training cost. It is a trade-off in practice and we recommend determining the number of teachers based on the expected service time to make the additional training cost more amortized by the improved inference speed. \n\nComparison to baseline approaches In order to further verify the effectiveness of multi-teacher distillation, we compare it to three related baseline approaches, including Ensemble Mean (EM), Embedding Distillation (ED), and Contrastive Learning (CL). EM is a classical ensemble method, which averages the similarity scores produced by teacher models during inference. ED is a feature-based distillation method, which jointly minimizes the Euclidean distances between output embeddings of teacher models and the student model. CL is a representation learning method widely used in recent works (Chen, Xie, and He 2021), which is equivalent to replacing Q in Eq. 2 with an identity matrix. \n\nAs in Table 4, our proposed triple-teacher distillation consistently outperforms the counterparts with a clear margin. Among them, EM fusing knowledge from multiple models performs the best, but it brings heavy computation overhead as every ensembled model must be forwarded to calculate its similarity output.",
            "score": 0.5428355815837955,
            "section_title": "Single-teacher distillation vs. Multi-teacher distillation",
            "char_start_offset": 21379,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1418
                },
                {
                    "start": 1421,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2109
                },
                {
                    "start": 2112,
                    "end": 2230
                },
                {
                    "start": 2231,
                    "end": 2422
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35888671875
        },
        {
            "corpus_id": "265033252",
            "title": "Comparative Knowledge Distillation",
            "text": "In the era of large-scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally-heavy teacher models to lightweight, efficient student models while preserving performance. Yet KD settings often assume readily available access to teacher models capable of performing many in-ferences-a notion increasingly at odds with the realities of costly large-scale models. Addressing this gap, we study an important question: how KD algorithms fare as the number of teacher inferences decreases, a setting we term Reduced-Teacher-Inference Knowledge Distillation (RTI-KD). We observe that the performance of prevalent KD techniques and state-of-the-art data augmentation strategies suffers considerably as the number of teacher inferences is reduced. One class of approaches, termed \u201crelational\u201d knowledge distillation underperforms the rest, yet we hypothesize that they hold promise for reduced dependency on teacher models because they can augment the effective dataset size without additional teacher calls. We find that a simple change - performing high-dimensional comparisons instead of low-dimensional relations, which we term Comparative Knowledge Distillation - vaults performance well over existing KD approaches. We perform empirical evaluation across varied experimental settings and rigorous analysis to understand the learning outcomes of our method. All code is made publicly available.",
            "score": 0.5427449209993925,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62646484375
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "These predicted probabilities are defined by the feature representations of the training model. They reflect the similarities of data in feature embedding space. Yun et al. proposed class-wise self-knowledge distillation to match the output distributions of the training model between intra-class samples and augmented samples within the same source with the same model (Yun et al. 2020). In addition, the self-distillation proposed by Lee et al. (2019a) is adopted for data augmentation and the self-knowledge of augmentation is distilled into the model itself. Self distillation is also adopted to optimize deep models (the teacher or student networks) with the same architecture one by one (Furlanello et al. 2018;Bagherinezhad et al. 2018). Each network distills the knowledge of the previous network using a teacher-student optimization. \n\nBesides, offline, online and self distillation can also be intuitively understood from the perspective of human beings teacher-student learning. Offline distillation means the knowledgeable teacher teaches a student knowledge; online distillation means both teacher and student study together with each other; self-distillation means student learn knowledge by oneself. Moreover, just like the human beings learning, these three kinds of distillation can be combined to complement each other due to their own advantages. For example, both self-distillation and online distillation are properly integrated via the multiple knowledge transfer framework (Sun et al. 2021). In knowledge distillation, the teacher-student architecture is a generic carrier to form the knowledge transfer. In other words, the quality of knowledge acquisition and distillation from teacher to student is also determined by how to design the teacher and student networks. In terms of the habits of human beings learning, we hope that a student can find a right teacher. Thus, to well finish capturing and distilling knowledge in knowledge distillation, how to select or design proper structures of teacher and student is very important but difficult problem. Recently, the model setups of teacher and student are almost pre-fixed with unvaried sizes and structures during distillation, so as to easily cause the model capacity gap.",
            "score": 0.5418640896016407,
            "section_title": "Self-Distillation",
            "char_start_offset": 27181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 386,
                    "matchedPaperCorpusId": "214727822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4033203125
        },
        {
            "corpus_id": "271843206",
            "title": "UNIC: Universal Classification Models via Multi-teacher Distillation",
            "text": "Knowledge distillation (KD) was initially introduced as a model compression technique [7], where the goal is to train a smaller student model from the output of a teacher model [23]. While early work focused on predicting the final outputs of a classification model, the idea was rapidly extended to other forms of distillation, such as distilling intermediate representations [1,21,22,49,73,75,79]. These methods perform well but require careful layer selection and loss balancing [21]. In our work, instead of matching layer-wise representations between the student and teacher architectures, we add shortcut connections from intermediate layers of the student to the loss of each teacher. \n\nMulti-teacher knowledge distillation. KD can naturally be extended to an ensemble of teachers so that student can benefit from their potential complementarity. While the final outputs of teachers trained for the same task can simply be averaged [3,15,23,75], multi-teacher distillation with teachers trained for different tasks is more challenging. UDON [76] first trains domain-specialist teachers which are subsequently distilled in a student model using adaptive data sampling for balancing the different domains. In [60], contrastive learning is used for ensemble distillation while [56] proposes a framework tailored for teachers trained with masked image modeling and contrastive learning. But such approaches are not straightforward to extend to teachers learned differently. Similarly, [71] combines self-supervised teachers from arbitrary heterogeneous pretext tasks. [13,16,51] focus on jointly utilizing pseudo-and true labels for multi-teacher distillation. Roth et al. [51] formulate multi-teacher distillation as continual learning and further propose a novel method for data partitioning based on confidence. Here we develop a more generic method for combining teachers, that is not limited to certain types of teachers or losses, and, unlike [30,51], does not require labeled data, nor classifiers associated with each teacher for obtaining pseudo-labels. \n\nLoss balancing is shown to be crucial in multi-task learning [11,24,26,78]. Similar strategies to automatically balance losses have also been proposed for multi-teacher distillation [15,32].",
            "score": 0.541547098899984,
            "section_title": "Related Work",
            "char_start_offset": 4470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 2065
                },
                {
                    "start": 2068,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2258
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 181,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 377,
                    "end": 380,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 380,
                    "end": 383,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 383,
                    "end": 386,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 389,
                    "end": 392,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 392,
                    "end": 395,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 395,
                    "end": 398,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 482,
                    "end": 486,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 939,
                    "end": 942,
                    "matchedPaperCorpusId": "202660953"
                },
                {
                    "start": 945,
                    "end": 948,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 948,
                    "end": 951,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1214,
                    "end": 1218,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1281,
                    "end": 1285,
                    "matchedPaperCorpusId": "259274820"
                },
                {
                    "start": 1571,
                    "end": 1575,
                    "matchedPaperCorpusId": "85464175"
                },
                {
                    "start": 1575,
                    "end": 1578,
                    "matchedPaperCorpusId": "237291521"
                },
                {
                    "start": 1578,
                    "end": 1581,
                    "matchedPaperCorpusId": "264490946"
                },
                {
                    "start": 1676,
                    "end": 1680,
                    "matchedPaperCorpusId": "264490946"
                },
                {
                    "start": 1956,
                    "end": 1959,
                    "matchedPaperCorpusId": "264490946"
                },
                {
                    "start": 2129,
                    "end": 2133,
                    "matchedPaperCorpusId": "4703661"
                },
                {
                    "start": 2133,
                    "end": 2136,
                    "matchedPaperCorpusId": "44079150"
                },
                {
                    "start": 2136,
                    "end": 2139,
                    "matchedPaperCorpusId": "41794936"
                },
                {
                    "start": 2139,
                    "end": 2142,
                    "matchedPaperCorpusId": "267024006"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5810546875
        },
        {
            "corpus_id": "264172668",
            "title": "Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective",
            "text": "The parametric knowledge transfer paradigm in this paper is fundamentally distinct from traditional distillation methods, characterized by the following differences: \n\n\u2022 Purpose and Focus: Rather than proposing better distillation methods, our study is focused on exploring the transferability of implicit knowledge embedded in static parameters. \n\nWhile prior research has explored the detectability and editability of parametric knowledge, its transferability remains less explored. Our experiments provide empirical evidence in the knowledge transfer scenario, where student models show improved performance after receiving task-specific knowledge from the teacher model, as shown in Tables 1 and 2. \u2022 Process and Efficiency: Our approach differs from standard knowledge distillation, which typically requires fine-tuning or the direct involvement of the teacher model in student model training -a computationally intensive process. In contrast, our parametric knowledge transfer involves extracting task-specific parameters from the vanilla teacher model and integrating them into the student model. This method, requiring only 32 inferences from the teacher model, offers a significant efficiency advantage, especially in the context of LLMs. \n\nDespite these differences, we provide the results of the distillation methods as a reference in the knowledge transfer scenario. Table 1 contains the results for LLaMA-1 13B to 7B and 30B to 7B, where KD refers to vanilla knowledge distillation (Hinton et al., 2015) and SeqKD is sequence-level knowledge distillation (Kim & Rush, 2016). All models are fine-tuned with LoRA, using identical training data and hyperparameters. In this paper, we attempt to explore the evidence that knowledge in static parameters can be transferred between different LLMs, and knowledge transfer is the scenario in which we find and provide empirical evidence. Our focus is not on proposing to find better methods in this scenario. \n\nA.5 TRADE-OFF DISCUSSION BETWEEN PERFORMANCE AND RUNNING COST Considering that users may have varying computational resources in practical application scenarios, we discuss the trade-offs between performance and running costs as follows: \n\nExperimental details: \n\n\u2022 We conduct comparisons for two knowledge transfers: LLaMA-1 13B to 7B and 30B to 7B.",
            "score": 0.5415166298163354,
            "section_title": "A.4 COMPARISON WITH DISTILLATION METHODS",
            "char_start_offset": 30819,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1247
                },
                {
                    "start": 1250,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1963
                },
                {
                    "start": 1966,
                    "end": 2203
                },
                {
                    "start": 2206,
                    "end": 2227
                },
                {
                    "start": 2230,
                    "end": 2316
                }
            ],
            "ref_mentions": [
                {
                    "start": 1568,
                    "end": 1586,
                    "matchedPaperCorpusId": "8451212"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69921875
        },
        {
            "corpus_id": "252565150",
            "title": "Selective Cross-Task Distillation",
            "text": "Knowledge distillation society often assumes the existence of a suitable teacher model. It may be a deeper neural network [2], [23], [24], [54] or a previous generation of the student model along the training process [39], [46], [56]. Despite the rapid development of knowledge distillation algorithms [40], [41], [44], [45], how to obtain a good teacher remains an open problem, which raised attention recently [33], [57]. [49] claimed that an ideal teacher should approximate the true label distribution. However, this insight hardly guides practice because the gap between the current teacher and the Bayesian optimal solution is unknown. \n\nIn real-world applications, sometimes we construct the teacher model by ourselves, e.g., model compression, but sometimes we just fetch a teacher from existing models. In this case, the student's performance is determined by which teacher we select. This paper tackles this selection problem through efficient model assessment. We aim to pick out the most contributive teacher from the repository without trying each one to train the student repeatedly. An alternative to teacher selection is directly distilling knowledge from the ensemble of all the teachers [45], [46], [58], which costs lots Standard KD with Logits (e.g., [2]) Standard KD with Representations (e.g., [23]) Cross-Task KD (e.g., [24]) KD with Specially Designed Teachers (e.g., [33]) \n\nSelective Cross-Task Distillation (ours) \n\nof computing resources and is not practical for large model repository. We will give a more detailed discussion in Sec. 5. \n\n(3) What kind of knowledge is reused during distillation? In standard knowledge distillation [2] and other subsequent works [41], [59], we reuse the teacher's soft labels to help the student via distribution matching. This method does not apply to cross-task knowledge distillation since the teacher's label set differs from the student's.",
            "score": 0.5415100490865389,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1397
                },
                {
                    "start": 1400,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 125,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 127,
                    "end": 131,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 133,
                    "end": 137,
                    "matchedPaperCorpusId": "219965004"
                },
                {
                    "start": 139,
                    "end": 143,
                    "matchedPaperCorpusId": "247521335"
                },
                {
                    "start": 217,
                    "end": 221,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 223,
                    "end": 227,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 302,
                    "end": 306,
                    "matchedPaperCorpusId": "219636179"
                },
                {
                    "start": 308,
                    "end": 312,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 314,
                    "end": 318,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "233378598"
                },
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "233307448"
                },
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "235826057"
                },
                {
                    "start": 1211,
                    "end": 1215,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "219636007"
                },
                {
                    "start": 1271,
                    "end": 1274,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1316,
                    "end": 1320,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1343,
                    "end": 1347,
                    "matchedPaperCorpusId": "219965004"
                },
                {
                    "start": 1392,
                    "end": 1396,
                    "matchedPaperCorpusId": "233378598"
                },
                {
                    "start": 1661,
                    "end": 1664,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1692,
                    "end": 1696,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1698,
                    "end": 1702,
                    "matchedPaperCorpusId": "16391544"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.296142578125
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "Inspired by the success of neural architecture search (or NAS), the performances of small neural networks have been further improved by searching for a global structure based on efficient meta operations or blocks (Wu et al. 2019;Tan et al. 2019;Tan and Le 2019;Radosavovic et al. 2020). Furthermore, the idea of dynamically searching for a knowledge transfer regime also appears in knowledge distillation, e.g., automatically removing redundant layers in a data-driven way using reinforcement learning (Ashok et al. 2018), and searching for optimal student networks given the teacher networks (Liu et al. 2019i;Xie et al. 2020;Gu and Tresp 2020). \n\nMost previous works focus on designing either the structures of teacher and student models or the knowledge transfer scheme between them. To make a small student model well match a large teacher model for improving knowledge distillation performance, the adaptive teacher-student learning architecture is necessary. Recently, the idea of a neural architecture search in knowledge distillation, i.e., a joint search of student structure and knowledge transfer under the guidance of the teacher model, will be an interesting subject of future study.",
            "score": 0.5412121301060904,
            "section_title": "Self-Distillation",
            "char_start_offset": 32595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1197
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 230,
                    "matchedPaperCorpusId": "54461508"
                },
                {
                    "start": 262,
                    "end": 285,
                    "matchedPaperCorpusId": "214714446"
                },
                {
                    "start": 503,
                    "end": 522,
                    "matchedPaperCorpusId": "13352766"
                },
                {
                    "start": 594,
                    "end": 612,
                    "matchedPaperCorpusId": "208175624"
                },
                {
                    "start": 612,
                    "end": 628,
                    "matchedPaperCorpusId": "207853355"
                },
                {
                    "start": 628,
                    "end": 646,
                    "matchedPaperCorpusId": "211004051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5283203125
        },
        {
            "corpus_id": "211068747",
            "title": "Understanding and Improving Knowledge Distillation",
            "text": "Knowledge distillation is a model-agnostic technique to improve model quality while having a fixed capacity budget. It is a commonly used technique for model compression, where a higher capacity teacher model with better quality is used to train a more compact student model with better inference efficiency. Through distillation, one hopes to benefit from student's compactness, without sacrificing too much on model quality. Despite the large success of knowledge distillation, better understanding of how it benefits student model's training dynamics remains under-explored. In this paper, we dissect the effects of knowledge distillation into three main factors: (1) benefits inherited from label smoothing, (2) example re-weighting based on teacher's confidence on ground-truth, and (3) prior knowledge of optimal output (logit) layer geometry. Using extensive systematic analyses and empirical studies on synthetic and real-world datasets, we confirm that the aforementioned three factors play a major role in knowledge distillation. Furthermore, based on our findings, we propose a simple, yet effective technique to improve knowledge distillation empirically.",
            "score": 0.5402582780480588,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80078125
        },
        {
            "corpus_id": "274131310",
            "title": "Exploring Feature-based Knowledge Distillation for Recommender System: A Frequency Perspective",
            "text": "Let   and   denote the dimensions of student and teacher features, respectively. In this work, we use \n\nall the features of the student model and the teacher model, respectively. Note that, when performing knowledge for users, we have \n\nIn feature-based knowledge distillation methods [10,13], a projector is used to align the dimensionalities of the student model and the teacher model. Then, the student model is trained to minimize the MSE loss to make the student model mimic the teacher's features. Formally, the feature-based knowledge distillation loss is given by: L   = \u2225  (S) \u2212 T\u2225 2  , where   (\u2022) denotes the projector that aligns the dimensionalities of S and T, and is usually a linear transformation.",
            "score": 0.5400349429956508,
            "section_title": "Feature-based Knowledge Distillation",
            "char_start_offset": 12956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 714
                }
            ],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "224282941"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "245324791"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29052734375
        },
        {
            "corpus_id": "253734490",
            "title": "Scalable Collaborative Learning via Representation Sharing",
            "text": "The concept of knowledge distillation (KD) originated in Bucila et al. [8] as a way of compressing models, and was later generalized by Hinton et al. [19] (see Gou et al. [14] for an overview of the field). The standard use case for KD is that of a Teacher-Student (or offline) configuration, in which the teacher model (usually a large and well-trained model) transfers its knowledge to the student model (usually a smaller model) by sharing its last layer activations on a given transfer dataset (see Fig. 1a). The knowledge is then distilled into the student model using a divergence loss between the teacher and student models outputs (response-based KD) or intermediate layers (feature-based KD) on the transfer dataset. Traditional KD schemes use a transfer set that is similar (or identical) to the teacher training dataset, but some recent work has focused on data-free (or zero-shot) KD. This can be achieved either by looking at some of the teacher model statistics to generate synthetic transfer data [35,39,6], or by training a GAN in parallel [37,10,2]. It has also been shown that positive results can be obtained using mismatched or random unlabeled data for the distillation [27,40]. A key concept for our framework is the idea of online KD (or co-distillation [3], see Fig. 1b), where each model is treated as both a teacher and a student, meaning that the KD is performed synchronously with the training of each model (rather than after the training of the teacher model) [15,57,45]. Finally, ensemble KD (see Fig. 1c) refers to the setup where the knowledge is distilled from an ensemble of teacher (offline) or teacher-student (online) models. \n\nCollaborative Learning via Knowledge Distillation A growing body of literature has recently investigated ways of using online knowledge distillation (KD) [8,19] for collaborative learning in order to alleviate the need of sharing model updates (FL) or individual smashed data (SL).",
            "score": 0.539596373024265,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6216,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1947
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 74,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 171,
                    "end": 175,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1016,
                    "end": 1019,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "162183830"
                },
                {
                    "start": 1060,
                    "end": 1063,
                    "matchedPaperCorpusId": "91183944"
                },
                {
                    "start": 1063,
                    "end": 1065,
                    "matchedPaperCorpusId": "209500810"
                },
                {
                    "start": 1195,
                    "end": 1198,
                    "matchedPaperCorpusId": "227013462"
                },
                {
                    "start": 1490,
                    "end": 1494,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1494,
                    "end": 1497,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6015625
        },
        {
            "corpus_id": "270737867",
            "title": "InFiConD: Interactive No-code Fine-tuning with Concept-based Knowledge Distillation",
            "text": "Our goal is to develop a framework that efficiently supports interpretable KD from parent models and subsequent (i.e., post-distillation) fine-tuning of student models, in a way optimizes human effort and accessibility (i.e., AI/ML non-experts can perform it).To help constrain our efforts, we reviewed prior interactive, no-code, and visualizationdriven KD design studies (see Section 2 papers), identifying four key design challenges for such a framework.\n\nExisting methods (e.g., [39,61,64,82]) often prioritize student model performance at the cost of introducing complex architectures.This increased complexity hinders the adoption of KD [23] and negatively impacts subsequent fine-tuning processes, such as identifying the causes of underperformance, devising improvement strategies, and executing them effectively [76,84].Conversely, \"simpler\" student models can increase interpretability but also widen the capacity gap between the student and teacher models, making it challenging to transfer the desired knowledge effectively [56].Therefore, striking an appropriate balance between high student effectiveness and suitable interpretability is crucial for successful knowledge distillation and fine-tuning processes.\n\n(C2) Ensuring Generalizability in Knowledge Distillation.While using more information from the teacher model can lead to better student model performance, it can also make the KD method heavily dependent on the specific teacher model architecture [34,70], hindering its generalizability.Model-agnostic methods have been suggested for making KD more widely applicable [33,68,89], but they come with their own challenges.For example, a model-agnostic strategy means that only the input and output of the teacher model (and any information derived from them) can be used to train the student model.This constraint can significantly limit the effectiveness of KD [23], especially when there is an additional requirement for interpretability.Developing a system that is generalizable across a variety of pretrained models (PTMs) while still maintaining high effectiveness is a non-trivial challenge, requiring careful consideration of the trade-offs between generalizability and performance in the context of interpretable knowledge distillation.\n\n(C3) Supporting Efficient Analysis of the Student Model.",
            "score": 0.5395123450736194,
            "section_title": "FORMALIZING DESIGN CONSTRAINTS FOR INFICOND 3.1 Design Challenges",
            "char_start_offset": 14857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 260,
                    "end": 457
                },
                {
                    "start": 459,
                    "end": 590
                },
                {
                    "start": 590,
                    "end": 829
                },
                {
                    "start": 829,
                    "end": 1041
                },
                {
                    "start": 1041,
                    "end": 1224
                },
                {
                    "start": 1226,
                    "end": 1283
                },
                {
                    "start": 1283,
                    "end": 1513
                },
                {
                    "start": 1513,
                    "end": 1645
                },
                {
                    "start": 1645,
                    "end": 1821
                },
                {
                    "start": 1821,
                    "end": 1963
                },
                {
                    "start": 1963,
                    "end": 2267
                },
                {
                    "start": 2269,
                    "end": 2325
                }
            ],
            "ref_mentions": [
                {
                    "start": 483,
                    "end": 487,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 487,
                    "end": 490,
                    "matchedPaperCorpusId": "218487294"
                },
                {
                    "start": 643,
                    "end": 647,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 825,
                    "end": 828,
                    "matchedPaperCorpusId": "251643827"
                },
                {
                    "start": 1473,
                    "end": 1477,
                    "matchedPaperCorpusId": "258888057"
                },
                {
                    "start": 1477,
                    "end": 1480,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 1593,
                    "end": 1597,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 1600,
                    "end": 1603,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1885,
                    "end": 1889,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.734375
        },
        {
            "corpus_id": "267782743",
            "title": "TIE-KD: Teacher-Independent and Explainable Knowledge Distillation for Monocular Depth Estimation",
            "text": "A crucial goal of knowledge distillation is the accurate transfer of knowledge from the teacher to the student model. To evaluate how well our TIE-KD framework preserves the teacher's knowledge, we compared the similarity between the outputs of the teacher and the student models using three metrics: AbsRel, RMSE, and \u03b4 1 . For this evaluation, we excluded the top 110 pixels, consistent with the rationale provided in Sec. 4.2. The results, presented in Table 2, show that students trained with TIE-KD more closely mirror their respective teacher models than those trained by other teachers not involved in their training process. While Res-KD demonstrates some degree of correlation with the teachers' outputs, the TIE-KD students generally exhibit a higher level of similarity. These results underscore the efficacy of the TIE-KD framework in faithfully transferring the teacher's knowledge to the student model. \n\nNotably, TIE-KD DepthF ormer and Res-KD DepthF ormer students showed less alignment with their respective teacher model. This divergence is likely attributed to the substantial differences in parameter counts, consistent with the findings of Mirzadeh et al. [42]. However, when applying the TIE-KD framework to a student model equipped with a larger ResNet50 backbone, which encompasses approximately 78M parameters, there is a notable increase in the similarity of the student model's output to that of the DepthFormer. This observation, as presented in Table 3, suggests that minimizing the disparity in parameters between teacher and student models can potentially improve the efficiency of knowledge transfer in the TIE-KD framework.",
            "score": 0.539315492946598,
            "section_title": "Similarity to the Teacher Model",
            "char_start_offset": 20948,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1656
                }
            ],
            "ref_mentions": [
                {
                    "start": 1177,
                    "end": 1181,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "262084420",
            "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning",
            "text": "This technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) [84]. This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies. \n\nThe extraction of knowledge from teachers and its utilization for training student networks can be classified into three categories: response-based, feature-based, and relationbased. Specifically, response-based knowledge distillation involves mimicking the final predictions of the teacher model by capturing the neural response in the last output layer [87]. Feature-based knowledge expands upon this approach by using both the outputs of the last layer and intermediate layers to train thinner networks [81]. Finally, relation-based knowledge takes a step further by exploring the relationships between different layers or data samples in addition to the outputs of specific layers in the teacher model [86]. \n\nThe distillation schemes are also crucial for the student learning process. Depending on the training strategy, the following three different categories are presented: offline distillation, online distillation, self-distillation. Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits [80]. On the other hand, online distillation is an end-to-end approach where both the teacher and student models are updated simultaneously, making it suitable when the teacher model is not significantly larger or higher performing [85]. Finally, self-distillation is a special case of online distillation where the teacher and student networks have the same architecture [79]. \n\nIn general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy [88]. For this reason, it is widely adopted in several fields where existing models are well-performing but unable to be deployed ''as they are'' in resource-constrained hardware.",
            "score": 0.5393138291865188,
            "section_title": "3) Knowledge Distillation",
            "char_start_offset": 28218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2057
                },
                {
                    "start": 2060,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 115,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 858,
                    "end": 862,
                    "matchedPaperCorpusId": "232104927"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "235613518"
                },
                {
                    "start": 1209,
                    "end": 1213,
                    "matchedPaperCorpusId": "239486869"
                },
                {
                    "start": 1680,
                    "end": 1684,
                    "matchedPaperCorpusId": "198179767"
                },
                {
                    "start": 1912,
                    "end": 1916,
                    "matchedPaperCorpusId": "224914013"
                },
                {
                    "start": 2052,
                    "end": 2056,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 2183,
                    "end": 2187,
                    "matchedPaperCorpusId": "255266316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69580078125
        },
        {
            "corpus_id": "236034208",
            "title": "Representation Consolidation for Training Expert Students",
            "text": "Figure 1: Unlike knowledge distillation, which seeks to copy the end-to-end functionality a teacher, we learn a consolidated representation from multiple teachers that transfers to downstream tasks. \n\nGiven N teacher models and a large unlabeled proxy dataset, we train a single student model using multi-task distillation with a separate classifier head for each of the teacher tasks. To limit student forgetting and representation collapse, we always include an additional generalist teacher (ImageNet). We show that the resulting consolidated representation transfers better to downstream tasks than any of the individual teachers (including the generalist). \n\nTo carry out representation consolidation, we utilize multi-teacher multi-task model distillation (see Fig. 1). Here, a single student is trained to emulate multiple teachers, each of which operates on a different set of class labels. Previous work on multi-teacher knowledge distillation has focused on evaluating how well the student model performs the teacher's task. Instead, we evaluate how well the student representation generalizes to new downstream tasks (whether related or unrelated to the teachers' tasks). In this setting we demonstrate several surprising results: \n\n\u2022 While task-specific model representations transfer poorly, consolidating a task-specific teacher with a generalist teacher (ImageNet) is sufficient to rescue the student. The resulting representation transfers well, with improved downstream performance on teacher-relevant tasks while matching the performance of a strong generalist representation on unrelated tasks. \u2022 Consolidating multiple related task-specific teacher models can yield a student representation that exceeds the performance of any one teacher on downstream tasks. \u2022 Unlike knowledge distillation, which requires access to the teacher training data (or using data-free distillation [4,5] to carefully craft synthetic data) to achieve good performance, we avoid using these data and show effective representation consolidation can be carried out using a sufficiently diverse generic proxy dataset and is robust to the choice of the proxy.",
            "score": 0.5385188974044585,
            "section_title": "Introduction",
            "char_start_offset": 2104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 2152
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "237563200",
            "title": "Distilling Linguistic Context for Language Model Compression",
            "text": "State-of-the-art knowledge distillation objectives commonly used come with constraints in designing student networks since they directly match some parts of the teacher and student networks such as attention matrices or word representations. For example, DistilBERT  and PKD (Sun et al., 2019) match each word representation independently using their cosine similarities, n i=1 cos(r t l,i , r s l,i ), hence the embedding size of student network should follow that of given teacher network. Similarly, TinyBERT (Jiao et al., 2019) and MINI-LM  match the attention matrices via H h=1 KL(A t l,h , A s l,h ). Therefore, we should have the same number of attention heads for teacher and student networks (see Appendix A for more details on diverse distillation objectives).\n\nIn addition to the advantage of distilling contextual information, our CKD method has the advantage of being able to select the student network's structure more freely without the restrictions that appear in existing KD methods. This is because CKD matches the pair-wise or triple-wise relationships of words from arbitrary networks (student and teacher), as shown in Eq. (1), so it is always possible to match the information of the same dimension without being directly affected by the structure. Thanks to this advantage, in the experimental section, we show that CKD can further improve the performance of recently proposed Table 1: Comparisons for task-agnostic distillation. For the task-agnostic distillation comparison, we do not use task-specific distillation for a fair comparison. The results of TinyBERT and Truncated BERT are ones reported in . Other results are as reported by their authors. We exclude BERT-of-Theseus since the authors do not consider task-agnostic distillation. Results of development set are averaged over 4 runs. \"-\" indicates the result is not reported in the original papers and the trained model is not released. \u2020 marks our runs with the officially released model by the authors.   DynaBERT (Hou et al., 2020) that involves flexible architectural changes in the training phase.",
            "score": 0.5377099897304722,
            "section_title": "Architectural Constraints in Distillation Objectives",
            "char_start_offset": 17178,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 2003,
                    "end": 2020,
                    "matchedPaperCorpusId": "215415863"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48974609375
        },
        {
            "corpus_id": "222124879",
            "title": "Online Knowledge Distillation via Multi-branch Diversity Enhancement",
            "text": "Traditional knowledge distillation methods have two stages that require a pretrained teacher model to provide soft output for distillation. Different from above complex training methods, several works adopts collaboratively training strategy. Simultaneously training a group of student models based on each other's predictions is an effective single-stage distillation method, which can be a good substitute for pretrained teacher models. Some methods [16,18] solve this problem. The online knowledge distillation was completed through mutual instruction between two peers [16]. However, the lack of a high-capacity teacher model will decrease the distillation efficiency. In [17,40], each student model learns from the average of the predictions generated by a group of students and obtains a better teacher model effect. ONE found that simply averaging the results would reduce the diversity among students, affecting the training of branch-based models. ONE generates the importance score corresponding to each student through the gate module. By assigning different importance score to each branch, a high-capacity teacher model is constructed, which can leverage knowledge from training data more effectively. OKDDip [19] proposed the concept of two-level distillation. The ensemble results of auxiliary peer networks were distilled into the group leader. The diversified peer network plays a key role in improving distillation performance.",
            "score": 0.5375487061513294,
            "section_title": "Online Knowledge Distillation",
            "char_start_offset": 7086,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1445
                }
            ],
            "ref_mentions": [
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 573,
                    "end": 577,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 676,
                    "end": 680,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 680,
                    "end": 683,
                    "matchedPaperCorpusId": "44119099"
                },
                {
                    "start": 1222,
                    "end": 1226,
                    "matchedPaperCorpusId": "208526905"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1959228515625
        },
        {
            "corpus_id": "247315450",
            "title": "Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation",
            "text": "The knowledge of n complementary teachers can be transfered to the student through word-level knowledge distillation:\n\nwhere p is the prediction of student S and q i is the prediction of teacher T i . We use a hyperparameter \u03b1 to interpolate the distillation loss and the crossentropy loss:\n\nIn this way, the student model learns both new knowledge from the training set and complementary knowledge from teacher models. With an appropriate \u03b1, we can achieve a balance between the Algorithm 1 COKD Input: training set D, the number of teachers n Output: student model S 1: randomly initialize student S and teachers T 1:n 2: while not converge do for i = 1 to n do T i \u2190 S 9: return student model S two kinds of knowledge and alleviate the problem of imbalanced training. However, this method is based on knowledge distillation where knowledge is transferred unidirectionally from teachers to the student. Though the student can benefit from balanced training, these complementary teachers also set an upperbound to the student and prevent it from performing better.\n\nTo overcome this limitation, we follow the underlying idea of two-way knowledge transfer where the knowledge is also transferred from the student to teachers (Zhang et al., 2018;Zhu et al., 2018). We use a simple reinitialization method to achieve the two-way knowledge transfer. At the end of each epoch, we reinitialize teacher models with the parameters of the student model:\n\nThrough the reinitialization, the student and teachers are exactly the same at the beginning of each epoch. In this way, both the student and teachers are iteratively improved so the student performance is no longer limited by the fixed ability of teachers. We summarize the training process of COKD in Algorithm 1.",
            "score": 0.5371780926027382,
            "section_title": "Complementary Training",
            "char_start_offset": 17853,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1225,
                    "end": 1245,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1245,
                    "end": 1262,
                    "matchedPaperCorpusId": "48352434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56201171875
        },
        {
            "corpus_id": "271924409",
            "title": "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation",
            "text": "It is shown that our efficient trustworthy distillation (FIRST) demonstrates superior calibration ability and performance over direct distillation and standard fine-tuning methods. However, despite these exciting results, there are still some limitations in our current work, as well as potential opportunities for future research. \n\nExtend to Large Teacher Model : Due to the resource limitation, our largest teacher model is Llama 33B, which is not very large but already achieving exciting results by distillation to a 7B student model. We expect that employing a large teacher model such as 70B can lead to better calibration ability and performance since a larger model learns a better distribution. However, we are unable to explore how very large teachers perform due to resource limitations. \n\nTop-K Chosen in Offline Distillation: Another limitation of this work is that it does not provide a rigorous study on how many token probabilities to choose for one entry is optimal for knowledge distillation in large language models. Currently, we consistently choose the top-5 token probability to retrieve because of the reasons stated in \u00a75.4. However, how much token probability to use is optimal could be an important area for further exploration and development.",
            "score": 0.5363048078553225,
            "section_title": "Limitations",
            "char_start_offset": 27917,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 331
                },
                {
                    "start": 334,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1271
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58837890625
        },
        {
            "corpus_id": "250391099",
            "title": "Sparse Distillation: Speeding Up Text Classification by Using Bigger Student Models",
            "text": "We investigated a new way of using knowledge distillation to produce a faster student model by reversing the standard practice of having the student be smaller than the teacher and instead allowed the student to have a large table of sparsely-activated embeddings. This enabled the student model to essentially memorize task-related information that if an alternate architecture were used would have had to be computed. We tested this method on six single-sentence classification tasks with models that were up to 1 billion parameters in size, approximately 3x as big as the RoBERTa-Large teacher model, and found that the student model was blazing fast and performed favorably. We hope that our work can lead to further exploration of sparse architectures in knowledge distillation. There are multiple directions for future work, including extending the DAN architecture to better support tasks with long range dependencies like natural language inference or multiple inputs like text similarity. Additionally, more work is needed to test the idea on non-English languages where n-gram statistics can be different from English.",
            "score": 0.5349315193696053,
            "section_title": "Conclusions & Future Work",
            "char_start_offset": 26558,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6435546875
        },
        {
            "corpus_id": "215238905",
            "title": "Teacher-Class Network: A Neural Network Compression Mechanism",
            "text": "Knowledge distillation via single student: In knowledge distillation, as introduced by Hinton et al. [15], a single student either tries to mimic a single teacher's [6,15,17,18,19,21,23,26,35,36,44] or multiple teachers [9,39,42]. Most of such schemes transfer knowledge to student by minimizing the error between the knowledge of the teacher and the student [15,18]. Rather than matching actual representation, Passalis et al. [26] and Watanabe et al. [36] propose to model the knowledge using probability distribution and then match the distribution of teacher and student networks. Nikolaos et al. [26] try to cater non-classification problems in addition to classification problems. Wang et al. [35] argue that it is hard to figure out which student architecture is more suitable to quantify the information inherited from teacher networks, so they use generative adversarial network (GAN) to learn student network. Belagiannis et al. [3] even studied the distillation of dense features using GANs. Since, teacher can transfer limited amount of knowledge to student, so Mirzadeh et al. [23] propose multi-step knowledge distillation, which employs intermediate-sized networks. Peng et al. [27] propose a framework named correlation congruence for knowledge distillation (CCKD), which transfers the sample level knowledge, yet in addition, it also transfers the correlation between samples. \n\nFew studies that utilize the dense features along with soft-logits claim that the dense features help to generalize the student model [19,21,44]. Heo et al. [14] set out a novel feature distillation technique in which the distillation loss is intended to make an alliance among different aspects: teacher transform, student transform, distillation feature position and distance function. An online strategy has also been proposed that eliminates the need for a two-phase strategy and performs joint training of a teacher network as well as a single multi-branch student network [17]. All these methods, including multi-branch strategy [17] train only a single student on the final logits; we instead train multiple students, each on a chunk of dense representation.",
            "score": 0.5347751419302171,
            "section_title": "Related Work",
            "char_start_offset": 3215,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2161
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 168,
                    "matchedPaperCorpusId": "227276362"
                },
                {
                    "start": 171,
                    "end": 174,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 174,
                    "end": 177,
                    "matchedPaperCorpusId": "49869692"
                },
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "213068593"
                },
                {
                    "start": 180,
                    "end": 183,
                    "matchedPaperCorpusId": "224818016"
                },
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 186,
                    "end": 189,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 189,
                    "end": 192,
                    "matchedPaperCorpusId": "19182852"
                },
                {
                    "start": 192,
                    "end": 195,
                    "matchedPaperCorpusId": "9154797"
                },
                {
                    "start": 195,
                    "end": 198,
                    "matchedPaperCorpusId": "232417071"
                },
                {
                    "start": 220,
                    "end": 223,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 223,
                    "end": 226,
                    "matchedPaperCorpusId": "204788964"
                },
                {
                    "start": 226,
                    "end": 229,
                    "matchedPaperCorpusId": "145916274"
                },
                {
                    "start": 363,
                    "end": 366,
                    "matchedPaperCorpusId": "49869692"
                },
                {
                    "start": 428,
                    "end": 432,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 453,
                    "end": 457,
                    "matchedPaperCorpusId": "9154797"
                },
                {
                    "start": 601,
                    "end": 605,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 699,
                    "end": 703,
                    "matchedPaperCorpusId": "19182852"
                },
                {
                    "start": 939,
                    "end": 942,
                    "matchedPaperCorpusId": "4375646"
                },
                {
                    "start": 1090,
                    "end": 1094,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1193,
                    "end": 1197,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1530,
                    "end": 1534,
                    "matchedPaperCorpusId": "213068593"
                },
                {
                    "start": 1534,
                    "end": 1537,
                    "matchedPaperCorpusId": "224818016"
                },
                {
                    "start": 1537,
                    "end": 1540,
                    "matchedPaperCorpusId": "232417071"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1974,
                    "end": 1978,
                    "matchedPaperCorpusId": "48352434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36474609375
        },
        {
            "corpus_id": "203953149",
            "title": "Knowledge Distillation from Internal Representations",
            "text": "Transformer-based models have significantly advanced the field of natural language processing by establishing new state-of-the-art results in a large variety of tasks. Specifically, BERT (Devlin et al. 2018), GPT (Radford et al. 2018), GPT-2 (Radford et al. 2019), XLM (Lample and Conneau 2019), XLNet (Yang et al. 2019), and RoBERTa (Liu et al. 2019c) lead tasks such as text classification, sentiment analysis, semantic role labeling, question answering, among others. However, most of the models have hundreds of millions of parameters, which significantly slows down the training process and inference time. Besides, the large number of parameters demands a lot of memory consumption, making such models hard to adopt in production environments where computational resources are strictly limited.\n\nDue to these limitations, many approaches have been proposed to reduce the size of the models while still providing similar performance. One of the most effective techniques is knowledge distillation (KD) in a teacher-student setting Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. (Hinton, Vinyals, and Dean 2015), where a cumbersome already-optimized model (i.e., the teacher) produces output probabilities that are used to train a simplified model (i.e., the student). Unlike training with one-hot labels where the classes are mutually exclusive, using a probability distribution provides more information about the similarities of the samples, which is the key part of the teacher-student distillation.\n\nEven though the student requires fewer parameters while still performing similar to the teacher, recent work shows the difficulty of distilling information from a huge model. Mirzadeh et al. (2019) state that, when the gap in between the teacher and the student is large (e.g., shallow vs. deep neural networks), the student struggles to approximate the teacher. They propose to use an intermediate teaching assistant (TA) model to distill the information from the teacher and then use the TA model to distill information towards the student. However, we argue that the abstraction captured by a large teacher is only exposed through the output probabilities, which makes the",
            "score": 0.5346769263812754,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "273186993",
            "title": "Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher",
            "text": "Knowledge distillation. Knowledge distillation (KD) (Hinton et al., 2015) transfers knowledge from a teacher to a smaller student model. Methods improve this by focusing on logits or intermediate features (Sun et al., 2024;Jin et al., 2023;Zhao et al., 2022a;Li et al., 2023;Passalis et al., 2021;Tian et al., 2019;Zagoruyko & Komodakis, 2017a;Heo et al., 2019a;Chen et al., 2021b;Heo et al., 2019b;Kim et al., 2018). Standard methods prioritize fully converged teachers with high performance, yet the performance gap can hinder knowledge transfer (Wang et al., 2022;Gao et al., 2020;Cho & Hariharan, 2019;Yuan et al., 2019). Strategies to address this include using intermediatestage teachers (Cho & Hariharan, 2019;Zhao et al., 2022b), pre-training student-friendly teacher model (Yang et al., 2019a;Park et al., 2021;Dong et al., 2024), introducing intermediate-sized assistant teachers (Mirzadeh et al., 2020;Son et al., 2021) or introducing auxiliary networks (Gao et al., 2021). These methods often rely on specially designed and pre-trained intermediate models. Feature-based methods like DTSKD (Li et al., 2024) and DiffKD (Huang et al., 2023) focus on bridging semantic gaps or denoising features. SCKD (Zhu & Wang, 2021) optimizes transfer using gradient similarity. Recent works refine soft labels (Yuan et al., 2024;Rao et al., 2023) or student's output entropy (Zhu et al., 2024a) to enhance knowledge transfer. In contrast, our GPD constructs a trainable dynamic teacher based on the student model, maintaining an appropriate accuracy gap throughout distillation for effective knowledge transfer. Reparameterization.",
            "score": 0.5335147034221028,
            "section_title": "RELATED WORK",
            "char_start_offset": 6141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 223,
                    "matchedPaperCorpusId": "268247468"
                },
                {
                    "start": 223,
                    "end": 240,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 240,
                    "end": 259,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 259,
                    "end": 275,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 275,
                    "end": 297,
                    "matchedPaperCorpusId": "219169868"
                },
                {
                    "start": 315,
                    "end": 344,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 344,
                    "end": 362,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 362,
                    "end": 381,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 381,
                    "end": 399,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 399,
                    "end": 416,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 548,
                    "end": 567,
                    "matchedPaperCorpusId": "252846591"
                },
                {
                    "start": 584,
                    "end": 606,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 694,
                    "end": 717,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 717,
                    "end": 736,
                    "matchedPaperCorpusId": "198179767"
                },
                {
                    "start": 782,
                    "end": 802,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 802,
                    "end": 820,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 820,
                    "end": 838,
                    "matchedPaperCorpusId": "249642077"
                },
                {
                    "start": 890,
                    "end": 913,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 913,
                    "end": 930,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 965,
                    "end": 983,
                    "matchedPaperCorpusId": "229400079"
                },
                {
                    "start": 1102,
                    "end": 1119,
                    "matchedPaperCorpusId": "268439162"
                },
                {
                    "start": 1131,
                    "end": 1151,
                    "matchedPaperCorpusId": "258888057"
                },
                {
                    "start": 1212,
                    "end": 1230,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1328,
                    "end": 1345,
                    "matchedPaperCorpusId": "249209818"
                },
                {
                    "start": 1374,
                    "end": 1393,
                    "matchedPaperCorpusId": "258564799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35009765625
        },
        {
            "corpus_id": "256459285",
            "title": "Knowledge Distillation on Graphs: A Survey",
            "text": "For the number of teachers, employing multiple teachers enables the student to fuse information from diverse sources to establish a comprehensive understanding of the knowledge, similar to ensemble learning. However, how to trade off the instruction from different teachers and integrate them without performance degradation needs special designs. In contrast, a single teacher is sometimes better and more convenient by bypassing the intricacy of incorporating multiple ones. For the determination of teacher and student structures for KDG methods, GNNs demonstrate outstanding capability in capturing graph topological information and have become the most popular choice. Moreover, some methods introduce MLP as the student model to circumvent the scalability limitation imposed by GNNs or utilize KGEM to adapt to knowledge graphs. As long as the model performs well with the target graph data, the choice of the model structure is typically not a crucial aspect of KDG methods. For distillation schemes, offline distillation is typically used to transfer knowledge from a complex teacher model, whereas teacher and student models are comparable and can be trained concurrently in the setting of online distillation.",
            "score": 0.5329985657683491,
            "section_title": "Who to Whom",
            "char_start_offset": 24888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1219
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.448486328125
        },
        {
            "corpus_id": "264490946",
            "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
            "text": "While better than normal distillation, it is often too strong a constraint when the teachers have niche areas of expertise or are overall stronger. We thus propose to constrain distillation at the data level by partitioning it into two sets -one with samples where transfer from a teacher is desired, and one where we wish to retain the student behavior. This introduces significantly fewer constraints on the model weights to learn from arbitrary teacher context, while reducing forgetting by retaining initial performance on samples where the teacher has limited positive (even detrimental) impact. Moreover, our data partitioning can be achieved without any supervision. \n\nDoing so, we see significant increases in the success rate (non-zero gains of the student) for all teacher-student pairings -from 32.5% with normal distillation to 92.5% with data partitioning. Our data-level regularization is the only setting which allows for consistently positive transfer from weaker teachers, while retaining the transfer performance of normal distillation for much stronger teachers and even outperforming normal distillation for equiperformant ones. In addition, it allows for the transfer of specialized knowledge ( \u00a75.1) and requires no pairing-specific hyperparameters. Unlike ensembling methods (Lakshminarayanan et al., 2017;Gontijo-Lopes et al., 2022;Sinha et al., 2021;Dietterich, 2000), our approach maintains original inference costs and handles high performance differences. Finally, we study architectural properties and their impact on the transfer process ( \u00a75.1) beyond the transfer method, and look into scalability to knowledge transfer from multiple models, where we find that simple sequential transfer can perform favorably when leveraging our transfer method, achieving clear improvements over transfer from just the single best teacher model. Overall, our contributions can be summarized as: (1) We discover the consistent existence of complementary knowledge between arbitrary models pretrained on the same dataset -even if model families or performances differ. (2) We conduct extensive, exploratory studies to investigate the possibility of guaranteed model-and performance-independent transfer of the complementary knowledge without performance degradation.",
            "score": 0.5317584710695773,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2281
                }
            ],
            "ref_mentions": [
                {
                    "start": 1298,
                    "end": 1329,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 1375,
                    "end": 1392,
                    "matchedPaperCorpusId": "10765854"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "258486778",
            "title": "A First Look at the Impact of Distillation Hyper-Parameters in Federated Knowledge Distillation",
            "text": "It is worth mentioning that forgetting is sometimes inevitable, especially when students and teachers have different class distributions. Thus, learning from participants who were pre-trained with some similarities in their objectives yields better learning and less forgetting outcomes, which is related to teacher selection, and we leave for future work. However, for a given pair of participants, adding a copy of the student model as an additional teacher for itself when using an unlabelled transfer set, Figure 3 (b), or using the student data as a transfer set with the CE loss in addition to the distillation loss, Figure 3 (c), can remarkably mitigate the possible forgetting and improve overall accuracy. Also note that the second approach, Figure 3 (c), is sometimes worse than the first approach, Figure 3 (b), (e.g, student participant 1 and in some cases student participants 3 and 6), which usually happens with weak student participants that have a poor pre-trained model accuracy and limited data. However, the right positioning of the models (i.e., which model is the student and which model is the teacher) does mitigate this issue and improve the overall performance, according to our experimental analysis. For example, changing the position of participant 1 from a student to a teacher when paired with participant 5 reduced the forgetting (of participant 1 classes) from 88% to only 16%. Similarly, changing the position of participant 1 from a student to a teacher when paired with participant 3 reduced the forgetting (of participant 1 classes) from 87% to only 13%. Note that both proposed approaches are applicable for privacy-sensitive settings such as FL, as the participants never share their private datasets. For the rest of the paper, we continue our experiments using the student dataset as a transfer set, because suitable public datasets might not always be available in real scenarios [1] and require prior knowledge of participants' private data and careful consideration (i.e., to avoid the significant performance degradation caused by distribution variation between the public dataset participants' datasets) [18,25]. Temperature Effect. Recall that in knowledge distillation, the student learns from the soft-target outputs of the teacher model (Eq. 1). Temperature  is usually set to 1.",
            "score": 0.5299720212703296,
            "section_title": "Joint Distillation",
            "char_start_offset": 14187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2291
                },
                {
                    "start": 2292,
                    "end": 2295
                },
                {
                    "start": 2296,
                    "end": 2329
                }
            ],
            "ref_mentions": [
                {
                    "start": 2154,
                    "end": 2157,
                    "matchedPaperCorpusId": "247292268"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.233154296875
        },
        {
            "corpus_id": "270726811",
            "title": "Balancing Speed and Precision: Lightweight and Accurate Depth Estimation for Light Field Image",
            "text": "To design a knowledge distillation framework, adapting the optimal student model is a key step.Cho and Hariharan [29] indicated that when there is a large disparity in representation power between the teacher model and the student model, knowledge transfer is ineffective.In order to explore the extent of the difference that can be tolerated between teacher and student models in knowledge distillation, two variations of the student model were tested: one with a single convolution block removed, referred to as the 'compact model', and another with three convolution blocks removed, referred to as the 'ultra-compact model'.Using these lightweight student models and the same teacher model, the students were trained through the knowledge distillation framework.Subsequently, they were evaluated to assess the effectiveness of the training.The accuracy results of each model are presented in Table 3 and visualized in Figure 8.\n\nFrom these results, it was observed that while the compact model showed improvements in both MSE and BadPix through knowledge distillation, the ultra-compact model, on the other hand, experienced a decrease in accuracy.This indicates that in the case of the EPINET architecture, knowledge distillation does not work effectively for enhancing the accuracy when there is a difference of three 92158 VOLUME 12, 2024 Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.convolution blocks between the teacher and student models.In other words, it was found that the acceptable difference in convolution blocks is less than three.To investigate how the difference of three convolution blocks influences the effectiveness of knowledge distillation, an examination of the accuracy trends over epochs was conducted.The models compared include one with three blocks removed (middle) and another with one block removed (bottom).Red square areas highlight the significant difference in error distribution between the compact model and ultra-compact model.Inference was conducted on the light field synthetic dataset [22].That is, the student model, with three convolutional blocks removed, ceased to improve its accuracy at an early stage.These findings indicate that the large model discrepancy between the student and teacher models likely made it difficult for the student model to appropriately learn the abundant information held by the teacher model.",
            "score": 0.52925279426883,
            "section_title": "B. ACCURACY ENHANCEMENT EXPERIMENTS 1) THE EVALUATION OF THE LIMITS OF MODEL DISPARITY BETWEEN TEACHER AND STUDENT",
            "char_start_offset": 27663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 627
                },
                {
                    "start": 627,
                    "end": 765
                },
                {
                    "start": 765,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 930
                },
                {
                    "start": 932,
                    "end": 1151
                },
                {
                    "start": 1151,
                    "end": 1436
                },
                {
                    "start": 1436,
                    "end": 1455
                },
                {
                    "start": 1455,
                    "end": 1513
                },
                {
                    "start": 1513,
                    "end": 1614
                },
                {
                    "start": 1614,
                    "end": 1796
                },
                {
                    "start": 1796,
                    "end": 1907
                },
                {
                    "start": 1907,
                    "end": 2033
                },
                {
                    "start": 2033,
                    "end": 2099
                },
                {
                    "start": 2099,
                    "end": 2217
                },
                {
                    "start": 2217,
                    "end": 2434
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 117,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 2094,
                    "end": 2098,
                    "matchedPaperCorpusId": "26221504"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.482666015625
        },
        {
            "corpus_id": "237492204",
            "title": "On the Efficiency of Subclass Knowledge Distillation in Classification Tasks",
            "text": "Knowledge Distillation in Classification Tasks. Transferring knowledge from one model to another is a research topic that has obtained noteworthy attention during recent years. Ba and Caruana (Ba and Caruana 2014) trained a single and small neural network to imitate the logits of a large and complex neural network. Then, Hinton et al. (Hinton, Vinyals, and Dean 2015) introduced KD and dark knowledge to claim that the deeper teacher model can successfully distill its knowledge into the smaller student neural network by matching their soft targets (softmax distributions). Nowadays, a lot of successive papers have been written to propose different techniques to KD for model compression purposes. Romero et al. (Romero et al. 2015) distilled the feature representations of the teacher's intermediate layers to the student for improving the training stage of the student network. Transferring the attention maps (Zagoruyko and Komodakis 2017;Huang and Wang 2017;Tarvainen and Valpola 2017), the inner products of intermediate activation maps (Yim et al. 2017), and relational knowledge between training samples (Park et al. 2019;Tung and Mori 2019;Peng et al. 2019;Liu et al. 2019) are some other methods to promote the distillation process from one model to another. However, these approaches ignored the possibility of available subclass knowledge within the classes and therefore did not take advantage of hidden subclass knowledge to improve student performance. By contrast, in this study, we use subclass knowledge to enhance the generalization ability of the teacher network. \n\nSubclass Knowledge Distillation. The distillation of knowledge can be improved by increasing the amount of information that the teacher can transfer to the student. M\u00fcller et al. (M\u00fcller, Kornblith, and Hinton 2020) compelled the teacher to create semantically meaningful subclasses for each class during its training phase with auxiliary contrastive loss. The student is then trained to mimic the invented teacher's subclasses predictions (probabilities). When the number of training samples per class is the same, they measured the number of bits of label information about how the teacher generalizes through subclass distillation in binary classification tasks.",
            "score": 0.5288050941815956,
            "section_title": "Related Work",
            "char_start_offset": 4680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1586
                },
                {
                    "start": 1589,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2254
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 213,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 337,
                    "end": 369,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 916,
                    "end": 946,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 966,
                    "end": 993,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 1046,
                    "end": 1063,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1115,
                    "end": 1133,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1133,
                    "end": 1152,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1152,
                    "end": 1169,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1169,
                    "end": 1185,
                    "matchedPaperCorpusId": "198185886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56884765625
        },
        {
            "corpus_id": "237605152",
            "title": "Dynamic Knowledge Distillation for Pre-trained Language Models",
            "text": "To examine the influence of less-softened teacher predictions, we conduct distillation experiments with various temperature \u03c4 using the hyperparameters identical with previous experiments, to mimicking the sharpen impact introduced by the larger teacher size. In more detail, we setup a student model with 6 layers as before and select the BERT BASE as the teacher model. The results are illustrated in Figure 7. We observe on both datasets, the decreased temperature \u03c4 will lead a performance decrease. It indicates that the lesssoftened probability distribution indeed weakens the performance of knowledge distillation.",
            "score": 0.5282264355637358,
            "section_title": "B Impacts of Prediction Smoothness",
            "char_start_offset": 31211,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55859375
        },
        {
            "corpus_id": "214641252",
            "title": "Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?",
            "text": "In [18], the authors generalized this idea and brought it into the deep learning framework. \n\nIn KD, knowledge is transferred from the teacher model to the student model by minimizing a loss in which the target is the distribution of class probabilities induced by the teacher model. In was shown in [61] that KD has several benefits for optimization and knowledge transfer between tasks. BAN [13] introduced sequential distillation, which also improved the performance of teacher models. In natural language processing (NLP), BAM [7] used BAN to distill from single-task models to a multi-task model, helping the multi-task model surpass its single-task teachers. Another two related works are [30] which provides theoretical analysis of self-distillation and CRD [50] which shows distillation improves the transferability across datasets.",
            "score": 0.5280086973873277,
            "section_title": "Related works",
            "char_start_offset": 8838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 94,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 840
                }
            ],
            "ref_mentions": [
                {
                    "start": 3,
                    "end": 7,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 300,
                    "end": 304,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 531,
                    "end": 534,
                    "matchedPaperCorpusId": "85464175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34521484375
        },
        {
            "corpus_id": "251135436",
            "title": "General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings",
            "text": "The problem of knowledge distillation [9] or model compression [10] refers to learning a smaller model g that imitates the behavior of a larger model f , such that desirably g(x) \u2248 f (x). The smaller model g is called the student and the larger model f is called the teacher.\n\nThe distillation is carried out by aligning the teacher's and student's output, e. g., via a loss term L(f (x), g(x)), which we call the teacher signal. There are more techniques to foster distillation such as using the teacher's weights as initialization for the student [11,12,13], but those are only applicable when teacher and student are of the same model architecture. When student and teacher models have a different architecture, we call that cross-architecture distillation.\n\nTo contextualize knowledge distillation with transfer learning, we adopt the distinction between general distillation and taskspecific distillation from Tang et al. [14].\n\n\u2022 General distillation: The distillation is carried out only in the (self-supervised) pretraining stage. After pretraining, the teacher is not needed anymore. The student can be fine-tuned to downstream tasks independently. \u2022 Task-specific distillation: In task-specific distillation, the teacher model can be consulted during fine-tuning for each of the downstream tasks. The respective task's training data are used for (supervised) distillation. Note that the model for task-specific distillation can still be initialized with a model obtained by general distillation. However, this also falls under task-specific distillation, because, after all, the teacher has to be consulted during fine-tuning. Distinguishing between general and task-specific distillation is important because this affects how the methods can be applied in practice. Imagine that we want to fine-tune a model for a downstream task on a mobile device. A model from general distillation would be able to learn new tasks on its own. With a task-specific distillation approach, the larger teacher model would need to be consulted on the mobile device. Thus, both approaches differ in how they can be applied in practice and should",
            "score": 0.5279092918244465,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 6663,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 41,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 63,
                    "end": 67,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "203626972"
                },
                {
                    "start": 553,
                    "end": 556,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 556,
                    "end": 559,
                    "matchedPaperCorpusId": "215238853"
                },
                {
                    "start": 927,
                    "end": 931,
                    "matchedPaperCorpusId": "85543565"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74755859375
        },
        {
            "corpus_id": "272146417",
            "title": "MST-KD: Multiple Specialized Teachers Knowledge Distillation for Fair Face Recognition",
            "text": "Apart from the higher complexity of the training process and data considerations, the distillation itself is associated with a black-box behaviour that does not allow for an easy interpretation of which knowledge is being distilled. Specially if we consider a generalist teacher with no speciality, which means a teacher that is enforced to learn the highest number of common features between groups. In this sense, we hypothesize that having generalist teachers leads to lower performance and fairness. One can visualize this strategy as an educational system, where a student is provided with multiple professors/teachers highly specialized in a topic. Together, they contribute to define future of the student, and the student itself has the responsibility to learn from these multiple domains. Furthermore, this allows to trace, more accuratly, which teacher is distilling which information. \n\nIn this work, we use multiple specialized teachers to distill knowledge to a student network with reduced racial bias, resulting in the proposed multiple specialized teachers KD (MST-KD) framework. Each teacher is trained to distinguish identities from a specific ethnicity as each hyper-specialized model learns ethnicity-specific information that would not be easily gathered by a system addressing the more complex task of FR for all ethnicities. Distilling this knowledge to a single student might contribute to individually boost its performance for each ethnicity, resulting in increased global performance and racial fairness. \n\nWe designed a set of experiments to showcase the impact of having hyperspecialised teachers. In that sense, we demonstrate that students trained with information distilled from specialized/biased teachers have higher performance metrics and better bias indicators in comparison to students trained with information from multiple balanced/generalist teachers. This presents itself as an intriguing behaviour, that indicates an advantage of learning ethnicity-specific information in comparison to jointly optimising for all ethnic groups. In this paper, we present the following contributions: \n\n-A multi-teacher fusion approach that aggregates multiple N-dimensional spaces into a single N-dimensional space, allowing for KD; -We show that students that receive knowledge from hyper-specialised teachers achieve superior performance even if the teachers' individual performance is lower than that of balanced teachers;",
            "score": 0.5277855848733113,
            "section_title": "Introduction",
            "char_start_offset": 2136,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1531
                },
                {
                    "start": 1534,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2126
                },
                {
                    "start": 2129,
                    "end": 2452
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0970458984375
        },
        {
            "corpus_id": "258715315",
            "title": "Tailoring Instructions to Student\u2019s Learning Levels Boosts Knowledge Distillation",
            "text": "In this paper, we focus on task-specific distillation given pre-trained language models. Under this setting, the teacher model is already pre-trained in an unsupervised manner and the student model is either derived from part of the teacher model or pre-trained in an unsupervised manner as well.\n\nVanilla distillation The typical approach to knowledge distillation is a two-stage process. It involves first fine-tuning a pre-trained teacher model to maximize its performance on a specific task. Once the teacher model has converged, a student model is trained to closely imitate the output of the teacher model on the training data. The optimization objective for the student model at each mini-batch is:\n\n(1)\n\nThe update of the student follows:\n\nThe limitation of vanilla distillation is that it does not allow teacher to adjust its behavior according to student's feedback, as the teacher's parameters are fixed during the distillation process.\n\nOnline distillation To achieve student-aware distillation, online distillation Zhu et al., 2018;Shi et al., 2020) is proposed which involves the simultaneous fine-tuning of both the student and teacher models in one-stage.\n\nIn addition to minimizing the cross-entropy loss with respect to the ground truth labels, the target distribution of the teacher model is constrained to be close to that of the student model through the minimization of the cross-entropy loss between the outputs of the teacher and student models: L t (\u03b8 t , \u03b8 s , z r ) = \u03b1L ce (y r , T (x r ; \u03b8 t )) +(1 \u2212 \u03b1)L ce (T (x r ; \u03b8 t ), S(x r ; \u03b8 s )).\n\n(\n\nThe training process involves iteratively updating the parameters of both models:\n\nThrough iterative update, the student model is able to learn from the learning curve of the teacher model (Shi et al., 2020), which improves its performance on the given task. However, online distillation focuses on transferring the knowledge of the teacher to the student on training set without explicitly considering how well the student model will perform on unseen test data. This might lead to the student model only memorizing the training examples without generalizing well to new ones (",
            "score": 0.5277053576003501,
            "section_title": "Revisiting Learning to Teach",
            "char_start_offset": 6489,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1028,
                    "end": 1045,
                    "matchedPaperCorpusId": "48352434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "252565150",
            "title": "Selective Cross-Task Distillation",
            "text": "Knowledge distillation is an essential model reuse approach that extracts knowledge from a teacher and assists a student's training. It was first proposed in [2] and has been proven to be an effective technique for improving student performance. Recent advances in knowledge distillation involve exploring new types of knowledge [22], [23], [24], [34], [35], [36], [37], [38], developing new training styles [39], [40], [41], [42], [43], [44], [45], [46], [47], studying the reasons for its success [48], [49], [50], and applying it to new applications [51], [52], [53]. This paper studies a new setting called selective cross-task distillation, which is highly related to but different from conventional knowledge distillation. Now we discuss three key questions to clarify the position of our work in the existing literature. \n\n(1) What is the relationship between the teacher's and student's tasks? Most existing works [2], [22], [23], [37], [38] follow the \"same task assumption\", i.e., the teacher model is trained on the same task as the student, which restricts the application field of knowledge distillation. As an exception, several recent works [24], [54], [55] have started to explore cross-task knowledge distillation where the same task assumption is replaced by a weaker \"relevant task assumption\", which means the teacher's task shares semantic relatedness with the student's task. \n\nRecall that our motivation to study the new setting is to utilize the massive model repository to the maximum extent. Thus, we must enable cross-task distillation since it is unlikely to obtain a teacher from the repository that perfectly fits our target task. In this paper, we propose a method for generalized cases, which means the teacher's label space can be the same as, overlapped with, or totally different from the student's label space. \n\n(2) How to obtain a good teacher? Knowledge distillation society often assumes the existence of a suitable teacher model.",
            "score": 0.5266962008492349,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6901,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1397
                },
                {
                    "start": 1400,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1846
                },
                {
                    "start": 1849,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 161,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 329,
                    "end": 333,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 335,
                    "end": 339,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 341,
                    "end": 345,
                    "matchedPaperCorpusId": "219965004"
                },
                {
                    "start": 347,
                    "end": 351,
                    "matchedPaperCorpusId": "229679667"
                },
                {
                    "start": 353,
                    "end": 357,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 359,
                    "end": 363,
                    "matchedPaperCorpusId": "231839582"
                },
                {
                    "start": 365,
                    "end": 369,
                    "matchedPaperCorpusId": "3603145"
                },
                {
                    "start": 371,
                    "end": 375,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 414,
                    "end": 418,
                    "matchedPaperCorpusId": "219636179"
                },
                {
                    "start": 420,
                    "end": 424,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 426,
                    "end": 430,
                    "matchedPaperCorpusId": "49869692"
                },
                {
                    "start": 432,
                    "end": 436,
                    "matchedPaperCorpusId": "30258763"
                },
                {
                    "start": 438,
                    "end": 442,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 450,
                    "end": 454,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 499,
                    "end": 503,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "235826057"
                },
                {
                    "start": 511,
                    "end": 515,
                    "matchedPaperCorpusId": "211068747"
                },
                {
                    "start": 553,
                    "end": 557,
                    "matchedPaperCorpusId": "3619097"
                },
                {
                    "start": 565,
                    "end": 569,
                    "matchedPaperCorpusId": "250297995"
                },
                {
                    "start": 922,
                    "end": 925,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 927,
                    "end": 931,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 933,
                    "end": 937,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 939,
                    "end": 943,
                    "matchedPaperCorpusId": "3603145"
                },
                {
                    "start": 945,
                    "end": 949,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1156,
                    "end": 1160,
                    "matchedPaperCorpusId": "219965004"
                },
                {
                    "start": 1162,
                    "end": 1166,
                    "matchedPaperCorpusId": "247521335"
                },
                {
                    "start": 1168,
                    "end": 1172,
                    "matchedPaperCorpusId": "247011924"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32470703125
        },
        {
            "corpus_id": "271208532",
            "title": "Efficient image classification through collaborative knowledge distillation: A novel AlexNet modification approach",
            "text": "Knowledge distillation mitigates this risk by enabling the student model to learn from the detailed and nuanced knowledge encoded in the teacher model. This includes learning from the teacher intermediate representations and final outputs, which helps the student model achieve comparable accuracy to the teacher model despite having a simpler architecture. \n\nAnother significant aspect is the generalization and robustness of models. The process of knowledge distillation often leads to models that generalize better to new data, as the student model learns not just the final predictions but also the intermediate processes and feature representations from the teacher model. This comprehensive learning process makes the student model more robust and better equipped to handle variations and noise in the input data. Deploying large models can be impractical due to hardware limitations in real-world scenarios. Knowledge distillation allows for the creation of smaller, efficient models that can be easily deployed across various devices and platforms without sacrificing performance. This scalability is crucial for the widespread adoption of deep learning technologies in consumer electronics, autonomous systems, and other applications where resources are constrained. The versatility of knowledge distillation is also noteworthy. It is not limited to a single type of neural network or application but has been successfully applied across various domains, including image classification, natural language processing, and speech recognition. This makes it a valuable tool for improving the efficiency of machine-learning models across a wide range of tasks. In addition to facilitating model compression and optimization, knowledge distillation complements other techniques like pruning, quantization, and low-rank factorization. It helps create lightweight models that retain high performance, essential for applications requiring real-time processing and low latency. \n\nKnowledge distillation bridges the gap between the high performance of complex models and the practical constraints of deploying these models in real-world environments. It ensures models remain efficient, accurate, and robust while being suitable for various applications with limited computational resources. This ability to create high-performing yet resource-efficient models underscores the critical importance of knowledge distillation in advancing the field of machine learning and artificial intelligence. \n\nMost extant knowledge distillation strategies prompt the student network to emulate the teacher network representational domain to match its adept performance [9].",
            "score": 0.5265478097092589,
            "section_title": "Introduction",
            "char_start_offset": 2291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 357
                },
                {
                    "start": 360,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1976
                },
                {
                    "start": 1979,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2289
                },
                {
                    "start": 2290,
                    "end": 2492
                },
                {
                    "start": 2495,
                    "end": 2658
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8359375
        },
        {
            "corpus_id": "270371436",
            "title": "DeviceBERT: Applied Transfer Learning With Targeted Annotations and Vocabulary Enrichment to Identify Medical Device and Component Terminology in FDA Recall Summaries",
            "text": "Transfer learning is a widely adopted technique in machine learning that utilizes pre-trained model weights from models trained on large-scale datasets to fine-tune the model on smaller, downstream tasks (Zhuang et al. [2021]).This approach takes advantage of the knowledge captured by the pre-trained model on the larger dataset and adapts it to the specific requirements of the target task, thereby reducing the need for extensive retraining and improving performance.\n\nKnowledge distillation is a related technique that involves training a smaller student model to approximate the output of a pre-trained teacher model (Hinton et al. [2015]; Tian et al. [2022], Beyer et al. [2022]).The teacher model, typically a larger and more complex model, serves as a reference for the student model, providing guidance on how to map inputs to outputs.Through this process, the student model learns to mimic the behavior of the teacher model, capturing its knowledge and expertise.The key advantage of knowledge distillation lies in its ability to retain the performance of the teacher model while significantly reducing computational requirements, making it an attractive approach for deploying models in resource-constrained environments.",
            "score": 0.5261978983872485,
            "section_title": "Related Work",
            "char_start_offset": 3476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 227,
                    "end": 470
                },
                {
                    "start": 472,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 844
                },
                {
                    "start": 844,
                    "end": 973
                },
                {
                    "start": 973,
                    "end": 1232
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 225,
                    "matchedPaperCorpusId": "207847753"
                },
                {
                    "start": 645,
                    "end": 663,
                    "matchedPaperCorpusId": "204838340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53564453125
        },
        {
            "corpus_id": "272593472",
            "title": "Applied Federated Model Personalisation in the Industrial Domain: A Comparative Study",
            "text": "Generally speaking, a large difference in model size between the student and instructor networks in (KD) can lead to subpar results. An enhanced KD framework [19] was proposed, which incorporates a teacher assistant and a multistep process. Additionally, the integration of multi-teacher KD technology with dual-stage progressive KD has been suggested [20] to improve the performance of KD under limited data conditions. This approach takes advantage of the benefits provided by multi-teacher KD. \n\nThere have also been attempts to apply self-learning to a model via KD [21]. The aforementioned methodology employs a teacher-student paradigm with identical network structures to derive a distilled student model. This distilled model is then leveraged as a teacher to facilitate the training of a new student model, and this cycle is iteratively repeated to gradually enhance model performance. In an attempt to avoid using exceptionally large models in Neural Machine Translation (NMT) tasks, the paper at [22] utilized KD, introducing two new variations of the technique in the process. \n\nAdditional variations include Relational Knowledge Distillation (RKD) [23], which transfers mutual relations between data examples. Experiments results show that via RKD, student models can often outperform the teacher. Another technique is knows as Similarity-Preserving Knowledge Distillation [24] and it enables the training of a student network by ensuring that input pairs that generate comparable, or distinct, activations in the teacher network yield similar, or dissimilar, activations in the student network. \n\nWhile exploring the field of Logit Distillation, researchers proposed a reformulation of the conventional KD loss [25], splitting it into two components referred to as Target Class Knowledge Distillation (TCKD) and Non-Target Class Knowledge Distillation (NCKD). Also a separate technique dubbed Virtual Knowledge Distillation (VKD) [26] leverages a softened distribution generated by a virtual knowledge generator that is conditioned on the class label, in an attempt to improve the student's performance.",
            "score": 0.5258840305254789,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 11724,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "250034651"
                },
                {
                    "start": 570,
                    "end": 574,
                    "matchedPaperCorpusId": "244805869"
                },
                {
                    "start": 1007,
                    "end": 1011,
                    "matchedPaperCorpusId": "8451212"
                },
                {
                    "start": 1161,
                    "end": 1165,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1725,
                    "end": 1729,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1944,
                    "end": 1948,
                    "matchedPaperCorpusId": "247874670"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62841796875
        },
        {
            "corpus_id": "263620565",
            "title": "Can a student Large Language Model perform as well as it's teacher?",
            "text": "Knowledge distillation is a technique used in machine learning to transfer knowledge from a larger, more complex model (called the teacher model) to a smaller, simpler model (called the student model). The goal is to create a lightweight, computationally efficient student model that retains as much of the teacher model's performance as possible. This is especially useful for deploying machine learning models on resource-constrained devices or in situations where inference time and energy consumption are critical. \n\nThe knowledge distillation approach entails training a smaller, more focused model to replicate the results of a bigger, more broad language model, like GPT-3 [Brown et al., 2020]. The bigger model's high-level symbolic knowledge should be reduced into a smaller, more effective model that can accurately carry out specific tasks. This method involves training a student Transformer model using knowledge distillation from a larger teacher model. The teacher model provides soft labels for the training data, which are used to train the student model. This allows the student model to learn from the teacher model's rich representation while being more efficient due to its smaller size [Freitag et al., 2017]. In our experiments we used the model introduced by Gholami and Omar [2023] (GPT-Efficio) as the teacher. \n\nHere we study a specific approach called The figurative distiller (FD) process that involves three main components: a teacher model, a student model, and a set of rules. The teacher model is the larger, more general language model, while the student model is the smaller, more specialized model being trained. The rules define the constraints and relationships between different concepts in the modeled domain. There are three steps in the FD procedure. The student model is first trained using a collection of training examples produced by the instructor model. In order to enhance the student model's performance, a task-specific dataset is used to fine-tune it. The output of the student model is then further refined using symbolic rules to ensure it adheres to the limitations and relationships specified by the rules. \n\nBoth supervised, and unsupervised learning can be included in the FD process. The teacher model creates a collection of labeled examples in the supervised environment, subsequently utilized for training the student model.",
            "score": 0.5255930619873028,
            "section_title": "Approach",
            "char_start_offset": 7499,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1336
                },
                {
                    "start": 1339,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2162
                },
                {
                    "start": 2165,
                    "end": 2242
                },
                {
                    "start": 2243,
                    "end": 2386
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70849609375
        },
        {
            "corpus_id": "264952263",
            "title": "A lightweight speech recognition method with target-swap knowledge distillation for Mandarin air traffic control communications",
            "text": "Experiments were carried out on the Mandarin ATC communications dataset to investigate the effectiveness of the TSKD training strategy proposed in this article. As shown in Table 3, two distinct teacher-student network structures, including homogeneous and heterogeneous architectures, were employed. The left set used the Conformer teacher model to distill the Conformer student model within homogeneous architectures, whereas the right group made use of the Transformer teacher model to distill the Conformer student model within heterogeneous network structures. The findings suggest that the effectiveness of knowledge distillation can be boosted by employing either TKD or SKD, with TKD bringing greater benefits. It also supports that non-target class knowledge distillation (NCKD) is the primary reason why classical logits-based distillation is effective but severely restricted, which coincides with the efficiency analysis of DKD (Zhao et al., 2022). Furthermore, integrating two training strategies into one, namely TSKD, can further strengthen the performance of knowledge distillation. In this article, TSKD focuses on the logit output exchange of the target class between the student and teacher models, which can effectively improve the distillation performance. Apart from the target class, the top-k information predicted by the teacher model for each segment of acoustic features is also a sort of valuable information (Reddi et al., 2021); it reflects which k characters the acoustic features input should be transcribed into, from the   (Hinton, Vinyals & Dean, 2015) 6.30 11.76 6.94 \n\nFitNets (Romero et al., 2015) 6.22 10.92 6.68 DKD (Zhao et al., 2022) 6.11 11.39 6.30 TSKD (Ours) 6.00 10.65 6.11 \n\nNotes. \n\nResults of the proposed model are in bold. \n\nperspective of the teacher model. On the basis of this assumption, another comparative experiment was conducted to verify the effectiveness of the TSKD, with the predictive top-k output of the teacher model for each segment as the information exchange.",
            "score": 0.525266662682718,
            "section_title": "Effectiveness of TSKD in homogeneous and heterogeneous architectures",
            "char_start_offset": 17657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1719
                },
                {
                    "start": 1722,
                    "end": 1728
                },
                {
                    "start": 1731,
                    "end": 1773
                },
                {
                    "start": 1776,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2028
                }
            ],
            "ref_mentions": [
                {
                    "start": 940,
                    "end": 959,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1437,
                    "end": 1457,
                    "matchedPaperCorpusId": "233237266"
                },
                {
                    "start": 1557,
                    "end": 1587,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1614,
                    "end": 1635,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1656,
                    "end": 1675,
                    "matchedPaperCorpusId": "247476179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.353271484375
        },
        {
            "corpus_id": "272753230",
            "title": "Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights",
            "text": "In this work, we demonstrated an effective approach to distilling knowledge into student models by calculating the attribution scores of the teacher model's input and using them as rationales for the student model. This method is based on the theory that a high gradient value for a token in the input indicates that minor changes in that token can significantly impact the model's prediction, emphasizing its importance in influencing the model's output. Conversely, a low gradient value suggests that the token has minimal effect on the prediction, signifying its lesser importance. We tested our method on four diverse datasets, which included tasks in natural language inference, commonsense question answering, and arithmetic word problems. The results showed that using t5-small as the student model, our method enhances performance across these datasets. Additionally, we explored why this method is effective by analyzing the top important tokens used as rationales. Particularly in the multiple-choice CQA dataset, where the answer is included in the input, we found that the top-5 extracted tokens contain the answer or a part of it 68% of the time. \n\nWhile our method offers a simple and effective means of knowledge distillation, there is still a significant performance gap between the teacher and student models, with the teacher model outperforming the student by a large margin. We hope that this work contributes to the field of knowledge distillation and inspires further research to narrow the gap between large and small models. This endeavor is crucial for advancing the efficiency and applicability of language models in various real-world scenarios.",
            "score": 0.5249649286507436,
            "section_title": "Conclusion and Outlook",
            "char_start_offset": 25246,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1672
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "85464175",
            "title": "BAM! Born-Again Multi-Task Networks for Natural Language Understanding",
            "text": "Main Results. We compare models trained with single-task learning, multi-task learning, and several varieties of distillation in Table 1. While standard multi-task training improves over single-task training for RTE (likely because it is closely related to MNLI), there is no improvement on the other tasks. In contrast, Single\u2192Multi knowledge distillation improves or matches the performance of the other methods on all tasks except STS, the only regression task in GLUE. We believe distillation does not work well for regression tasks because there is no distribution over classes passed on by the teacher to aid learning. \n\nThe gain for Single\u2192Multi over Multi is larger than the gain for Single\u2192Single over Single, suggesting that distillation works particularly well in combination with multi-task learning. Interestingly, Single\u2192Multi works substantially better than Multi\u2192Multi distillation. We speculate it may help that the student is exposed to a diverse set of teachers in the same way ensembles benefit from a diverse set of models, but future work is required to fully understand this phenomenon. In addition to the models reported in the table, we also trained Single\u2192Multi\u2192Single\u2192Multi models. However, the difference with Single\u2192Multi was not statistically significant, suggesting there is little value in multiple rounds of distillation.",
            "score": 0.5248620529062911,
            "section_title": "Results",
            "char_start_offset": 9540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1354
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22802734375
        },
        {
            "corpus_id": "252545066",
            "title": "PROD: Progressive Distillation for Dense Retrieval",
            "text": "The knowledge distillation [11] has been widely studied for decades. There are a variety of off-the-shelf practical techniques, such as the response-based method [19], the feature-based method [37] and distilling the attention scores [49] with Kullback-Leibler divergence (KLD) or mean squared error (MSE). Recently, a rising group of work focuses on the relationships between the teacher model and the student model. Some elaborate the knowledge transmission methods [4,56], some introduce middle models [28], and others work on distillation with multiple teachers [45]. It can be noted that how to use a more reasonable teacher model to guide the learning of the student model has gradually become the current key direction. \n\nAmong the studies on knowledge distillation, there is a branch of work on progressive distillation, in which teachers are dynamically adjusted to distill students. In spite of the success on the workflow of pretraining [43], progressive distillation methods are widely proven to be effective in down-stream tasks, such as image classification [36], image generation [39], GLUE [14,36], question answering [36], and cross-modal representation learning [1]. In each task, the main idea of the existing progressive distillation work is to use stronger teachers while educating students. Different from the above studies, PROD is adopt to the research field of dense retrieval, using a progressive method from two perspectives, teacher and data, in which teachers with different architectures are applied in a progressive order and more and more confusing data is mined to fill the performance gap between the teacher and the student.",
            "score": 0.5246350401874842,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1659
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "234777937"
                },
                {
                    "start": 468,
                    "end": 471,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "237250417"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1106,
                    "end": 1110,
                    "matchedPaperCorpusId": "239009625"
                },
                {
                    "start": 1180,
                    "end": 1183,
                    "matchedPaperCorpusId": "248085000"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39599609375
        },
        {
            "corpus_id": "273233176",
            "title": "SNN-PAR: Energy Efficient Pedestrian Attribute Recognition via Spiking Neural Networks",
            "text": "Knowledge Distillation is a technique for model compression that facilitates a smaller student model in learning from a larger teacher model. The student acquires knowledge by imitating various aspects of the teacher, such as its intermediate features [41], prediction logits [22], or activation boundaries [20]]. This approach was originally put forth by Hinton et al. [21] to supervise students based on the hard and soft label's output by the teacher, and nowadays there is a lot of work on using distillation for knowledge transfer to help the model get better performance. Earlier knowledge distillation (KD) techniques can be classified into three distinct categories: distillation from logits, distillation from features, and distillation based on attention. In terms of logit distillation, DIST [24] employs the Pearson correlation coefficient in place of KL divergence, combining both inter-and intra-class relationships. SRRL [62] ensures that the logits output from the teacher and the features of the student, after the teacher's linear layer, are identical. WSLD [75] examines soft labels and assigns varying weights to them based on the bias-variance trade-off. In addition to logit distillation, Several studies [5,43,64,66] concentrate on transferring knowledge through intermediate features. FitNet [41] directly distills semantic information from these intermediate features. AT [68] shifts the attention from feature maps to the student model. RKD [40] extracts relationships from the feature maps. MGD [65] masks the features of the student model, compelling it to replicate the features of the teacher model. To our knowledge, AT [68] is the sole knowledge distillation method that focuses on transferring attention, defining the attention map as a spatial representation that highlights the areas of the input that the model concentrates on the most. Wang et al. propose the HDETrack [56] which employs a hierarchical knowledge distillation strategy to augment the student tracking network from multi-modal or multi-view teacher network. In this paper, we employ both logits and intermediate features for knowledge distillation, believing that the integration of these two methods can greatly enhance knowledge transfer and improve the effectiveness of the student model. 3 Our Proposed Approach",
            "score": 0.5241446787445438,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2293
                },
                {
                    "start": 2294,
                    "end": 2317
                }
            ],
            "ref_mentions": [
                {
                    "start": 307,
                    "end": 311,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 803,
                    "end": 807,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "235613564"
                },
                {
                    "start": 1227,
                    "end": 1230,
                    "matchedPaperCorpusId": "250279758"
                },
                {
                    "start": 1230,
                    "end": 1233,
                    "matchedPaperCorpusId": "236882796"
                },
                {
                    "start": 1233,
                    "end": 1236,
                    "matchedPaperCorpusId": "244488341"
                },
                {
                    "start": 1467,
                    "end": 1471,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1522,
                    "end": 1526,
                    "matchedPaperCorpusId": "248506080"
                },
                {
                    "start": 1906,
                    "end": 1910,
                    "matchedPaperCorpusId": "262822525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.529296875
        },
        {
            "corpus_id": "252110723",
            "title": "On the effectiveness of compact biomedical transformers",
            "text": "Knowledge distillation (Hinton et al., 2015) is the process of transferring knowledge from a larger model called \"teacher\" to a smaller one called \"student\" using the larger model's outputs as soft labels. Distillation can be done in a task-specific way where the pre-trained model is first fine-tuned on a task and then the student attempts to imitate the teacher network. This is an effective method, however, fine-tuning of a pre-trained model can be computationally expensive. Task-agnostic distillation, on the other hand, allows the student to mimic the teacher by looking at its masked language predictions or intermediate representations. The student can subsequently be directly fine-tuned on the final task (Wang et al., 2020;Yao et al., 2021). \n\nDistilBERT is a well-known example of a compressed model that uses knowledge distillation to transfer the knowledge within the BERT base model to a much smaller student network which is about 40% smaller and 60% faster. It uses a triple loss which is a linear combination of language modeling, distillation and cosine-distance losses.",
            "score": 0.5237154214911952,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 13990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1091
                }
            ],
            "ref_mentions": [
                {
                    "start": 717,
                    "end": 736,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 736,
                    "end": 753,
                    "matchedPaperCorpusId": "235652233"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42578125
        },
        {
            "corpus_id": "244714888",
            "title": "Improved Knowledge Distillation via Adversarial Collaboration",
            "text": "The vanilla KD method is a two-stage process in which a high capacity teacher model is trained and then used for distillation. However, due to the high capacity divergence between the teacher and the student, it results in a common phenomenon that the student is hard to reach the teacher's performance after the distillation process. \n\nTo alleviate this divergence, many researchers try to distill knowledge from the teacher to the student in a more smooth way. One of the most well-known methods is TAKD [28], which utilizes proxy models to alleviate the gap, aiming to provide an easier learning curve for the student. Another insightful method is to reduce the teacher's capacity by using early-stop teacher regularization [5], in this way, the student can gain better performance. \n\nA major advantage of these methods is to smooth the student's learning curve and mitigate the mismatched capacity between teacher and student. However, there are certain drawbacks associated with the use of the proxy model and early-stop teacher model. Firstly, although these approaches alleviate the student's learning burdens, they also lose abundant knowledge of the original teacher in the distillation process, which may cause that the student model can not draw some key information. As Table 1 showed, we use qualitative analysis in order to gain insights into the information loss in the distillation process. We conduct experiments with the teacher-student pair of WRN-28-4 and resnet56 on CIFAR-100. Specifically, we use both CKA [20] and KL divergence to measure the representational similarity among the teacher, the proxy, and the student. The larger the CKA, the more similar the representations among the teacher, the proxy, and the student. The larger the KL divergence, the greater the difference between the two distributions. Besides, we use the relative accuracy (AccGap) to evaluate the capacity gap among the teacher and the proxy. From Table 1, the CKA of the teacher-proxy pair is higher than the teacher-student pair while KL is quite the reverse, which means that the student model suffers the loss of information during the proxy distillation process.",
            "score": 0.523253657170696,
            "section_title": "Analysis on Proxy-based Approaches",
            "char_start_offset": 10007,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 727,
                    "end": 730,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.332275390625
        },
        {
            "corpus_id": "252846591",
            "title": "Efficient Knowledge Distillation from Model Checkpoints",
            "text": "Knowledge distillation. Hinton et al. [2] proposed to transfer \"dark knowledge\" of a strong capacity teacher model to a compact student model by minimizing the Kullback-Leibler divergence between the soft targets of the two models. Since then, many variants of KD methods were proposed to improve the distillation performance [16], such as Fitnets [17], AT [18], CCM [19], FSP [20], SP [21], CCKD [22],TC 3 KD [23]. As a promising technique to improve model generalization, ensemble learning is often combined with knowledge distillation to improve the distillation performance [3,11,4]. \n\nIn the online knowledge distillation framework [24], efforts were made to boost the distillation performance by increasing the diversity between multiple models to improve the ensemble performance [6,7]. Most existing methods commonly assumed that a high performing teacher is preferred for KD. On the contrary, some researchers thought that the model capacity gap between strong teachers and small students usually degrades knowledge transfer [8,9,12,25]. Some of them have experimentally verified that poor teachers can also perform KD tasks well [12,25]. Several methods are proposed to compress this gap by introducing an assistant network [8,9] or designing a student-friendly teacher [10,11]. However, they did not explain theoretically why gap exists and how gap affects KD. In the self-distillation framework [26,27], it is essentially the intermediate model that is used as the teacher, but no one has a theoretical explanation for why the intermediate model works. In this paper, we link KD and IB theory through extensive observations and experiments. From the perspective of mutual information, we explain why intermediate models serve as better teachers than full models, and how to select an suitable intermediate model to reduce the negative impact of model gap. \n\nInformation bottleneck. Tishby et al. [28] firstly proposed the information bottleneck concept and provided a tabular method to numerically solve the IB Lagrangian (Eq. ( 3)).",
            "score": 0.5228324385730709,
            "section_title": "Related Work",
            "char_start_offset": 5723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1867
                },
                {
                    "start": 1870,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 330,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 367,
                    "end": 371,
                    "matchedPaperCorpusId": "237416639"
                },
                {
                    "start": 377,
                    "end": 381,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 386,
                    "end": 390,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 397,
                    "end": 401,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 410,
                    "end": 414,
                    "matchedPaperCorpusId": "250631682"
                },
                {
                    "start": 578,
                    "end": 581,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 581,
                    "end": 584,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 584,
                    "end": 586,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 787,
                    "end": 790,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 790,
                    "end": 792,
                    "matchedPaperCorpusId": "222124879"
                },
                {
                    "start": 1034,
                    "end": 1037,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1039,
                    "end": 1042,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1139,
                    "end": 1143,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1234,
                    "end": 1237,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1280,
                    "end": 1284,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 1284,
                    "end": 1287,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 1407,
                    "end": 1411,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1411,
                    "end": 1414,
                    "matchedPaperCorpusId": "54436113"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.308349609375
        },
        {
            "corpus_id": "272987178",
            "title": "Classroom-Inspired Multi-Mentor Distillation with Adaptive Learning Strategies",
            "text": "Knowledge distillation (KD) [1] is a widely adopted model compression technique in deep learning, where a smaller, more efficient student model learns to replicate the behavior of a larger, more complex teacher model. While traditional KD methods [1][2] [3] typically employ a single teacher, multi-teacher (or multi-mentor) distillation has been proposed to further enhance performance by leveraging an ensemble of teachers [4]. This setup is expected to provide richer and more diverse knowledge, improving the student's generalization and robustness. We use the term mentor to describe all networks involved in teaching the student, regardless of their size or role. Despite its potential benefits, multi-mentor distillation faces several significant challenges: \n\nLarge Capacity Gap: Employing multiple large mentors can create a substantial capacity gap between the collective representation power of the mentors and that of the student. This gap can hinder the student's ability to effectively mimic the combined knowledge of the mentors, leading to suboptimal learning outcomes. To bridge this gap, some works [5,6] have introduced intermediate-sized mentors alongside a large teacher. However, smaller mentors may be less effective, potentially introducing additional errors into the student's knowledge. \n\nError Accumulation: The lower performance of smaller mentors can contribute to cumulative errors in the distillation process. This is particularly problematic in sequential distillation frameworks like TAKD (Figure 1(b)), where each mentor teaches only the subsequent smaller model. Such setups can lead to an \"error avalanche,\" where inaccuracies from lower-performing mentors degrade the student's performance [6]. Although DGKD (Figure 1(c)) attempts to mitigate this by allowing each mentor to teach all smaller models and randomly dropping some mentors during training, these strategies can result in valuable information loss and reduced learning efficiency.",
            "score": 0.5227647036296696,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1312
                },
                {
                    "start": 1315,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 31,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 254,
                    "end": 257,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 425,
                    "end": 428,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1117,
                    "end": 1120,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1120,
                    "end": 1122,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 1727,
                    "end": 1730,
                    "matchedPaperCorpusId": "221802641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55517578125
        },
        {
            "corpus_id": "267406362",
            "title": "MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models",
            "text": "Our primary results demonstrate the effectiveness of MAGDI across five reasoning benchmarks over different Single-Teacher (Li et al., 2023;Magister et al., 2023;Fu et al., 2023;Ho et al.,  single-teacher and multi-teacher distillation setups. For the main results, we use Mistral-7B-Instruct as the student model and train task-specific distilled models (see Sec. 5.2 for experiments with multi-task models with larger students belonging to different model families). We report accuracy for each task. Based on Table 1 results, we summarize our main conclusions below, addressing our research question posed in Section 1: How can we distill diverse teacher interactions into a smaller and more efficient student, utilizing the increasing levels of interaction structure in a MAG? \n\nLevel 1: Distillation from multiple teachers outperforms distillation from the single strongest teacher. Knowledge distillation from the correct reasoning chains of multiple teachers i.e., GPT-4, Bard, and Claude2, outperforms distillation from the single strongest teacher by an average of 1.50% (see MAGDI-R0 row versus SIT-GPT4 row). Different teachers bring diversity in their reasoning, leading to improved reasoning capabilities of the student model. \n\nLevel 2: Distillation from pre-and post-interaction reasoning outperforms only pre-interaction reasoning. Training a student model on all correct reasoning chains after multiple teacher models have interacted further improves the student's reasoning. Fine-tuning on post-interaction reasoning chains effectively increases the training data and its usefulness is validated through an overall 2.54% improvement, compared to the best single-teacher model (see MAGDI-CN row versus SIT-GPT4 row). \n\nLevel 3: Negative reasoning chains help. Additionally, learning by contrasting between positive and negative chains improves over all previous levels. Our MAGDI-AN model obtains an overall average improvement of 3.20% (see MAGDI-AN row versus SIT-GPT4 row). \n\nLevel 4: Structured distillation from interactions outperforms all multi-teacher baselines.",
            "score": 0.5226747403775577,
            "section_title": "Main Results",
            "char_start_offset": 25888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1238
                },
                {
                    "start": 1241,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1992
                },
                {
                    "start": 1995,
                    "end": 2086
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 139,
                    "matchedPaperCorpusId": "259251773"
                },
                {
                    "start": 139,
                    "end": 161,
                    "matchedPaperCorpusId": "254823156"
                },
                {
                    "start": 161,
                    "end": 177,
                    "matchedPaperCorpusId": "256390607"
                },
                {
                    "start": 177,
                    "end": 187,
                    "matchedPaperCorpusId": "254877399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.290771484375
        },
        {
            "corpus_id": "255546131",
            "title": "ERNIE 3.0 Tiny: Frustratingly Simple Method to Improve Task-Agnostic Distillation Generalization",
            "text": "Larger Teacher or Better Knowledge? There are many ways to improve the performance of the student. One straightforward way is to enlarge the teacher to obtain a stronger teacher with better downstream performance: for example, MiniLMv2-L outperforms MiniLMv2-B by simply switching to a larger teacher, shown in Table 4. However, several works (Mirzadeh et al., 2020;Li et al., 2021) witness failure when the size gap between student and teacher are too large. On the other hand, we propose to enhance the teacher with more generalizable knowledge through MTL. Table 4 shows that although our teacher has inferior performance compared to RoBERTa Large , the produced student shows better performance on indomain and out-domain tasks. This suggests that besides chasing after a larger and stronger teacher, we should also pay attention to the knowledge embedded in the teacher. \n\nPerformance on Chinese Datasets To verify the effectiveness of MITKD on Chinese datasets, we apply MITKD on ERNIE 3.0 (Sun et al., 2021) to produce ERNIE 3.0 Tiny. In particular, we follow ERNIE",
            "score": 0.5222824630047767,
            "section_title": "Tasks and Datasets",
            "char_start_offset": 12064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1072
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 366,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 366,
                    "end": 382,
                    "matchedPaperCorpusId": "237605152"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.161376953125
        },
        {
            "corpus_id": "245855883",
            "title": "WeChat Neural Machine Translation Systems for WMT21",
            "text": "Knowledge Distillation (KD) has proven to be a powerful technique for NMT (Kim and Rush, 2016;Wang et al., 2021) to transfer knowledge from the teacher model to student models. In particular, we first use the teacher models to generate synthetic corpus in the forward direction (i.e., En\u2192Zh). Then, we use the generated corpus to train our student models. \n\nNotably, Right-to-Left (R2L) knowledge distillation is a good complement to the Left-to-Right (L2R) way and can further improve model performance.",
            "score": 0.5220742461468111,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 504
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 112,
                    "matchedPaperCorpusId": "235212428"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2841796875
        },
        {
            "corpus_id": "265128505",
            "title": "DONUT-hole: DONUT Sparsification by Harnessing Knowledge and Optimizing Learning Efficiency",
            "text": "Alternatively training of small models has empirically been shown to be a hard optimization problem Ba and Caruana [2014] while also being limited in their expressivity. \n\nTo circumvent this issue, lately there has been significant interest in utilizing knowledge distillation Hinton et al. [2015] for model compression. The works in knowledge distillation can be classified into two distinct categories of task-oriented and task-agnostic approaches. Kim et al. proposed a sequence-level knowledge distillation for sequence-to-sequence language models Kim and Rush [2016]. In Xia et al. [2022] a task-oriented approach utilizing structured pruning has been introduced which jointly prune coarse-and fine-grained layers in the network, to effectively control the pruning decision of each parameter. To facilitate knowledge transfer from un-pruned to pruned models during optimization they suggested a layer-wise distillation strategy. In Chen et al. [2021] proposed a novel task-agnostic approach for sharing knowledge between teacher and student networks by utilizing connection path cross levels. Zafrir et al. presented an architecture-agnostic method for producing compressed up-stream language models using unstructured pruning Zafrir et al. [2021]. Huang et al. suggested a loss function based on correlation that explicitly captures the inherent relationships between different classes, as provided by the teacher Huang et al. [2022]. Also, they discussed that large gap between the student's and the teacher's size negatively impacts the knowledge distillation results. To alleviate this issue in Liu et al. [2022] an Embedding Assistant module was proposed, which aids to build a pseudo teacher assistant model by combining the student's transformer blocks and seamlessly bridge teacher and student models. Exploring the literature on knowledge distillation reveals the potential benefits of upstream pruning over downstream pruning Zafrir et al. [2021], Liang et al. [2023]. \n\nIn this work, we aim to use the aforementioned methods to reduce the model density and size and consequently computational requirements of DONUT while preserving its performance for reading and document image KIE applications.",
            "score": 0.5220428658538258,
            "section_title": "arXiv:2311.05778v1 [cs.CV] 9 Nov 2023",
            "char_start_offset": 2197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1983
                },
                {
                    "start": 1986,
                    "end": 2212
                }
            ],
            "ref_mentions": [
                {
                    "start": 552,
                    "end": 571,
                    "matchedPaperCorpusId": "141460329"
                },
                {
                    "start": 937,
                    "end": 955,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 1232,
                    "end": 1252,
                    "matchedPaperCorpusId": "243938339"
                },
                {
                    "start": 1420,
                    "end": 1439,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 1941,
                    "end": 1961,
                    "matchedPaperCorpusId": "243938339"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.376953125
        },
        {
            "corpus_id": "266210136",
            "title": "Unraveling Key Factors of Knowledge Distillation",
            "text": "In knowledge Distillation, the choice of student decoding strategies profoundly affects the results. We focused on evaluating three main decoding strategies: Beam Search, Greedy Search, and Teacher Forcing. \n\n1) Experimental Setup and Methodology: In our investigation of the impact of decoding strategies on knowledge distillation, we conducted experiments across a range of datasets, including IWSLT14 de\u2192en, IWSLT14 en\u2192de, sentence-level distilled versions of these datasets, and WMT14 en\u2192de. Each dataset was tested under two conditions: a 1k random sample (IWSLT14-1k) and the full test set (IWSLT14-test). The performance was evaluated using the BLEU metric to assess the effectiveness of different decoding strategies: Beam Search, Greedy Search, and Teacher Forcing. 3) Summary: The analysis of the results, as shown in Figure 3, reveals several key insights: \n\n\u2022 Beam Search vs. Greedy Search: Beam Search consistently outperforms Greedy Search across all datasets and subsets. This suggests that the exploration of multiple paths in decoding, as done in Beam Search, is generally more effective for knowledge distillation. \u2022 Teacher Forcing's Superiority: Teacher Forcing shows a significant advantage, particularly in smaller datasets. Its ability to leverage correct historical outputs seems to contribute to its effectiveness, indicating its potential as a powerful decoding strategy in knowledge distillation. \u2022 Variation Across Datasets: The preference for specific decoding strategies varies with the dataset. For example, IWSLT14 de\u2192en shows better results with Beam Search, while IWSLT14 en\u2192de and WMT14 en\u2192de favor Teacher Forcing. This highlights the influence of datasetspecific characteristics on the effectiveness of decoding strategies. \u2022 Impact of Sentence-Level Distillation: The influence of sentence-level distillation on decoding strategies is mixed. \n\nIn some cases, it enhances performance, while in others, it leads to a decrease. This suggests that the compatibility of distillation methods with decoding strategies can vary.",
            "score": 0.5219228204862391,
            "section_title": "D. Impact of the Student Model's Decoding Strategies on Distillation Performance",
            "char_start_offset": 14541,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 206
                },
                {
                    "start": 209,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1879
                },
                {
                    "start": 1882,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2058
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343017578125
        },
        {
            "corpus_id": "278501800",
            "title": "Knowledge Distillation for Enhancing Walmart E-commerce Search Relevance Using Large Language Models",
            "text": "\u2022 We propose a knowledge distillation framework in which the teacher model is trained with soft-labeling, enabling more effective distillation through the teacher's single output. The distillation process uses a extended Margin-MSE loss on an augmented, large-scale unlabeled dataset. Building on the work of Hofst\u00e4tter et al. [7], where both the student and teacher models share the same training data and require true labels for distillation, we significantly expand the dataset used to train the student model. Specifically, we generate a large-scale unlabeled dataset and use the teacher model's predictions to label the data. The Margin-MSE loss is then computed on all item pairs for a given query, eliminating the need for true labels. \u2022 We conduct an ablation study that reveals the following findings: (1) using Margin-MSE loss to align the margin with the teacher model significantly improves knowledge distillation compared to using pointwise cross-entropy (CE) loss. \n\n(2) distilling knowledge through a large-scale dataset labeled with the teacher's predictions leads to a substantial performance boost, enabling the student model to achieve results comparable to the teacher model. ( 3) the student model performs on par with the teacher model, even without the 'item description' field, which has been shown to be beneficial. These results demonstrate that increasing the model's capacity is not necessary to effectively learn from the teacher model. \u2022 Our proposed model has been successfully deployed in production at Walmart.com with significantly positive metrics. \n\nThe rest of this paper is organized as follows: in Section 2, we review related work and discuss the previous production model. Section 3 outlines our model architecture and loss functions. In Section 4, we present our offline experiment setup and evaluation metrics, followed by the online test results in Section 5. Finally, Section 6 concludes with a discussion of future work.",
            "score": 0.5217190477441557,
            "section_title": "Introduction",
            "char_start_offset": 4234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1583
                },
                {
                    "start": 1586,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59619140625
        },
        {
            "corpus_id": "248683566",
            "title": "Teacher-student collaborative knowledge distillation for image classification",
            "text": "Ensembles have been widely used to improve model performance [27][28][29]. Since different models could be complementary, the outputs of multiple models with the same structure and different initialization training ensembles can be used to improve test performance. Several studies have found that ensembles are also effective in improving knowledge distillation performance. [30] found that ensemble teacher networks can effectively improve student network classification performance. To overcome offline-distillation issues, a strong teacher network is needed, [31] combined knowledge distillation and an ensemble approach to train a multi-branch network and then built a strong teacher model based on the branches of the ensemble to enhance the learning capabilities of the target network. However, this approach undoubtedly leads to a complex teacher model. In contrast with multiple teacher network ensembles [32], our student network constructs multiple exit outputs from shallow to deep, and only a small number of parameters are added to achieve the effect of multiple model ensembles. \n\nFinally we use ensembles to vote on the classification results of multiple sub-models in the student network, and obtain a strong classifier.",
            "score": 0.5215825496569025,
            "section_title": "Ensemble",
            "char_start_offset": 8904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1237
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 65,
                    "matchedPaperCorpusId": "10765854"
                },
                {
                    "start": 65,
                    "end": 69,
                    "matchedPaperCorpusId": "15455464"
                },
                {
                    "start": 69,
                    "end": 73,
                    "matchedPaperCorpusId": "11149239"
                },
                {
                    "start": 914,
                    "end": 918,
                    "matchedPaperCorpusId": "224818016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5458984375
        },
        {
            "corpus_id": "235262724",
            "title": "Distilling Knowledge via Intermediate Classifiers",
            "text": "There has been growing interest in model compression techniques for compressing the larger neural networks to smaller ones. Model pruning, as an optimization framework, intends to reduce the unnecessary structure and parameters of a large model to gain an efficient smaller network [3]. Model quantization compresses the neural network by reducing the precision of its parameters at the expense of accuracy [29]. Designing efficient deep neural architectures [22] intends to satisfy the specified accuracy requirement given resource limitations. Knowledge distillation [10], that can be combined with others, aims to preserve the accuracy of a large trained teacher model in a smaller student model. Knowledge Distillation. Knowledge distillation is a training framework in which a huge pre-trained teacher model transfers its knowledge to a small student network [4,10] to preserve teacher's accuracy in a resource-limited student model. The student trained under this framework is expected to have higher accuracy compared to if it was trained by the conventional training framework. The student owes this success to the teacher's output class probabilities for each input data. The relative class probabilities tell the student how the teacher generalizes and views the similarities between classes for an input sample. Many different extensions to knowledge distillation are proposed to leverage intermediate representations [16,36,20], shared representations [33,31], representation grafting [18], adversarial learning [9,27,28], and contrastive learning [24]. Some other variants have improved the robustness of the student by injecting noise in the teacher's outputs [17], aggregating multiple teachers' knowledge by voting mechanisms [32], deploying teacher assistants [14], or allowing peers to teach each other [38]. Our work is closely related to those extensions of knowledge distillations, which exploit the knowledge of intermediate representations (e.g., [16,36,20,33]). We attempt to address their practical challenges when (1) the dimension mismatch between student's and teacher's intermediate layers is large, (2) the choices of intermediate/shared layers are not obvious for the teacher and student models. \n\nKnowledge Distillation via Intermediate Classifiers.",
            "score": 0.5215421155736869,
            "section_title": "Related Work",
            "char_start_offset": 3016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2226
                },
                {
                    "start": 2229,
                    "end": 2281
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 285,
                    "matchedPaperCorpusId": "212628335"
                },
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 864,
                    "end": 867,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 867,
                    "end": 870,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1429,
                    "end": 1433,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1433,
                    "end": 1436,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1436,
                    "end": 1439,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 1464,
                    "end": 1468,
                    "matchedPaperCorpusId": "28101867"
                },
                {
                    "start": 1468,
                    "end": 1471,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1524,
                    "end": 1527,
                    "matchedPaperCorpusId": "21679091"
                },
                {
                    "start": 1527,
                    "end": 1530,
                    "matchedPaperCorpusId": "53976534"
                },
                {
                    "start": 1530,
                    "end": 1533,
                    "matchedPaperCorpusId": "19182852"
                },
                {
                    "start": 1560,
                    "end": 1564,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1742,
                    "end": 1746,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1777,
                    "end": 1781,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1821,
                    "end": 1825,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1970,
                    "end": 1974,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1974,
                    "end": 1977,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1977,
                    "end": 1980,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 1980,
                    "end": 1983,
                    "matchedPaperCorpusId": "28101867"
                },
                {
                    "start": 2040,
                    "end": 2043,
                    "matchedPaperCorpusId": "9794990"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65771484375
        },
        {
            "corpus_id": "273186993",
            "title": "Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher",
            "text": "In contrast, a weaker teacher, together with a smaller performance gap from the student, has been shown to be a better choice (Son et al., 2021;Yang et al., 2019b;Mirzadeh et al., 2020;Gao et al., 2020;Wang et al., 2022). In fact, it can be theoretically proved by (Wang et al., 2022) that weak models \"have higher mutual information regarding the input\" compared to stronger teacher models, which can enhance knowledge distillation. Interestingly, this phenomenon can be intuitively understood, just like it is hard for a child to learn advanced mathematics directly from a senior professor. Instead, it would be better to teach him/her to count or learn elementary mathematics first. In other words, for distillation, a student model should learn from a teacher that has an appropriate knowledge/performance gap over it. Nevertheless, even with a relatively weak teacher that has a small performance gap, a fixed model that is commonly used would eventually become useless since the student will gradually improve and surpass the teacher at some time. Thus, how to adaptively control the performance gap within a reasonable range throughout the whole distillation process is a critical problem. \n\nTo address this, we seek to explicitly control the performance gap to boost knowledge transfer. However, it is non-trivial to determine what performance gap should be regarded as \"too-large\". Although the gap can be measured by some criteria, e.g., accuracy for classification, the threshold to distinguish a too-large gap is still ambiguous. To solve this, we cast the problem of determining a threshold into measuring the gap in terms of model size between student and teacher. We propose to introduce a learnable dynamic teacher (DT) model besides the pre-trained static teacher model, training it from scratch together with the student. Since DT is a larger model and often converges faster than the student, we are able to keep a promising performance gap between DT and student during training. In addition, we hope to build a stronger connection between DT and student to transfer knowledge in a more explicit way, in order to further enhance the performance. To achieve this, we develop a hard strategy for distillation that enforces DT and the student to share their parameters and encourage parameter inheritance.",
            "score": 0.5208482371959402,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1438,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1196
                },
                {
                    "start": 1199,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2322
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 144,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 144,
                    "end": 163,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 163,
                    "end": 185,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 202,
                    "end": 220,
                    "matchedPaperCorpusId": "252846591"
                },
                {
                    "start": 265,
                    "end": 283,
                    "matchedPaperCorpusId": "252846591"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "258762557",
            "title": "Student-friendly Knowledge Distillation",
            "text": "The results on the CIFAR-100 dataset are shown in Table 3 and Table 4, where Table 3 shows the results where the teacher and student models had the same architectures, and Table 4 shows the results where the teacher and student models had different architectures. \n\nComparing the experimental results of KD with those of SKD, SKD performs significantly better when using the same distillation loss function as KD. The largest improvement occurred when the teacher model was ResNet32\u00d74 and the student model was ResNet8\u00d74. The improvement reached 3.28%. This indicates that SKD can improve the learning performance of the student model simply by modifying the logits of the teacher model. \n\nOf the six experiments where the teacher and student models had the same architectures, SKD achieved the best performance compared to all feature-based and logit-based methods in five experiments. It ranked third in only one experiment where the feature-based method OFD [13] and Furthermore, SKD outperformed other state-of-the-art knowledge distillation methods, such as ReviewKD [6] and DKD [53], in 8 and 9 out of 11 teacher-student model combination experiments, respectively, achieving the best performance. Only in one teacher-student model combination did SKD not achieve a top-two result. This suggests that SKD can achieve the best distillation effect while keeping the design and training simple. \n\nThe results on the ImageNet dataset are shown in Table 5. Our SKD continued to perform significantly better than the classical KD. Moreover, compared with other distillation methods, SKD achieved the first and second-best results in two experiments based on top-1 accuracy and the second and third-best results in two experiments based on top-5 accuracy. This suggests that the performance of SKD is superior to that of most of the current best methods and that it achieved the best performance among logit-based methods.",
            "score": 0.5203958115346432,
            "section_title": "Comparison with state-of-the-art methods",
            "char_start_offset": 13230,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 266,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 687
                },
                {
                    "start": 690,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1397
                },
                {
                    "start": 1400,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1921
                }
            ],
            "ref_mentions": [
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1072,
                    "end": 1075,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 1084,
                    "end": 1088,
                    "matchedPaperCorpusId": "247476179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7509765625
        },
        {
            "corpus_id": "253116921",
            "title": "Semi-Supervised Learning Based on Reference Model for Low-resource TTS",
            "text": "Knowledge distillation (KD) [34] can make student modle get the information from the teacher model. Its success is usually attributed to the privileged information of similarity between the class distribution of the teacher model and the student model. It was first proposed by Hinton et al. [34] transfer knowledge from large teacher networks to smaller student networks. It works by training students to predict target classification labels and imitate teachers' class probabilities because these features contain additional information about how teachers generalize [34].\n\nLiu et al. [35] tried the method of the teacher-student model for resolving the problem of exposure bias. There is an existing problem of exposure bias of autoregressive, due to the unmatched training and inference phase. This problem cloud leads to an unpredictable error for the model during the inference and accumulates the error frame by frame along the time axis.",
            "score": 0.5203302722562942,
            "section_title": "A. Knowledge Distillation",
            "char_start_offset": 4599,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 587,
                    "end": 591,
                    "matchedPaperCorpusId": "207847470"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4677734375
        },
        {
            "corpus_id": "235262724",
            "title": "Distilling Knowledge via Intermediate Classifiers",
            "text": "Improvement on knowledge distillation. We first examine if distillation via intermediate heads (DIH) improves canonical knowledge distillation (KD) [10] over different sets of student-teacher pairs with a relatively large capacity gap. For this purpose, we trained Res8 as a small student with teachers of various depths and architectures. Table 2 shows the accuracy of students for KD, DIH, and regular cross-entropy (CE) training (i.e., no distillation) for three datasets. DIH has improved both KD and CE in all datasets for any teacher-student pairs. The improvements are more noticeable for more complex datasets (e.g., Tiny-ImageNet and CIFAR-100) and when the capacity gap is larger. For example, compare the absolute improvements between Res110 and Res34 teachers: 0.14% vs. 0.74% in CIFAR-10 and 1.05% vs. 1.38% in CIFAR-100. We also note that KD fails to improve the student model when both the number of classes and the capacity gap are large. For example, KD optimizes the Res8 student poorly with Res34 teacher in Tiny-ImageNet when compared to regular cross-entropy (40.01% vs. 40.45%). However, DIH outperforms both CE and KD using the same settings by a large margin of 2.55 and 2.99, respectively. \n\nDIH vs. KD variants. We also study how DIH compares to other variants and extensions of knowledge distillation, which either take the advantage of intermediate representations for knowledge distillation or intend to address the capacity gap issue: FitNets [16], Attention distillation (AT) [36], Teacher Assistant Knowledge Distillation (TAKD) [14], Multi-head Knowledge Distillation (MHKD) [26], Task-Oriented Feature Distillation (TOFD) [37], and Contrastive Representation Distillation [25]. 9 Note that MHKD and TOFD, similar to DIH, benefit from intermediate classifier heads, and TAKD is specifically designed to address the capacity gap.",
            "score": 0.5201341564305283,
            "section_title": "Results",
            "char_start_offset": 15861,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 152,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1473,
                    "end": 1477,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1507,
                    "end": 1511,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1561,
                    "end": 1565,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1656,
                    "end": 1660,
                    "matchedPaperCorpusId": "227276466"
                },
                {
                    "start": 1706,
                    "end": 1710,
                    "matchedPaperCorpusId": "204838340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53564453125
        },
        {
            "corpus_id": "266174074",
            "title": "Dynamic Corrective Self-Distillation for Better Fine-Tuning of Pretrained Models",
            "text": "This flexibility in the choice of teacher models allows for broader applicability and ease of implementation within the DCS framework. \n\n\u273d How can we expect the student network to achieve better results on downstream tasks if we choose a teacher network that is not performing well? \n\n\u27a0 Knowledge distillation can still be beneficial for the student network's optimization process. The main purpose of knowledge distillation is to transfer valuable insights learned by the teacher down to the student (Furlanello et al., 2018;Stanton et al., 2021). Although the teacher network did not achieve high performance on its own, it still provides valuable patterns and special relationships in the data. By considering this distilled knowledge, the student has the ability to improve its performance and achieve better results on the downstream task. One main thing to note, is that in our DCS framework, we perform a weighted distillation in which we give higher weights on discordant samples. The latter can be one of the following scenarios: (a) The teacher network correctly classified the sample whereas the student misclassified the sample, or (b) vice versa. In this case, the student network gets to attend to both scenarios and learn from both the good and the bad throughout its learning process, which contributes to its effectiveness and robustness. Furthermore, it is worth mention-ing that our student networks receive guidance solely from the soft labels provided by the teacher but also from the hard labels known as the ground truth labels. This dual guidance alongside the re-weighting strategy serves as an effective regularization strategy. \n\n\u273d What type of knowledge are you using for distilling information from teacher to student? \n\n\u27a0 In DCS, we solely use response-based knowledge. We mainly exploit the logits (i.e., the raw pre-softmax outputs) to align the knowledge between the teacher and the student networks. \n\nOur main goal is to propose a straightforward and less complicated framework for fine-tuning. Therefore, we opted for this type of knowledge rather than feature based distillation, which can bring more computational complications to the overall framework. In addition, by aligning the logits, the student model can gain more insights into the finer details of the task guided by the teacher model.",
            "score": 0.519796534152567,
            "section_title": "Conclusion",
            "char_start_offset": 12438,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 137,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1654
                },
                {
                    "start": 1657,
                    "end": 1747
                },
                {
                    "start": 1750,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1933
                },
                {
                    "start": 1936,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2191
                },
                {
                    "start": 2192,
                    "end": 2333
                }
            ],
            "ref_mentions": [
                {
                    "start": 501,
                    "end": 526,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59765625
        },
        {
            "corpus_id": "260802774",
            "title": "Efficient RGB-T Tracking via Cross-Modality Distillation",
            "text": "fusion strategy as that in the teacher model [35]. In particular, each layer adopts the same fusion strategy in the student model. It can be seen that the exploitation of the multi-path fusion strategy can well improve the performance of the student model. In addition, the performance gap between 'Student-MPSD' and 'Student-TF' is much smaller, which indicates that our proposed MPSD module can better mimic the fused features in the teacher model to compensate for the performance penalty from simple fusion operations.\n\nTeacher-Student knowledge distillation experiments. Table 4 shows the performance of using some other knowledge distillation methods in the feature extraction and feature fusion stages for comparisons, including KD [8], Fit-Nets [21], ReviewKD [3] and MD [29]. It is observed from Table 4 that the proposed distillation strategy performs the best. Due to the absence of cross-modal interactions, these existing knowledge distillation methods usually achieve some modest performance gains. In addition, we notice that the student model with a single-stream feature extractor performs obviously well than the student model with a two-stream feature extractor after knowledge distillation. This may be due to the fact that the single-stream network can narrow the modality difference to a certain extent and better acquire the knowledge from the teacher model.",
            "score": 0.518692978818579,
            "section_title": "Ablation Experiments and Analyses",
            "char_start_offset": 25851,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 49,
                    "matchedPaperCorpusId": "249800120"
                },
                {
                    "start": 768,
                    "end": 771,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 779,
                    "end": 783,
                    "matchedPaperCorpusId": "246868573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5810546875
        },
        {
            "corpus_id": "269761365",
            "title": "Exploring Graph-based Knowledge: Multi-Level Feature Distillation via Channels Relational Graph",
            "text": "In the field of deep learning, the impressive success of large neural networks has come at the cost of increased computational complexity, which poses significant challenges for deployment in resourceconstrained environments.While these heavyweight models, often referred to as teacher networks, set state-of-the-art benchmarks on various tasks, their practical applicability is limited by their demanding requirements for memory, processing power, and energy.Knowledge Distillation (KD) [16] emerges as a promising solution to address this dichotomy by transferring the knowledge from a cumbersome model to a more compact and efficient student networks.\n\nThe quintessence of knowledge distillation lies in its ability to encapsulate the representational power of a larger model into a smaller one without incurring a substantial loss in performance.Pioneered by Hinton et al. [16], the process involves training a smaller model to mimic the behavior of the pre-trained larger model by softening the output logits [19], thus leveraging the rich information embedded in the output distributions of the teacher network.\n\nRecent advances in KD techniques have extended beyond the mere replication of output distributions.Contemporary works explore the distillation of intermediate representations [1,49,31], attention mechanisms [50,20], and even the inculcation of adversarial robustness from teacher to student [33,55,37].The underlying hypothesis is that the intermediate layers of a neural network embody a wealth of information about the data manifold that, when transferred effectively, can endow the student with nuanced understanding akin to its teacher.\n\nLarge and complex teacher models often capture a wealth of features and deep information, crucial for enhancing task performance [7].However, attempting to directly distill this rich information into smaller capacity student models often results in suboptimal performance reproduction due to the student model's capacity limitations.The student model may struggle to process the complex information present in the teacher model, leading to ineffective distillation.Moreover, simply mimicking all features of the teacher model overlooks the differences in relationships and structures between the information, particularly when there is a significant gap between the teacher and student models [12].",
            "score": 0.5185738023716493,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 654
                },
                {
                    "start": 656,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 1117
                },
                {
                    "start": 1119,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1421
                },
                {
                    "start": 1421,
                    "end": 1659
                },
                {
                    "start": 1661,
                    "end": 1794
                },
                {
                    "start": 1794,
                    "end": 1994
                },
                {
                    "start": 1994,
                    "end": 2126
                },
                {
                    "start": 2126,
                    "end": 2359
                }
            ],
            "ref_mentions": [
                {
                    "start": 1014,
                    "end": 1018,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 1294,
                    "end": 1297,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1297,
                    "end": 1300,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "198185886"
                },
                {
                    "start": 1330,
                    "end": 1333,
                    "matchedPaperCorpusId": "125950115"
                },
                {
                    "start": 1414,
                    "end": 1417,
                    "matchedPaperCorpusId": "237194985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "247596810",
            "title": "DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization",
            "text": "The second task we consider is knowledge distillation, where we train a smaller student model to mimic the behavior of a larger teacher model; specifically, we want to reproduce the output logits, attentions, and hidden states of the teacher model. Following Shleifer and Rush (2020), we initialize the student model by copying the weights from maximally spaced layers of the teacher model, e.g., when initializing a 3-layer student encoder (decoder) from a 6-layer teacher encoder (decoder), we copy the 0th, 3th and 5th layers from the teacher to the student. When copying only 1 layer, we choose the last instead of the first, which has been shown empirically to yield better performance. Different than Shleifer and Rush (2020) who only distill the decoder, we distill both the encoder and the decoder. After initialization, we fine-tune the student model with the combined objective of task loss and distillation loss, i.e. L data + L dist , with\n\nwhere the RHS are MSE losses measuring the difference between the student and teacher with regard to output logits, attention scores (including  We abbreviate the number of bits for weights, word embedding and activations as \"W-E-A (#bits)\", followed by the number of encoder and decoder layers as \"E-D (#layers)\". We use the rouge-{1,2,L} as evaluation metrics (Lin, 2004). We found that distillation-aware quantized models achieves comparable or even better performance compared with the full precision models, and combining quantization and distillation, e.g., from \"2-2-8 6-6\" to \"2-2-8 6-3\", gives us a further boost in model footprint compression ratio without significant sacrifice in performance. See \u00a73.2 for details. encoder attention, decoder attention and cross attention), and hidden states (including all encoder and decoder layers). 1 We include the details of the loss in Appendix B for completeness.",
            "score": 0.5182749839640356,
            "section_title": "Distillation",
            "char_start_offset": 5893,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1315,
                    "end": 1326,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52880859375
        },
        {
            "corpus_id": "249847955",
            "title": "Improving Generalization of Metric Learning via Listwise Self-distillation",
            "text": "Conventional Knowledge Distillation (KD) [20], [21], [22] methods aim at model compression: they use knowledge from the teacher model to generate compact models, achieving a good performance-efficiency trade-off. Further, some works has found that the student model can outperform the teacher model if the student is configured with the same capacity as its teacher. Born Again Network [5] proposed to use pretrained models as teacher models to supervise the training of student models. [23] proposed deep mutual learning in which an ensemble of students learn collaboratively and teach each other throughout the training process. PS-KD [24], BYOT [25] and CS-KD [26] proposed the self-distillation where students becoming teachers themselves. [27] proposed to apply random-walk to refine the output of model as refined learning target. These works produced student models that achieved better accuracy on classification problems. More recently, some works start to focus attention on retrieval task: A novel work [6], named Born Again neural Rankers (BAR), applied born-again-based knowledge distillation technology on learning to rank (LTR) task. Another work [28], called Simultaneous Similarity-based Self-distillation (S2SD), extends DML with knowledge distillation from auxiliary, high-dimensional embedding and feature spaces to leverage complementary context during training while retaining testtime cost. Remarks. Here, we would like to highlight the marked difference between our LSD and most existing KD methods. First, most existing knowledge distillation methods, either for model compression or performance improvement, focus on classification tasks, while our LSD is designed for metric learning. Second, these approaches typically use teachers to predict the probability distribution over all classes and then use this \"dark knowledge\" to enhance students. Therefore, these methods require classifiers or memory to output the probability distribution and are not applicable to most metric learning methods. Since our LSD does not require an additional classifier or memory module, it is more general. \n\nBesides, the closely related works are BAR [6] and S2SD [27]: 1).",
            "score": 0.5166284650923602,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 7298,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2116
                },
                {
                    "start": 2119,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 57,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 487,
                    "end": 491,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 637,
                    "end": 641,
                    "matchedPaperCorpusId": "233714221"
                },
                {
                    "start": 648,
                    "end": 652,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 663,
                    "end": 667,
                    "matchedPaperCorpusId": "214727822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44921875
        },
        {
            "corpus_id": "259378480",
            "title": "Review of Recent Distillation Studies",
            "text": "The use of Simple Knowledge Distillation (SimKD) allows the compression of powerful teacher models into lightweight student models without sacrificing performance. This is achieved by reusing the discriminative classifier of the teacher model for student inference and training the student encoder by feature alignment with a single loss. A projector was also developed to help match the student encoder with the teacher classifier, making the technique applicable to a variety of teacher and student architectures. Experiments show that with the addition of the projector, the technique achieves state-of-the-art results at the cost of a reduced compression ratio. Vanilla KD aligns the logit or class projections of the two models, but the performance gap between the original teacher model and the refined student model is still large. Various methods have been proposed to overcome this problem, but they require detailed knowledge representation and optimized hyperparameters. The SimKD technique is proposed, which trains the student model by feature alignment in the previous layer of the classifier and directly replicates the teacher classifier for student inference. On a standard benchmark dataset, SimKD outperforms all state-of-the-art methods compared with various combinations of teacher-student architectures. The concept of knowledge distillation (KD), a technique for compressing knowledge from a powerful teacher model into a smaller student model, is discussed here. Transferred knowledge, usually in the form of soft targets, is thought to capture the relationships between different categories and serve as effective regularization during student training. Feature distillation is a common solution to prevent performance degradation in teacherto-student compression, which utilizes more information from the middle layer of the teacher model. One of the proposed methods, SimKD, is related to hypothetical transfer learning (HTL) and aims to close the performance gap between teachers and students on the same dataset. Here, by analyzing the limitations of the existing method and focusing on its improvement, a new method, logit distillation, is proposed, and the knowledge distillation loss function is re-represented as a weighted sum of two components, target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD).",
            "score": 0.5165954708734831,
            "section_title": "Simple knowledge distillation",
            "char_start_offset": 36280,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2374
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "235390933",
            "title": "Does Knowledge Distillation Really Work?",
            "text": "While knowledge distillation does often improve generalization, understanding the relationship between fidelity and generalization, and how to maximize fidelity, is important for several reasonsincluding better generalization! \n\nBetter generalization in distilling large teacher models and ensembles. Knowledge distillation was initially motivated as a means to deploy powerful models to small devices or low-latency controllers [e.g., 10,19,24,46,48]. While in self-distillation generalization and fidelity are in tension, there is often a significant disparity in generalization between large teacher models, including ensembles, and smaller students. We have seen this disparity in Figure 1(b). We additionally show in Figure 10 in Appendix B.1 that as we increase the number of ensemble components, the generalization disparity between teacher and distilled student increases. Improving student fidelity is the most obvious way to close the generalization disparity between student and teacher in these settings. Even if one exclusively cares about student accuracy, fidelity is a key consideration outside self-distillation. \n\nInterpretability and reliability. Knowledge distillation has been identified as a means to transfer representations discovered by large black-box models into simpler more interpretable models, for example to provide insights into medical diagnostics, or discovering rules for understanding sentiment in text [e.g., 21,22,6,30,8]. The ability to perform this transfer could have extraordinary scientific consequences: large models can often discover structure in data that we would not have anticipated a priori. Moreover, we often want to transfer properties such as well-calibrated uncertainties or robustness, which have been well-established for larger models, so that we can safely deploy more efficient models in their place. In both cases, achieving good distillation fidelity is crucial. \n\nUnderstanding. The name knowledge distillation implies we are transferring knowledge from the teacher to the student. For this reason, improved student generalization as a consequence of a distillation procedure is sometimes conflated with fidelity. Decoupling fidelity and generalization, and explicitly studying fidelity, is foundational to understanding how knowledge distillation works and how we can make it more useful across a variety of applications.",
            "score": 0.5164643683834165,
            "section_title": "If distillation already improves generalization, why care about fidelity?",
            "char_start_offset": 12138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 229,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1926
                },
                {
                    "start": 1929,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2387
                }
            ],
            "ref_mentions": [
                {
                    "start": 436,
                    "end": 439,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 439,
                    "end": 442,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 442,
                    "end": 445,
                    "matchedPaperCorpusId": "8451212"
                },
                {
                    "start": 445,
                    "end": 448,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 448,
                    "end": 451,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1450,
                    "end": 1453,
                    "matchedPaperCorpusId": "801539"
                },
                {
                    "start": 1455,
                    "end": 1458,
                    "matchedPaperCorpusId": "57189239"
                },
                {
                    "start": 1458,
                    "end": 1460,
                    "matchedPaperCorpusId": "208526905"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "266450524",
            "title": "Topological persistence guided knowledge distillation for wearable sensor data",
            "text": "We tested the proposed method, TPKD, with different datasets including different sizes of window length, the number of classes, and complexity.In most of the cases, TPKD outperformed baselines in classification.Even with smaller or the different number of classes, TPKD showed the best accuracy.This implies that using orthogonality properties to transfer feature relationship can improve performance significantly.The results also showed that larger capacity of teachers does not guarantee the generation of a better student.This corroborates previous studies [12,56].Among different combinations of teachers and students, in most cases, WRN16-3 teachers distilled a superior student.When the window length of a sample is large and the model performs with a smaller number of classes, the smaller network (WRN16-1) showed good performance.This indicates that large-sized models can increase the knowledge gap, and smaller teachers can perform better for easy problems.For different architectures of teachers, in overall cases, the proposed method generated a better student.However, for WRN28-1 Teacher1 and WRN16-3 Teacher2 on GENEActiv, TPKD without Orth.outperformed with Orth.This showed that TPKD with Orth.performs better when the width of teachers is similar to a student among various combinations of teachers.Also, even though improvements for large Teacher1 with TPKD are smaller than other combinations, TPKD achieved better results compared to baselines.Therefore, when two teachers are similar or the width of teacher models is similar to a student, the proposed method performs better than the other combinations.And, TPKD can alleviate the negative effects from the knowledge gap in distillation.\n\nFor TPKD, best cases showed with different \u03b2 and k across datasets.In general, when k is 4, TPKD performs the best.However, the optimal \u03b2 is different for datasets.For dataset including 40 channels (PAMAP2), when \u03b2 is 200, it showed best performance in overall cases, whereas it is 700 for GENEActiv with 3 channels.Finding optimal parameters for training can consume time which is a limitation of the method.",
            "score": 0.5158767481724692,
            "section_title": "Discussion",
            "char_start_offset": 44739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 415
                },
                {
                    "start": 415,
                    "end": 526
                },
                {
                    "start": 526,
                    "end": 569
                },
                {
                    "start": 569,
                    "end": 685
                },
                {
                    "start": 685,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 969
                },
                {
                    "start": 969,
                    "end": 1075
                },
                {
                    "start": 1075,
                    "end": 1158
                },
                {
                    "start": 1158,
                    "end": 1181
                },
                {
                    "start": 1181,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1319
                },
                {
                    "start": 1319,
                    "end": 1467
                },
                {
                    "start": 1467,
                    "end": 1628
                },
                {
                    "start": 1628,
                    "end": 1712
                },
                {
                    "start": 1714,
                    "end": 1781
                },
                {
                    "start": 1781,
                    "end": 1829
                },
                {
                    "start": 1829,
                    "end": 1878
                },
                {
                    "start": 1878,
                    "end": 2030
                },
                {
                    "start": 2030,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 561,
                    "end": 565,
                    "matchedPaperCorpusId": "245604889"
                },
                {
                    "start": 565,
                    "end": 568,
                    "matchedPaperCorpusId": "53473116"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.369384765625
        },
        {
            "corpus_id": "255546131",
            "title": "ERNIE 3.0 Tiny: Frustratingly Simple Method to Improve Task-Agnostic Distillation Generalization",
            "text": "Pretrained language models (PLMs) (Devlin et al., 2019;Liu et al., 2019d;Clark et al., 2020;He et al., 2021) have achieved great success in a wide range of natural language processing tasks, however, their enormous parameters often bring challenges to serving them in real-life applications where computational resources are limited. \n\nKnowledge distillation (KD) (Hinton et al., 2015) has been widely utilized to tackle this problem. KD aims to compress a large PLM called  teacher into a smaller one called student by transferring knowledge from teacher to student. In the context of PLM compression, KD is usually applied in two different settings: task-specific (Sun et al., 2019;Tang et al., 2019;Jiao et al., 2020;Su et al., 2021) and task-agnostic (Sanh et al., 2019;Wang et al., 2020). The former transfers task-specific knowledge from teacher to student for a given task and often yields student with better performance than the latter, but poses one disadvantage: task-specific KD needs to be performed for every downstream task. Task-agnostic KD, on the other hand, eliminates the need of distillation for every single task by transferring general knowledge to the student in a once-for-all fashion that the student needs to be distilled only once and can be directly finetuned on downstream tasks as similar to the pretrain-finetune paradigm. A natural research question is whether we can combine the advantage of favorable downstream performance and easy-to-deploy from these two types of distillation. \n\nPrevious attempt (Mukherjee et al., 2021) injects downstream task knowledge into task-agnostic distillation by performing task-agnostic distillation on a teacher finetuned in a downstream task. Although this approach can improve the performance of the student, knowledge from a single task may not be sufficient to yield a generalizable student. In this work, we show that the downstream generalization of the student can be further improved by fusing multi-task learning (MTL) into task-agnostic distillation.",
            "score": 0.51529671908747,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 333
                },
                {
                    "start": 336,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2028
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 55,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 73,
                    "end": 92,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 92,
                    "end": 108,
                    "matchedPaperCorpusId": "219531210"
                },
                {
                    "start": 666,
                    "end": 684,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 702,
                    "end": 720,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 774,
                    "end": 792,
                    "matchedPaperCorpusId": "211296536"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.431884765625
        },
        {
            "corpus_id": "226262400",
            "title": "Distilling Multiple Domains for Neural Machine Translation",
            "text": "Our proposed multi-domain knowledge distillation models train deep in-domain teachers and distill these teachers into the shallower students. In this section, we aim to understand how much of the gains seen from the MDKD models can be attributed to the knowledge distillation component. To this end, we train a knowledge distillation baseline model that builds a single deep teacher for the entire data. The training data is distilled using this teacher model and a student is trained on the concatenation of the distilled data and the original data, similarly to the MDKD models. We train unbalanced and upsampled teachers, from which we distill unbalanced and upsampled students, respectively. Table 7 shows the BLEU scores on the DE\u2192EN multi-domain test set for these single-domain knowledge distillation models, as well as for the MDKD models and the unbalanced and upsampled baselines. For both the unbalanced and upsampled cases, the single-domain knowledge distillation approach yields improvements in quality over the baseline, and the multi-domain knowledge distillation models give further improvements. This trend also broadly holds across individual domains. Thus,  we attribute the improved quality of the MDKD models partially but not completely to the knowledge distillation component of the models.",
            "score": 0.515282413310556,
            "section_title": "Improvements Due to Distillation",
            "char_start_offset": 20118,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3046875
        },
        {
            "corpus_id": "252118863",
            "title": "Exploring Target Representations for Masked Autoencoders",
            "text": "Knowledge distillation (KD) is widely employed in model knowledge compression [21], which improves the performance of the smaller student model by distilling the knowledge learned from a well-trained large teacher network. Further study on e.g. relational KD [30], contrastive KD [34], and latent feature KD [33] is conducted to improve the performance of vanilla KD. Beyond its prominence in the field of supervised learning, KD recently cuts a figure in self-supervised learning. Concurrent work manages to adopt conventional feature distillation [40] to match contrastive models with MIM-trained ones. Nevertheless, it shows negligible gains on MIM-trained models such as MAE. BEiT [3], MaskFeat [38] and MVP [39] could be seen as distilling knowledge from dVAE [32], HOG features [11] and language-induced model CLIP [31] within the discourse of MKD, respectively. Until now, there exists no work conferring a system-level study on the importance of how to choose adequate target representation or teacher networks to guide the learning of MKD. Beyond that, we propose using a randomly initialized model as the teacher and bootstraps the teacher for stages, demonstrating superiority over other practices.",
            "score": 0.51522741819874,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1209
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 82,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 259,
                    "end": 263,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 699,
                    "end": 703,
                    "matchedPaperCorpusId": "245218767"
                },
                {
                    "start": 765,
                    "end": 769,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 784,
                    "end": 788,
                    "matchedPaperCorpusId": "206590483"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33447265625
        },
        {
            "corpus_id": "273638702",
            "title": "SWITCH: Studying with Teacher for Knowledge Distillation of Large Language Models",
            "text": "In this section, we introduce SWITCH, a novel approach designed to enhance the knowledge distillation process for language models. We begin by formalizing the problem of sequence-level knowledge distillation and then delve into the specifics of SWITCH, explaining how it addresses the limitations of traditional student-generated output (SGO) methods. Our methodology emphasizes the strategic involvement of the teacher model during sequence generation, aiming to reduce the accumulation of errors and improve the overall performance of the student model.",
            "score": 0.5147688777234175,
            "section_title": "Methodology",
            "char_start_offset": 4656,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 555
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58935546875
        },
        {
            "corpus_id": "274655747",
            "title": "Optimising TinyML with quantization and distillation of transformer and mamba models for indoor localisation on edge devices",
            "text": "Knowledge distillation (KD) is a model compression technique in which a smaller, less complex model (referred to as the \"student\") is trained to replicate the behaviour of a larger, more complex model (referred to as the \"teacher\") 10 . The process involves training the student model to mimic the teacher model's predictions rather than relying on the original training data labels. This approach allows the student model to achieve similar performance levels to the teacher model while significantly reducing computational resources required for inference. \n\nTraditional model compression techniques, such as pruning and quantization, primarily focus on reducing model size by removing redundant parameters or lowering numerical precision. While effective, these methods may lead to a substantial loss in model accuracy, particularly for complex tasks. In contrast, knowledge distillation leverages the rich information embedded in the teacher model's output to train the student model, providing more nuanced guidance and helping the student model to generalize better. KD also offers significant flexibility in both the size and architecture of the teacher and student models, making it a superior model compression technique. \n\nThe teacher model can be any large, high-accuracy model with no constraints on its architecture, allowing the use of state-of-the-art models. The student model, in contrast, can be smaller and simpler, tailored to the specific constraints of the deployment environment, such as limited memory and computational power. This flexibility enables KD to significantly reduce the student model's size and computational complexity without substantial performance degradation. The architectures of the teacher and student models can differ, with the teacher being a complex model like a state-of-the-art transformer, and the student being a simpler neural network, such as a small transformer with a minimal attention core. This adaptability makes KD highly suitable for various use cases and deployment constraints. \n\nWe perform knowledge distillation using a distillation loss to guide the student model to mimic the behaviour of the teacher model, as demonstrated in Eq. ( 4): \n\nHere, \u03b1 is the proportion between student loss (LS) and distillation loss (LD). LS is calculated from the student predictions matching the raw labels, while LD measures the match between teacher and student predictions.",
            "score": 0.5147633983786529,
            "section_title": "Knowledge distillation",
            "char_start_offset": 26575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 558
                },
                {
                    "start": 561,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2041
                },
                {
                    "start": 2044,
                    "end": 2204
                },
                {
                    "start": 2207,
                    "end": 2286
                },
                {
                    "start": 2287,
                    "end": 2426
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.630859375
        },
        {
            "corpus_id": "258887701",
            "title": "On the Impact of Knowledge Distillation for Model Interpretability",
            "text": "Several recent studies have elucidated why knowledge distillation (KD) improves model performance. However, few have researched the other advantages of KD in addition to its improving model performance. In this study, we have attempted to show that KD enhances the interpretability as well as the accuracy of models. We measured the number of concept detectors identified in network dissection for a quantitative comparison of model interpretability. We attributed the improvement in interpretability to the class-similarity information transferred from the teacher to student models. First, we confirmed the transfer of class-similarity information from the teacher to student model via logit distillation. Then, we analyzed how class-similarity information affects model interpretability in terms of its presence or absence and degree of similarity information. We conducted various quantitative and qualitative experiments and examined the results on different datasets, different KD methods, and according to different measures of interpretability. Our research showed that KD models by large models could be used more reliably in various fields.",
            "score": 0.5147504532353945,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5810546875
        },
        {
            "corpus_id": "226226888",
            "title": "MixKD: Towards Efficient Distillation of Large-scale Language Models",
            "text": "Previous efforts on distilling large-scale LMs mainly focus on designing better training objectives, such as matching intermediate representations (Sun et al., 2019a;Mukherjee & Awadallah, 2019), learning multiple tasks together (Liu et al., 2019a), or leveraging the distillation objective during the pre-training stage (Jiao et al., 2019;Sanh et al., 2019). However, much less effort has been made to enrich task-specific data, a potentially vital component of the knowledge distillation procedure. In particular, tasks with fewer data samples provide less opportunity for the student model to learn from the teacher. Even with a well-designed training objective, the student model is still prone to overfitting, despite effectively mimicking the teacher network on the available data. \n\nIn response to these limitations, we propose improving the value of knowledge distillation by using data augmentation to generate additional samples from the available task-specific data. These augmented samples are further processed by the teacher network to produce additional soft labels, providing the student model more data to learn from a large-scale LM. Intuitively, this is akin to a student learning more from a teacher by asking more questions to further probe the teacher's answers and thoughts. In particular, we demonstrate that mixup (Zhang et al., 2018) can significantly improve knowledge distillation's effectiveness, and we show with a theoretical framework why this is the case. We call our framework MixKD. \n\nWe conduct experiments on 6 GLUE datasets (Wang et al., 2019) across a variety of task types, demonstrating that MixKD significantly outperforms knowledge distillation (Hinton et al., 2015) and other previous methods that compress large-scale language models. In particular, we show that our method is especially effective when the number of available task data samples is small, substantially improving the potency of knowledge distillation. We also visualize representations learned with and without MixKD to show the value of interpolated distillation samples, perform a series of ablation and hyperparameter sensitivity studies, and demonstrate the superiority of MixKD over other BERT data augmentation strategies.",
            "score": 0.5141663121444711,
            "section_title": "Pre-print",
            "char_start_offset": 1657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1517
                },
                {
                    "start": 1520,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2239
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.475830078125
        },
        {
            "corpus_id": "271544434",
            "title": "Accelerating the Low-Rank Decomposed Models",
            "text": "Knowledge Distillation (KD) uses a teacher-student framework in order to transfer the knowledge from a larger network (teacher) into a compact and efficient one (student) by adding and auxiliary loss to imitate softmax outputs or logits from the teacher as a representation of class distributions (Hinton et al., 2015;Rashid et al., 2021). In this method, the architecture of the student could be totally different from that of the teacher. The student model tries to mimic the behaviour of the teacher through distilling knowledge from the teachers output or intermediate layers (Mirzadeh et al., 2020). \n\nThe theory behind how KD works is still an open question in the literature, however, there are some works explaining the contribution of adding class similarity information in the output of the teacher (which is reffered to as the dark knowledge as well), or regularization effects of the KD loss as potential reasons. However, these methods have poor mathematical support and could face some serious limitations in high compression ratios.",
            "score": 0.5138140005295627,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1047
                }
            ],
            "ref_mentions": [
                {
                    "start": 580,
                    "end": 603,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34765625
        },
        {
            "corpus_id": "268723621",
            "title": "Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation",
            "text": "The high-level idea of our study is presented in Figure 2. Given a pair of PLMs, a teacher and a student, we aim to transfer the parameter-efficient (PE) modules from the teacher to the student.First, we use a PEFT technique to train the teacher and its PE modules.Then, we \"move\" the modules from the teacher and insert them into the student, followed by PEFT of the student.This approach means that PE modules of the student have nonrandom prior initialisation during training.We consider two setups: (1) matching PLMs and (2) incompatible PLMs.The former uses a shallow version of a teacher with task-agnostic distillation as a student (Kim and Hassan, 2020).This case means that the models represent the same knowledge, have the same hidden dimensionality, and the only difference is the depth of the model.The latter represents a generalised version, where the models are differently parameterised (in terms of latent space size) and they are independently trained.We propose a parameter-free, sample-based pruning and alignment method to answer dimensionality mismatch.",
            "score": 0.5136452331746204,
            "section_title": "Transferable Modularity",
            "char_start_offset": 3069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 265
                },
                {
                    "start": 265,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 547
                },
                {
                    "start": 547,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 811
                },
                {
                    "start": 811,
                    "end": 970
                },
                {
                    "start": 970,
                    "end": 1075
                }
            ],
            "ref_mentions": [
                {
                    "start": 639,
                    "end": 661,
                    "matchedPaperCorpusId": "225070666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.583984375
        },
        {
            "corpus_id": "258715315",
            "title": "Tailoring Instructions to Student\u2019s Learning Levels Boosts Knowledge Distillation",
            "text": "It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student\u2019s generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher\u2019s learning process. By prioritizing samples that are likely to enhance the student\u2019s generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.",
            "score": 0.5135143536422777,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2293701171875
        },
        {
            "corpus_id": "270923792",
            "title": "Supporting Cross-language Cross-project Bug Localization Using Pre-trained Language Models",
            "text": "In general, for task-specific distillation, given a prediction or classification downstream task (e.g., bug localization [37]), the large teacher model is obtained by fine-tuning a pre-trained language model (e.g., UniXcoder [9] having 12 transformer layers).After that, the teacher model is utilized to extract the knowledge (e.g., logits or intermediate states) which are then used to guide fine-tuning the student model during distillation.Here, we assume that the teacher model is already fine-tuned and pre-defined for the given downstream task.We illustrate the overview of the model architecture in Figure 4 and the overview of proposed fine-tuning process in Algorithm 7 To this end, we aim to answer three key questions.The first question is: How can we initialize the student model?Initializing the student model is an crucial step for improving student performance.To address this question, we aim to distill the teacher model into a student model with fewer transformer layers (i.e., k layers, k < 12), wherein each student layer has the same architecture as its teacher layer (e.g., the same hidden size (i.e., d) and the same number (i.e., h) of attention heads).This benefits from initializing the student layers directly with teacher layers.In particular, we follow [32] to directly utilize k bottom teacher layers to initialize k student layers, respectively.In addition, we also initialize the student's embedding layer with the teacher's embedding layer.\n\nNext, we address the second question: How can we map the layers in the teacher model into the layers in the student model?We follow [29] to perform last-layer knowledge distillation, i.e., only distilling the knowledge from the teacher last layer to the student last layer.This helps to avoid the effort of finding the best layer mapping for each downstream task.Lastly, we address the third question: How  Prediction layer transfer minimizes the mean-squared-error (MSE) between the logits from the teacher and student models:\n\nwherein z T and z S denotes logits from the teacher and student, respectively.",
            "score": 0.5132153105511496,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 14528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 259,
                    "end": 443
                },
                {
                    "start": 443,
                    "end": 550
                },
                {
                    "start": 550,
                    "end": 729
                },
                {
                    "start": 729,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 876
                },
                {
                    "start": 876,
                    "end": 1177
                },
                {
                    "start": 1177,
                    "end": 1257
                },
                {
                    "start": 1257,
                    "end": 1376
                },
                {
                    "start": 1376,
                    "end": 1473
                },
                {
                    "start": 1475,
                    "end": 1597
                },
                {
                    "start": 1597,
                    "end": 1748
                },
                {
                    "start": 1748,
                    "end": 1838
                },
                {
                    "start": 1838,
                    "end": 2002
                },
                {
                    "start": 2004,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 125,
                    "matchedPaperCorpusId": "8037297"
                },
                {
                    "start": 1607,
                    "end": 1611,
                    "matchedPaperCorpusId": "229923069"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64501953125
        },
        {
            "corpus_id": "235694419",
            "title": "Fair Visual Recognition in Limited Data Regime using Self-Supervision and Self-Distillation",
            "text": "The authors in [13] propose to use knowledge distillation for knowledge transfer from a teacher network to a student network. This process involves training the student network to match the output logits/soft predictions of the teacher network in addition to the training objective of the primary task. When the teacher and student architectures are the same, the knowledge distillation process is referred to as self-distillation. The authors in [20] demonstrate that self-distillation improves the test set performance of the network. The distillation process increases the generalization ability of the network without requiring additional labeled data for training. We use self-distillation in our approach to reduce the impact of biases in the model (see Sec. 4.1).",
            "score": 0.5130278248120534,
            "section_title": "Self-Distillation",
            "char_start_offset": 7755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 770
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 19,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80712890625
        },
        {
            "corpus_id": "236782270",
            "title": "Adaptive Teacher Finetune: Towards high-performance knowledge distillation through adaptive fine-tuning",
            "text": "Knowledge distillation is a widely used method to transfer knowledge from a large model to a small model. Traditional methods use pre-trained large models to supervise the training of small models, called Offline Knowledge Distillation, However, the structural gap between teachers and students limits its performance. After that, Oneline Knowledge Distillation retrained the teacher-student network from the beginning and the method of echo teaching greatly improved the performance. But there is very little work to explore the difference between the two. In this paper, we first point out that the essential difference between Offline and Oneline Knowledge Distillation is actually whether the weight of the teacher-student network has a process of mutual adaptation. If they adopt the teacher network and the student network jointly train to implement Offline Knowledge Distillation, there is no obvious difference in the final performance, no matter whether it is a joint distillation training. This shows that teacher-student network adaptation is important for Knowledge Distillation. Then, we propose an Adaptive Teacher Finetune (ATF) to adapt the teacher model to the student network. It will use student model information for Tinetune during the Offline Knowledge Distillation process. With normalized logical distribution and alpha-divergence, the performance improvement of ATF clearly exceeds the existing Offline and Oneline Knowledge Distillation method. Extensive experiments conducted on cifar and ImageNet support our aforementioned analysis and conclusions. With the newly introduced ATF, we obtained state-of-the-art performance on ResNet 18 on ImageNet.",
            "score": 0.5130252215566916,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62890625
        },
        {
            "corpus_id": "264952263",
            "title": "A lightweight speech recognition method with target-swap knowledge distillation for Mandarin air traffic control communications",
            "text": "On the basis of this assumption, another comparative experiment was conducted to verify the effectiveness of the TSKD, with the predictive top-k output of the teacher model for each segment as the information exchange. As shown in Table 4, the experimental results show that exchanging top-k information is an effective knowledge distillation approach, nevertheless, its performance does not surpass that of the target class, i.e., TSKD. \n\nTo further validate the effectiveness of TSKD proposed in this article, several typical knowledge distillation methods were implemented in homogeneous and heterogeneous network structures. The experimental results are presented in Tables 5 and 6, respectively. The results demonstrate that under these six homogeneous or heterogeneous architectures, the TSKD enhanced the recognition accuracy of all student models by an average of 1.13%, achieving the best distillation results in most experiments. \n\nSpecifically, the TSKD dramatically boosts the Transformer student model under heterogeneous architectures, reducing the SER by 2.13%. A plausible explanation is that the Conformer teacher model may enable the Transformer student model to capture",
            "score": 0.512850584405175,
            "section_title": "Effectiveness of TSKD in homogeneous and heterogeneous architectures",
            "char_start_offset": 19467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2349853515625
        },
        {
            "corpus_id": "263677297",
            "title": "Efficient and Controllable Model Compression through Sequential Knowledge Distillation and Pruning",
            "text": "Tables 1 and 2 provide a comprehensive performance comparison of various distillation approaches using 11 network combinations where teacher and student models have either similar or completely different architectures. We compare nine different knowledge distillation methods to determine which one gives the best performance. We observe that SimKD consistently outperforms all competitors on CIFAR-100 and CIFAR-10. Then, we choose these distilled students for the next step and apply pruning to that.",
            "score": 0.5122122345328015,
            "section_title": "Distillation",
            "char_start_offset": 21583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 502
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55419921875
        },
        {
            "corpus_id": "102483181",
            "title": "A Comprehensive Overhaul of Feature Distillation",
            "text": "We analyze possible factors which would have lead to the performance improvement by our proposed method. The first analysis is the output similarity between the teacher and the student learned by distillation. By this, we verify how well our method forces the student to follow the teacher. After that, we provide an ablation study of our proposed method. We measure how much each component of our proposed method contributes to the performance. Finally, we discuss about how the mode of batch normalization affects knowledge distillation, as mentioned in Section 3.3. All experiments are based on setting (c) of Table 2.",
            "score": 0.512060912549946,
            "section_title": "Analysis",
            "char_start_offset": 24468,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.202392578125
        },
        {
            "corpus_id": "218469974",
            "title": "MKD: a Multi-Task Knowledge Distillation Approach for Pretrained Language Models",
            "text": "We evaluate our approach on two different student model architectures. One uses three layers Transformers (Vaswani et al., 2017), since most of the KD works (Sun et al., 2019;Jiao et al., 2019) use Transformers as their students. Another is LSTM based network with bi-attention mechanism. Previously Tang et al. (2019) examine the representation capacity of a simple, single-layer Bi-LSTM only, so we are interested in whether adding more previous effective modules, such as an attention mechanism, will further improve its effectiveness. It exemplifies that our approach is model agnostic, i.e., the choice of student model does not depend on the teacher model architecture; The teacher model can be easily switched to other powerful language models other than BERT. \n\nWe further study several important problems in knowledge distillation, such as the choice of modules in student model, the influence of different tokenization methods, and the influence of MTL in KD. We evaluate our approach on seven datasets across four different tasks. For LSTM based student, our approach keeps the advantage of inference speed while maintaining comparable performances as those specifically designed for Transformer methods. For our Transformer based student, it does provide a modest gain, and outperforms other KD methods without using external training data.",
            "score": 0.5120158065695258,
            "section_title": "Introduction",
            "char_start_offset": 2149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1352
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 128,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58251953125
        },
        {
            "corpus_id": "267523447",
            "title": "Beyond Answers: Transferring Reasoning Capabilities to Smaller LLMs Using Multi-Teacher Knowledge Distillation",
            "text": "To solve these issues, we propose TinyLLM, a paradigm that facilitates the learning of a small student LLM by distilling knowledge from multiple large teacher LLMs with rationale guidance. Specifically, TinyLLM mitigates the limited knowledge diversity issue by involving multiple teacher models as co-advisors, which introduces a richer, varied knowledge source for the student to learn from. To fully exploit each teacher model and mitigate the lack of rich contextual information problem, TinyLLM asks the teacher for the credible rationales to support the answers, thereby providing the student with a deeper understanding of the problemsolving process. By learning from multiple teachers, the student model can inherit a broader range of skills and knowledge, leading to better generalization capabilities. In addition, to ensure the rationales are grounded in contextually appropriate scenarios and reflect the true underlying reasoning procedure, TinyLLM features an in-context example generator and a teacher-forcing Chain-of-Thought strategy, making the teachers understand the task through demonstrations and therefore generate the accurate rationales. \n\nTo thoroughly evaluate our approach, we conduct experiments on six datasets in commonsense and biomedical reasoning tasks. The results show that the usage of our paradigm enhances performance by +5.07% to +15.69% compared to full fine-tuning. Compared to the teacher models, TinyLLM achieves superior performance improvement, e.g., up to +23.40% with significantly smaller model size, e.g., 1.1% to 26.0%. In addition, compared to the state-ofthe-art distillation methods, our approach improves the distillation performance by +10.00% to +11.79% across different model sizes. With the aim of further validating the effectiveness of our method, we perform efficiency analyses, ablation studies, parameter sensitivities, and case studies to provide a comprehensive evaluation across multiple dimensions. To summarize, our main contributions are as follows: \n\n\u2022 We identify two critical limitations in the existing knowledge distillation landscape for LLMs: 1) limited knowledge diversity, and 2) lack of rich contextual information. \u2022 To solve these two problems, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs.",
            "score": 0.5117598888919981,
            "section_title": "Introduction",
            "char_start_offset": 2225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 1162
                },
                {
                    "start": 1165,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2019
                },
                {
                    "start": 2022,
                    "end": 2195
                },
                {
                    "start": 2196,
                    "end": 2349
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48828125
        },
        {
            "corpus_id": "251463676",
            "title": "Study of Deep Learning-Based Legal Judgment Prediction in Internet of Things Era",
            "text": "In natural language processing area, using a pretraining model with a huge amount of data can effectively improve the performance of the model in the target task. However, the huge number of parameters of the pretraining model also makes it difficult to directly apply it to online tasks. Using knowledge distillation for the pretraining model can effectively improve the practicability of the model. e purpose of knowledge distillation is to achieve knowledge transfer between models by letting the untrained student model learn the trained teacher model. Generally, the structure of the student model is simpler than that of the teacher model, and it has fewer layers or parameters.\n\nrough knowledge distillation, the student model can obtain similar performance to the teacher model, accelerate the model inference, and reduce the memory usage of the model. e structure of knowledge distillation is shown in Figure 1.\n\nAs shown in the figure, knowledge distillation generally uses the output layer distribution of the teacher model as a soft label, and the labels in the dataset as hard labels. Imitation of the teacher model. In order to further narrow the gap between the teacher model and the student model, the intermediate layer distribution loss of the teacher model and the student model can also be added in the distillation process. e loss function L of knowledge distillation can be expressed as follows:\n\n(1)\n\nAmong them, L qa , L 1 , and L 2 represent the distillation loss from the soft label, the supervision loss from the dataset label, and the intermediate layer distribution loss of the teacher model and the student model, respectively, and \u03bc, \u03c0, and \u03c1 represent the weights of each loss function.\n\nOn the premise of ensuring the performance, we try to use the knowledge distillation strategy. e BERTmodel is used as the teacher model for knowledge distillation to reduce the redundant parameters and enhance the inference speed.\n\nReferring to the existing research, we tried to use two mainstream knowledge distillation strategies for pretraining models: knowledge distillation and BERT knowledge distillation strategy to compress the model and combined the characteristics of legal judgment prediction to further improve the predicition accuracy.",
            "score": 0.5116235331757242,
            "section_title": "BERT Knowledge Distillation-Based Legal Decision Prediction",
            "char_start_offset": 10225,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5234375
        },
        {
            "corpus_id": "275790007",
            "title": "Quantification of Large Language Model Distillation",
            "text": "Model distillation is a fundamental technique in building large language models (LLMs), transferring knowledge from a teacher model to a student model. However, distillation can lead to model homogenization, reducing diversity among models and impairing their ability to robustly handle complex or novel tasks. These limitations underscore the need to systematically quantify the distillation process and its impact. In this work, we propose a framework to evaluate and quantify model distillation. Our method addresses two key aspects: (1) Identifying identity cognition contradictions to assess discrepancies in how models perceive and represent identity-related information, and (2) Analyzing multi-granularity response similarities across models to measure the extent of homogenization. Experimental results demonstrate two key insights: (1) Well-known closed-source and open-source LLMs usually exhibit high distillation degrees, except for Claude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees compared to aligned LLMs. By offering a systematic approach to improve the transparency of LLM data distillation, we call for LLMs with more independent development and more transparent technical reports to improve LLMs' robustness and safety. The code and data are available under https://github.com/Aegis1863/LLMs-Distillation-Quantification.",
            "score": 0.5106234151152564,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494140625
        },
        {
            "corpus_id": "249017991",
            "title": "CDFKD-MFS: Collaborative Data-Free Knowledge Distillation via Multi-Level Feature Sharing",
            "text": "Knowledge distillation, which is a practical approach for compressing a large pretrained teacher model into a tiny student model, was first proposed by Hinton et al. [2]. The original KD method takes predictions of the teacher model as soft labels to train the tiny student model from a sketch. Based on this framework, some works simulates soft labels by regularizing [15] or label smoothing [16], [17]. In addation to the output layer, there are also some approaches for transferring the knowledge in intermediate layers of the teacher. Based on this consideration, Fit-Net [18] and some follow-up works [19], [20] adopted feature matching losses to utilize intermediate knowledge. Moreover, intermediate-layer supervision can also be achieved through attention map matching [21], feature distribution matching [22], and cross-layer knowledge transferring [23].\n\nMultiple pretrained teacher models can provide diverse knowledge and improve the performance of the student [5]. Hinton et al. [2] proposed using the average of teacher prediction as the supervising signal. You et al. [24] and Park et al. [25] introduced feature supervision in multi-teacher KD. To better transfer knowledge from multiple teachers, some works added additional branches to the student [6], [26]. However, these branches were directly grafted onto the student without careful consideration, which may not be sufficient for preserving the diverse knowledge of teachers. Hence, multi-teacher KD requires a student to learn diverse knowledge from multiple teachers while remaining lightweight. To tackle this problem, we design a multi-header student with a multi-level feature-sharing structure to learn from multiple teachers.",
            "score": 0.5102834107423995,
            "section_title": "A. Data-Required KD",
            "char_start_offset": 6201,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 393,
                    "end": 397,
                    "matchedPaperCorpusId": "174802983"
                },
                {
                    "start": 399,
                    "end": 403,
                    "matchedPaperCorpusId": "40427649"
                },
                {
                    "start": 576,
                    "end": 580,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "227232038"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "221559239"
                },
                {
                    "start": 777,
                    "end": 781,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 858,
                    "end": 862,
                    "matchedPaperCorpusId": "227335337"
                },
                {
                    "start": 1083,
                    "end": 1087,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1104,
                    "end": 1108,
                    "matchedPaperCorpusId": "220378802"
                },
                {
                    "start": 1271,
                    "end": 1275,
                    "matchedPaperCorpusId": "202660953"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50537109375
        },
        {
            "corpus_id": "272368391",
            "title": "On-Device Language Models: A Comprehensive Review",
            "text": "Knowledge Distillation (KD) is a technique for transferring knowledge from a large, computationally intensive model (teacher) to a smaller, more efficient model (student). This method is crucial for condensing the capabilities of large language models (LLMs) into more manageable forms without significantly impacting performance. \n\n1. Black-box Knowledge Distillation: This approach involves the student model learning solely from the outputs of the teacher model, without access to its internal mechanics or parameters. It is particularly advantageous when the teacher model's details are proprietary or when the architectures of the teacher and student models differ markedly. For instance, Gu et al. (2023) demonstrated that black-box KD could effectively train models using only the output data from LLM APIs like ChatGPT. The student model trains to emulate the teacher's output distribution based on input-output pairs, a process that, while effective, limits learning to external behaviors without tapping into the teacher's deeper internal states. 2. White-box Knowledge Distillation: In contrast, White-box Knowledge Distillation allows the student model to access the internal states and workings of the teacher, facilitating a deeper and more precise learning process. This method enables the student to mimic not just the outputs but also the internal state distributions of the teacher, enhancing learning efficacy and depth. The increased access to the teacher's detailed workings helps guide the student's learning, resulting in more accurate and robust models. However, this technique requires a careful alignment of model architectures to ensure effective knowledge transfer and is generally more complex to implement.",
            "score": 0.5097078083217186,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 38449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 330
                },
                {
                    "start": 333,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1736
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76025390625
        },
        {
            "corpus_id": "235608384",
            "title": "Meta-Teacher For Face Anti-Spoofing",
            "text": "Traditionally, the training of neural network models is supervised by handcrafted labels. Several recent studies [24]- [30], [32], [53] show that using one or more well-trained deep and wide models to supervise another lighter model's training would benefit the lighter model's performance. This kind of training is commonly referred to as knowledge distillation (KD), as the lighter model distills knowledge from the cumbersome models. Because these methods simulate teachers' teaching process, where the cumbersome models act as teachers and the lighter models act as students, these methods are also called as teacher-student methods. \n\nHinton et al. [24] propose the earliest teacher-student method. The authors utilize the teacher's output logits to supervise the student's training. In addition to the teacher's logits, FitNets [25] demonstrates that intermediate features of the teacher supervises the student more efficiently. Researchers often make the teacher's network much larger than the student's network to guarantee the teacher's superior capacity and performance. BANs [26], however, shows that the teacher network does not have to be larger than the student model. A teacher that has the same network as the student can still improve the student's learning. Furthermore, [29] and [32] demonstrate that improving the teacher's performance does not always enable the student to learn better. Teacher-student methods will lose efficacy when the representation ability gap between the teacher models and the student turns too large. \n\nAlthough the existing teacher-student methods are promising, they train the teacher to focus on learning how to perform better but not how to reliably teach the student. In this paper, we propose to train a FAS meta-teacher to focus on providing better-suited supervision signals for the student (PA detector).",
            "score": 0.5097072142627984,
            "section_title": "Teacher-Student Methods",
            "char_start_offset": 12246,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1859
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 123,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 125,
                    "end": 129,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 131,
                    "end": 135,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1298,
                    "end": 1302,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64404296875
        },
        {
            "corpus_id": "231925118",
            "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
            "text": "Although our teacher network obtained from the student-aware training procedure is specialized for a specific student model, it is also effective to transfer knowledge to the students models with substantially different architectures. Table 7 shows that the benefit of our method is also preserved well as long as the student branch has similar capacity to the student models, where the model capacity is defined by the achievable accuracy via independent training without distillation. In addition, it presents that larger students branches are often effective to enhance distillation performance while smaller student branches are not always helpful. In summary, these results imply that a teacher network in SFTN trained for a specific architecture of student network has the potential to transfer its knowledge to other types of student networks.",
            "score": 0.5097038137514184,
            "section_title": "Versatility of SFTN",
            "char_start_offset": 21082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 850
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62744140625
        },
        {
            "corpus_id": "269187844",
            "title": "Dynamic Self-adaptive Multiscale Distillation from Pre-trained Multimodal Large Model for Efficient Cross-modal Representation Learning",
            "text": "The successful application of deep learning models across various domains has highlighted the challenge of deploying these large models in resource-constrained environments.Knowledge distillation emerges as a pivotal technique for this, efficiently compressing models by transferring knowledge from a complex teacher model to a simpler, lightweight student model, thus preserving the teacher's performance within a reduced model size.\n\nExisting knowledge distillation methods are categorized into three types: response-based, relationship-based, and feature-based.KD [18] employs response-based distillation using the teacher's final classifier outputs for richer soft labels.DKD [59] categorizes logits into target and non-target classes, emphasizing the importance of non-target information.BAN [14] uses target class logits as weighting factors for samples.In relationship-based methods, FSP [56] and RKD [38] construct relationship matrices between teacher and student features to capture and distill structural knowledge, enhancing understanding through differences in sample relationships and geometrical configurations within the same batch.\n\nIn feature-based distillation, FitNets [44] uses middle layer outputs of the teacher model as \"Hints\" to guide the student model, noting differences in feature activations.FT [21] enhances knowledge transfer by pre-training connectors in the teacher model.AB [17] employs a binarization technique to filter features between the teacher and student models, while AT [57] transforms features into attention values to optimize distillation.CRD [46] utilizes contrastive learning to align corresponding features between teacher and student as positive examples and separate non-corresponding ones as negatives.OFD [16] reviews these techniques, analyzing how feature location, connector composition, and loss functions influence the efficiency of knowledge transfer.\n\nHowever, previous distillation methods are tailored for unimodal scenarios, operate at a single scale, and often require additional features or information, limiting their applicability to multimodal Two people in the photo are playing the guitar and the other is poking at him.",
            "score": 0.5096231126550923,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7292,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 173,
                    "end": 434
                },
                {
                    "start": 436,
                    "end": 564
                },
                {
                    "start": 564,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 793
                },
                {
                    "start": 793,
                    "end": 860
                },
                {
                    "start": 860,
                    "end": 1148
                },
                {
                    "start": 1150,
                    "end": 1322
                },
                {
                    "start": 1322,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1587
                },
                {
                    "start": 1587,
                    "end": 1756
                },
                {
                    "start": 1756,
                    "end": 1912
                },
                {
                    "start": 1914,
                    "end": 2192
                }
            ],
            "ref_mentions": [
                {
                    "start": 680,
                    "end": 684,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 895,
                    "end": 899,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 908,
                    "end": 912,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1325,
                    "end": 1329,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 1409,
                    "end": 1413,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1515,
                    "end": 1519,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1591,
                    "end": 1595,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1760,
                    "end": 1764,
                    "matchedPaperCorpusId": "102483181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58984375
        },
        {
            "corpus_id": "272367233",
            "title": "Efficient Point Cloud Classification via Offline Distillation Framework and Negative-Weight Self-Distillation Technique",
            "text": "This section investigates the impact of distillation weights on the classification performance of the student model. The experimental design follows conventional settings in knowledge distillation, where the teacher-student distillation weight \u03b1 is set to a high value without self-distillation. This ensures that the weight is comparable to the classification loss weight, providing strong guidance for the student model. Once the teacher-student distillation parameters are established, the model further introduces a self-distillation with a negative weight \u03b2 of low magnitude to prevent interference with the training of the student model. This section explores the effects of varying weights on model performance through a series of experiments. The results are presented in Tab. 5 and 6, corresponding to the accuracy of the student model under different \u03b1 and \u03b2 values, respectively. As shown in Tab. 5, without self-distillation, the accuracy varies with different \u03b1 values, indicating that the strength of guidance has a direct impact on the knowledge transfer from the teacher model and the learning of the student model. The table demonstrates that an appropriately high \u03b1 is crucial for effective knowledge distillation and student model guidance. \n\nTab. 6 illustrates that when the teacher model provides strong guidance, the model's performance is generally better with a negative self-distillation weight \u03b2 than with a positive \u03b2. This confirms that negative-weight selfdistillation serves a regularizing role, enhancing the model's ability to generalize from training data and preventing rapid convergence to suboptimal solutions under the teacher model's guidance.",
            "score": 0.5095375913109867,
            "section_title": "Effects of Distillation Weights on Model Performance:",
            "char_start_offset": 18652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1681
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68115234375
        },
        {
            "corpus_id": "211068747",
            "title": "Understanding and Improving Knowledge Distillation",
            "text": "Recent advances in artificial intelligence have largely been driven by deep neural networks using multi-layer perceptron, and thus, current state-of-the-art models typically require a high inference cost in computation and memory. Therefore, several works have been devoted to find a better quality and computation trade-off, such as pruning (Han et al., 2015b;a) and quantization-based approaches (Han et al., 2015a;Jacob et al., 2018). One promising and commonly used method for addressing this computational burden is called Knowledge Distillation (KD), proposed by Hinton et al. (2015), which uses a larger capacity teacher model (ensembles) to transfer its 'dark knowledge' to a more compact student model. Through distillation, one hopes to achieve a student model that not only inherits better quality from the teacher, but is also more efficient for inference due to its compactness. Recently, we have witnessed a huge success of knowledge distillation, irrespective of the model architecture and application domain (Kim & Rush, 2016;Chen et al., 2017;Tang & Wang, 2018;Anil et al., 2018;He et al., 2019). \n\nDespite the large success of knowledge distillation, surprisingly sparse theoretical research has been done to better understand the mechanism of how it works, which could limit the applications of KD; and also raise unexpected or unexplainable results. For example, to successfully 'distill' a better student, one common practice is to have a teacher model with as good quality as possible. However, recently Mirzadeh et al. (2019) and M\u00fcller et al. (2019) have found this intuition would fail under certain circumstances. Furthermore, Anil et al. (2018) and Furlanello et al. (2018) have analyzed that even without using a powerful teacher, distilling a student model to itself using mutual or self-distillation also improves quality. To this end, some researchers have made attempts on understanding the mechanism of KD. For example, Yuan et al. (2019) connects label smoothing to KD. Furlanello et al. (2018) conjectures KD's effect on re-weighting training examples.",
            "score": 0.5095201539987683,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1113
                },
                {
                    "start": 1116,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 342,
                    "end": 361,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 417,
                    "end": 436,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1060,
                    "end": 1078,
                    "matchedPaperCorpusId": "50778760"
                },
                {
                    "start": 1096,
                    "end": 1112,
                    "matchedPaperCorpusId": "54445469"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.560546875
        },
        {
            "corpus_id": "232428032",
            "title": "Fixing the Teacher-Student Knowledge Discrepancy in Distillation",
            "text": "Existing knowledge distillation methods ignore the discrepancy and directly train the student with the supervision of the teacher network. We claim that such discrepancy will impede the learning of the student. To describe the discrepancy quantitatively, we propose consistency measurements between teacher and student models. Based on the consistency measurements, a channel-based feature transformation is explored and deployed on the teacher feature. The transformation can alleviate the discrepancy problem between the teacher and student dramatically and provide a suitable feature that is easier for the student to learn and obtain better performance. As shown in Figure 1, the feature of teacher model after transformation is more consistent with the student model. Besides, our approach is independent of other knowledge distillation methods. We can integrate our method into other methods and achieve further performance improvement. \n\nThe contributions of this work can be summarized as threefold. \n\n\u2022 We demonstrate the existence of knowledge discrepancy among different teacher and student models in the knowledge distillation process. \n\n\u2022 We propose a new method to solve the discrepancy problem and analyze the effectiveness of the proposed feature transformation operation. \n\n\u2022 Extensive experiments on different benchmarks and visual tasks demonstrate the advantage and generalization ability of our method. Moreover, it can be easily integrated into most of the existing state-of-the-art knowledge distillation methods to further improve the performance.",
            "score": 0.5092573569875105,
            "section_title": "Introduction",
            "char_start_offset": 2297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1571
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6064453125
        },
        {
            "corpus_id": "254815049",
            "title": "Sound Event Detection System Based on VGGSKCCT Model Architecture with Knowledge Distillation",
            "text": "We next disclose the effect of using knowledge distillation and incorporating both the mean teacher method and knowledge distillation.The pre-trained model in knowledge distillation is similar to the aforementioned VGGSK system.The difference is it replaces the SK blocks with RepVGG (Reparameterized VGG) blocks (Ding et al. 2021) in order to decrease the number of model parameters from 2.5 m to 943k.This pre-trained model is also trained on the same DESED data.The first two rows in Tables 4 and 5 list the PSDS scores of VGGSKCCT system and pre-trained system with mean teacher framework.The third row is the knowledge distillation model with VGGSKCCT as a student model.This is referred to as 2-pass approach, since mean-teacher method and knowledge distillation method are applied in separate passes.The last row is a result of the model that includes a mean teacher model of VGGSKCCT and a pre-trained teacher model.This is referred to as 1-pass approach, since mean-teacher method and knowledge distillation method are applied simultaneously.The results in these tables indicate that when employing a pre-trained model to distill a VGGSKCCT model, there is no significant improvement.As we can see, the score of the student model and the pre-trained teacher model is roughly the same.The student model can only predict like a pre-trained teacher model if only knowledge distillation is used.However, the 1-pass approach, which incorporates both the mean teacher method and knowledge distillation, does improve performance.Here we also show the supervised losses of using 1-pass approach or not (mean teacher model with/without knowledge distillation) in Figure 5.The lighter line in the background is the true value of the supervised loss between strong labels and strong predictions.The darker line is the trend of the loss value.We can see that the pre-trained teacher model can assist the student model with the mean teacher structure in learning better, as the model that incorporates knowledge The terms \"Baseline,\" \"VGGSK\" and \"VGGSKCCT\" denote the three systems in our experiment.Details can be found in Section Materials and methods.",
            "score": 0.5091782351120557,
            "section_title": "Comparison of Different Training Methods",
            "char_start_offset": 21429,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 465
                },
                {
                    "start": 465,
                    "end": 593
                },
                {
                    "start": 593,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 924
                },
                {
                    "start": 924,
                    "end": 1051
                },
                {
                    "start": 1051,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1400
                },
                {
                    "start": 1400,
                    "end": 1531
                },
                {
                    "start": 1531,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1793
                },
                {
                    "start": 1793,
                    "end": 1840
                },
                {
                    "start": 1840,
                    "end": 2096
                },
                {
                    "start": 2096,
                    "end": 2150
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.403564453125
        },
        {
            "corpus_id": "212628302",
            "title": "Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation",
            "text": "In this work, we conducted a large-scale empirical investigation to determine best practices when using sequence-level knowledge distillation and domain adaptation in combination. We found that adapting models from the general-domain makes them better teachers and that distilling using generaldomain data does not impact a model's adaptability. This leads us to recommend distilling twice for best results: once in the general-domain to possibly improve student performance, and again using an adapted in-domain teacher. The results are robust among multiple language pairs, student sizes, in-domain settings.",
            "score": 0.509063954548701,
            "section_title": "Conclusion",
            "char_start_offset": 15131,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 610
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.282470703125
        },
        {
            "corpus_id": "263909173",
            "title": "Leveraging Vision-Language Models for Improving Domain Generalization in Image Classification",
            "text": "These methods can be integrated with the proposed approach. Knowledge Distillation (KD): Knowledge distillation aims to transfer knowledge from a powerful teacher model to a compact student model. Output-distillation [20] uses the teacher's softmax outputs as guidance for the student. Feature-distillation methods [12-14, 37, 43, 51] use the teacher's intermediate representations as guidance since they contain richer information. The proposed method operates in the black box distillation setup that allows only input-output access to the teacher network and restricts access to intermediate features. We thus compare the proposed black-box approach only with methods that do not access intermediate features [13,20,22].",
            "score": 0.5089829006622255,
            "section_title": "Related Works",
            "char_start_offset": 7281,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 723
                }
            ],
            "ref_mentions": [
                {
                    "start": 712,
                    "end": 716,
                    "matchedPaperCorpusId": "247762862"
                },
                {
                    "start": 719,
                    "end": 722,
                    "matchedPaperCorpusId": "262217079"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.354736328125
        },
        {
            "corpus_id": "262044685",
            "title": "Energy Concerns with HPC Systems and Applications",
            "text": "Knowledge distillation refers to the approach of transferring the knowledge from a large but unwieldy model or set of models to a single smaller model that can be deployed under realworld constraints. In many recent publications on the topic, the teacher/student analogy is used to describe how knowledge distillation learning models work. There are three different ways that the larger teacher model is used to help training the smaller student model: response-based knowledge, feature-based knowledge and relation-based knowledge [49]. Through a varying combination of these three techniques, it has been shown that some very large models can be migrated to smaller representations. Probably the most well-known of these is DistilBERT [114], which is able to keep 97% of its language understanding versus BERT, while having a model which is 40% smaller and 60% faster. Using processors that are optimized for ML training such as tensor processing units (TPUs) and recent GPUs (e.g. V100 and A100) instead of general-purpose processors can improve performance/watt by factors 2 to 5 [103].",
            "score": 0.5087398585505195,
            "section_title": "Knowledge distillation.",
            "char_start_offset": 83960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1090
                }
            ],
            "ref_mentions": [
                {
                    "start": 532,
                    "end": 536,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1084,
                    "end": 1089,
                    "matchedPaperCorpusId": "246840687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.414306640625
        },
        {
            "corpus_id": "264405606",
            "title": "DistillCSE: Distilled Contrastive Learning for Sentence Embeddings",
            "text": "The advantage of DistillCSE is its self-enhancing feature: using a base model to provide additional supervision signals, a stronger model can be learned through knowledge distillation. Specifically, our framework can be divided into three steps ( \u00a72): First, it learns a base model as a teacher using standard contrastive learning; Second, it learn a stronger student model through knowledge distillation (Gao et al., 2023); Thrid, it iteratively repeats the process of knowledge distillation by treating the student model as a teacher. \n\nHowever, it is far from easy to put DistillCSE into practice: our preliminary experiment shows that the vanilla implementation of the proposed framework only achieves marginal improvements. We identify that the vanilla distillation method suffers from the severe overfitting on the training corpus (See Table 9 later). This motivates us to investi-gate the in-depth reason why overfitting occurs. One possible intuition is that in the contrastive learning scenario, the logits of the teacher model are defined on a pair of examples; whereas in the usual scenario these logits are defined on a single example. This essential difference may lead to a relatively large variance of the teacher model's logits in the contrastive learning scenario. To demonstrate our insight, two metrics are designed to quantify the variance of logits: one variance measures the change of logits between training and testing examples which controls over-fitting, and the other measures the change of logits with respect to different teacher models. Through our quantitative analysis, it is observed that logits defined on an example pair indeed have a much larger variance than those defined on a single example ( \u00a73.1). To mitigate these two high-variance problems, we respectively proposed two simple yet effective solutions for the knowledge distillation step: a group-p shuffling strategy as an implicit regularization to prevent overfitting issue ( \u00a73.2) and the averaging logits from multiple teacher components to reduce the second variance w.r.t different teacher models ( \u00a73.3). Experiments on standard benchmarks demonstrate that the proposed DistillCSE outperforms many strong baseline methods and yields a new state-of-the-art performance ( \u00a74).",
            "score": 0.5086627572928103,
            "section_title": "Introduction",
            "char_start_offset": 1734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2275
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39990234375
        },
        {
            "corpus_id": "269302768",
            "title": "Sentence-Level or Token-Level? A Comprehensive Study on Knowledge Distillation",
            "text": "However, empirical studies have revealed divergent performances between sentence-level and token-level distillation across different scenarios.Specifically, while some scenarios benefit more from the global structure and semantic consistency provided by sentence-level distillation [Kim and Rush, 2016;Chen et al., 2020;Xu et al., 2021b;Lei et al., 2022;Mhamdi et al., 2023], other scenarios require the finegrained knowledge transfer that token-level distillation offers [Liao et al., 2020;Tang et al., 2021;Li et al., 2021;Ma et al., 2023].This variation in performance has led to confusion regarding the empirical selection of knowledge distilla-tion methods.In this study, we conduct analytical experiments to explore the general suitable scenario of two knowledge distillation methods.Given that the training target of sentencelevel distillation (simplified sentence by teacher model) is easier than that of the token-level distillation (detailed token distribution of teacher model).We hypothesize that sentencelevel distillation is suitable for \"complex\" scenarios and the token-level distillation is suitable for \"simple\" scenarios.\n\nWe define the \"complex\" or \"simple\" scenarios from three perspectives: 1) model size of student model, as the student model becomes small, it is harder for the student model to learn the knowledge, and thus the scenario become more complex; 2) the complexity of text, more complex text will make the learning procedure of student model harder; 3) the difficulty of decoding, which is determined by the amount of auxiliary information available during decoding.The more auxiliary information available, the simpler the decoding process becomes.Experiments on the above three perspectives consistently verify our hypothesis, showing that the tokenlevel distillation performs better in simple scenarios with the sentence-level distillation is better for complex scenarios.\n\nAlthough the analytical experiments provide deep understanding and reveal the general suitable scenarios for two distillation method, how to empirically define the complexity of a machine translation task is challenging.To address this challenge, we further explore the hybridization of two distillation methods, aiming at taking the advantage of both the distillation methods to enhance overall translation accuracy.",
            "score": 0.5086116898228769,
            "section_title": "Introduction",
            "char_start_offset": 2024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 542
                },
                {
                    "start": 542,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 790
                },
                {
                    "start": 790,
                    "end": 989
                },
                {
                    "start": 989,
                    "end": 1140
                },
                {
                    "start": 1142,
                    "end": 1602
                },
                {
                    "start": 1602,
                    "end": 1685
                },
                {
                    "start": 1685,
                    "end": 1911
                },
                {
                    "start": 1913,
                    "end": 2133
                },
                {
                    "start": 2133,
                    "end": 2330
                }
            ],
            "ref_mentions": [
                {
                    "start": 337,
                    "end": 354,
                    "matchedPaperCorpusId": "253097876"
                },
                {
                    "start": 472,
                    "end": 491,
                    "matchedPaperCorpusId": "243830667"
                },
                {
                    "start": 509,
                    "end": 525,
                    "matchedPaperCorpusId": "243830667"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43701171875
        },
        {
            "corpus_id": "258480404",
            "title": "Minimal Distillation Schedule for Extreme Language Model Compression",
            "text": "Pretrained language models (LMs) (Devlin et al., 2019;Liu et al., 2019;Radford et al., 2019;Brown et al., 2020;Raffel et al., 2020) have achieved promising results in various downstream tasks (Wang et al., 2019;Rajpurkar et al., 2018), but are inapplicable to those requiring limited computational resources (Liu et al., 2021b). To address * Dawei Song is the corresponding author. In the study, a BERT base model is used as the teacher and distilled to a pruned student (10% parameters of the teacher) via different teacher assistants (Mirzadeh et al., 2020) on MRPC and QQP. There are several observations: (1) The blue curve shows that the performance of the teacher assistant degrades with the decreasing of its scale, which is obvious. \n\n(2) The green curve validates that the performance of the student varies with different teacher assistants. \n\n(3) The red curve represents \u03bb-tradeoff of the teacher assistant, which is positively correlated with the performance of the student. \n\nthis issue, LMs can be compressed using a range of strategies such as model quantization (Zafrir et al., 2019;Bai et al., 2021), pruning (Michel et al., 2019;Hou et al., 2020), etc., among which knowledge distillation (Sun et al., 2019;Wang et al., 2020) has gained significant attention. It operates within the teacher-student framework, where a large model acts as the teacher, transferring its knowledge to a smaller student model. Recent advances (Mirzadeh et al., 2020) have shown a significant performance decline in conventional distillation methods when dealing with a substantial capacity gap between the teacher and the student models. To alleviate this, teacher assistantbased distillation (Son et al., 2021) has been proposed. This approach involves distilling the teacher model into an intermediate-scale teacher assistant, which then serves as an intermediary to transfer knowledge to the student model.",
            "score": 0.5084814582931519,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1906
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 54,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 92,
                    "end": 111,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 111,
                    "end": 131,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 192,
                    "end": 211,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 211,
                    "end": 234,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 536,
                    "end": 559,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1078,
                    "end": 1099,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 1099,
                    "end": 1116,
                    "matchedPaperCorpusId": "229923538"
                },
                {
                    "start": 1126,
                    "end": 1147,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 1147,
                    "end": 1164,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 1207,
                    "end": 1225,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 1225,
                    "end": 1243,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 1440,
                    "end": 1463,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1690,
                    "end": 1708,
                    "matchedPaperCorpusId": "221802641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302490234375
        },
        {
            "corpus_id": "270094801",
            "title": "Aligning in a Compact Space: Contrastive Knowledge Distillation between Heterogeneous Architectures",
            "text": "Knowledge distillation aims to transfer knowledge from a large-size teacher model T to a small-size student model S.This process often uses logits and features as carriers for knowledge transfer.\n\nLogits-based Distillation.Logits carry category distribution information that directly impacts classification probabilities.Logit-based approach has proven to be an effective strategy to circumvent the discrepancies in the spatial distribution of features among heterogeneous models, as recent research has demonstrated.Specifically, the output category distribution of the student model can be made to approximate that of the teacher model, thereby improving the performance of the student model.\n\nwhere p s and p t are the predicted logits of student model and teacher model, respectively.L KLD is the Kullback-Leibler divergence loss function.y is the one-hot groundtruth labels.\u03bb is the hyperparameter adjusts the weight of soft label p t .\n\nFeature-based Distillation.By minimizing the Euclidean distance between features, feature distillation offers a finer-grained supervision for the student model through the introduction of sample-level comparisons.Formally, the objective of feature distillation approaches can be written as:\n\nwhere F s and F t are the features extracted from the i-th layer of student model and teacher model.\u03d5(\u2022) is the projector that transform student feature to match the dimension of teacher feature.\u03bb KLD and \u03bb MSE balance the logit loss and feature loss.\n\nExisting feature distillation methods simply align feature dimensions through convolution or MLP modules, which is effective when both the teacher and student models belong to the same metaarchitecture.However, models with different meta-architectures exhibit significant differences in terms of model inputs, inductive biases, and spatial distributions, among others.Consequently, enforcing complete feature-level alignment between the student and teacher models may not be the optimal approach.",
            "score": 0.5083162547704783,
            "section_title": "Recap of knowledge distillation",
            "char_start_offset": 8706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 116,
                    "end": 195
                },
                {
                    "start": 197,
                    "end": 223
                },
                {
                    "start": 223,
                    "end": 321
                },
                {
                    "start": 321,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 694
                },
                {
                    "start": 696,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 879
                },
                {
                    "start": 879,
                    "end": 941
                },
                {
                    "start": 943,
                    "end": 970
                },
                {
                    "start": 970,
                    "end": 1156
                },
                {
                    "start": 1156,
                    "end": 1233
                },
                {
                    "start": 1235,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1486
                },
                {
                    "start": 1488,
                    "end": 1690
                },
                {
                    "start": 1690,
                    "end": 1856
                },
                {
                    "start": 1856,
                    "end": 1984
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.779296875
        },
        {
            "corpus_id": "212736952",
            "title": "Pretraining Image Encoders without Reconstruction via Feature Prediction Loss",
            "text": "This method most commonly uses the prediction values of a pretrained network either as a replacement for or together with the ground truth when training a new model. In this setup, the pretrained model is referred to as the teacher and the model being trained is the student. Knowledge distillation has been shown to give faster training time and higher performance of the student model than training using only the ground truth [19].",
            "score": 0.5079522326213712,
            "section_title": "Contribution",
            "char_start_offset": 7983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 434
                }
            ],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50634765625
        },
        {
            "corpus_id": "271212917",
            "title": "Enhancing Weakly-Supervised Histopathology Image Segmentation with Knowledge Distillation on MIL-Based Pseudo-Labels",
            "text": "Knowledge distillation [23] is a technique that relies on teacher-student (T-S) architecture and targets to transfer knowledge from one model (teacher model) to another (student model).As shown in Fig. 2, beyond labels, the student model can learn various feature representations from the teacher model, such as prediction probabilities, feature vectors, and more [50].Since the student model usually exhibits superior performance over the teacher model [16], methods like momentum update have been widely used to further improve teacher's performance by updating it through the student model [7].Particularly, in iterative knowledge distillation, the student itself acts as the teacher, leveraging its past predictions for more informative supervision during training for generalization capability enhancement [28].To prevent the occurrence of model collapses in such self-supervised learning tasks, various strategies including contrastive loss [46], clustering constraints [6], predictor [17] or batch normalizations [42] are adopted in model optimization process.\n\nIt's important to highlight that, in contrast to the ensemble teacher distillation approach for classification mentioned in [28], our knowledge distillation method maintains the teacher model unchanged during the student's training phase.Once the student model outperforms the teacher, their roles are reversed, and a new cycle of knowledge distillation begins.",
            "score": 0.5078540488100631,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8660,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 185,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 816
                },
                {
                    "start": 816,
                    "end": 1067
                },
                {
                    "start": 1069,
                    "end": 1307
                },
                {
                    "start": 1307,
                    "end": 1430
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "232302458"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 593,
                    "end": 596,
                    "matchedPaperCorpusId": "233444273"
                },
                {
                    "start": 976,
                    "end": 979,
                    "matchedPaperCorpusId": "219721240"
                },
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "219687798"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58935546875
        },
        {
            "corpus_id": "277510177",
            "title": "UNDO: Understanding Distillation as Optimization",
            "text": "In this section, we first outline the standard knowledge distillation process and discuss its key limitations. We then introduce our proposed UNDO: UNderstanding Distillation as Optimization framework, which addresses these limitations by enabling the teacher to iteratively identify the student's weaknesses and refine the knowledge being distilled accordingly. \n\nProblem Setup. We consider a large teacher model p L (LLM) and a smaller student model p \u03b8 sm (with learnable parameters \u03b8). We have a dataset of math questions D = {(q i , a i )} n i=1 , and an associated held-out validation set V = {(q j , a j )} m j=1 to monitor progress at each iteration. Our goal is to distill the teacher's reasoning capabilities into the student iteratively by constantly moderating the errors made by the student and targeted refinements from the teacher.",
            "score": 0.5078009877703703,
            "section_title": "Methodology",
            "char_start_offset": 3315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 362
                },
                {
                    "start": 365,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 846
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59375
        },
        {
            "corpus_id": "259203703",
            "title": "Query Encoder Distillation via Embedding Alignment is a Strong Baseline Method to Boost Dense Retriever Online Efficiency",
            "text": "The results in Table 1 reveal the within-group performance comparison. Since all student models undergo the same embedding-alignment distillation process, the final performance preservation rate can serve as a proxy for the \"well-preparedness\" of students. msmarco-dot performs the best. Its tuning configuration is the same as its teacher's, i.e., the same dataset, distance function, and objective function. msmarco-tas-b, tuned with the balanced topicaware sampling technique (Hofst\u00e4tter et al., 2021b), closely follows. Such a variation poses a slightly greater challenge in embedding alignment. On the other side of the spectrum, changing a distance measurement alone makes alignment drastically harder, as shown from msmarco-cos. Interestingly, using a different objective function (msmarco-base  and nli-stsb) appears to alleviate misalignment, suggesting the potential interaction between objective and distance functions. Additionally, a clean, pretrained-only student (base-uncased) performs better when a perfect replication of the teacher's fine-tuning setting is not present. Notably, all DistilBERT-based students perform worse than the top-performing extractive students. The 2layer extractive student outperforms the 6-layer msmarco-dot with a performance gap of 2.5%.\n\nWhere are the Well-prepared Students? Student pretraining has been demonstrated to be crucial for knowledge distillation in language understanding tasks (Turc et al., 2019). However, in our asymmetric DE system, the student encoder operates in conjunction with the document encoder of the teacher system, deviating from the conventional distillation procedure. In this case, the effectiveness of student models lies not in their sheer capability but rather in their compatibility. Dong et al. (2022) employed t-SNE (van der Maaten and Hinton, 2008) to visualize the embedding spaces of DE encoders in the context of QA tasks. They observed that the two encoders of an asymmetric system tend to map questions and answers onto distinct parameter spaces, even when trained jointly. This observation elucidates the reason why extractive initialization significantly reduces the difficulty",
            "score": 0.5077567680389344,
            "section_title": "Initializing from DistilBERTs",
            "char_start_offset": 9474,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1767,
                    "end": 1785,
                    "matchedPaperCorpusId": "248177840"
                },
                {
                    "start": 1810,
                    "end": 1834,
                    "matchedPaperCorpusId": "5855042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.457275390625
        },
        {
            "corpus_id": "212908749",
            "title": "Improved Knowledge Distillation via Teacher Assistant",
            "text": "Given a fixed student network, e.g., a Convolutional Neural Network (CNN) with 2 layers to be deployed on a small embedded device, and a pool of larger pre-trained CNNs, which one should be selected as the teacher in the knowledge distillation framework? The first answer is to pick the strongest which is the biggest one. However, this is not what we observed empirically as showing in Figure 2. Here, a plain CNN student with 2 convolutional layers is being trained via distillation with similar but larger teachers of size 4, 6, 8, and 10 on both CIFAR-10 and CIFAR-100 datasets. By size, we mean the number of convolutional layers in the CNN. This number is roughly proportional to the actual size or number of parameters of the neural network and proxy its capacity. Note that they are usually followed by max-pooling or fully connected layers too. We defer the full details on experimental setup to experiments section. With increasing teacher size, its own (test) accuracy increases (plotted in red on the right axis). However, the trained student accuracy first increases and then decreases (depicted in blue on the left axis). To explain this phenomenon, we can name a few factors that are competing against each other when enlarging the teacher: 1. Teacher's performance increases, thus it provides better supervision for the student by being a better predictor. 2. The teacher is becoming so complex that the student does not have the sufficient capacity or mechanics to mimic her behavior despite receiving hints. 3. Teacher's certainty about data increases, thus making its logits (soft targets) less soft. This weakens the knowledge transfer which is done via matching the soft targets. Factor 1 is in favor of increasing the distillation performance while factors 2 and 3 are against it. Initially, as the teacher size increases, factor 1 prevails; as it grows larger, factors 2 and 3 dominate. \n\nSimilarly, imagine the dual problem. We are given a large teacher network to be used for training smaller students, and we are interested in knowing for what student size this teacher is most beneficial in the sense of boosting the accuracy against the same student learned from scratch.",
            "score": 0.5073263057906814,
            "section_title": "The Gap Between Student and Teacher",
            "char_start_offset": 9879,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1909
                },
                {
                    "start": 1912,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2199
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47412109375
        },
        {
            "corpus_id": "237845150",
            "title": "Deep Collaborative Learning for Randomly Wired Neural Networks",
            "text": "The implicit ensemble is usually seen as a regularization method to reduce overfitting. Moreover, it can be used along with an explicit ensemble approach. \n\nKnowledge distillation refers to training a smaller model (i.e., a student) to mimic the performance of a large model or an ensemble (i.e., a teacher). The student model is trained with an additional loss function to prompt the model to be identical to the teacher model. Various distillation methods have been introduced to examine different types of loss functions [21,22], different forms of teacher model [23,24], and the best way to train the student model [25,26]. For example, in [27], the authors introduced an approach (called AvgMKD) to distill knowledge from multiple teachers. \n\nThey integrated softened outputs of each teacher equally and imposed constraints on the intermediate layers of the student models using the relative dissimilarity learned from the teacher networks. However, by treating each teacher equally, the differences between teacher models could be lost. In [14], authors proposed an adaptive multi-teacher knowledge distillation method (named AMTML-KD) that extended the previous method by adding an adaptive weight for each teacher model and transferring the intermediatelevel knowledge from hidden layers of the teacher models to the student models. \n\nAnother distillation variant is co-distillation [12,13] where the teacher and student had the same network architecture and were trained in parallel using distillation loss before any model converged. It has shown improvement in the speed of model training and its accuracy. Zhang's method [12] can be seen as co-distillation of models that have different architectures. Our proposed method can be seen as the co-distillation of randomly generated models, but the distillation method is using transfer learning instead of an extra loss function. \n\nKnowledge transfer is another student-teacher paradigm, where the knowledge is transferred by passing the parameters of each layer of a trained teacher model to the student model as initialization before beginning training the student model. The knowledge is transferred from a smaller model to a larger model with function preserving transformations to accelerate the training of the student model. The expansion of the student network can be achieved by increasing its depth, width, or kernel size.",
            "score": 0.5071290696158306,
            "section_title": "Related Work",
            "char_start_offset": 9045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 154
                },
                {
                    "start": 157,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1340
                },
                {
                    "start": 1343,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2290
                },
                {
                    "start": 2291,
                    "end": 2391
                }
            ],
            "ref_mentions": [
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1046,
                    "end": 1050,
                    "matchedPaperCorpusId": "224818016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68017578125
        },
        {
            "corpus_id": "245334959",
            "title": "Learning with Label Noise for Image Retrieval by Selecting Interactions",
            "text": "Now comes the question of what to use for f \u22c6 . In practice a perfect model f \u22c6 is not available, otherwise the retrieval problem would already be solved. To create a model which approaches it, we take inspiration from knowledge distillation, where a strong teacher network provides knowledge to a student network that learns the knowledge [18]. More specifically, we take inspiration from Mean Teacher [38]. It consists of a teacher-student setup with two similar model architectures, where the student model weights are updated through gradient descent and the teacher model weights as an exponential moving average of the student  weights. Mean Teacher uses two loss functions, a classification loss on the student model and a consistency loss that compares the softmax outputs of both networks, usually the MSE loss or KL-divergence loss. Strictly speaking, Mean Teacher is not a self-distillation method, since it optimizes for consistency during training, while for self-distillation methods the consistency regularization is usually performed after training. We decide to use a teacher only for the task-specific loss and do not aim to optimize for consistency between the teacher and student models outputs. Therefore, we do not consider this as a typical knowledge distillation approach. Our main model and teacher model have the same architecture and same initialization. Our main model is updated by backpropagation and we choose the update for the teacher model to be an exponential moving average of the parameters of the main model every iteration. \n\nwhere \u03b8, \u03b8 \u22c6 are the main model and teacher's weights, respectively. Our network updating step is similar to [38].",
            "score": 0.5065225074794271,
            "section_title": "Using a Teacher",
            "char_start_offset": 13038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1562
                },
                {
                    "start": 1565,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1679
                }
            ],
            "ref_mentions": [
                {
                    "start": 340,
                    "end": 344,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5546875
        },
        {
            "corpus_id": "251741056",
            "title": "Design Automation for Fast, Lightweight, and Effective Deep Learning Models: A Survey",
            "text": "Knowledge distillation (KD) is extended from knowledge transfer (KT) [143] by Ba and Caruana [144] to compress a cumbersome network (teacher) into a smaller and simpler network (student). This is done by making the student model mimic the function learned by the teacher model in order to achieve a competitive accuracy. It is later formally popularized by Hinton et al. [21] as a student-teacher paradigm, where the knowledge is transferred from the teacher to the student by minimizing the difference between the logits (features before the final softmax) of the teacher and student. In many situations, the performance of the teacher is almost perfect with a very high classification probability for the correct class and flat probabilities for the other classes. Therefore, the teacher is not able to provide much more information than the ground truth labels. Hinton et al. [21] introduce the concept of softmax temperature to transfer knowledge, which can better deliver the information of which classes the teacher find similar to the correct class. Formally, given the logits of the teacher model, the classification probability p i of the class i is: \n\nwhere \u03c4 is the temperature parameter. It controls how soft the labels from the teacher are. The soft labels together with the ground truth labels are used to supervise a compact student model. \n\nVanilla knowledge distillation mostly focuses on transferring knowledge to a student model with a fixed small architecture, which is manually designed in advance. However, different teachers and tasks favour different student architectures, and hand-crafted architectures are prone to be suboptimal. Considering these limitations, there is a growing trend to automate the architecture design of a student model [145], [146], [147], [148], [149], [150], [151]. The ground-truth labels are combined with the distillation labels to guide the automatic design process. AKDNet [145] proposes to search optimal student architectures for distilling a given teacher by RL-based NAS. It adopts the efficient search space of [48] and designs a KD-guided reward with a teacher network.",
            "score": 0.5063988098317072,
            "section_title": "B. Automated Knowledge Distillation",
            "char_start_offset": 84556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 74,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 93,
                    "end": 98,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 371,
                    "end": 375,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 879,
                    "end": 883,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1768,
                    "end": 1773,
                    "matchedPaperCorpusId": "208175624"
                },
                {
                    "start": 1782,
                    "end": 1787,
                    "matchedPaperCorpusId": "235428017"
                },
                {
                    "start": 1810,
                    "end": 1815,
                    "matchedPaperCorpusId": "233877900"
                },
                {
                    "start": 1929,
                    "end": 1934,
                    "matchedPaperCorpusId": "208175624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53369140625
        },
        {
            "corpus_id": "276580552",
            "title": "AfroXLMR-Comet: Multilingual Knowledge Distillation with Attention Matching for Low-Resource languages",
            "text": "Knowledge distillation (KD) refers to the process where a smaller model learns from a larger one, transferring knowledge from a teacher model to a student model to enhance the student's performance. The central concept is that the student model emulates the teacher model, using the teacher's insights to achieve competitive or even superior results. A typical KD framework includes three main components: knowledge, a distillation algorithm, and the teacher-student model architecture. The key challenge is to effectively convey knowledge from the large teacher model to the smaller student model while ensuring the student retains or improves its performance (Bucilu\u01ce et al., 2006;Ba and Caruana, 2014;Hinton, 2015;Urban et al., 2017). \n\nResponse-based knowledge distillation focuses on aligning the final output (logits) of the student model with the teacher's logits, typically using soft targets and temperature scaling to capture the \"dark knowledge\" from the teacher. This approach has been widely applied in tasks like image classification, but it is limited by only utilizing the output of the last layer, potentially missing intermediatelevel supervision that could benefit deeper networks (Hinton, 2015;Ba and Caruana, 2014). \n\nTo address this, feature-based knowledge distillation extends the method by transferring feature maps from intermediate layers of the teacher model to the student model, enhancing representation learning. Techniques like attention maps and activation boundaries are used to match features between layers, but challenges persist in selecting the right layers and handling size differences between layers (Romero et al., 2015;Zagoruyko and Komodakis, 2017). \n\nIn this work, we propose a novel hybrid distillation framework that integrates knowledge distillation and attention matching to improve multilingual model compression. Existing approaches often focus on either response-based distillation, which transfers knowledge through soft output distributions, or feature-based distillation, which aligns internal representations. Our method integrates both, enabling a more comprehensive transfer of knowledge from teacher to student models. Additionally, we introduce a highly compact multilingual student model with a significantly smaller hidden dimension, optimized for low-resource African languages. Our contributions can be summarized as follows:",
            "score": 0.5063857838046999,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 1565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1236
                },
                {
                    "start": 1239,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1694
                },
                {
                    "start": 1697,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2342
                },
                {
                    "start": 2343,
                    "end": 2390
                }
            ],
            "ref_mentions": [
                {
                    "start": 661,
                    "end": 683,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 683,
                    "end": 704,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1214,
                    "end": 1235,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64013671875
        },
        {
            "corpus_id": "257038997",
            "title": "HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers",
            "text": "Combining pruning and distillation. While we are the first work to combine pruning with distillation in task-agnostic setting, there have been similar explorations in task-specific setting. One stream of explorations first prune the model to the target size and then distill the subnetwork (Hou et al., 2020;Lagunas et al., 2021). In this case, pruning solely serves as an architecture selection strategy independent of distillation. Another stream simultaneously prunes and distills the model (Xu et al., 2021;Xia et al., 2022), which is more comparable to ours. The main differences are that they do not initialize the student with the teacher and often prune at a large granularity, e.g., a Transformer layer. In task-agnostic setting, however, an undesirable initialization and a large granularity will induce a huge discrepancy, which is difficult to minimize on large amount of open-domain data. Furthermore, after each layer pruning, the remaining layers need to match a different set of teacher layers to ensure the learning of comprehensive knowledge. However, suddenly switching the layer to learn from can be difficult on large amount of open-domain data. How to prune the student's height in task-agnostic setting remains an interesting open problem. A comprehensive comparison of these methods is deferred to Appendix A.5. \n\nResolving prediction discrepancy. Recent research has shown that distillation from a large teacher to a small student has only marginal benefits (Jin et al., 2019;Cho & Hariharan, 2019), mainly due to the large prediction discrepancy (Guo et al., 2020). Traditional solutions have resorted to introducing auxiliary teacher assistant models (Mirzadeh et al., 2020;Rezagholizadeh et al., 2021;Li et al., 2021), but training and storing auxiliary models can be memory and computational costly. \n\nTable 9: The evaluation performance of HomoDistil and the commonly used task-specific distillation baseline methods on the GLUE development set.",
            "score": 0.5062901289186138,
            "section_title": "DISCUSSION",
            "char_start_offset": 21356,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1335
                },
                {
                    "start": 1338,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1828
                },
                {
                    "start": 1831,
                    "end": 1975
                }
            ],
            "ref_mentions": [
                {
                    "start": 1483,
                    "end": 1501,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 1501,
                    "end": 1523,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1678,
                    "end": 1701,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303466796875
        },
        {
            "corpus_id": "272753449",
            "title": "Enhancing SLM via ChatGPT and Dataset Augmentation",
            "text": "Attempting to enable these smaller, more applicable models to comprehend natural language more profoundly opens up a new area of possible research. The concept of knowledge distillation (originally called model compression [3]) is used by a variety of authors to improve the performance of smaller student models by transferring (distilling) knowledge from an ensemble of models [9] or in more recent cases from a magnitudes stronger teacher model [11]. In the latter case a transfer function [24] is utilized. In most cases, parameters or complete layers of the teacher are embedded into the student model [1]. \n\nA similar and rather thorough approach to transfer knowledge to smaller models through reasoning is given by [10] that proposes a further developed Chain-of-thought method [27] for knowledge distillation. In the same fashion, [12] elicit increased reasoning abilities in small models utilizing the capabilities of GPT-3 while also generating free-text high-quality explanations. \n\nAnother approach to NLP problem-solving using only text-to-text models is introduced in [19], where capacities of transfer learning from data-rich environments to different downstream tasks are used. \n\nDifferent concepts of rationales can be applied to bolster a model's performance while fine-tuning an NLI dataset. While extractive explanations are fairly limited in their expressiveness and chance to lay open the model decisions, they are more straightforward to measure with tools/benchmarks like those proposed by [6]. At the same time, Free-Text Rationales can provide more insight into the background of the model's decision. Still, especially in model-generated Free-Text rationale, it is hard to check the relevance and coherence of these rationales without human supervision [16]. A detailed view into the reasons and ways for models benefiting from specific explanations is further tested and given by [8].",
            "score": 0.5059487654692526,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1913
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 226,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 493,
                    "end": 497,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 1909,
                    "end": 1912,
                    "matchedPaperCorpusId": "231786356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4609375
        },
        {
            "corpus_id": "258564799",
            "title": "DynamicKD: An Effective Knowledge Distillation via Dynamic Entropy Correction-Based Distillation for Gap Optimizing",
            "text": "Therefore, this paper focuses on knowledge distillation-based model compression. \n\nIn the knowledge distillation, how the teacher network better guides the student network training has become vital research. Several researchers have studied various knowledge used in the distillation process. \n\nRomero et al. applied the middle layer features of the teacher network as knowledge to guide the student network learning [26]. Komodakis et al. transferred attention from the large-scale teacher network to the student network [27]. Damiano et al. viewed the knowledge transfer between teacher and student networks as maximizing the mutual information between teacher and student networks [28]. Zagoruyko et al. used the attention mechanism as a learnable knowledge, and they used the teacher network's attention knowledge to guide the student network's training. In neural networks, the convolutional layers map one feature to another. Yim et al. treated the mapping processing of features between layers as knowledge and used the FSP matrix to describe this knowledge so that the student network could imitate it [29]. The rich and varied knowledge exchange between the teacher and student networks helps the student network training. \n\nBut, these methods do not focus on the performance gap between the teacher and student networks, which may affect student network learning. \n\nSeveral works have studied the performance gap and proposed corresponding improvements. Cho et al. found that the underperformance teacher with early-stop training benefits the student [30]. Mirzadeh et al. found that when the gap between the teacher network and the student network is large, the student trained by a lower-performance lightweight teacher network performs better than the one taught by a higher-performance large-scale teacher network [16]. For this reason, he utilized a medium-sized neural network (called Teacher Assistant) to help the student network cross the large performance gap. Both methods mentioned above show that reducing the performance gap can improve distillation performance. \n\nHowever, these static methods do not correct the performance gap further during the distillation. The performance gap keeps changing with the performance improvement of the student network, so these strategies may still hinder the student network from imitating the high-performance teacher network. \n\nVarious knowledge distillation algorithms are available to continuously update the knowledge applied for the student network training during the distillation.",
            "score": 0.5056212282674248,
            "section_title": "Introduction",
            "char_start_offset": 2212,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 83,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2086
                },
                {
                    "start": 2089,
                    "end": 2186
                },
                {
                    "start": 2187,
                    "end": 2388
                },
                {
                    "start": 2391,
                    "end": 2549
                }
            ],
            "ref_mentions": [
                {
                    "start": 522,
                    "end": 526,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 684,
                    "end": 688,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1561,
                    "end": 1565,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1828,
                    "end": 1832,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53857421875
        },
        {
            "corpus_id": "258048760",
            "title": "A Survey on Recent Teacher-student Learning Studies",
            "text": "The use of Simple Knowledge Distillation (SimKD) allows the compression of powerful teacher models into lightweight student models without sacrificing performance. This is achieved by reusing the discriminative classifier of the teacher model for student inference and training the student encoder by feature alignment with a single loss. A projector was also developed to help match the student encoder with the teacher classifier, making the technique applicable to a variety of teacher and student architectures. Experiments show that with the addition of the projector, the technique achieves state-of-the-art results at the cost of a reduced compression ratio. Vanilla KD aligns the logit or class projections of the two models, but the performance gap between the original teacher model and the refined student model is still large. Various methods have been proposed to overcome this problem, but they require detailed knowledge representation and optimized hyperparameters. The SimKD technique is proposed, which trains the student model by feature alignment in the previous layer of the classifier and directly replicates the teacher classifier for student inference. On a standard benchmark dataset, SimKD outperforms all state-of-the-art methods compared with various combinations of teacher-student architectures. The concept of knowledge distillation (KD), a technique for compressing knowledge from a powerful teacher model into a smaller student model, is discussed here. Transferred knowledge, usually in the form of soft targets, is thought to capture the relationships between different categories and serve as effective regularization during student training. Feature distillation is a common solution to prevent performance degradation in teacher-to-student compression, which utilizes more information from the middle layer of the teacher model. One of the proposed methods, SimKD, is related to hypothetical transfer learning (HTL) and aims to close the performance gap between teachers and students on the same dataset. Here, by analyzing the limitations of the existing method and focusing on its improvement, a new method, logit distillation, is proposed, and the knowledge distillation loss function is re-represented as a weighted sum of two components, target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD).",
            "score": 0.5055771230057073,
            "section_title": "SIMPLE KNOWLEDGE DISTILLATION",
            "char_start_offset": 36338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2375
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67529296875
        },
        {
            "corpus_id": "267782419",
            "title": "Wisdom of Committee: Distilling from Foundation Model to Specialized Application Model",
            "text": "In contrast, specialized application models are trained for specific application distributions, leading to a significant distribution gap. Lastly, potential differences in input features and modalities further increase the challenge as foundation models and the student could consume different features. For instance, LLMs can make predictions based on text features such as item descriptions and comments, while standard recommender systems rely on categorical and numerical features such as user history [25]. \n\nIn this paper, we explore adding complementary teachers who are more similar to the specialized application students in the model characteristics discussed above. A teacher committee with both foundation models and complementary models could bridge the vast differences between foundation models and specialized application models, facilitating a more effective knowledge transfer. Prior research in knowledge distillation also provides supportive evidence; for example, teachers with a smaller gap in model capacity lead to more effective student learning [34,30,31]. In addition, relying on a single teacher may limit the student's full potential. This is analogous to forming a Ph.D. committee, which comprises multiple members with diverse expertise, allowing diversity in the teachers to provide unique knowledge and perspective. Foundation models pose a more general understanding, whereas application teachers specialize in downstream tasks. \n\nHowever, harnessing the benefits of diverse teachers has its challenges.Since the complementary teachers are supposed to be more similar to the student, the teachers (the foundation model teachers and application model teachers) themselves could exhibit considerable differences in capacity, architecture, training distribution, input features, and modalities. For instance, for a rating prediction task where the student employs a standard recommendation architecture, a teaching committee comprising a large language model and a larger recommendation system would teach the student. \n\nThe recommendation system teacher is more similar to the student regarding architecture, capacity, and data distribution. At the same time, it is evident that these two teachers also possess significant differences. This breaks the common assumption of multi-teacher distillation methods where teachers share similar architecture and training distribution. Under this assumption, standard multi-teacher distillation methods enjoy the performance benefits mainly from a variance reduction perspective, while lack the understanding on teachers' expertise [47,9,14]. Thus, we propose an interactive distillation process, allowing the student to model each teacher's expertise and ask tailored questions to each teacher.",
            "score": 0.5050389877887302,
            "section_title": "Introduction",
            "char_start_offset": 2453,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1462
                },
                {
                    "start": 1465,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 2049
                },
                {
                    "start": 2052,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2267
                },
                {
                    "start": 2268,
                    "end": 2408
                },
                {
                    "start": 2409,
                    "end": 2615
                },
                {
                    "start": 2616,
                    "end": 2768
                }
            ],
            "ref_mentions": [
                {
                    "start": 1078,
                    "end": 1081,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 2605,
                    "end": 2609,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 2611,
                    "end": 2614,
                    "matchedPaperCorpusId": "227276362"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4814453125
        },
        {
            "corpus_id": "273350649",
            "title": "Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling",
            "text": "Text generation capabilities of large language models (LLMs) have seen continuous improvement, largely driven by scaling up the number of parameters and the amount of training data (Kaplan SKD addresses the limitations of on-policy knowledge distillation (KD) by filtering out low-quality student samples and replacing them with teacher generated tokens. Right: illustration of how SKD generalizes to both supervised KD (replacing with all teacher tokens) and on-policy (accepting all student tokens). et al., 2020;Anil et al., 2023;Dubey et al., 2024;OpenAI et al., 2024). However, the substantial inference-time costs and memory footprint associated with LLMs present significant challenges for practical deployment (Agarwal et al., 2024). Therefore, compressing LLMs while maintaining their performance is crucial for real-time practical applications. \n\nKnowledge Distillation (KD) (Hinton et al., 2015) is a widely used method to compress LLMs by transferring knowledge from a larger teacher model to a smaller student model. Traditional KD approaches, such as supervised KD (Sanh et al., 2020) and SeqKD (Kim & Rush, 2016b), rely on a static dataset of outputs to train the student model. However, this fixed dataset can lead to a distribution mismatch between the training data and the student's generated samples at inference time, hindering the student's learning. On-policy KD (Agarwal et al., 2024) attempts to address this traininference mismatch by training the student on its self-generated samples, with the teacher provides feedback through its token-level probabilities. However, the student may generate low-quality samples that are out-of-distribution (OOD) for the teacher, especially early in the training, leading to inaccurate teacher feedback. Additionally, the performance of on-policy is highly sensitive to the student's initialization, as illustrated in Figure 4. \n\nIn this work, we introduce Speculative Knowledge Distillation (SKD) that uses interleaved sampling between teacher and student to overcome the challenges faced by supervised and on-policy distillation (Figure 2).",
            "score": 0.5040408656046544,
            "section_title": "INTRODUCTION",
            "char_start_offset": 593,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 854
                },
                {
                    "start": 857,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1890
                },
                {
                    "start": 1893,
                    "end": 2105
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "246035505",
            "title": "It's All in the Head: Representation Knowledge Distillation through Classifier Sharing",
            "text": "The second approach aims at tackling the limited capacity of the student in the distillation process. In a conventional knowledge distillation, the teacher model is trained independently and in isolation from the student training process. Typically, the capacity of the teacher model is higher than the capacity of the student model, and thus the features learned by the teacher may not be applicable for the student training.\n\nTo this end, we propose to train a teacher while considering the limited capacity of the student, by initializing the teacher's classifier with the weights of a temporary student's head and fixing them during the training. This process can be viewed as a regularization mechanism that enforces the teacher to learn useful features suited for the student's limitations. The method can be depicted as a three-step training procedure:\n\nstep-I 0 : A student model is trained, with KD or without, providing a backbone and a classifier head with parameters, {\u03c6 0 s , \u03b8 s }. step-I 1 : A teacher model is trained by initializing and fixing its classifier with \u03b8 s to obtain the teacher model parameters, {\u03c6 t , \u03b8 s }.\n\nstep-I 2 : A student model is trained using the loss in equation (1), with the teacher model obtained in step I 1 , to produce the final student parameters:\n\nThe approach, named SH-KD, is illustrated in Fig. 1b. Note that for cases where the embedding dimensions of the teacher and the student are not the same, we add a linear transformation to the model's backbone. SH-KD method offers an easy-to-implement yet effective scheme that enables to tailor the teacher model to comply with the student's capacity during the teacher's training at the cost of an additional training iteration.\n\nIndeed, the accuracy of the teacher may be decreased when using the SH-KD scheme compared to the accuracy obtained by a teacher that was trained conventionally. However, in knowledge distillation, we do not aim at optimizing the teacher, but rather to improve the accuracy of the student. Specifically, a lower accuracy of the teacher does not necessarily lead to a lower accuracy of the student. The same observation was made in the teacher assistant (TA) technique [25]. Instead of using a teacher model with the largest capacity which produces the highest",
            "score": 0.5039405628922258,
            "section_title": "Student-Head Sharing (SH-KD)",
            "char_start_offset": 11758,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5810546875
        },
        {
            "corpus_id": "258947093",
            "title": "Improving Knowledge Distillation via Regularizing Feature Norm and Direction",
            "text": "In this subsection, we present simple comparisons for mainstream knowledge distillation methods, as illustrated in Fig. 6. Fig. 6a and Fig. 6b showcase examples of homogeneous distillation (ResNet-34 \u2192 ResNet-18) and heterogeneous distillation (ResNet-50 \u2192 MobileNet-V1) on the Im-ageNet dataset. We measure the average time cost per batch iteration over the entire dataset as the horizontal axis and the Top-1 accuracy as the vertical axis. The varying sizes of circular markers representing different methods are proportional to the actual model parameter sizes. It is clear that our approach (KD++) delivers better performance with a small amount of time expense. It is important to highlight that in heterogeneous knowledge distillation tasks, there is typically a disparity in feature dimensions. Consequently, the inclusion of a bridging linear dimension transformation layer becomes imperative, attributing to the marginal increment in parameterization observed in our method, KD++, as compared to the classical KD approach. In earlier sections, we discover that improving the student model's norm benefits knowledge distillation. Therefore, a natural question arises: does increasing the teacher model norm also contribute to improving student performance? To investigate this, we conduct simple experiments where we introduce a scaling factor, denoted as m, to the norm of the teacher model in Eq.8 as follows: \n\nInterestingly, our experimental results indicate that in the context of homogeneous knowledge distillation, altering the norm of the teacher model, whether increasing or decreasing it, does not lead to better improvement in student performance compared to maintaining the original norm of the on the training set (73%, for example). In such cases, using all training samples to evaluate class centers would inevitably impact the distribution of each class center. We investigate two methods for computing class centers on ImageNet: (1) utilizing all samples and (2) only considering the correctly predicted samples by the teacher model. It is important to note that all samples are derived from the training set. The teacher and student models are ResNet-34 and ResNet-18, with a teacher accuracy of approximately 73% on the ImageNet training set.",
            "score": 0.5037327448979669,
            "section_title": "B.2 Complexity Comparisons",
            "char_start_offset": 24565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1419
                },
                {
                    "start": 1422,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2269
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447265625
        },
        {
            "corpus_id": "258841668",
            "title": "Learning from Mistakes via Cooperative Study Assistant for Large Language Models",
            "text": "Benefiting the LLMs' ability to follow instructions, recent researchers add textual feedback into the prompt and directly ask models to revise their response (Peng et al., 2023;Shinn et al., 2023). Moreover, the feedback can be one time (Saunders et al., 2022), or multiple times (Scheurer et al., 2023;Madaan et al., 2023). In this work, we use feedback as the instruction of the main LLM and ask it to refine its answer. \n\nTeacher-student Learning Teacher-student learning is a knowledge distillation method to transfer knowledge from a larger teacher model to a smaller student model (Hinton et al., 2015;Gou et al., 2021). The goal is to produce similar results as the powerful teacher model with fewer parameters and computational costs. The teaching assistant is an intermediate model mimicking the behavior of the teacher model and then teaches the student model. It usually has a medium size between the student and the teacher (Mirzadeh et al., 2020). Recently, a lot of work has tried to distill knowledge in large language models to enhance the capability of small models, such as commonsense (Bhagavatula et al., 2023;West et al., 2022) and the reasoning ability (Shridhar et al., 2023;Magister et al., 2022). Unlike knowledge distillation, the study assistant in SALAM does not need a stronger capability on the downstream tasks. It is designed to analyze the output of the base model given the ground truth, providing a guideline for the base model to avoid similar mistakes.",
            "score": 0.5037078363266019,
            "section_title": "Related Work",
            "char_start_offset": 7873,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1489
                }
            ],
            "ref_mentions": [
                {
                    "start": 608,
                    "end": 625,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 936,
                    "end": 959,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1104,
                    "end": 1130,
                    "matchedPaperCorpusId": "254854264"
                },
                {
                    "start": 1130,
                    "end": 1148,
                    "matchedPaperCorpusId": "238857304"
                },
                {
                    "start": 1175,
                    "end": 1198,
                    "matchedPaperCorpusId": "254125395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2318115234375
        },
        {
            "corpus_id": "231986112",
            "title": "Exploring Knowledge Distillation of a Deep Neural Network for Multi-Script identification",
            "text": "Earlier papers used ensemble methods for model compression [8,9]. Distillation of knowledge from a teacher network and transferring it to a student network to mimic the teacher network is a basic fundamental concept of knowledge distillation. The first proposed concept of knowledge distillation [7] introduces the concept of compressing the knowledge of a more in-depth or larger model to a single computational efficient neural network. It has introduced the concept of dark knowledge transfer from a deep teacher network to a smaller student network by taking the softmax of the results of the teacher network with a specific temperature value and calculating loss between it and the predicted outputs of the student network. They validated their findings by running on MNIST dataset and, JFT dataset by google and other speech recognition tasks. Since then, knowledge distillation has progressed a lot, and adversarial methods [17,18] also have utilized for modelling knowledge transfer between teacher and student. After this study, extensive research has conducted on knowledge distillation. In the paper [11] has introduced the transfer of a hidden activation output and other has proposed transferring attention information as knowledge [20]. \n\nArticle [19] has briefly described the advantages and efficiency of the knowledge distillation. It has described importance of knowledge transfer from teacher to student model using distilled knowledge. They have compared two student deep neural networks trained with teacher network and without teacher model with same size.They have proposed a method of transferring the distilled knowledge between two layers to shows three important points. The student model is more efficient than the original model and it also outperform the original model which is trained from scratch. The student network understand the flow of solving the problem and it start learning with good initial weights. It can learnt and optimized faster than original or normal deep neural network.This paper proves that, the student model reports better efficiency than a normal network without a teacher model.They have compared various knowledge transfer techniques with a normal network without any teacher model for knowledge transfer.They have learned their model with two main condition. First, the teacher model must pretrained with some different dataset and second condition is the teacher model is shallower or deeper than the student model. Their approach contains two step training.",
            "score": 0.5033949587819156,
            "section_title": "Related Study",
            "char_start_offset": 4015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2318
                },
                {
                    "start": 2319,
                    "end": 2476
                },
                {
                    "start": 2477,
                    "end": 2519
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 62,
                    "matchedPaperCorpusId": "9433631"
                },
                {
                    "start": 62,
                    "end": 64,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 931,
                    "end": 935,
                    "matchedPaperCorpusId": "53976534"
                },
                {
                    "start": 1261,
                    "end": 1265,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.418212890625
        },
        {
            "corpus_id": "267312283",
            "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
            "text": "Knowledge distillation (KD) is a technique that transfers useful knowledge from a large teacher model T to a small student model S to improve the accuracy of S. This process encourages the student model to generate outputs that are similar to that of the teacher model to obtain the teacher model's generalized knowledge. KD matches the outputs of classifiers, embedding layers, and sublayers in T and S to distill diverse types of knowledge within different distillation sources. We explain diverse types of distillation sources used for KD in Section 5.1.1. \n\nWhen we apply KD for language models, we need to consider how to match sublayers in T and S since they have different numbers of sublayers. We categorize the sublayer matching strategies into three groups: 1:1, many:1, and many:many strategies. We explain more details about each of the sublayer matching strategies in Section 5.1.2. \n\nWe summarize the performance of diverse KD algorithms for encoder-only Transformers in Table 6. We denote the distillation sources and the sublayer-matching strategies of each KD algorithm in columns 2 and 3, respectively. We Table 6. Comparison of the performance of knowledge distillation (KD) algorithms for encoder-only Transformers on MNLI, QQP, and SQuAD 1.1 (SQD) benchmarks.",
            "score": 0.5033040063176358,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 53658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1280
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64306640625
        },
        {
            "corpus_id": "276902993",
            "title": "Asymmetric Decision-Making in Online Knowledge Distillation:Unifying Consensus and Divergence",
            "text": "In conventional offline knowledge distillation paradigms (Hinton et al., 2015;Tian et al., 2019;Chen et al., 2021), a pre-trained teacher model provides soft supervision to guide student training through post-hoc knowledge transfer. This two-stage paradigm inherently suffers from computational redundancy and temporal decoupling between teacher-student interactions. In contrast, recently proposed Online Knowledge Distillation (OKD) methods simplify to one-stage distillation where both teacher and student models learn simultaneously from scratch, while achieving competitive or even superior performance (Guo et al., 2020;Qian et al., 2022;Song et al., 2023;Zhang et al., 2023;2018b). Typically, Deep Mutual Learning (DML) (Zhang et al., 2018b) encourages each network to mutually learn from each other by matching their output logit via KL divergence. In this paper, we propose a novel Asymmetric Decision-Making strategy that unifies consensus and divergence feature learning to achieve better OKD performance. This paper investigates a fundamental question: How does intermediate feature alignment affect model behavior in online distillation? Through systematic analysis using Class Activation Mapping(CAM) (Zhou et al., 2016) (Figure 1 and 2), we reveal two pivotal findings: (1) the similar features between student and teacher models are predominantly concentrated on foreground objects, suggesting students prioritize learning these \"easy\" patterns. (2) teacher models highlight foreground objects more than students, indicating an underutilized capacity for discriminative feature discovery. Surprisingly, the most informative divergent features (which may explain teacher superiority) still concentrate on foreground areas rather than background contexts. \n\nBuilding on these insights, we propose Asymmetric Decision-Making (ADM). It is a unified framework that coordinates consensus enhancement and divergence exploration through spatially-aware feature modulation. Unlike conventional symmetric distillation, ADM implements rolespecific learning strategies: For students, we amplify feature attribution in teacher-aligned foreground regions to accelerate confidence building.",
            "score": 0.5032265040389379,
            "section_title": "Introduction",
            "char_start_offset": 1533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1769
                },
                {
                    "start": 1772,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 114,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 608,
                    "end": 626,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 626,
                    "end": 644,
                    "matchedPaperCorpusId": "252198825"
                },
                {
                    "start": 644,
                    "end": 662,
                    "matchedPaperCorpusId": "258108186"
                },
                {
                    "start": 681,
                    "end": 687,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 727,
                    "end": 748,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5048828125
        },
        {
            "corpus_id": "270257777",
            "title": "PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs",
            "text": "Large language models (LLMs) have shown remarkable abilities across a wide range of tasks (OpenAI, 2022;Anil et al., 2023).However, their huge parameter sizes and computational requirements pose significant challenges for practical deployment, especially in environments with limited resources.Knowledge distillation (KD) has emerged as a technique for addressing these challenges by transferring insights from a large, sophisticated teacher model to a compact student model with reduced memory footprints and inference costs.The seminal work (Hinton et al., 2015) proposes to train a student model to match the output class distribution of the teacher model.Kim and Rush (2016) further extends this idea to the sequence level and teaches the student to directly produce teachers' decoded sequences.Another line of work (Jiao et al., 2019;Wang et al., 2020) seeks to align the student model's intermediate-layer representations with the teacher's.All these approaches employ a teacher-forcing strategy, training the student to fully match the outputs or representations of the teacher model.\n\nApplying conventional KD methods to LLMs presents several significant challenges.First, those LLM teachers are typically only available through API calls.The absence of direct access to the full output logits or internal states of LLM teachers hinders the implementation of traditional distillation techniques.Second, the capacity gap between the student model and LLM teachers becomes significantly larger compared to the previous instances when a relatively smaller teacher model was employed.This disparity exacerbates the student model's limited ability to fully match the teacher LLM's output distribution.Third, as LLMs increase in size, they often encounter a mis-calibration issue (Zhao et al., 2023) where sequences that are highly likely according to the model don't necessarily exhibit high quality for target tasks.Consequently, when a student model is trained to mimic these outputs from the teacher LLM, arXiv:2406.02886v2[cs.CL] 6 Jun 2024 it inherits this mis-calibration, leading to suboptimal performance.",
            "score": 0.5028715798374042,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 123,
                    "end": 294
                },
                {
                    "start": 294,
                    "end": 526
                },
                {
                    "start": 526,
                    "end": 659
                },
                {
                    "start": 659,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 947
                },
                {
                    "start": 947,
                    "end": 1091
                },
                {
                    "start": 1093,
                    "end": 1174
                },
                {
                    "start": 1174,
                    "end": 1247
                },
                {
                    "start": 1247,
                    "end": 1403
                },
                {
                    "start": 1403,
                    "end": 1588
                },
                {
                    "start": 1588,
                    "end": 1704
                },
                {
                    "start": 1704,
                    "end": 1920
                },
                {
                    "start": 1920,
                    "end": 2029
                },
                {
                    "start": 2029,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 839,
                    "end": 857,
                    "matchedPaperCorpusId": "211296536"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.708984375
        },
        {
            "corpus_id": "267657497",
            "title": "Maximizing discrimination capability of knowledge distillation with energy function",
            "text": "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks. Over the years, KD methods have evolved to narrow the performance gap between student and teacher models by utilizing both final predictions, known as logits-based distillation [10,15,16,11,17], and intermediate features, known as features-based distillation [18,19,20,21,22,23,24,12,25,26,27,28]. \n\nPrevious works on logits-based distillations include the following: DML [15] proposed a mutual learning strategy for collaboratively teaching and learning between student and teacher models; TAKD [16] introduced a multi-step method with an intermediate-size network (i.e., assistant network) to bridge the gap between teachers and students; DKD [11] decomposed the soft-label distillation loss into two components: target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD), enabling each part to independently harness its effectiveness; Multi KD [17] proposed multi-level prediction alignment, containing instance, batch, and class levels, and prediction augmentation. While these approaches emphasize effective knowledge transfer, they do not consider dividing entire datasets or provide mechanisms to distinguish and transfer knowledge from specific samples. \n\nFitNet [18] was groundbreaking as it leveraged not only the final outputs but also intermediate representations. Since the introduction of FitNet, various feature-based KD methods have emerged as follows: AT [22] prompted the student to mimic the attention map of the teacher network; PKT [19] employed various kernels to estimate the probability distributions, employing different divergence metrics for distillation; RKD [20] focused on transferring the mutual relations of data samples; CRD [21] framed the objective as contrastive learning for distillation; VID [23] took a different approach by maximizing mutual information; OFD [24] introduced a novel loss function incorporating teacher transform and a new distance function; Review KD [12] introduced a review mechanism that leverages past features for guiding current ones through residual learning.",
            "score": 0.5026520602273098,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 3764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 366,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 439,
                    "end": 442,
                    "matchedPaperCorpusId": "219169868"
                },
                {
                    "start": 442,
                    "end": 445,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 451,
                    "end": 454,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 454,
                    "end": 457,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 457,
                    "end": 460,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 460,
                    "end": 463,
                    "matchedPaperCorpusId": "258298441"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "269167845"
                },
                {
                    "start": 469,
                    "end": 472,
                    "matchedPaperCorpusId": "269206209"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1058,
                    "end": 1062,
                    "matchedPaperCorpusId": "260933721"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.451416015625
        },
        {
            "corpus_id": "273185547",
            "title": "Accelerating Diffusion Models with One-to-Many Knowledge Distillation",
            "text": "All students within the group collaborate to generate high-quality images similar to their respective teachers while preserving the same acceleration ratio as one-toone knowledge distillation. In O2MKD, since each student is solely responsible for learning the teacher in a subset of timesteps, the learning complexity for each student is significantly reduced. This, in turn, results in superior image fidelity compared to traditional knowledge distillation. \n\nExtensive experimental results demonstrate the effectiveness of O2MKD in various experimental scenarios including CIFAR10, LSUN Church, CelebA-HQ with DDPM, and COCO2013 with Stable Diffusion. It's worth noting that O2MKD presents three notable advantages: \n\n\u2022 Compatibility with other acceleration techniques: O2MKD is designed to alleviate the computational overhead associated with a single step (i.e., the computation of the UNet within the diffusion model) instead of reducing the number of sampling steps. This enables its compatibility with other acceleration techniques such as DDIM, as demonstrated in the discussion section. \n\n\u2022 Compatibility with previous KD: O2MKD introduces a novel framework involving multiple students and a single teacher, which can be directly applied to most existing knowledge distillation methods. \n\n\u2022 Beyond Model Compression: O2MKD can also be utilized in the common diffusion model training to improve the generalization performance, extending its possible utility to the settings beyond model compression. Besides, in the discussion section, we also demonstrate that the additional memory overhead introduced by using multiple students is acceptable, and can be further reduced by using the model merging technique, which merges multiple students into one student after training (Ilharco et al. 2022). \n\nThis paper brings two insights as the takeaway for readers: \n\n\u2022 We find that the effectiveness of O2MKD can be explained from the perspective of distillation from a general model (i.e the teacher for all the timesteps) into multiple domain experts (i.e. multiple students for different timestep ranges), where a hyper-parameter should be utilized to balance the domain knowledge and the general knowledge, as discussed in Figure 7. \n\n\u2022 As discussed in Figure 9, we demonstrate that the supervision from knowledge distillation is significantly more stable than the supervision from traditional training loss of diffusion models.",
            "score": 0.5023325312330313,
            "section_title": "Introduction",
            "char_start_offset": 6168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1804
                },
                {
                    "start": 1807,
                    "end": 1866
                },
                {
                    "start": 1869,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2238
                },
                {
                    "start": 2241,
                    "end": 2434
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26220703125
        },
        {
            "corpus_id": "229679667",
            "title": "ALP-KD: Attention-Based Layer Projection for Knowledge Distillation",
            "text": "To utilize intermediate layers' information (and other components in general), a family of models exists that defines a dedicated loss function to measure how much a student diverges from its teacher in terms of internal representations. In particular, if the goal is to distill from an n-layer teacher into an m-layer student, a subset of m (out of n) teacher layers is selected whose outputs are compared to those of student layers (see Equation 3 for more details). Figure 1 illustrates this concept.\n\nFigure 1: Student and teacher models have m and n layers, respectively. Each node is an intermediate layer and links are cross-model connections. In this example, every other layer of the teacher is skipped in order to match the size of the student. The output of nodes connected to each other are compared via a loss function (shown with \u2194) to ensure that the student model has similar internal representations as its teacher.\n\nAs the figure shows, each student layer is connected to a single, dedicated peer on the teacher side, e.g. the n-th teacher layer corresponds to the m-th student layer. Since outputs of these two layers are compared to each other, we hope that both models generate as similar outputs as possible at points n and m. With this simple technique, teacher's knowledge can be used to supervise student's intermediate layers.\n\nExperimental results show that intermediate layer matching could be quite effective, but in our study we realized that it may suffer from two shortcomings:\n\n\u2022 If n m, multiple layers in T have to be ignored for distillation but we know that those layers consist of precious information for which we spend expensive resources to learn. This issue is referred to as the skip problem in this paper.\n\n\u2022 Moreover, it seems the way teacher layers are kept/skipped is somewhat arbitrary as there is no particular strategy behind it. Before training, we lack enough knowledge to judge which subset of teacher layers contributes more to the distillation process, so there is a good chance of skipping significant layers if we pick them in an arbitrary fashion. Finding the best subset of layers to distill from requires an exhaustive search or an expert in the field to signify connections. We refer to this issue as the search problem.\n\nIn order to resolve the aforementioned issues we propose an alternative, which is the main contribution of this paper.",
            "score": 0.5019413174440175,
            "section_title": "Problem Definition",
            "char_start_offset": 2648,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7919921875
        },
        {
            "corpus_id": "260815602",
            "title": "Learning Lightweight Object Detectors via Multi-Teacher Progressive Distillation",
            "text": "In a successful distillation process, it is crucial to have a degree of similarity between the student and teacher models to facilitate the transfer of knowledge. Our similarity-based teacher sequence design, which incorporates feature-level measurements, ensures a more effective learning process for the student. Consequently, BGS proves to be more effective than algorithms that solely focus on performance ranking. \n\nIn summary, our greedy backward construction, BGS, works the best in our setting, outperforming globally optimized shortest-path algorithms or a performance ranking based algorithm. The final target teacher has the most profound impact on the distilled student's performance. In order to fully assist the final teacher, we need to use another teacher with the minimal adaptation cost to the final teacher before it, which is exactly the behavior of BGS.",
            "score": 0.501908660632398,
            "section_title": "Conclusion",
            "char_start_offset": 32436,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 418
                },
                {
                    "start": 421,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 874
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.295654296875
        },
        {
            "corpus_id": "259766297",
            "title": "The Staged Knowledge Distillation in Video Classification: Harmonizing Student Progress by a Complementary Weakly Supervised Framework",
            "text": "Different types of KD, including response-based [34], feature-based [31], [32], [35], and relation-based [33], [36], have been proposed to transfer knowledge from the teacher to the student. Researchers have also proposed various loss functions to enhance the student's performance, such as distilling knowledge with dense representations [37], attention-based knowledge distillation [38], and temperature-damping normalization [39]. Furthermore, different teacher-student architectures have been designed, including multiple teachers-students [40]- [46], selftraining [47], [48], and mutual learning [49]. However, despite the abundance of research on the topic, there has been little attention paid to the internal relationship between the type of distillation method and the structural design of the teacher- KD is a process that mimics human learning by systematically transferring knowledge from a teacher model to a student model. To maximize the effectiveness of this process, we propose a novel framework called Staged Knowledge Distillation (SKD), which consists of three distinct learning stages, as shown in Figure 1a. We argue that a blind, singlestage approach may be inefficient and ineffective due to its increased cost and potential for bias. \n\nTo address this issue, we adopt a parallel multi-branch structure training method divided into three substages (see Figure 2), which emulates the human learning process and improves the algorithm's generalization ability. However, if the capacity gap between the teacher and the student is too large, the training effect will be diminished [46]. To overcome this limitation, we introduce the teaching assistant method [46], which we refer to as cascade training. Additionally, we use a weakly supervised label enhancement technique that generates high-quality pseudo labels to supervise the classification in the video. \n\nThe final student model in SKD is obtained by combining the results of each stage, which reduces the model's gen-eralization error. However, the same learning framework is used internally for each parallel training branch, leading to the excessive correlation between models [50] and increased inference time and memory occupation when deployed.",
            "score": 0.5018260156165266,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 1675,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1258
                },
                {
                    "start": 1261,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1880
                },
                {
                    "start": 1883,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 52,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 105,
                    "end": 109,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 339,
                    "end": 343,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 384,
                    "end": 388,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 428,
                    "end": 432,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 550,
                    "end": 554,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "207853355"
                },
                {
                    "start": 1601,
                    "end": 1605,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1679,
                    "end": 1683,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46337890625
        },
        {
            "corpus_id": "246485656",
            "title": "How to Improve Deep Learning for Software Analytics (a case study with code smell detection)",
            "text": "This leads to simpler models that run faster. We now discuss yet another motivation for using feedforward networks: knowledge distillation. \n\nKnowledge distillation [23] is a technique that allows a \"teacher\" model to train a \"student\" model. Specifically, a large number of inputs is generated and fed to the originally trained, more complex model (a deep learner), and the outputs are captured. These input-output pairs are used to train a simpler, often faster or more interpretable model (such as a smaller deep learner or a decision tree). Surprisingly, the student model often matches or even outperforms the teacher model. Knowledge distillation has been widely studied [27,38,41], and shown to be useful for various tasks [10,21]. Knowledge distillation also has the benefits of yielding simpler, faster, models, and being amenable to transfer learning [55]. \n\nHowever, there have been shown to be limitations to knowledge distillation [11]. Specifically, a key insight of Cho and Hariharan [11] is that larger models may not be effective teachers, especially for smaller students, and typical solutions such as multiple rounds of distillation fail. From this, we infer that because convolutional models are typically significantly larger than their feedforward counterparts (due to the additional parameters introduced by the convolutional layers), they may be less effective teachers for decision tree learners. Therefore, we advocate instead for obtaining performance by leveraging hyper-parameter optimization on feedforward networks, which yield fast, simple models; then, if an interpretable model is required, knowledge distillation can be used to obtain a decision tree with similar performance. This is because our feedforward networks are significantly smaller than deep learners such as convolutional neural networks, and so can be effective teacher models in knowledge distillation.",
            "score": 0.5016699779766682,
            "section_title": "BROADER IMPLICATIONS",
            "char_start_offset": 31324,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1902
                }
            ],
            "ref_mentions": [
                {
                    "start": 681,
                    "end": 684,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 684,
                    "end": 687,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 734,
                    "end": 737,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 861,
                    "end": 865,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 944,
                    "end": 948,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 999,
                    "end": 1003,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63330078125
        },
        {
            "corpus_id": "264107179",
            "title": "Applying Knowledge Distillation to Improve Weed Mapping with Drones",
            "text": "This variant of KD consists of two distillation stages [13], where the first stage involves the teacher model distilling its knowledge to an intermediate assistant model, and the second stage involves the assistant model further distilling knowledge to the final student model. This approach is designed to leverage the expertise of the teacher model while mitigating the impact of the capability gap between the teacher and student models. Introducing the assistant model as an intermediary aims to minimize the loss of information and enable a more effective transfer of knowledge to the student model. As an assistant, we used Lawin, specifically the B0 variant, which falls between the teacher and student models in terms of complexity.",
            "score": 0.5013755908212039,
            "section_title": "D. Teacher Assistant Knowledge Distillation (TAKD)",
            "char_start_offset": 17489,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 740
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 59,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.370849609375
        },
        {
            "corpus_id": "273549440",
            "title": "Knowledge Distillation Using Frontier Open-source LLMs: Generalizability and the Role of Synthetic Data",
            "text": "Large Language Models (LLMs) have gradually become more capable of showing remarkable abilities to understand and generate language, and being able to solve a variety of natural language processing (NLP) tasks such as questionanswering and summarization. The largest and most capable LLMs, such as Llama-3.1-405B-Instruct, are setting new standards in AI performance. With its massive 405 billion parameters, the model not only holds immense pre-trained knowledge, but according to Meta's benchmark evaluations (Meta, 2024), also performs competitively with leading closed-source models. Beyond the direct use of the model for inference and text generation, which is associated with significant computational demands, the 405B model can also be utilized for knowledge distillation. \n\nKnowledge distillation is a promising direction to alleviate inference cost and latency without sacrificing accuracy (Hinton et al., 2015;Papamakarios, 2015). In knowledge distillation, the knowledge from a large, complex model (the \"teacher\" model) is transferred to a smaller, simpler model (the \"student\" model). The objective is to develop a student model that closely approximates the performance of the teacher model while being smaller, cheaper and faster for inference. In knowledge distillation, knowledge is transferred from a teacher model to a student model using one Our methodical study highlights the critical role of high-quality synthetic data in knowledge distillation and the necessity of accurate evaluation processes. Through extensive experiments on various well-known benchmarking datasets, we demonstrate that enhanced task-specific synthetic data helps the student LLM learn the teacher's reasoning abilities. Rest of the paper is organized as follows. Section 2 describes prior related work on similar methods. Section 3 describes our distillation methodology that uses task-specific prompts, metrics, and synthetic data generation procedure. Section 4 covers the results for all the tasks and datasets showing generalizability of distillation using the Llama-3.1 teacher-student pairs. In Section 5, we present the conclusions and future work, while Section 6 discusses the limitations of our work. All the prompts used in our experimentation are provided in the Appendix.",
            "score": 0.5011596449536888,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2209
                },
                {
                    "start": 2210,
                    "end": 2283
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65478515625
        },
        {
            "corpus_id": "275007781",
            "title": "Knowledge Reasoning- and Progressive Distillation-Integrated Detection of Electrical Construction Violations",
            "text": "To verify the effectiveness of progressive distillation, we conducted experiments using different teacher models. The results are shown in Table 5, and further demonstrate the impact of selecting various teacher models on the performance of the student network. This comparison highlights how different teacher models contribute to guiding the student network in learning more effectively, improving overall detection accuracy. It can be observed that compared to directly using a single advanced-teacher or primary-teacher model for knowledge distillation, the progressive distillation strategy achieves better results. Through phased learning, progressive distillation first leverages the rich expressive capabilities of the advanced teacher model to establish a solid knowledge foundation for the primary-teacher model. Subsequently, the primary teacher gradually guides the student model, enabling effective knowledge transfer. \n\nThis multi-stage process ensures that the student model can effectively absorb knowledge from both teacher models, leading to improved performance in complex scenarios such as detecting safety behaviors in power construction environments. The gradual knowledge transfer helps mitigate the gap between the advanced and student models, optimizing both the learning efficiency and detection accuracy.",
            "score": 0.5009085982902466,
            "section_title": "Public Dataset Experiments",
            "char_start_offset": 48734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1331
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55810546875
        },
        {
            "corpus_id": "272146417",
            "title": "MST-KD: Multiple Specialized Teachers Knowledge Distillation for Fair Face Recognition",
            "text": "In this section, the methodology followed in this work is presented. Section 4.1 describes the data used. In Section 4.2, the proposed multi-teacher scheme is presented. Section 4.3 details how the four teachers' spaces were adapted to a common multi-teacher space. Section 4.4 describes the student's architecture as well as the proposed knowledge distillation procedures. The baseline experiments are detailed in Section 4.5. Finally, Section 4.6 describes the experimental setup followed during the networks' training, as well as the performance and fairness metrics used to evaluate the students.",
            "score": 0.500588227464196,
            "section_title": "Methodology",
            "char_start_offset": 17510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 600
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16650390625
        },
        {
            "corpus_id": "268532573",
            "title": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation",
            "text": "We consider three dimension for scaling model size: student, teacher, and data generator.For the following experiments, we keep two dimensions constant at the default setting (depth=24), allowing variation in just one.We utilize the models of the scaling study evaluated in [13].\n\nFig. 7 presents the results.Student model size significantly impacts performance, surpassing both data quality and teacher model size in influence.Consequently, larger student models do not only demonstrate superior performance as diffusion models [13], but that performance advantage is Figure 6: Comparing latent distillation approaches.We distill an MMDiT (depth=24) with both LCM and LADD.For LADD, we use the same model as a teacher and data generator.We find that LADD consistently outperforms LCM in a single step.\n\nFigure 7: Scaling behaviour.We ablate the size of student, teacher, and data generator model.Our default setting is a depth of 24 for all models and we vary one dimension at a time.A tangible difference is particularly noticeable when varying student depth.We show samples for a fixed seed and the following prompts: \"a cat patting a crystal ball with the number 7 written on it in black marker\", \"an astronaut riding a horse in a photorealistic style\", and \"a dog wearing a baseball cap backwards and writing BONEZ on a chalkboard\" (left, top to bottom).\n\neffectively transferred to their distilled versions.While teacher models and data quality contribute to improvements, their benefits plateau, indicating diminishing returns beyond certain thresholds.This pattern suggests a strategy for optimizing resource allocation, especially under memory constraints, by prioritizing larger student models while allowing for smaller teacher models without substantially compromising performance.",
            "score": 0.500524482499981,
            "section_title": "Scaling Behavior",
            "char_start_offset": 17987,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 218
                },
                {
                    "start": 218,
                    "end": 279
                },
                {
                    "start": 281,
                    "end": 309
                },
                {
                    "start": 309,
                    "end": 428
                },
                {
                    "start": 428,
                    "end": 620
                },
                {
                    "start": 620,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 738
                },
                {
                    "start": 738,
                    "end": 802
                },
                {
                    "start": 804,
                    "end": 832
                },
                {
                    "start": 832,
                    "end": 897
                },
                {
                    "start": 897,
                    "end": 985
                },
                {
                    "start": 985,
                    "end": 1061
                },
                {
                    "start": 1061,
                    "end": 1359
                },
                {
                    "start": 1361,
                    "end": 1413
                },
                {
                    "start": 1413,
                    "end": 1560
                },
                {
                    "start": 1560,
                    "end": 1793
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.373046875
        },
        {
            "corpus_id": "250391099",
            "title": "Sparse Distillation: Speeding Up Text Classification by Using Bigger Student Models",
            "text": "further support this claim, if we consider knowledge distillation as a \"substitute\" for pre-training that provides the student model with stronger inductive biases, and interpret the remaining teacher-student performance gap as the difference brought by architectural advances. On the other hand, we believe the power of DANs are previously undermined due to the challenges in optimizing large sparse models with limited supervision. Our experiments show that knowledge distillation effectively densify the supervision and greatly improve the performance of DANs.\n\nAdditional Analysis and Specifications. Due to space limit, we leave some additional analysis and specifications in Appendix C. We discuss tokenization speed (Table 9) and impact of n in n-grams (Table 10). We provide more detailed speed comparison in Table 12, model storage and memory usage information in Table 11. We provide fine-grained n-gram coverage information in Table 13.",
            "score": 0.5003905569985118,
            "section_title": "Limitations and Discussions",
            "char_start_offset": 22743,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3583984375
        },
        {
            "corpus_id": "235390933",
            "title": "Does Knowledge Distillation Really Work?",
            "text": "In Figure 1 we presented an experiment distilling ResNet-56 networks on CIFAR-100 augmented with synthetic GAN-generated images. We saw that enlarging the distillation dataset leads to improved teacher-student agreement on test, but the agreement remains relatively low (below 80%) even for the largest distillation dataset that we considered. In Figure 5 (left panel), we report the teacher-student agreement for the same experiment, but now on the distillation dataset. We now observe the opposite trend: as the distillation dataset becomes larger, it becomes more challenging for the student to match the teacher. Even when the student has identical capacity to the teacher, the student only achieves 95% agreement with the teacher when we use 50k synthetic images for distillation. \n\nThe drop in train agreement is even more pronounced when we use extensive data augmentation. In Figure 5, right panel, we report the teacher-student agreement on the train set with data augmentation for a subset of augmentation strategies presented in Section 5.1. We use the CIFAR-100 dataset and the ResNet-56 model for the teachers and the students (for details, see Section 5.1). In each case, we measure agreement on the augmented training set that was used during distillation. While for the baseline augmentation strategy, we can achieve almost perfect teacher-student agreement, for heavier augmentations the agreement drops dramatically. For the Rotation, Vertical Flip and Color Jitter augmentations, the agreement is between 80% and 90% for all the considered teacher sizes. For Combined Augs, the combination of these three augmentation strategies, the agreement drops even further, to just 60% in self-distillation! Our intuition about how knowledge distillation should work largely hinges on the assumption that after distillation the student matches the teacher on the distillation set. However, the results presented in this section suggest that in practice the optimization method is unable to achieve high fidelity even on the distillation dataset when extensive data augmentation or synthetic data is used.",
            "score": 0.5003750880651171,
            "section_title": "More distillation data lowers train agreement",
            "char_start_offset": 22268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2113
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4384765625
        },
        {
            "corpus_id": "254685937",
            "title": "FlexiViT: One Model for All Patch Sizes",
            "text": "Knowledge distillation [23] is a popular technique, where a typically smaller student model is trained to mimic the predictions of a typically larger teacher model. This can significantly improve the performance of the student model compared to standard label-supervised training [5,12,60]. It was recently shown that knowledge distillation corresponds to a much more challenging optimization problem than standard supervised training [5,49], and that initializing the student close to the teacher simplifies alleviates this [49]. Unfortunately, this solution is impractical since the teacher usually has a different (larger) architecture than the student [5]. However, with FlexiViT, we can initialize a student FlexiViT with the weights of a powerful ViT teacher and significantly improve distillation performance.\n\nUnless otherwise stated, the model we use for the remaining experiments in this paper is a FlexiViT-B initialized and distilled from the powerful ViT-B/8 model of [50]. At initialization, we PI-resize the teacher's patch embedding weights to 32 \u00d7 32, and bilinearly resample its position embeddings to 7 \u00d7 7. We then train the student model following the FunMatch [5] approach, minimizing the KLdivergence between the predictions of the teacher and the student FlexiViT with a randomized patch size:\n\nwhere f FlexiViT (x, p) is the distribution over classes for the FlexiViT model on an input x with patch size p, f ViT-B/8 (x) is the predictive distribution of the teacher on the exact same input, D is the training data distribution with random flips, crops, and mixup, and P is the distribution over patch sizes used for training the FlexiViT model. Figure 5 compares the effect of distilling using teacher initialization to random initialization and to supervised training from labels. The comparison was performed for 90 epochs and shows considerable benefits of this unique initialization capability of FlexiViT. Since distillation needs patience [5,54], we additionally run for 300 and 1000 epochs, shown as pale green curves in the figure. FlexiViT matches the teacher's performance at small patch sizes, and teacher initialization provide a large improvement",
            "score": 0.5002243927013115,
            "section_title": "Connection to knowledge distillation",
            "char_start_offset": 13729,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 283,
                    "matchedPaperCorpusId": "235376877"
                },
                {
                    "start": 283,
                    "end": 286,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 286,
                    "end": 289,
                    "matchedPaperCorpusId": "207853355"
                },
                {
                    "start": 435,
                    "end": 438,
                    "matchedPaperCorpusId": "235376877"
                },
                {
                    "start": 438,
                    "end": 441,
                    "matchedPaperCorpusId": "235390933"
                },
                {
                    "start": 525,
                    "end": 529,
                    "matchedPaperCorpusId": "235390933"
                },
                {
                    "start": 656,
                    "end": 659,
                    "matchedPaperCorpusId": "235376877"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "235485156"
                },
                {
                    "start": 1182,
                    "end": 1185,
                    "matchedPaperCorpusId": "235376877"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64599609375
        },
        {
            "corpus_id": "269302768",
            "title": "Sentence-Level or Token-Level? A Comprehensive Study on Knowledge Distillation",
            "text": "In this subsection, we explore the impact of student model size on the effectiveness of token-level and sentence-level distillation.We adjust the size of the student model following the model size reduction approach in [Zhou et al., 2020] to observe the impact of model size on knowledge distillation across different datasets.The results are shown in Table 1.",
            "score": 0.5000234060481228,
            "section_title": "Impact of Model Size",
            "char_start_offset": 12478,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 327
                },
                {
                    "start": 327,
                    "end": 360
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339599609375
        },
        {
            "corpus_id": "224818016",
            "title": "Adaptive multi-teacher multi-level knowledge distillation",
            "text": "As complementary experiments, we investigate how the number of teacher models affects the performance of our  learning approach. We first show whether the effect is consistent across different student networks. Table 4 shows the results corresponding to the first aspect, from which we can observe a clear trend that as the number of teachers increases, all the student networks achieve better results. Besides, the performance gap between 2 teachers and 3 teachers is larger than the gap between 3 teachers and 5 teachers, showing the performance gain becomes smaller when the number of teachers continues to grow. We further show the comparison of multiple teacher based knowledge distillation methods when having different number of teachers. \n\nThe results revealed in Figure 3 verify that our approach is consistently better as well. \n\nTable 5 shows the comparison of GPU memory and training time cost when training ResNet20 (Stu1) on CIFAR-100. The single teacher KD methods which usually have uncompetitive performance cost less training time and GPU memory. But in multi-teacher methods, ours only need a little more training time and memory, due to the ex- tra parameters from FitNets and adapter which even can be ignored compared with the large models. Since DML needs to update the parameters of all student networks at the same time, it will consume more resources.",
            "score": 0.4999064681197086,
            "section_title": "Effect of the number of teachers",
            "char_start_offset": 22924,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1377
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.257568359375
        },
        {
            "corpus_id": "231572861",
            "title": "Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals",
            "text": "12,31].\n\nTeacher-student Framework. Knowledge distillation (KD) is introduced and generalized by early work [10]. They proposed a teacher-student framework, where a complicated high-performance model and a light-weight model are treated as teacher and student respectively. The predictions of the teacher model are treated as \"soft labels\" and the student model is trained to fit the soft labels. While knowledge distillation was initially proposed for model compression, recent work [9,39] found that applying the soft labels as the training target can help the student achieve better performance.\n\nSeveral studies also apply the teacher-student framework in question answering task. Yang et al. [35] designed a multi-teacher knowledge distillation paradigm in a Web Question Answering system. Do et al. [4] and Hu et al. [11] applied the teacher-student framework to visual question answering task and reading comprehension task, respectively. In this work, we try to address spurious reasoning caused by weak supervision in the multi-hop KBQA task with an elaborate teacher-student framework.",
            "score": 0.4998344006380325,
            "section_title": "RELATED WORK",
            "char_start_offset": 8094,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 3,
                    "end": 6,
                    "matchedPaperCorpusId": "52154304"
                },
                {
                    "start": 487,
                    "end": 490,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 697,
                    "end": 701,
                    "matchedPaperCorpusId": "204788964"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.477783203125
        },
        {
            "corpus_id": "270245756",
            "title": "NTCE-KD: Non-Target-Class-Enhanced Knowledge Distillation",
            "text": "This underscores the robustness within a certain range and the importance of the distillation loss in effectively transferring knowledge from the teacher to the student model.Conversely, the accuracy in Table 7 remains relatively stable across varying \u03b1 values, indicating a lower sensitivity to this parameter.The sensitivity analysis reveals that optimizing \u03b2 is crucial for maximizing knowledge distillation effectiveness, as the distillation loss plays a pivotal role in capturing the knowledge transfer.Shrinkage coefficients.D \u03bb is set to [1, 0.5, 0] for the best performance in our experiments.This choice of D \u03bb indicates that our method is relatively robust to variations in the shrinkage coefficients, as the accuracies remain close across different settings in Table 8.However, the small improvement in accuracy when using [1, 0.5, 0] compared to other configurations suggests that the selection of these coefficients could have a non-negligible impact on the overall performance.Therefore, further exploration of optimal shrinkage coefficients for different datasets and model architectures remains an interesting direction for future work.",
            "score": 0.49976452652706604,
            "section_title": "Analysis of Magnitude Enhancement",
            "char_start_offset": 20595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 175,
                    "end": 311
                },
                {
                    "start": 311,
                    "end": 508
                },
                {
                    "start": 508,
                    "end": 531
                },
                {
                    "start": 531,
                    "end": 601
                },
                {
                    "start": 601,
                    "end": 780
                },
                {
                    "start": 780,
                    "end": 991
                },
                {
                    "start": 991,
                    "end": 1152
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.482421875
        },
        {
            "corpus_id": "256460933",
            "title": "Calibrating Student Models for Emotion-related Tasks",
            "text": "Knowledge and Self-distillation with Vanilla Models \n\nFigure 3 compares knowledge and self-distillation with vanilla models and illustrates that distillation settings yield better-calibrated models with lower ECE in all of the experiments with our setup (with the self-distillation being the best setting). Figure 4 shows that the errors obtained with our mixup method are much smaller in general compared to the other settings and yields best calibrated models. Table 3: Accuracy in percentage (%) for in-domain (GoEmotions, Yelp, EPITOME) and out-of-domain (Meld, SST, NewsEmpathy) datasets. LS, and TS refer to the label smoothing, and temperature scaling, respectively. KD refers to knowledge distillation from a teacher model that is specified at the subscript and a student model with a different architecture (for example, in KD RoBERTa , RoBERTa is used as the teacher in the knowledge distillation setting and BERT is used a the student model). For space restrictions, we do not include the student model. All reported values for the methods are mean \u00b1 std of three repetitions. \n\nIn",
            "score": 0.49971107580971696,
            "section_title": "C Visualisations of Comparison of the",
            "char_start_offset": 30661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 54,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1092
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.312255859375
        },
        {
            "corpus_id": "235490043",
            "title": "Teacher's pet: understanding and mitigating biases in distillation",
            "text": "Knowledge distillation is a technique for improving the performance of a \"student\" model using the predictions from a \"teacher\" model. At its core, distillation involves replacing the one-hot training labels with the teacher's predicted distribution over labels. Empirically, distillation has proven successful as a means of model compression [Bucil\u01ce et al., 2006, Hinton et al., 2015], improving the performance of a fixed model architecture [Anil et al., 2018, Furlanello et al., 2018], and semisupervised learning [Radosavovic et al., 2018]. Theoretically, considerable recent effort [Lopez-Paz et al., 2016, Mobahi et al., 2020, Tang et al., 2020, Menon et al., 2020, Zhang and Sabuncu, 2020, Ji and Zhu, 2020, Allen-Zhu and Li, 2020, Zhou et al., 2021, Dao et al., 2021] has focused on understanding how distillation affects learning. Put together, both strands of work further the understanding of when and why distillation helps. \n\nIn this paper, we are similarly motivated to better understand the mechanics of distillation, but pose a slightly different question: does distillation help all data subgroups uniformly? Or, do its overall gains come at the expense of degradation of performance on certain subgroups? To our knowledge, there has been no systematic study (empirical or otherwise) of this question. This consideration is topical given the study of fairness of machine learning algorithms on under-represented subgroups [Hardt et al., 2016, Buolamwini and Gebru, 2018, Chzhen et al., 2019, Sagawa et al., 2020a]. \n\nOur first finding is that even in standard settings -e.g., on image classification benchmarks such as CIFAR -distillation can disproportionately harm performance on subgroups defined by the individual classes (see Figure 1). To discern the source of this behaviour, we ablate the teacher and student architectures ( \u00a73.3), dataset complexity ( \u00a73.4), and label frequencies ( \u00a73.5).",
            "score": 0.499468941206599,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1531
                },
                {
                    "start": 1534,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1915
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 363,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 443,
                    "end": 461,
                    "matchedPaperCorpusId": "2331610"
                },
                {
                    "start": 517,
                    "end": 543,
                    "matchedPaperCorpusId": "7350432"
                },
                {
                    "start": 587,
                    "end": 610,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 670,
                    "end": 695,
                    "matchedPaperCorpusId": "219558831"
                },
                {
                    "start": 695,
                    "end": 713,
                    "matchedPaperCorpusId": "224802975"
                },
                {
                    "start": 737,
                    "end": 756,
                    "matchedPaperCorpusId": "231740588"
                },
                {
                    "start": 756,
                    "end": 775,
                    "matchedPaperCorpusId": "233307448"
                },
                {
                    "start": 1439,
                    "end": 1458,
                    "matchedPaperCorpusId": "7567061"
                },
                {
                    "start": 1458,
                    "end": 1486,
                    "matchedPaperCorpusId": "3298854"
                },
                {
                    "start": 1486,
                    "end": 1507,
                    "matchedPaperCorpusId": "186206648"
                },
                {
                    "start": 1507,
                    "end": 1530,
                    "matchedPaperCorpusId": "208176471"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6181640625
        },
        {
            "corpus_id": "265105398",
            "title": "Multi-Task Knowledge Distillation with Embedding Constraints for Scholarly Keyphrase Boundary Classification",
            "text": "In standard knowledge distillation (KD) (Hinton et al., 2015), a student model is trained to imitate a teacher model's output distribution under the assumption that this output distribution provides better signal to the student than the gold label itself. Clark et al. (2019b) showed that using knowledge distillation with MTL when learning from multiple related tasks can improve the performance of models that use MTL with only one related task. We describe MTL+KD (Clark et al., 2019b) as follows: Given a set of K + 1 tasks, i.e., a target task and K auxiliary tasks (see Figure 1), each with its own training set D k , k = 1, \u2022 \u2022 \u2022 , K + 1, we train a single-task (teacher) model on each task, denoted \u03b8 k . Then we use the single-task models \u03b8 k to teach a multi-task shared (student) model with parameters \u03b8 using cross-entropy loss L CE given as follows: \n\nwhere f k (x k i , * ) denotes the output for task k produced by a neural model with parameters *. However, Clark et al. (2019b) pointed out that there is a limitation that the student might not be able to overcome the teacher model's performance and suggested to use teacher annealing KD, which combines gold labels with predictions. Thus, the crossentropy loss with teacher annealing becomes: \n\nwhere \u03bb is linearly increased from 0 to 1 through training (see Figure 1).",
            "score": 0.49939723837348754,
            "section_title": "Multi-Task Learning with Knowledge Distillation",
            "char_start_offset": 10765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1336
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 276,
                    "matchedPaperCorpusId": "85464175"
                },
                {
                    "start": 467,
                    "end": 488,
                    "matchedPaperCorpusId": "85464175"
                },
                {
                    "start": 973,
                    "end": 993,
                    "matchedPaperCorpusId": "85464175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63623046875
        },
        {
            "corpus_id": "272310354",
            "title": "How Knowledge Distillation Mitigates the Synthetic Gap in Fair Face Recognition",
            "text": "Fig. 1: Visual representation of the proposed approach. Includes the merge of several real or synthetic datasets using a ethnicity aware sampling approach. On the top, it is visible a frozen (snowflake icon) pretrained teacher that distils knowledge to a training student (fire icon). \n\nAcknowledging the diverse advantages of using knowledge distillation to train new models, we have designed a set of methodological strategies to assess empir-ically these advantages. Our strategies focus on several aspects such as: various degrees of model compression, the influence of having a dataset composed of real data, synthetic data or a mix of both, and the fairness impact of each of the previous elements. Fundamentally, the following subsections and subsequent experiments were designed to answer the following questions: \"Does KD significantly improves models trained with Synthetic Data?\" and \"In a scenario where only parts of real data is missing, is a mixture of synthetic and real data beneficial?\". Figure 1 shows a visualisation of the proposed strategy described in this work.",
            "score": 0.4991752656793378,
            "section_title": "Methodology",
            "char_start_offset": 8273,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 1085
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1494140625
        },
        {
            "corpus_id": "276575418",
            "title": "CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport Alignment for Language Models with Different Tokenizers",
            "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing (NLP) tasks (Achiam et al., 2023;Touvron et al., 2023;Jiang et al., 2023Jiang et al., , 2024;;Guo et al., 2025). However, their deployment in real-world applications is often hindered by high computational costs, memory constraints, and latency issues. These limitations pose significant challenges to deploying LLMs efficiently on resource-constrained devices like mobile phones and IoT devices. Knowledge distillation (KD) (Hinton, 2015) has emerged as a promising solution to this challenge by transferring knowledge from a large teacher model to a more compact student model, thereby retaining essential performance while reducing computational overhead. Conventional KD approaches generally seek to align the output distributions of teacher and student models through methods like Kullback-Leibler (KL) divergence (Zhang et al., 2023;Hsieh et al., 2023;Ko et al., 2024). \n\nFor LLMs, knowledge distillation can be categorized into two main approaches: black-box KD and white-box KD. In black-box KD, the student model learns by mimicking the teacher model's outputs, as it has no access to the teacher's internal structure or variables (Fu et al., 2023;Kim and Rush, 2016). In contrast, white-box KD enables the student model to utilize the teacher model's architecture and variables while constructing regularization constraints during training. Theoretically, this approach facilitates more comprehensive knowledge transfer, leading to superior performance (Wen et al., 2023;Gu et al., 2024;Ko et al., 2024). However, a fundamental limitation of these approaches is that they assume both models share the same vocabulary and tokenizer, a requirement that is increasingly impractical given the diversity of architectures and tokenization schemes used in contemporary LLMs. \n\nSeveral recent studies have attempted to address this issue by enabling KD across models with different tokenizers.",
            "score": 0.4990129736938408,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1899
                },
                {
                    "start": 1902,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 1262,
                    "end": 1279,
                    "matchedPaperCorpusId": "256390607"
                },
                {
                    "start": 1603,
                    "end": 1619,
                    "matchedPaperCorpusId": "258865989"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7275390625
        },
        {
            "corpus_id": "201666324",
            "title": "Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation",
            "text": "While recent work has distilled pre-trained teacher models (Chia et al., 2018;Tang et al., 2019;Hu et al., 2018;Yang et al., 2019a), or learned representations from unlabeled text for compact models without using a teacher (Gururangan et al., 2019), the interactions between pre-training and distillation for compact students have yet to be studied thoroughly. The main research question in this work is whether unsupervised representation learning for the student is still important in the presence of a strong-and usually also pre-trained-teacher. Towards answering this question, we perform controlled experiments on 4 NLP tasks, including ones requiring inferences for text pairs (natural language language inference and textual entailment) and single texts (sentiment classification). For each experiment, we train 24 Transformer-based student models with sizes ranging from 4m to 110m parameters. \n\nOur experiments show that the student initialization strategy in distillation for compact models is crucial, even when the teacher models are already pre-trained. When the amount of in-domain unlabeled task-relevant data for transfer is limited (less than 2 million examples in our experiments), pre-training+fine-tuning alone outperforms distillation with a randomly initialized student. We find that the best strategy across all settings is a sequence of three standard training operations: MLM pre-training, distillation, and optional fine-tuning. In this work, we will refer to this method as Pretrained Distillation (PD) (Figure 1). PD not only outperforms all of its constituent operations applied in isolation, but also displays robustness to various aspects of the data: small and slightly out-ofdomain transfer sets have less negative impact on student performance than in standard distillation. \n\nMore interestingly, we observe that PD still ranks highest even when the same unlabeled dataset is used for pre-training and distillation. This indicates that their compound effect cannot be explained solely by their ability to take advantage of complementary resources, and that their different training objectives contribute to the overall success.",
            "score": 0.49801230661791246,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2162
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65234375
        },
        {
            "corpus_id": "267617086",
            "title": "Cooperative Knowledge Distillation: A Learner Agnostic Approach",
            "text": "Knowledge distillation is a simple but powerful way to transfer knowledge between a teacher model to a student model. Existing work suffers from at least one of the following key limitations in terms of direction and scope of transfer which restrict its use: all knowledge is transferred from teacher to student regardless of whether or not that knowledge is useful, the student is the only one learning in this exchange, and typically distillation transfers knowledge only from a single teacher to a single student. We formulate a novel form of knowledge distillation in which many models can act as both students and teachers which we call cooperative distillation. The models cooperate as follows: a model (the student) identifies specific deficiencies in it's performance and searches for another model (the teacher) who encodes learned knowledge into instructional virtual instances via counterfactual instance generation. Because different models may have different strengths and weaknesses, all models can act as either students or teachers (cooperation) when appropriate and only distill knowledge in areas specific to their strengths (focus). Since counterfactuals as a paradigm are not tied to any specific algorithm, we can use this method to distill knowledge between learners of different architectures, algorithms, and even feature spaces. We demonstrate our approach not only outperforms baselines such as transfer learning, self-supervised learning, and multiple knowledge distillation algorithms on several datasets, but it can also be used in settings where the aforementioned techniques cannot.",
            "score": 0.4978518684924337,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5927734375
        },
        {
            "corpus_id": "218684987",
            "title": "Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition",
            "text": "Knowledge Distillation (KD) [1], also known as teacherstudent training, is commonly used to narrow the gap of performance between a smaller model and a larger one [2,3,4,5,6]. A typical KD training procedure consists of two stages. First, a deep neural network referred as the teacher is trained in line with standard supervised training rules based on numerous samples and their corresponding ground truth labels. Second, a compressed network, the student model, is trained on a selection of original ground truths and soft targets labelled by the teacher. These soft targets are the posterior probabilities obtained from the pre-trained teacher. Knowledge distillation has been shown to be particularly efficient to reduce the student size while matching its performance to that of the teacher. Common applications include Computer Vision (CV) [4,7,8], Natural Language Processing [9,10,11] (NLP) and Automatic Speech Recognition (ASR) [12,13,14,15]. \n\nAn alternative approach to KD focuses solely on increasing the performances of the student model without considering its complexity. Distillation from ensembles of teachers has been commonly conducted under this approach. This method is referred to as the multi-teacher distillation [16,17]. \n\nModern deep learning based ASR systems have been shown to strongly benefit from multi-teacher distillation strategies [12,17]. Empirically, ensembles of teacher models capture complementary information by making different errors that can be further distill to a student model. A critical aspect of multi-teacher distillation in the context of ASR is to find suitable strategies to maximize the distillation with respect to a specific set of teachers. For instance, [12] proposed to pre-assign weights to teachers to control their impact on the distilled information. Another strategy is to sample the considered teachers randomly [17]. However, both strategies may give higher weighting, and thus higher importance, to teachers that are performing worse than others in the teacher set when applied to specific sentences. \n\nEnd-to-End (E2E) ASR models are particularly well suited for KD as the whole pipeline is composed of neural networks only [13,15,18].",
            "score": 0.4975832425989121,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1246
                },
                {
                    "start": 1249,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2069
                },
                {
                    "start": 2072,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 170,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 172,
                    "end": 174,
                    "matchedPaperCorpusId": "53288870"
                },
                {
                    "start": 846,
                    "end": 849,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 851,
                    "end": 853,
                    "matchedPaperCorpusId": "52290108"
                },
                {
                    "start": 883,
                    "end": 886,
                    "matchedPaperCorpusId": "1174931"
                },
                {
                    "start": 945,
                    "end": 948,
                    "matchedPaperCorpusId": "145893667"
                },
                {
                    "start": 948,
                    "end": 951,
                    "matchedPaperCorpusId": "52286229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52880859375
        },
        {
            "corpus_id": "254366823",
            "title": "Adaptive Mixing of Auxiliary Losses in Supervised Learning",
            "text": "Effect of optimal \u03bbs on Knowledge Distillation In the first experiment, we examine effective transfer of learned knowledge from various teachers to a student model which has fewer parameters. We compares test accuracies obtained with KD, AMAL , TAKD (Mirzadeh et al. 2019) KD with ResNet (20,32,56,110) as teacher and ResNet8 as student models on CIFAR100, Wide Residual Networks (WRN-16-3,WRN-16-4,WRN-16-6,WRN-16-8) as teacher and WRN-16-1 as student models on Stanford Cars and with Wide Residual Networks (WRN-16-3,WRN-16-4,WRN-16-6,WRN-16-8) as teachers and Resnet8 as student on FGVC-Aircraft. For TAKD and DGKD we use ResNet14 for CI-FAR100 and WRN-16-2 for Stanford Cars and FGVC-Aircraft as teaching assistant models. In all our knowledge distillation experiments we use temperature \u03c4 = 4 and \u03bb a = 0.9 (weights associated with KD loss) except in case of AMAL. For DGKD we use set the binomial variable to be 0.75, best reported in the paper. Figure 3 shows that AMAL consistently outperforms other techniques when a much smaller model learns from large teacher model (CIFAR100, Stanford Cars) and is comparable to DGKD in FGVC-Aircraft dataset. The figure shows plot relative test accuracies (w.r.t. non-KD students) vs model compression ratio1 .Interestingly, methods such as KD, SSKD and TAKD actually perform worse than training a student model with standard cross entropy loss. This observation is consistent with (Cho and Hariharan 2019), where authors claim KD may fail if the student is too weak. This problem gets worse when techniques such as SSKD bring even more additional information for the student model to learn. TAKD tries to address this issue by bring in teaching assistant model, which have already gone through with knowledge distillation from the teacher model.",
            "score": 0.4970168006160845,
            "section_title": "Results with Knowledge Distillation",
            "char_start_offset": 20265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1792
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 272,
                    "matchedPaperCorpusId": "211259075"
                },
                {
                    "start": 1428,
                    "end": 1452,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49853515625
        },
        {
            "corpus_id": "277652299",
            "title": "On Explaining Knowledge Distillation: Measuring and Visualising the Knowledge Transfer Process",
            "text": "The Student's performance often declines when there is a large architecture (capacity) gap between the Teacher and the Student [29,38]. The drop in the Student's performance may stem from either its own challenges in learning relevant features or the overwhelming knowledge of the Teacher. To investigate this issue, we employ two distillation strategies in our experiments using ResNet-101 as the Teacher and ResNet-18 as the Student, which have a significant capacity disparity. In the first approach, we conduct direct distillation from ResNet-101 to ResNet-18. The second approach introduces an intermediate \"Teacher assistant\" [37] to help bridge the capacity gap between ResNet-101 and ResNet-18. We use UniCAM and RS to analyse the KD process in these settings, with a focus on how well the smaller model manages to learn relevant features. \n\nUsing the proposed methods, we first examine the impact of a large capacity gap on the knowledge transfer between Teacher and Student. We use ResNet-101 as the Teacher and ResNet-18 as the Student and apply KD to train the Student model. Fig. 6 demonstrates that, in this setting, the Base Model captures more relevant features than the Student model. This suggests that a large capacity gap impedes knowledge transfer, as the Student model cannot effectively learn from the complex Teacher's knowledge. \n\nTo bridge the capacity gap, we use an intermediate Teacher assistant [29] to enable a more effective and focused knowledge transfer from ResNet-101 to ResNet-18 via ResNet-50. Figure 7 compares the saliency maps of the distilled features learned by two Students: ResNet-18 directly distilled from ResNet-101 (R18-R101) and ResNet-18 distilled from ResNet-101 through Teacher assistant ResNet-50 (R18-R50-R101). The saliency maps, visualised using UniCAM, reveal that the Teacher assistant helps learn more relevant features that highlight the object parts. In contrast, R18-R101 learns some irrelevant features and misses the salient features for the gt prediction. In fact,  We compare the relevance of features learned by the Student trained with the Teacher assistant and its equivalent Base model.",
            "score": 0.49695696248746835,
            "section_title": "Exploring the capacity gap impact",
            "char_start_offset": 25588,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2157
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 131,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 131,
                    "end": 134,
                    "matchedPaperCorpusId": "235390933"
                },
                {
                    "start": 632,
                    "end": 636,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 1425,
                    "end": 1429,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "237563200",
            "title": "Distilling Linguistic Context for Language Model Compression",
            "text": "DistilBERT  uses logit distillation loss (Logit KD), masked language modeling loss, and cosine loss between the teacher and student word representations in the learning process. The cosine loss serves to align the directions of the hidden state vectors of the teacher and student. Since the cosine of the two hidden state vectors is calculated in this process, they have the constraint that the embedding size of the teacher and the student model must be the same.\n\nPKD (Sun et al., 2019) transfers teacher knowledge to the student with Logit KD and patient loss. The patient loss is the mean-square loss between the normalized hidden states of the teacher and student. To calculate the mean square error between the hidden states, they have a constraint that the dimensions of hidden states must be the same between teacher and student.\n\nTinyBERT (Jiao et al., 2019) uses additional loss that matches word representations and attention matrices between the teacher and student. Although they acquire flexibility on the embedding size, using an additional parameter, since the attention matrices of the teacher and student are matched through mean square error loss, the number of attention heads of the teacher and student must be the same.\n\nMobileBERT (Sun et al., 2020) utilizes a similar objective with TinyBERT (Jiao et al., 2019) for task-agnostic distillation. However, since they match the hidden states with l2 distance and attention matrices with KL divergence between teacher and student, they have restrictions on the size of hidden states and the number of attention heads.\n\nMiniLM  proposes distilling the self-attention module of the last Transformer layer of the teacher. In self-attention module, they transfer attention matrices such as TinyBERT and MobileBERT and Value-Value relation matrices. Since they match the attention matrices of the teacher and student in a one-to-one correspondence, the number of attention heads of the teacher and student must be the same.\n\nThe methods introduced in Table 5 have constraints by their respective knowledge distillation objectives. However, our CKD method which uti-lizes the relation statistics between the word representations (hidden states) has the advantage of not having any constraints on student architecture.",
            "score": 0.49695056545626476,
            "section_title": "A Explanation of previous methods and their constraints",
            "char_start_offset": 28040,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50634765625
        },
        {
            "corpus_id": "261682391",
            "title": "Addressing the Accuracy-Cost Tradeoff in Material Property Prediction: A Teacher-Student Strategy",
            "text": "Knowledge distillation is an emerging strategy aimed at constructing efficient smallscale networks 27 . The central idea is to transfer the \"knowledge\" from complex teacher models into simpler student models. This concept, initially brought to light by Hinton 28 and his colleagues, primarily involves training a smaller model to emulate the behavior of its larger counterpart. The main advantage of this strategy is its ability to reduce model complexity and computational requirements while maintaining high performance. In the wake of this, various knowledge distillation methods have been applied across different fields with the goal of maximizing model performance while minimizing model size [29][30][31] . Furthermore, cross-modal knowledge distillation models have emerged, where the inputs for the teacher and student models are entirely different, yet share an underlying semantic connection [32][33][34] . \n\nOur strategy, while drawing inspiration from these works, is distinct in its approach. \n\nThe key to our strategy is that the material structure input into the SPM contains the composition information input into the CPM, with the composition also having the capability to infer the structure to a certain degree. This inclusion yet interrelation of input information makes past strategies no longer applicable. The volume of input information results in inconsistent upper limits of model accuracy, but the interrelation between composition and structure also offers a theoretical avenue for enhancing the performance of weaker models.",
            "score": 0.49678111244468337,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1554
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 101,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5595703125
        },
        {
            "corpus_id": "212908749",
            "title": "Improved Knowledge Distillation via Teacher Assistant",
            "text": "Unfortunately, there is not a principled way to do this. To solve this issue, Yim et al.;Yu et al. (2017;2017) used a shared representation of layers, however, it's not straightforward to choose the appropriate layer to be matched. Czarnecki et al. (2017) minimized the difference between teacher and student derivatives of the loss combined with the divergence from teacher predictions while Tarvainen and Valpola (2017) uses averaging model weights instead of target predictions. Urban et al. (2017) trained a network consisting of an ensemble of 16 convolutional neural networks and compresses the learned function into shallow multilayer perceptrons. To improve the student performance, Sau and Balasubramanian (2016) injected noise into teacher logits to make the student more robust. Utilizing multiple teachers were always a way to increase robustness. Zhang et al. (2017) proposed deep mutual learning which allows an ensemble of student models to learn collaboratively and teach each other during training. KL divergences between pairs of students are added into the loss function to enforce the knowledge transfer among peers. You et al. (2017) proposed a voting strategy to unify multiple relative dissimilarity information provided by multiple teacher networks. Anil et al. (2018) introduced an efficient distributed online distil-lation framework called co-distillation and argue that distillation can even work when the teacher and student are made by the same network architecture. The idea is to train multiple models in parallel and use distillation loss when they are not converged, in which case the model training is faster and the model quality is also improved. \n\nHowever, the effectiveness of distilling a large model to a small model has not yet been well studied. Our work differs from existing approaches in that we study how to improve the student performance given fixed student and teacher network sizes, and introduces intermediate networks with a moderate capacity to improve distillation performance. Moreover, our work can be seen as a complement that can be combined with them and improve their performance. \n\nDistillation Theory. Despite its huge popularity, there are few systematic and theoretical studies on how and why knowledge distillation improves neural network training.",
            "score": 0.49666208553459534,
            "section_title": "Related Work",
            "char_start_offset": 4812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1683
                },
                {
                    "start": 1686,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2141
                },
                {
                    "start": 2144,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2314
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 89,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 89,
                    "end": 105,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 232,
                    "end": 255,
                    "matchedPaperCorpusId": "21596346"
                },
                {
                    "start": 393,
                    "end": 421,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 482,
                    "end": 501,
                    "matchedPaperCorpusId": "16550689"
                },
                {
                    "start": 860,
                    "end": 879,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1137,
                    "end": 1154,
                    "matchedPaperCorpusId": "28101867"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47900390625
        },
        {
            "corpus_id": "255125239",
            "title": "BD-KD: Balancing the Divergences for Online Knowledge Distillation",
            "text": "Knowledge distillation (KD) is a simple and yet powerful technique for model compression [7]. It has become an accessible tool, which facilitates the creation of compact student networks for deployment onto limited memory and computation edge devices. While distillation is traditionally performed offline, in recent years online methods have gained popularity for their improved performance and computational efficiency. Deep mutual learning [25]is one of the first techniques to propose collaborative learning of two or more networks This technique has opened an opportunity for distillation methods to reduce the training process to a single one-time simultaneous training of teacher and student. Each network is minimizing an objective function which consists of the traditional Cross-Entropy (CE) loss and a forward KL loss which encourages the network to mimic the other network's behaviour. Online Knowledge Distillation with Diverse Peers (OKDDip) is another online method proposed by Chen et al. [2]; a teacher-free two-level distillation method where the first-level happens among multiple students with identical architectures. The group knowledge of these diverse students is then transferred to the \"group leader\" in the second level. In a recent work by Chung et. al. [3] the distillation goes beyond class logit matching and is used to match the feature maps of two networks using an adversarial framework. \n\nNone of these online methods study the impact of a large capacity gap. Guo and colleagues were the first ones to show large gap harm the performance of the networks. To combat this phenomenon, they present Distillation Collaborative learning (KDCL) [6] where a series of ensembling methods is employed to generate high-quality soft targets at once for distillation. Although, KDCL improves the performance of both networks, due to the nature of the loss the focus is mostly on the larger network and therefore the improvement is more enhanced for the teacher. The first work that introduces an online distillation model that attempts to close the accuracy gap during training is Switchable Knowledge distillation model (SWITOKD) [15]. This model mitigates the accuracy gap problem by slowing down the training of the teacher.",
            "score": 0.49660837712086825,
            "section_title": "Related Work",
            "char_start_offset": 5468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2249
                }
            ],
            "ref_mentions": [
                {
                    "start": 443,
                    "end": 447,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1282,
                    "end": 1285,
                    "matchedPaperCorpusId": "209319166"
                },
                {
                    "start": 1673,
                    "end": 1676,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 2153,
                    "end": 2157,
                    "matchedPaperCorpusId": "252198825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29736328125
        },
        {
            "corpus_id": "234557589",
            "title": "A Distilled BERT with Hidden State and Soft Label Learning for Sentiment Classification",
            "text": "We perform the procedures as described in the method section and evaluate the performance of the compressed model, namely the 6-layer student model. We explore the effectives of the student models with different fine-tuned teacher models and various numbers of distillation epochs. The student models are obtained through three steps: (1) teacher model fine-tuning, (2) knowledge distillation and (3) student model fine-tuning. Three teacher models are used, which are full-size BERT models fine-tuned for 1, 2, or 4 epochs, respectively. Then, each teacher model is distilled for 1, 2, 4, 8 or 16 epochs. The resultant student models are fine-tuned for 5 epochs and evaluated on the dev set of SST-2. Results are shown in Table 1. Overall, as the increase of distillation epochs, increased performances of the student models are observed for all the three teacher models. The performance gains range from 0.1% to 0.6%. Interestingly, the teacher model fine-tuned with 1 epoch has more advantage than those with more than 1 epochs. Nearly all the distilled student models learning from teacher 1 perform better than those learn from teacher 2 or 3. A possible reason is that the teacher model is overfitting for knowledge distillation if the number of epochs is greater than 1. Among all the settings we tested in  1. We try 32 and 64 epochs. Results are shown in Table 2. Distillation for more epochs only leads to marginal increase to the performance, indicating that distillation for 16 or 32 epochs may be enough. Although distillation for 64 epochs also achieves similar performance, it is not recommended, since it is time-consuming and has the possibility of overfitting.",
            "score": 0.49647433170378097,
            "section_title": "Results and Discussion",
            "char_start_offset": 5408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1678
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52880859375
        },
        {
            "corpus_id": "174800781",
            "title": "Online Distilling from Checkpoints for Neural Machine Translation",
            "text": "Knowledge distillation is a class of methods which transfers knowledge from a pre-trained teacher model T , to a student model S. The teacher model can be a model with large capacity (Bucila et al., 2006) or an ensemble of several models (Hinton et al., 2015). In knowledge distillation, the student model learns to match the predictions of the teacher model. Concretely, assuming that we learn a classification model (parameterized by \u03b8) on a set of training samples in the form of (x, y) with |V| classes. Instead of minimizing the cross-entropy loss between one-hot label y and model's output probability p(y|x; \u03b8), knowledge distillation uses the teacher model's distribution q(\u2022|x) as \"soft targets\" and optimizes the loss: \n\nwhere \u03b8 T parameterizes the teacher model and p(\u2022|x) is the distribution of the student model. Kim and Rush (2016) proposed that, as the loss of NMT model (Equation 4) can be factored into minimizing cross-entropy loss between the target words and word-level probabilities of the NMT model for every position at target side, knowledge distillation on multi-class classification can be naturally applied. They defined word-level knowledge distillation (W-KD) on a sentence as: \n\n\u2022 log p(y j = k|y <j , H(x); \u03b8), where V is the target vocabulary. They further proposed sequence-level knowledge distillation (S-KD), which optimizes the student model by matching the predictions of the teacher model in the probability distribution over the space of all possible target sequences: \n\nwhere \u03c4 is the space of target side sentences. As summing over exponential numbers of samples here is intractable, they proposed to train student model on samples generated by teacher model as an approximation. \n\n3 Online Distillation from Checkpoints",
            "score": 0.4964695467431779,
            "section_title": "Knowledge Distillation in Neural Machine Translation",
            "char_start_offset": 5201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1761
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 204,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5078125
        },
        {
            "corpus_id": "273654531",
            "title": "Data-Efficient Low-Complexity Acoustic Scene Classification via Distilling and Progressive Pruning",
            "text": "In order to explore the data-efficiency ability of models with different structures for teacher models in knowledge distillation, we compare the performance of common CNNs (CP-ResNet [6], CP-Mobile [15] and BC-ResNet) and Transformers (PaSST [20]) structured networks under different training data size, and the corresponding results are shown in Table III. It should be noted that for CNN based models, we adjusted the width of them to maximize their performance for better teacher models. From the results, it can be seen that both CNNs and Transformers achieve comparable results when there is sufficient data (25%-100%), while Transformers perform better when training data is limited (5% and 10%), demonstrating better data-efficiency ability. Meanwhile, we also compare whether to use AudioSet [36] for pre-training (PaSST vs PaSST-PT), and find that the performance gain of PaSST is not obtained by pre-training with more data, but by the architectural advantages of Transformer itself. In Table IV, we also adopted different ensemble methods to explore the effectiveness of different models when distilling knowledge to Rep-Mobile. By comparing with the baseline, using larger models for knowledge distillation on low-complexity models can effectively improve the model performance, and different teacher models exhibit different characteristics. Although the CNN ensemble teacher model has shown great superiority compared to the Transformer teacher, it has not shown corresponding advantages in knowledge distillation for the student model, especially for the setups with less training data. This also indicates that the selection of teacher models is very important in knowledge distillation, and Transformer-based systems is more appropriate in this task. In addition, we have explored pruning strategies for building low-complexity models and showed the results in Table V. It should be noted that these results are obtained by distilling from Transformer to Rep-Mobile with different model size. From the analysis of the results, the strategy of training a larger model (64 or 96 channels) first and then pruning and compressing to smaller one (channels 32) can effectively improve the capacity of small model, by comparing with method of directly distilling to smaller one.",
            "score": 0.4964651386733274,
            "section_title": "A. Comparison with other models",
            "char_start_offset": 12194,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 202,
                    "matchedPaperCorpusId": "235195760"
                },
                {
                    "start": 800,
                    "end": 804,
                    "matchedPaperCorpusId": "21519176"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.421875
        },
        {
            "corpus_id": "264490946",
            "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
            "text": "Knowledge Distillation (KL distillation) was pioneered by Hinton et al. (2015), which suggests minimizing the KL divergence between a teacher and a student model's soft targets \u03c3(z t ) and \u03c3(z s ): with temperature T . We use Eq. 1 as our base transfer objective (KL-Dist transfer), as it still remains popular and competitive (Beyer et al., 2022;Rajasegaran et al., 2020;Tian et al., 2020) (see Supp. for other objectives). KL distillation is often extended with auxiliary classification losses (e.g. crossentropy L XE , (Hinton et al., 2015;Tian et al., 2020;Rajasegaran et al., 2020;Beyer et al., 2022)) to stabilize the distillation process. We denote the \u03bb-weighted combination as XE-KL distillation, and the associated transfer as XE-KL-Dist. transfer or XE-KL: \n\nMost knowledge distillation research considers the distillation from a trained teacher to an untrained student, in stark contrast to our goal of knowledge transfer between pretrained models while retaining student knowledge already gained a priori. And indeed, when applied to knowledge transfer between pretrained models in \u00a75.1, standard knowledge distillation struggles to transfer knowledge without performance drops for most teacher-student pairs. We measure this via the transfer delta \u2206 transf = acc(f s kt ) \u2212 acc(f s ), which quantifies the change in the student's top-1 accuracy, with f s kt being the student model following the knowledge transfer.",
            "score": 0.4961764237823569,
            "section_title": "KNOWLEDGE TRANSFER THROUGH KNOWLEDGE DISTILLATION",
            "char_start_offset": 14411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 372,
                    "matchedPaperCorpusId": "219720852"
                },
                {
                    "start": 372,
                    "end": 390,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 543,
                    "end": 561,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 561,
                    "end": 586,
                    "matchedPaperCorpusId": "219720852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.382568359375
        },
        {
            "corpus_id": "239016062",
            "title": "FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metrics for Automatic Text Generation",
            "text": "Knowledge distillation (KD) (Hinton et al., 2015) is the process of transferring knowledge from a large teacher model to a smaller student model to accomplish model compression (Bucilu\u01ce et al., 2006). It was originally proposed in the domain of computer vision and speech recognition, then successfully adapted to NLP (Sanh et al., 2019). Distillation can be accomplished in three ways: (1) offline, where a teacher is first pre-trained, then a student is trained under the guidance of the teacher (Hinton et al., 2015); (2) online, where the student and the teacher are trained simultaneously (Zhang et al., 2018); and (3) self, where the same model plays the role of student and teacher, e.g., transferring the knowledge of a later exit layer into an earlier one of the same multi-exit network (Phuong and Lampert, 2019). Previous studies on KD mainly focused on classification problems (Gou et al., 2021). A few attempts have been made on regression problems (Chen et al., 2017;Saputra et al., 2019;Takamoto et al., 2020), in which special losses were proposed to train the student with respect to both the teacher's regression outputs and ground truth scores. Different from conventional distillation, our work is more similar to data-free KD (Kang and Kang, 2021), where the student is trained in the absence of the dataset used to train the teacher. To transfer knowledge, we first create a synthetic dataset by annotating sequence pairs with a large model (teacher), and then train a miniature model (student) on that dataset, in an offline and regression setting.",
            "score": 0.4961021047060926,
            "section_title": "Knowledge distillation",
            "char_start_offset": 8112,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3994140625
        },
        {
            "corpus_id": "267200066",
            "title": "Progressive Distillation Based on Masked Generation Feature Method for Knowledge Graph Completion",
            "text": "In recent years, knowledge graph completion (KGC) models based on pre-trained language model (PLM) have shown promising results. However, the large number of parameters and high computational cost of PLM models pose challenges for their application in downstream tasks. This paper proposes a progressive distillation method based on masked generation features for KGC task, aiming to significantly reduce the complexity of pre-trained models. Specifically, we perform pre-distillation on PLM to obtain high-quality teacher models, and compress the PLM network to obtain multi-grade student models. However, traditional feature distillation suffers from the limitation of having a single representation of information in teacher models. To solve this problem, we propose masked generation of teacher-student features, which contain richer representation information. Furthermore, there is a significant gap in representation ability between teacher and student. Therefore, we design a progressive distillation method to distill student models at each grade level, enabling efficient knowledge transfer from teachers to students. The experimental results demonstrate that the model in the pre-distillation stage surpasses the existing state-of-the-art methods. Furthermore, in the progressive distillation stage, the model significantly reduces the model parameters while maintaining a certain level of performance. Specifically, the model parameters of the lower-grade student model are reduced by 56.7\\% compared to the baseline.",
            "score": 0.49606119394316417,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5888671875
        },
        {
            "corpus_id": "258048760",
            "title": "A Survey on Recent Teacher-student Learning Studies",
            "text": "Here, by analyzing the limitations of the existing method and focusing on its improvement, a new method, logit distillation, is proposed, and the knowledge distillation loss function is re-represented as a weighted sum of two components, target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). The effectiveness of each component is investigated and compared with vanilla knowledge distillation. A new technique called simple knowledge distillation (SimKD) is discussed here, which allows for state-of-the-art results without the need for complex knowledge representations or hyperparameters.SimKD involves student inference using a pre-trained teacher classifier, which eliminates the need for labeling information and makes feature alignment the only source of generated gradients. The discriminative information contained in the teacher classifier is considered important here, but has been neglected in the knowledge distillation (KD) literature. It is assumed that there is ability-invariant information in the data that can be easily obtained across different models, while the teacher model contains additional necessary ability-specific information that is difficult to obtain for a simple student model. Empirical evidence is provided here to support this hypothesis and to show that SimKD greatly mitigates the performance degradation in teacher-student compression.",
            "score": 0.49598402451870627,
            "section_title": "SIMPLE KNOWLEDGE DISTILLATION",
            "char_start_offset": 38381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1415
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.498291015625
        },
        {
            "corpus_id": "268181068",
            "title": "Heterogeneous Student Knowledge Distillation From BERT Using a Lightweight Ensemble Framework",
            "text": "Over the past decade, significant advancements in artificial intelligence have transformed the world and reshaped our lives, largely attributed to the progress of neural networks through deep learning. This revolution has greatly enhanced the capacity of computers to perceive, listen, and comprehend their environments, leading to remarkable advancement in the integration of AI across various scientific disciplines and other aspects of human achievement [1]. Especially in recent years, large language models including the BERT [2] and GPT series (GPT-1 [3], GPT-2 [4], GPT-3 [5] and ChatGPT) \n\nThe associate editor coordinating the review of this manuscript and approving it for publication was Maria Chiara Caschera . \n\nhave achieved significant success in many natural language processing tasks. Nevertheless, the computing costs, substantial space requirements and increased inference time associated with large neural networks impose limitations on their deployment on edge devices for different downstream applications. With recognition of these drawbacks, in both industry and academia, there has been a particular focus on researching and developing models that can be efficiently operated on resource-limited devices. \n\nVarious approaches have been suggested to resolve this problem such as low-rank networks [6], [7], efficient convolutional neural networks [8], [9], and pruning based methods [10], [11]. Among those efforts to build more efficient models by taking advantages of large networks, knowledge distillation is a popular approach where a complex teacher model transfers its knowledge to a simpler student model for helping the student model imitate the teacher's behavior and making it more computationally efficient [12]. In general, knowledge distillation can be divided into three key categories: feature-based, response-based and relation-based distillation methods [13]. Feature-based knowledge distillation concentrates on incorporating the internal representations and features learned by the teacher model into the student model [14], [15], response-based knowledge focuses on transferring output responses generated by the teacher model to guide the student model's predictions [16], [17], and relation-based knowledge emphasizes on providing essential structural correlation information existing within the teacher model to the student model [18], [19].",
            "score": 0.49561552788744695,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2388
                }
            ],
            "ref_mentions": [
                {
                    "start": 457,
                    "end": 460,
                    "matchedPaperCorpusId": "248377871"
                },
                {
                    "start": 568,
                    "end": 571,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1321,
                    "end": 1324,
                    "matchedPaperCorpusId": "247490662"
                },
                {
                    "start": 1326,
                    "end": 1329,
                    "matchedPaperCorpusId": "256261670"
                },
                {
                    "start": 1376,
                    "end": 1379,
                    "matchedPaperCorpusId": "245634301"
                },
                {
                    "start": 1407,
                    "end": 1411,
                    "matchedPaperCorpusId": "248203672"
                },
                {
                    "start": 1413,
                    "end": 1417,
                    "matchedPaperCorpusId": "256390345"
                },
                {
                    "start": 1742,
                    "end": 1746,
                    "matchedPaperCorpusId": "259357156"
                },
                {
                    "start": 1895,
                    "end": 1899,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.426025390625
        },
        {
            "corpus_id": "232428032",
            "title": "Fixing the Teacher-Student Knowledge Discrepancy in Distillation",
            "text": "Training a small student network with the guidance of a larger teacher network is an effective way to promote the performance of the student. Despite the different types, the guided knowledge used to distill is always kept unchanged for different teacher and student pairs in previous knowledge distillation methods. However, we find that teacher and student models with different networks or trained from different initialization could have distinct feature representations among different channels. (e.g. the high activated channel for different categories). We name this incongruous representation of channels as teacher-student knowledge discrepancy in the distillation process. Ignoring the knowledge discrepancy problem of teacher and student models will make the learning of student from teacher more difficult. To solve this problem, in this paper, we propose a novel student-dependent distillation method, knowledge consistent distillation, which makes teacher's knowledge more consistent with the student and provides the best suitable knowledge to different student networks for distillation. Extensive experiments on different datasets (CIFAR100, ImageNet, COCO) and tasks (image classification, object detection) reveal the widely existing knowledge discrepancy problem between teachers and students and demonstrate the effectiveness of our proposed method. Our method is very flexible that can be easily combined with other state-of-the-art approaches.",
            "score": 0.4955495614251634,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.482177734375
        },
        {
            "corpus_id": "276533262",
            "title": "Decoupled Classifier Knowledge Distillation",
            "text": "Therefore, the key step in knowledge distillation lies in how to effectively transfer the rich knowledge of the teacher model to the student model. Based on this consideration, we propose a novel approach that decouples the classifier's output from the distillation process by fixing the knowledge acquired through feature alignment and further mining the knowledge from the teacher model on both correct and incorrect categories to supplement the student's learning. By simultaneously aligning the deep features and outputs of both the teacher and student models, we strive to minimize the performance gap between the two, and even surpass the teacher model in some experiments. \n\nTo achieve the above goals, we divide the structure into two core components: (1) retaining the target class knowledge already acquired by the student model, without fully covering all target classes; (2) utilizing both target and non-target class information to deeply align the teacher and student models. For better understanding, we refer to these two components as Stable Knowledge Distillation (SKD) and Polarized Knowledge Distillation (PKD), respectively. \n\nBased on the aforementioned challenges, we propose an optimized output distillation method for feature-based knowledge distillation, called Decoupled Classifier Knowledge Distillation (DCKD), as illustrated in Fig 1 . DCKD avoids the information redundancy caused by repetitive learning when using both feature-based and output-based distillation methods simultaneously. It also reinforces learned knowledge and excludes potential ambiguous or incorrect knowledge. The separate weighting of SKD and PKD helps determine the importance of each part. \n\nIn summary, our contributions are as follows: \n\n(1)We decouple the classifier's output into two parts, and split traditional KD into SKD and PKD. Through comparative analysis of these two components, we provide a suitable method to improve feature-based knowledge distillation by utilizing output-based distillation. (2)We reveal the limitations and even negative effects caused by redundant learning when combining output-based and feature-based distillation. PKD effectively avoids redundant learning of critical knowledge, while SKD further strengthens the knowledge based on PKD.",
            "score": 0.4953939111034653,
            "section_title": "Introduction",
            "char_start_offset": 4596,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1695
                },
                {
                    "start": 1698,
                    "end": 1743
                },
                {
                    "start": 1746,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2281
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68212890625
        },
        {
            "corpus_id": "236477925",
            "title": "MergeDistill: Merging Language Models using Pre-trained Distillation",
            "text": "In this paper, we use knowledge distillation (KD) (Hinton et al., 2015) to achieve this. In the context of language modeling, KD methods can be broadly classified into two categories: task-specific and task-agnostic. In task-specific distillation, the teacher LM is first fine-tuned for a specific task and is then distilled into a student model which can solve that task. Task-agnostic methods perform distillation on the pre-training objective like masked language modeling (MLM) in order to obtain a task-agnostic student model. Prior work has either used task-agnostic distillation to compress singlelanguage teachers (Sanh et al., 2019;Sun et al., 2020) or used task-specific distillation to combine multiple fine-tuned teachers into a multi-task student (Liu et al., 2019b;Clark et al., 2019). The former prevents positive language transfer while the latter restricts the student's capabilities to the tasks and languages in the fine-tuned teacher LMs (as shown in Figure 1). \n\nWe focus on the problem of merging multiple pre-trained LMs into a single multilingual student LM in the task-agnostic setting. To the best of our knowledge, this is the first effort of its kind, and makes the following contributions: \n\n\u2022 We propose MERGEDISTILL, a task-agnostic distillation approach to merge multiple teacher LMs at the pre-training stage, to train a strong multilingual student LM that can then be finetuned for any task on all languages in the student LM. Our approach is more maintainable (fewer models), compute efficient and teacherarchitecture agnostic (since we obtain offline predictions). \n\n\u2022 We use MERGEDISTILL to i) combine monolingual teacher LMs into a single multilingual student LM that is competitive with or outperforms individual teachers, ii) combine multilingual teacher LMs, such that the overlapping languages can learn from multiple teachers. \n\n\u2022 Through extensive experiments and analysis, we study the importance of typological similarity in building multilingual models, and the impact of strong teacher LM vocabularies and predictions in our framework.",
            "score": 0.4953429031462707,
            "section_title": "fer.",
            "char_start_offset": 3006,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 981
                },
                {
                    "start": 984,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1600
                },
                {
                    "start": 1603,
                    "end": 1869
                },
                {
                    "start": 1872,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 641,
                    "end": 658,
                    "matchedPaperCorpusId": "215238853"
                },
                {
                    "start": 760,
                    "end": 779,
                    "matchedPaperCorpusId": "59523594"
                },
                {
                    "start": 779,
                    "end": 798,
                    "matchedPaperCorpusId": "85464175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58251953125
        },
        {
            "corpus_id": "253254911",
            "title": "Reduce, Reuse, Recycle: Improving Training Efficiency with Distillation",
            "text": "Knowledge distillation (Hinton et al., 2015) is a wellestablished practice for model compression and improving the quality of models (Gou et al., 2021). Distillation is often an ingredient in training recipes that push the limits of model quality (Xie et al., 2020;Touvron et al., 2021;Beyer et al., 2021). Teacher models are traditionally larger than student models, though too large of a size discrepancy between teacher and student models can reduce the efficacy of distillation (Mirzadeh et al., 2020). As such, numerous approaches have been proposed to more effectively utilize models of similar or identical architecture as teachers (Yim et al., 2017;Yang et al., 2018;Furlanello et al., 2018;Zhang et al., 2019;Wang et al., 2022).",
            "score": 0.49506449754752213,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 737
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 151,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 247,
                    "end": 265,
                    "matchedPaperCorpusId": "207853355"
                },
                {
                    "start": 482,
                    "end": 505,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 639,
                    "end": 657,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 675,
                    "end": 699,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 699,
                    "end": 718,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62890625
        },
        {
            "corpus_id": "208175624",
            "title": "Search to Distill: Pearls Are Everywhere but Not the Eyes",
            "text": "Table 3 summarizes the knowledge distillation performance for each pair of teacher and student. Distilling the same teacher model to different students leads to different performance results, and no student architecture produces the best results across all teacher networks. Although the reason behind it could be complicated, we argue its not solely due to: \n\n\u2022 Distribution: Fig. 2",
            "score": 0.49506449754752213,
            "section_title": "T(A) T(B) T(C) T(D) T(E) T(F) T(G) T(H) GT T(A) T(B) T(C) T(D) T(E) T(F) T(G) T(H) GT #### #### ####",
            "char_start_offset": 8557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 383
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18701171875
        },
        {
            "corpus_id": "240419662",
            "title": "Rethinking the Knowledge Distillation From the Perspective of Model Calibration",
            "text": "As aforementioned, knowledge distillation can minimize the distance between teacher's and student's output, and completing the process of knowledge transfer. (Cho and Hariharan 2019;Mirzadeh et al. 2020) have observed that: when the size of the teacher model increases, the accuracy of the student model may decrease if the performance gap between the teacher and the student is large. They speculated that it was due to the mismatch of the capacity, as the tiny student model cannot imitate the large teacher model well. In our experiments, we also reproduced the experiment results and observed similar phenomenon. In the vanilla knowledge distillation, we find that the large-scale teacher model is not necessarily better than the small-scale teacher model, as shown in the experimental sections.",
            "score": 0.49506449754752213,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 1584,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 799
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 182,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 182,
                    "end": 202,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57666015625
        },
        {
            "corpus_id": "236772780",
            "title": "Semi-Supervising Learning, Transfer Learning, and Knowledge Distillation with SimCLR",
            "text": "In this experiment, we re-evaluate the effect of student architecture in knowledge distillation via teacher-forcing. Specifically, we want to experiment if student has better performance when it has the same architecture (but with much smaller size) with its teacher.",
            "score": 0.49506449754752213,
            "section_title": "Hypothesis",
            "char_start_offset": 15517,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 267
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "248987313",
            "title": "The Importance of Being Parameters: An Intra-Distillation Method for Serious Gains",
            "text": "We define the degree of contribution balance to be simply evaluating the standard deviation of all parameter sensitivity. A lower standard deviation means that there is a more balanced contribution. \n\nA Case Study on Knowledge Distillation We here take naive knowledge distillation (KD) (Hinton et al., 2015) as a case study. We tie the success of KD to the more balanced contribution  among parameters. KD aims to transfer knowledge from a teacher model to a student model. Specifically, the student model tries to minimize the Kullback-Leibler (KL) divergence between its output p s and the gold label y, and between p s and output of the teacher p t . We here formulate a naive KD objective. \n\nmin KL(y \n\nCommonly, the teacher is a high-capacity model and student is more compact. However, recent studies (Furlanello et al., 2018;Zhang et al., 2019;Fang et al., 2020) show that the student model can significantly outperform the teacher when the student use the same architecture (and consequently, number of parameters) as the teacher, termed self-distillation. \n\nUsing the previously described machine translation task in Section 2.2, we conduct self-distillation experiments and iterate self-distillation twice, i.e., the student taught by the regular transformer model becomes a new teacher for the next student. In Table 1, we report sacreBLEU (Post, 2018). Similar to the previous literature, model performance substantially increase after each round of self-distillation. This surprising result is referred to in the literature as 'dark knowledge' (Gotmare et al., 2018;Zhang et al., 2019). Some studies try to understand the 'dark knowledge', e.g., in the view of regularization (Yuan et al., 2020) or ensemble (Allen-Zhu and Li, 2020), but they only explain how it leads to performance improvements instead of how the model itself changes. Here, we argue that the 'dark knowledge' transferred from teachers to such students is actually due to the more balanced contribution among parameters. We visualize the sensitivity distribution of all models via violin plots with their standard deviation in Figure 3 4 .",
            "score": 0.4949570111453748,
            "section_title": "Degree of Contribution Balance",
            "char_start_offset": 7244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 808,
                    "end": 833,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 833,
                    "end": 852,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 852,
                    "end": 870,
                    "matchedPaperCorpusId": "231592453"
                },
                {
                    "start": 1352,
                    "end": 1364,
                    "matchedPaperCorpusId": "13751870"
                },
                {
                    "start": 1558,
                    "end": 1580,
                    "matchedPaperCorpusId": "53107519"
                },
                {
                    "start": 1580,
                    "end": 1599,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1690,
                    "end": 1709,
                    "matchedPaperCorpusId": "219962714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52294921875
        },
        {
            "corpus_id": "246273687",
            "title": "Attention Based Data Augmentation for Knowledge Distillation with Few Data",
            "text": "In this paper, we explore knowledge distillation between heterogeneous models with few data from the perspective of data augmentation. A data augmentation method is proposed based on the attentional response of teacher model. Combining the proposed data augmentation method with different knowledge distillation methods, gratifying performances are achieved in knowledge distillation between heterogeneous models with few data. Besides, we discuss some characteristics of the proposed method, including that this method needs to be used in conjunction with knowledge distillation and the semantic random interferences is one explanation of how the proposed method improves the generalization of the model. \n\nFor future study, on the one hand, we will work on improving the performance of few-shot knowledge distillation between heterogeneous models with only one or five samples per class. On the other hand, we will intensively study the influence mechanism of knowledge in samples and teacher on student training process.",
            "score": 0.494814352606733,
            "section_title": "Conclusion",
            "char_start_offset": 13652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1023
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.359375
        },
        {
            "corpus_id": "222066807",
            "title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation on BERT",
            "text": "Knowledge Distillation (KD) is one of the widely known methods for model compression. In essence, KD trains a smaller student model based on a larger teacher model and tries to retain the teacher model\u2019s level of performance as much as possible. However, existing KD methods suffer from the following limitations. First, since the student model is smaller in absolute size, it inherently lacks model capacity. Second, the absence of an initial guide for the student model makes it difficult for the student to imitate the teacher model to its fullest. Conventional KD methods yield low performance due to these limitations. In this paper, we propose Pea-KD (Parameter-efficient and accurate Knowledge Distillation), a novel approach to KD. Pea-KD consists of two main parts: Shuffled Parameter Sharing (SPS) and Pretraining with Teacher\u2019s Predictions (PTP). Using this combination, we are capable of alleviating the KD\u2019s limitations. SPS is a new parameter sharing method that increases the student model capacity. PTP is a KD-specialized initialization method, which can act as a good initial guide for the student. When combined, this method yields a significant increase in student model\u2019s performance. Experiments conducted on BERT with different datasets and tasks show that the proposed approach improves the student model\u2019s performance by 4.4% on average in four GLUE tasks, outperforming existing KD baselines by significant margins.",
            "score": 0.4946253700021551,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81640625
        },
        {
            "corpus_id": "268512785",
            "title": "Multiple Teachers-Meticulous Student: A Domain Adaptive Meta-Knowledge Distillation Model for Medical Image Classification",
            "text": "In general, the knowledge distillation process requires a distillation approach and a teacher-student architecture (Wang & Yoon, 2021).The knowledge learned by the teacher(s) is transferred through a distillation approach to the student model, which generally has a simpler architecture.After the knowledge distillation process, the student model is expected to perform well on the tasks learned by the teacher or multiple teachers.Besides, there is no need to access the source datasets used in multiple-teacher training to distil knowledge, which can preserve patients' privacy (Chen et al., 2023).\n\nAccording to the issues raised in the above paragraphs, in this study, we propose a comprehensive model that can cover these issues.The knowledge distillation process in this study is achieved through an architecture including multiple teachers and a meticulous student.The contribution of this study is to propose a novel model that can simultaneously meet the following goals:\n\n-The proposed model can handle the domain shift problem.In other words, the model can perform properly in facing the input dataset with a different distribution than the datasets based on which the teachers were trained.-Due to the use of meta-learning, the proposed model can achieve proper efficiency even in the conditions of lack of access to considerable annotated data.-By distilling knowledge, the model takes advantage of the knowledge learned by teachers to learn a new task.Besides, the knowledge of multiple teachers, a set of complex models, is consolidated into a simpler model with almost the same efficiency.-The proposed model can work, utilising the trained teachers (models' weights) without accessing source datasets.It means there will be no need for the source datasets in the knowledge distillation process, which can lead to the preservation of patients' privacy.\n\nThe rest of this article is organised as follows.Section 2 reviews related studies.In section 3, the proposed model, the exploited datasets, and the implementation details are presented.The results of the extensive experiments are given in section 4. Section 5 is dedicated to a discussion of the proposed model and the achieved goals.Finally, the current study is concluded in section 6.",
            "score": 0.49460379827914625,
            "section_title": "Introduction",
            "char_start_offset": 2044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 287
                },
                {
                    "start": 287,
                    "end": 432
                },
                {
                    "start": 432,
                    "end": 600
                },
                {
                    "start": 602,
                    "end": 734
                },
                {
                    "start": 734,
                    "end": 872
                },
                {
                    "start": 872,
                    "end": 980
                },
                {
                    "start": 982,
                    "end": 1038
                },
                {
                    "start": 1038,
                    "end": 1202
                },
                {
                    "start": 1202,
                    "end": 1357
                },
                {
                    "start": 1357,
                    "end": 1466
                },
                {
                    "start": 1466,
                    "end": 1605
                },
                {
                    "start": 1605,
                    "end": 1718
                },
                {
                    "start": 1718,
                    "end": 1868
                },
                {
                    "start": 1870,
                    "end": 1919
                },
                {
                    "start": 1919,
                    "end": 1953
                },
                {
                    "start": 1953,
                    "end": 2056
                },
                {
                    "start": 2056,
                    "end": 2205
                },
                {
                    "start": 2205,
                    "end": 2258
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 134,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 580,
                    "end": 599,
                    "matchedPaperCorpusId": "249848140"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.293701171875
        },
        {
            "corpus_id": "277043690",
            "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression",
            "text": "Model In this paper, we primarily focus on SmolLM-series models2 (SmolLM-135M-Instruct, SmolLM-360M-Instruct) and Llama 3.2-series models3 (Llama3.2-1B-Instruct, Llama3.2-3B-Instruct) as our target models which span a variety of scales. All the models employ GQA as their attention module. For MLA, we utilize the same number of attention heads and the same head dimension for values. We tried multiple r q , r kv , and d qk (d r ) while maintaining a similar amount of parameters as the target model. \n\nAs mentioned earlier, we employ knowledge distillation, transferring knowledge from teacher models to the target model. In our experiments, we explore the impact of different teacher model sizes, including Llama3.2-1B-Instruct, Llama3.2-3B-Instruct, and Llama3.1-8B-Instruct Grattafiori et al. (2024). This approach allows us to analyze how the varying sizes of the teacher model impact the performance and efficiency of the distilled student model. \n\nTraining procedure Our approach begins with a pre-trained model, so we focus primarily on post-training rather than pre-training the target model again. We use a two-stage training procedure. Specifically, in the first stage (knowledge distillation), our X-EcoMLA model is trained with SFT using the KL loss between the output of the target model and the teacher model. We use AdamW optimizer with \u03b2 = (0.9, 0.  (2023). We set the training batch size at 64 and still use AdamW as the optimizer. We unfreeze all parameters in our X-EcoMLA model and train them for one epoch for both SFT and DPO. Self-distillation Evaluation Table 1 shows the benchmark performance of our proposed X-EcoMLA when we use the target models themselves as the teacher model, which we refer to as self-distillation.",
            "score": 0.4943033224665867,
            "section_title": "Experimental Setup",
            "char_start_offset": 14575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1747
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33056640625
        },
        {
            "corpus_id": "273969804",
            "title": "Quantifying Knowledge Distillation Using Partial Information Decomposition",
            "text": "The teacher may or may not have been trained for the same task as the student. \n\nit has remained a challenge to maximize these measures as a regularizer since computing them itself requires solving an optimization. Our optimization leads to a novel knowledge distillation framework -Redundant Information Distillation (RID) -which precisely captures task-relevant knowledge and filters out the task-irrelevant information from the teacher. In summary, our main contributions are as follows: \n\n\u2022 Quantifying transferred knowledge and knowledge to distill: Given a downstream task, and a teacher and a student model, we formally define the knowledge to distill as the unique information in the teacher (Definition 3.1) and the transferred knowledge as the redundant information (Definition 3.2). Through examples and theoretical results (Theorem 3.1), we first show that redundant information succinctly captures taskrelated knowledge transferred to the student as opposed to the existing frameworks which directly align teacher (T ) and student (S) representations, e.g., Variational Information Distillation (VID) [3] maximizes the mutual information I(T ; S). Theorem 3.1 points out a fundamental limitation of the existing knowledge distillation frameworks for capacity-limited students: they blindly align the student and the teacher without precisely capturing the task-related knowledge. \n\n\u2022 Maximizing redundant information as a regularizer: To alleviate this limitation, we propose a strategy to incorporate redundant information as a regularizer during model distillation to precisely maximize the task-relevant transferred knowledge. We first circumvent the challenge of computing the redundant information measure proposed in [12] by utilizing the quantity termed intersection information defined in [13], which we prove (in Theorem 4.1) to be a lower-bound for redundant information. The significance of Theorem 4.1 is that it enables us to obtain an optimization formulation to maximize a lower-bound of redundant information without making distributional assumptions, a contribution that is also of independent interest outside the domain of knowledge distillation. \n\n\u2022 A novel knowledge distillation framework: We propose a new framework called Redundant Information Distillation (RID) whose distillation loss is tailored to maximize the redundant information (i.e., the transferred knowledge as per Definition 3.2).",
            "score": 0.4940671306732148,
            "section_title": "Introduction",
            "char_start_offset": 2228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 81,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2178
                },
                {
                    "start": 2181,
                    "end": 2430
                }
            ],
            "ref_mentions": [
                {
                    "start": 1114,
                    "end": 1117,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1736,
                    "end": 1740,
                    "matchedPaperCorpusId": "10901107"
                },
                {
                    "start": 1810,
                    "end": 1814,
                    "matchedPaperCorpusId": "8480380"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.475341796875
        },
        {
            "paperId": "139be9f98735c72aa9932b7a9c2b6de33ca2245c",
            "corpusId": 265622441,
            "title": "Breast cancer diagnosis through knowledge distillation of Swin transformer-based teacher\u2013student models",
            "venue": "Machine Learning: Science and Technology",
            "year": 2023,
            "referenceCount": 11,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://iopscience.iop.org/article/10.1088/2632-2153/ad10cc/pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/2632-2153/ad10cc?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/2632-2153/ad10cc, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2244523635",
                    "name": "Bhavannarayanna Kolla"
                },
                {
                    "authorId": "2262699276",
                    "name": "V. P"
                }
            ],
            "abstract": "Breast cancer is a significant global health concern, emphasizing the crucial need for a timely and accurate diagnosis to enhance survival rates. Traditional diagnostic methods rely on pathologists analyzing whole-slide images (WSIs) to identify and diagnose malignancies. However, this task is complex, demanding specialized expertise and imposing a substantial workload on pathologists. Additionally, existing deep learning models, commonly employed for classifying histopathology images, often need enhancements to ensure their suitability for real-time deployment on WSI, especially when trained for small regions of interest (ROIs). This article introduces two Swin transformer-based architectures: the teacher model, characterized by its moderate size, and the lightweight student model. Both models are trained using a publicly available dataset of breast cancer histopathology images, focusing on ROIs with varying magnification factors. Transfer learning is applied to train the teacher model, and knowledge distillation (KD) transfers its capabilities to the student model. To enhance validation accuracy and minimize the total loss in KD, we employ the state\u2013action\u2013reward\u2013state\u2013action (SARSA) reinforcement learning algorithm. The algorithm dynamically computes temperature and a weighting factor throughout the KD process to achieve high accuracy within a considerably shorter training timeframe. Additionally, the student model is deployed to analyze malignancies in WSI. Despite the student model being only one-third the size and flops of the teacher model, it achieves an impressive accuracy of 98.71%, slightly below the teacher\u2019s accuracy of 98.91%. Experimental results demonstrate that the student model can process WSIs at a throughput of 1.67\u2009samples\u2009s\u22121 with an accuracy of 82%. The proposed student model, trained using KD and the SARSA algorithm, exhibits promising breast cancer classification and WSI analysis performance. These findings indicate its potential for assisting pathologists in diagnosing breast cancer accurately and effectively.",
            "corpus_id": "265622441",
            "text": "Breast cancer is a significant global health concern, emphasizing the crucial need for a timely and accurate diagnosis to enhance survival rates. Traditional diagnostic methods rely on pathologists analyzing whole-slide images (WSIs) to identify and diagnose malignancies. However, this task is complex, demanding specialized expertise and imposing a substantial workload on pathologists. Additionally, existing deep learning models, commonly employed for classifying histopathology images, often need enhancements to ensure their suitability for real-time deployment on WSI, especially when trained for small regions of interest (ROIs). This article introduces two Swin transformer-based architectures: the teacher model, characterized by its moderate size, and the lightweight student model. Both models are trained using a publicly available dataset of breast cancer histopathology images, focusing on ROIs with varying magnification factors. Transfer learning is applied to train the teacher model, and knowledge distillation (KD) transfers its capabilities to the student model. To enhance validation accuracy and minimize the total loss in KD, we employ the state\u2013action\u2013reward\u2013state\u2013action (SARSA) reinforcement learning algorithm. The algorithm dynamically computes temperature and a weighting factor throughout the KD process to achieve high accuracy within a considerably shorter training timeframe. Additionally, the student model is deployed to analyze malignancies in WSI. Despite the student model being only one-third the size and flops of the teacher model, it achieves an impressive accuracy of 98.71%, slightly below the teacher\u2019s accuracy of 98.91%. Experimental results demonstrate that the student model can process WSIs at a throughput of 1.67\u2009samples\u2009s\u22121 with an accuracy of 82%. The proposed student model, trained using KD and the SARSA algorithm, exhibits promising breast cancer classification and WSI analysis performance. These findings indicate its potential for assisting pathologists in diagnosing breast cancer accurately and effectively.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.374267578125
        },
        {
            "paperId": "5a43ca456550c7a0104baab7a11e6cf7aa23b512",
            "corpusId": 278960012,
            "title": "Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles",
            "venue": "",
            "year": 2025,
            "referenceCount": 31,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.22027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1580399064",
                    "name": "Miika Toikkanen"
                },
                {
                    "authorId": "2360507276",
                    "name": "June-Woo Kim"
                }
            ],
            "abstract": "Respiratory sound datasets are limited in size and quality, making high performance difficult to achieve. Ensemble models help but inevitably increase compute cost at inference time. Soft label training distills knowledge efficiently with extra cost only at training. In this study, we explore soft labels for respiratory sound classification as an architecture-agnostic approach to distill an ensemble of teacher models into a student model. We examine different variations of our approach and find that even a single teacher, identical to the student, considerably improves performance beyond its own capability, with optimal gains achieved using only a few teachers. We achieve the new state-of-the-art Score of 64.39 on ICHBI, surpassing the previous best by 0.85 and improving average Scores across architectures by more than 1.16. Our results highlight the effectiveness of knowledge distillation with soft labels for respiratory sound classification, regardless of size or architecture.",
            "corpus_id": "278960012",
            "text": "Respiratory sound datasets are limited in size and quality, making high performance difficult to achieve. Ensemble models help but inevitably increase compute cost at inference time. Soft label training distills knowledge efficiently with extra cost only at training. In this study, we explore soft labels for respiratory sound classification as an architecture-agnostic approach to distill an ensemble of teacher models into a student model. We examine different variations of our approach and find that even a single teacher, identical to the student, considerably improves performance beyond its own capability, with optimal gains achieved using only a few teachers. We achieve the new state-of-the-art Score of 64.39 on ICHBI, surpassing the previous best by 0.85 and improving average Scores across architectures by more than 1.16. Our results highlight the effectiveness of knowledge distillation with soft labels for respiratory sound classification, regardless of size or architecture.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.55078125
        },
        {
            "paperId": "3b743866f26ea1ce9ab300842b1e69d1f62c9a4e",
            "corpusId": 277954831,
            "title": "Learning from Stochastic Teacher Representations Using Student-Guided Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 38,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.14307, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2057319769",
                    "name": "Muhammad Haseeb Aslam"
                },
                {
                    "authorId": "2356582101",
                    "name": "Clara Martinez"
                },
                {
                    "authorId": "3048367",
                    "name": "Marco Pedersoli"
                },
                {
                    "authorId": "2263129961",
                    "name": "A. Koerich"
                },
                {
                    "authorId": "2269469250",
                    "name": "Ali Etemad"
                },
                {
                    "authorId": "2256991724",
                    "name": "Eric Granger"
                }
            ],
            "abstract": "Advances in self-distillation have shown that when knowledge is distilled from a teacher to a student using the same deep learning (DL) architecture, the student performance can surpass the teacher particularly when the network is overparameterized and the teacher is trained with early stopping. Alternatively, ensemble learning also improves performance, although training, storing, and deploying multiple models becomes impractical as the number of models grows. Even distilling an ensemble to a single student model or weight averaging methods first requires training of multiple teacher models and does not fully leverage the inherent stochasticity for generating and distilling diversity in DL models. These constraints are particularly prohibitive in resource-constrained or latency-sensitive applications such as wearable devices. This paper proposes to train only one model and generate multiple diverse teacher representations using distillation-time dropout. However, generating these representations stochastically leads to noisy representations that are misaligned with the learned task. To overcome this problem, a novel stochastic self-distillation (SSD) training strategy is introduced for filtering and weighting teacher representation to distill from task-relevant representations only, using student-guided knowledge distillation (SGKD). The student representation at each distillation step is used as authority to guide the distillation process. Experimental results on real-world affective computing, wearable/biosignal datasets from the UCR Archive, the HAR dataset, and image classification datasets show that the proposed SSD method can outperform state-of-the-art methods without increasing the model size at both training and testing time, and incurs negligible computational complexity compared to state-of-the-art ensemble learning and weight averaging methods.",
            "corpus_id": "277954831",
            "text": "Advances in self-distillation have shown that when knowledge is distilled from a teacher to a student using the same deep learning (DL) architecture, the student performance can surpass the teacher particularly when the network is overparameterized and the teacher is trained with early stopping. Alternatively, ensemble learning also improves performance, although training, storing, and deploying multiple models becomes impractical as the number of models grows. Even distilling an ensemble to a single student model or weight averaging methods first requires training of multiple teacher models and does not fully leverage the inherent stochasticity for generating and distilling diversity in DL models. These constraints are particularly prohibitive in resource-constrained or latency-sensitive applications such as wearable devices. This paper proposes to train only one model and generate multiple diverse teacher representations using distillation-time dropout. However, generating these representations stochastically leads to noisy representations that are misaligned with the learned task. To overcome this problem, a novel stochastic self-distillation (SSD) training strategy is introduced for filtering and weighting teacher representation to distill from task-relevant representations only, using student-guided knowledge distillation (SGKD). The student representation at each distillation step is used as authority to guide the distillation process. Experimental results on real-world affective computing, wearable/biosignal datasets from the UCR Archive, the HAR dataset, and image classification datasets show that the proposed SSD method can outperform state-of-the-art methods without increasing the model size at both training and testing time, and incurs negligible computational complexity compared to state-of-the-art ensemble learning and weight averaging methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.6591796875
        },
        {
            "paperId": "2708cacffc1777b0cbe873116b3ede8244767276",
            "corpusId": 274859280,
            "title": "Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation on Language Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "referenceCount": 42,
            "citationCount": 8,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.14528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279961107",
                    "name": "Xiao Cui"
                },
                {
                    "authorId": "2336255303",
                    "name": "Mo Zhu"
                },
                {
                    "authorId": "2267903811",
                    "name": "Yulei Qin"
                },
                {
                    "authorId": "2336262172",
                    "name": "Liang Xie"
                },
                {
                    "authorId": "38272296",
                    "name": "Wen-gang Zhou"
                },
                {
                    "authorId": "2210048071",
                    "name": "Houqiang Li"
                }
            ],
            "abstract": "Knowledge distillation (KD) has become a prevalent technique for compressing large language models (LLMs). Existing KD methods are constrained by the need for identical tokenizers (i.e., vocabularies) between teacher and student models, limiting their versatility in handling LLMs of different architecture families. In this paper, we introduce the Multi-Level Optimal Transport (MultiLevelOT), a novel approach that advances the optimal transport for universal cross-tokenizer knowledge distillation. Our method aligns the logit distributions of the teacher and the student at both token and sequence levels using diverse cost matrices, eliminating the need for dimensional or token-by-token correspondence. At the token level, MultiLevelOT integrates both global and local information by jointly optimizing all tokens within a sequence to enhance robustness. At the sequence level, we efficiently capture complex distribution structures of logits via the Sinkhorn distance, which approximates the Wasserstein distance for divergence measures. Extensive experiments on tasks such as extractive QA, generative QA, and summarization demonstrate that the MultiLevelOT outperforms state-of-the-art cross-tokenizer KD methods under various settings. Our approach is robust to different student and teacher models across model families, architectures, and parameter sizes.",
            "corpus_id": "274859280",
            "text": "Knowledge distillation (KD) has become a prevalent technique for compressing large language models (LLMs). Existing KD methods are constrained by the need for identical tokenizers (i.e., vocabularies) between teacher and student models, limiting their versatility in handling LLMs of different architecture families. In this paper, we introduce the Multi-Level Optimal Transport (MultiLevelOT), a novel approach that advances the optimal transport for universal cross-tokenizer knowledge distillation. Our method aligns the logit distributions of the teacher and the student at both token and sequence levels using diverse cost matrices, eliminating the need for dimensional or token-by-token correspondence. At the token level, MultiLevelOT integrates both global and local information by jointly optimizing all tokens within a sequence to enhance robustness. At the sequence level, we efficiently capture complex distribution structures of logits via the Sinkhorn distance, which approximates the Wasserstein distance for divergence measures. Extensive experiments on tasks such as extractive QA, generative QA, and summarization demonstrate that the MultiLevelOT outperforms state-of-the-art cross-tokenizer KD methods under various settings. Our approach is robust to different student and teacher models across model families, architectures, and parameter sizes.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.708984375
        },
        {
            "paperId": "f28719936905904a688777f93e38b87561b23b41",
            "corpusId": 273225519,
            "title": "Knowledge Distillation with Student-Teacher Model for Whole Heart and Great Vessel Segmentation in Congenital Heart Disease Using CT-Scan Image",
            "venue": "2024 International Conference on Artificial Intelligence, Blockchain, Cloud Computing, and Data Analytics (ICoABCD)",
            "year": 2024,
            "referenceCount": 28,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICoABCD63526.2024.10704464?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICoABCD63526.2024.10704464, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325084726",
                    "name": "Syifa Salsabila Putri Priambodo"
                },
                {
                    "authorId": "2969172",
                    "name": "I. Ardiyanto"
                },
                {
                    "authorId": "2322267962",
                    "name": "Nahar Taufiq"
                }
            ],
            "abstract": "Congenital Heart Disease (CHD) results from imper-fect heart anatomy during early fetal development. According to the Perhimpunan Dokter Spesialis Kardiovaskular Indonesia (PERKI), around 8\u201310 babies out of every 1000 births in Indonesia suffer from CHD. However, with advances in medical technology, many children with heart defects can survive. Cardiac CT-Scan images are used as the basis for appropriate treatment, while image analysis allows evaluation of abnormalities and monitoring of disease progression posttherapy. Developing a system with lightweight computing and good performance that can segment the cardiac CT scan image to support the doctor's analysis is necessary. For more specific or complex tasks, the model will use a huge memory. Because the model should use more parameters. Bigger memory that model used, more difficult to implement in the device with limited storage. By reducing the number of parameters and removing redundant connections, it is possible to improve model performance while saving computational load significantly. This research applies Knowledge Distillation (KD), which is a model compression method where knowledge from the teacher model is transferred to the student model. KD aims to teach student models to generalize based on the teacher model's knowledge. This research uses the U-Net architecture as the teacher model and the CFPNet-M architecture as the student model. The KD model performs better IoU than the student model, 0.872 or 3% higher. Judging from the resulting model file size, the KD model file is 131.4MB or 33.33 % smaller than the teacher model file size.",
            "corpus_id": "273225519",
            "text": "Congenital Heart Disease (CHD) results from imper-fect heart anatomy during early fetal development. According to the Perhimpunan Dokter Spesialis Kardiovaskular Indonesia (PERKI), around 8\u201310 babies out of every 1000 births in Indonesia suffer from CHD. However, with advances in medical technology, many children with heart defects can survive. Cardiac CT-Scan images are used as the basis for appropriate treatment, while image analysis allows evaluation of abnormalities and monitoring of disease progression posttherapy. Developing a system with lightweight computing and good performance that can segment the cardiac CT scan image to support the doctor's analysis is necessary. For more specific or complex tasks, the model will use a huge memory. Because the model should use more parameters. Bigger memory that model used, more difficult to implement in the device with limited storage. By reducing the number of parameters and removing redundant connections, it is possible to improve model performance while saving computational load significantly. This research applies Knowledge Distillation (KD), which is a model compression method where knowledge from the teacher model is transferred to the student model. KD aims to teach student models to generalize based on the teacher model's knowledge. This research uses the U-Net architecture as the teacher model and the CFPNet-M architecture as the student model. The KD model performs better IoU than the student model, 0.872 or 3% higher. Judging from the resulting model file size, the KD model file is 131.4MB or 33.33 % smaller than the teacher model file size.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.29931640625
        },
        {
            "paperId": "bf215340ff952911787452c66b2c151f24324618",
            "corpusId": 275545880,
            "title": "Enhancing Speech Emotion Recognition Through Knowledge Distillation",
            "venue": "Information and Communication Technology Convergence",
            "year": 2024,
            "referenceCount": 15,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICTC62082.2024.10826904?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICTC62082.2024.10826904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2340467257",
                    "name": "Trung Minh Nguyen"
                },
                {
                    "authorId": "2280926816",
                    "name": "Phuong-Nam Tran"
                },
                {
                    "authorId": "3103584",
                    "name": "Duc Ngoc Minh Dang"
                }
            ],
            "abstract": "The importance of Speech Emotion Recognition (SER) is growing across diverse applications, which has resulted in the development of multiple methodologies and models to improve SER performance. Nevertheless, some modern SER models require significant processing resources and exhibit poor performance, making them unsuitable for real-time applications. To address this, we propose a novel approach that leverages Knowledge Distillation (KD) to create lightweight student models derived from the 3M-SER architecture. Our method focuses on compressing the text embedding component by replacing BERTBASE with smaller variants while maintaining VGGish for audio embedding. Experiments conducted on the IEMOCAP dataset demonstrate that our student model, which reduces model size by up to 44.9%, achieves performance remarkably close to that of the teacher model while improving inference time by up to 40.2% when trained with KD. These results underscore the effectiveness of KD in creating efficient and accurate SER models suitable for resource-constrained environments and real-time applications. Our work contributes to the ongoing effort to make advanced SER technology more accessible and deployable in practical settings.",
            "corpus_id": "275545880",
            "text": "The importance of Speech Emotion Recognition (SER) is growing across diverse applications, which has resulted in the development of multiple methodologies and models to improve SER performance. Nevertheless, some modern SER models require significant processing resources and exhibit poor performance, making them unsuitable for real-time applications. To address this, we propose a novel approach that leverages Knowledge Distillation (KD) to create lightweight student models derived from the 3M-SER architecture. Our method focuses on compressing the text embedding component by replacing BERTBASE with smaller variants while maintaining VGGish for audio embedding. Experiments conducted on the IEMOCAP dataset demonstrate that our student model, which reduces model size by up to 44.9%, achieves performance remarkably close to that of the teacher model while improving inference time by up to 40.2% when trained with KD. These results underscore the effectiveness of KD in creating efficient and accurate SER models suitable for resource-constrained environments and real-time applications. Our work contributes to the ongoing effort to make advanced SER technology more accessible and deployable in practical settings.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5302734375
        },
        {
            "paperId": "e8e31cb728eeeb6d0fb3e3343cbee8a6df655d67",
            "corpusId": 271762338,
            "title": "A Lightweight Pipeline Edge Detection Model Based on Heterogeneous Knowledge Distillation",
            "venue": "IEEE Transactions on Circuits and Systems - II - Express Briefs",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSII.2024.3439361?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSII.2024.3439361, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115803045",
                    "name": "Chengyuan Zhu"
                },
                {
                    "authorId": "2176938878",
                    "name": "Yanyun Pu"
                },
                {
                    "authorId": "2269228133",
                    "name": "Zhuoling Lyu"
                },
                {
                    "authorId": "2315918943",
                    "name": "Aonan Wu"
                },
                {
                    "authorId": "50293711",
                    "name": "Kaixiang Yang"
                },
                {
                    "authorId": "2188010072",
                    "name": "Qinmin Yang"
                }
            ],
            "abstract": "The pipeline safety warning system (PSEW) is an important guarantee for the safe transportation of energy pipelines. Given the constraints of deploying detection models at resource-limited pipeline stations, there is a compelling need to develop efficient, lightweight models suitable for edge device applications. This brief introduces an adaptive heterogeneous model knowledge distillation network (AHKDnet) for edge deployment of pipeline network detection models. The global information and long-distance dependency relationships from the ViT-based teacher network are transferred to the CNN-based shallow student network. We introduce the learnable modulation parameters to optimize target information enhancement, reducing the impact of irrelevant information. By embedding the model selection at each stage of knowledge distillation, the performance collapse of student models caused by misleading cross-architecture knowledge is avoided, and model convergence is accelerated. Experiments on three actual scene datasets of pipeline networks show that AHKDnet outperforms the state-of-the-art KD methods and has strong generalization ability. Notably, AHKDnet enhances the recognition performance of shallow student networks by an average of 10%, highlighting its efficacy and potential for practical applications. Our method can provide a new reference for edge deployment of PSEW.",
            "corpus_id": "271762338",
            "text": "The pipeline safety warning system (PSEW) is an important guarantee for the safe transportation of energy pipelines. Given the constraints of deploying detection models at resource-limited pipeline stations, there is a compelling need to develop efficient, lightweight models suitable for edge device applications. This brief introduces an adaptive heterogeneous model knowledge distillation network (AHKDnet) for edge deployment of pipeline network detection models. The global information and long-distance dependency relationships from the ViT-based teacher network are transferred to the CNN-based shallow student network. We introduce the learnable modulation parameters to optimize target information enhancement, reducing the impact of irrelevant information. By embedding the model selection at each stage of knowledge distillation, the performance collapse of student models caused by misleading cross-architecture knowledge is avoided, and model convergence is accelerated. Experiments on three actual scene datasets of pipeline networks show that AHKDnet outperforms the state-of-the-art KD methods and has strong generalization ability. Notably, AHKDnet enhances the recognition performance of shallow student networks by an average of 10%, highlighting its efficacy and potential for practical applications. Our method can provide a new reference for edge deployment of PSEW.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.298583984375
        },
        {
            "paperId": "7ac903ece0726b1ca9d1084f2b90efee895e6cc4",
            "corpusId": 276290020,
            "title": "Disparity-constrained Knowledge Distillation for Cyber-Physical Systems and Edge Devices",
            "venue": "2024 2nd International Conference on Advancements and Key Challenges in Green Energy and Computing (AKGEC)",
            "year": 2024,
            "referenceCount": 44,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/AKGEC62572.2024.10868709?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/AKGEC62572.2024.10868709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "67228342",
                    "name": "K. Sunami"
                },
                {
                    "authorId": "143632380",
                    "name": "B. Lall"
                }
            ],
            "abstract": "Miniscule wearable devices with vast data from Cyber-Physical Systems and Internet of Things face challenges in storage, power, and compute due to limited resources. To adapt the state-of-the-art pre-trained models, models and data need to be processed using compression techniques like Knowledge Distillation for its application on edge devices. Knowledge Distillation is limited by student-teacher learning capabilities and several limitations highlighted in introduction. This paper introduces a novel experimental investigation on the impact of size and structural disparity on the performance of knowledge distillation in wearable devices which are otherwise deprived form using these for the given constraints. It uses 12 models of different sizes and architectures as student and teacher networks to distil knowledge from scratch utilizing image augmentation, random subsampling, and checkpoint restoration of student models while training from differently-abled teacher networks to quantify the best accuracy of distilled models at varying size and structural-disparities. The study shows that selecting models with smaller size and structural disparity can significantly enhance the accuracy of knowledge distilled models. The authors suggest further research to develop a new metric for these disparities, which can help identify potential teachers for students constrained by storage and compute limits. We achieved some significant results on standard benchmark datasets like CIFAR10, FMNIST and MNIST that demonstrate that knowledge-distilled models\u2019 accuracy can be improved by selecting student-teacher models with smaller size and structural disparity and suggests further research to develop a new metric.)",
            "corpus_id": "276290020",
            "text": "Miniscule wearable devices with vast data from Cyber-Physical Systems and Internet of Things face challenges in storage, power, and compute due to limited resources. To adapt the state-of-the-art pre-trained models, models and data need to be processed using compression techniques like Knowledge Distillation for its application on edge devices. Knowledge Distillation is limited by student-teacher learning capabilities and several limitations highlighted in introduction. This paper introduces a novel experimental investigation on the impact of size and structural disparity on the performance of knowledge distillation in wearable devices which are otherwise deprived form using these for the given constraints. It uses 12 models of different sizes and architectures as student and teacher networks to distil knowledge from scratch utilizing image augmentation, random subsampling, and checkpoint restoration of student models while training from differently-abled teacher networks to quantify the best accuracy of distilled models at varying size and structural-disparities. The study shows that selecting models with smaller size and structural disparity can significantly enhance the accuracy of knowledge distilled models. The authors suggest further research to develop a new metric for these disparities, which can help identify potential teachers for students constrained by storage and compute limits. We achieved some significant results on standard benchmark datasets like CIFAR10, FMNIST and MNIST that demonstrate that knowledge-distilled models\u2019 accuracy can be improved by selecting student-teacher models with smaller size and structural disparity and suggests further research to develop a new metric.)",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.51318359375
        },
        {
            "paperId": "c919be2068f6395286a7ba8c3a38f7602bcafe59",
            "corpusId": 267364776,
            "title": "Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 29,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.00448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2151789151",
                    "name": "Liyi Yao"
                },
                {
                    "authorId": "2282156422",
                    "name": "Shaobing Gao"
                }
            ],
            "abstract": "Due to the data imbalance and the diversity of defects, student-teacher networks (S-T) are favored in unsupervised anomaly detection, which explores the discrepancy in feature representation derived from the knowledge distillation process to recognize anomalies. However, vanilla S-T network is not stable. Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data. To address this problem, we propose a novel dual-student knowledge distillation (DSKD) architecture. Different from other S-T networks, we use two student networks a single pre-trained teacher network, where the students have the same scale but inverted structures. This framework can enhance the distillation effect to improve the consistency in recognition of normal data, and simultaneously introduce diversity for anomaly representation. To explore high-dimensional semantic information to capture anomaly clues, we employ two strategies. First, a pyramid matching mode is used to perform knowledge distillation on multi-scale feature maps in the intermediate layers of networks. Second, an interaction is facilitated between the two student networks through a deep feature embedding module, which is inspired by real-world group discussions. In terms of classification, we obtain pixel-wise anomaly segmentation maps by measuring the discrepancy between the output feature maps of the teacher and student networks, from which an anomaly score is computed for sample-wise determination. We evaluate DSKD on three benchmark datasets and probe the effects of internal modules through ablation experiments. The results demonstrate that DSKD can achieve exceptional performance on small models like ResNet18 and effectively improve vanilla S-T networks.",
            "corpus_id": "267364776",
            "text": "Due to the data imbalance and the diversity of defects, student-teacher networks (S-T) are favored in unsupervised anomaly detection, which explores the discrepancy in feature representation derived from the knowledge distillation process to recognize anomalies. However, vanilla S-T network is not stable. Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data. To address this problem, we propose a novel dual-student knowledge distillation (DSKD) architecture. Different from other S-T networks, we use two student networks a single pre-trained teacher network, where the students have the same scale but inverted structures. This framework can enhance the distillation effect to improve the consistency in recognition of normal data, and simultaneously introduce diversity for anomaly representation. To explore high-dimensional semantic information to capture anomaly clues, we employ two strategies. First, a pyramid matching mode is used to perform knowledge distillation on multi-scale feature maps in the intermediate layers of networks. Second, an interaction is facilitated between the two student networks through a deep feature embedding module, which is inspired by real-world group discussions. In terms of classification, we obtain pixel-wise anomaly segmentation maps by measuring the discrepancy between the output feature maps of the teacher and student networks, from which an anomaly score is computed for sample-wise determination. We evaluate DSKD on three benchmark datasets and probe the effects of internal modules through ablation experiments. The results demonstrate that DSKD can achieve exceptional performance on small models like ResNet18 and effectively improve vanilla S-T networks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.734375
        },
        {
            "paperId": "4b0a627d67541e5315cef06113a5995f3085bda4",
            "corpusId": 271441660,
            "title": "Knowledge Distillation in Image Classification: The Impact of Datasets",
            "venue": "De Computis",
            "year": 2024,
            "referenceCount": 48,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2073-431X/13/8/184/pdf?version=1721890205",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/computers13080184?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/computers13080184, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293888061",
                    "name": "Ange-Gabriel Belinga"
                },
                {
                    "authorId": "2320520399",
                    "name": "St\u00e9phane C\u00e9dric Koum\u00e9tio T\u00e9kouabou"
                },
                {
                    "authorId": "144219703",
                    "name": "M. Haziti"
                },
                {
                    "authorId": "50729883",
                    "name": "M. Hassouni"
                }
            ],
            "abstract": "As the demand for efficient and lightweight models in image classification grows, knowledge distillation has emerged as a promising technique to transfer expertise from complex teacher models to simpler student models. However, the efficacy of knowledge distillation is intricately linked to the choice of datasets used during training. Datasets are pivotal in shaping a model\u2019s learning process, influencing its ability to generalize and discriminate between diverse patterns. While considerable research has independently explored knowledge distillation and image classification, a comprehensive understanding of how different datasets impact knowledge distillation remains a critical gap. This study systematically investigates the impact of diverse datasets on knowledge distillation in image classification. By varying dataset characteristics such as size, domain specificity, and inherent biases, we aim to unravel the nuanced relationship between datasets and the efficacy of knowledge transfer. Our experiments employ a range of datasets to comprehensively explore their impact on the performance gains achieved through knowledge distillation. This study contributes valuable guidance for researchers and practitioners seeking to optimize image classification models through kno-featured applications. By elucidating the intricate interplay between dataset characteristics and knowledge distillation outcomes, our findings empower the community to make informed decisions when selecting datasets, ultimately advancing the field toward more robust and efficient model development.",
            "corpus_id": "271441660",
            "text": "As the demand for efficient and lightweight models in image classification grows, knowledge distillation has emerged as a promising technique to transfer expertise from complex teacher models to simpler student models. However, the efficacy of knowledge distillation is intricately linked to the choice of datasets used during training. Datasets are pivotal in shaping a model\u2019s learning process, influencing its ability to generalize and discriminate between diverse patterns. While considerable research has independently explored knowledge distillation and image classification, a comprehensive understanding of how different datasets impact knowledge distillation remains a critical gap. This study systematically investigates the impact of diverse datasets on knowledge distillation in image classification. By varying dataset characteristics such as size, domain specificity, and inherent biases, we aim to unravel the nuanced relationship between datasets and the efficacy of knowledge transfer. Our experiments employ a range of datasets to comprehensively explore their impact on the performance gains achieved through knowledge distillation. This study contributes valuable guidance for researchers and practitioners seeking to optimize image classification models through kno-featured applications. By elucidating the intricate interplay between dataset characteristics and knowledge distillation outcomes, our findings empower the community to make informed decisions when selecting datasets, ultimately advancing the field toward more robust and efficient model development.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.6953125
        },
        {
            "paperId": "4d447862c9316c671f3d5dac7a9cd9e046e773be",
            "corpusId": 268230792,
            "title": "Differentially Private Knowledge Distillation via Synthetic Text Generation",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 35,
            "citationCount": 14,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.00932, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2289844412",
                    "name": "James Flemings"
                },
                {
                    "authorId": "2285971718",
                    "name": "Murali Annavaram"
                }
            ],
            "abstract": "Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy puts pressure on practitioners to train LLMs with Differential Privacy (DP) on private data. Concurrently, the exponential growth in parameter size of LLMs necessitates model compression before deployment of LLMs on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, simultaneously applying both schemes can compound the utility degradation. To this end, we propose DistilDP: a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private teacher LLM. The knowledge of a teacher LLM is transferred onto the student in two ways: one way from the synthetic data itself -- the hard labels, and the other way by the output distribution of the teacher evaluated on the synthetic data -- the soft labels. Furthermore, if the teacher and student share a similar architectural structure, we can further distill knowledge by aligning the hidden representations between both. Our experimental results demonstrate that DistilDP can substantially improve the utility over existing baselines, at least $9.0$ PPL on the Big Patent dataset, with strong privacy parameters, $\\epsilon=2$. These promising results progress privacy-preserving compression of autoregressive LLMs. Our code can be accessed here: https://github.com/james-flemings/dp_compress.",
            "corpus_id": "268230792",
            "text": "Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy puts pressure on practitioners to train LLMs with Differential Privacy (DP) on private data. Concurrently, the exponential growth in parameter size of LLMs necessitates model compression before deployment of LLMs on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, simultaneously applying both schemes can compound the utility degradation. To this end, we propose DistilDP: a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private teacher LLM. The knowledge of a teacher LLM is transferred onto the student in two ways: one way from the synthetic data itself -- the hard labels, and the other way by the output distribution of the teacher evaluated on the synthetic data -- the soft labels. Furthermore, if the teacher and student share a similar architectural structure, we can further distill knowledge by aligning the hidden representations between both. Our experimental results demonstrate that DistilDP can substantially improve the utility over existing baselines, at least $9.0$ PPL on the Big Patent dataset, with strong privacy parameters, $\\epsilon=2$. These promising results progress privacy-preserving compression of autoregressive LLMs. Our code can be accessed here: https://github.com/james-flemings/dp_compress.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.299560546875
        },
        {
            "paperId": "73e9524c1db93d9b2ab86c54dc5000a5ed755068",
            "corpusId": 256900630,
            "title": "ST-MFNET Mini: Knowledge Distillation-Driven Frame Interpolation",
            "venue": "International Conference on Information Photonics",
            "year": 2023,
            "referenceCount": 33,
            "citationCount": 10,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2302.08455",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.08455, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2206154068",
                    "name": "Crispian Morris"
                },
                {
                    "authorId": "2051683028",
                    "name": "Duolikun Danier"
                },
                {
                    "authorId": "2152987033",
                    "name": "Fan Zhang"
                },
                {
                    "authorId": "1756108",
                    "name": "N. Anantrasirichai"
                },
                {
                    "authorId": "2138393856",
                    "name": "David R. Bull"
                }
            ],
            "abstract": "Currently, one of the major challenges in deep learning-based video frame interpolation (VFI) is the large model size and high computational complexity associated with many high performance VFI approaches. In this paper, we present a distillation-based two-stage workflow for obtaining compressed VFI models which perform competitively compared to the state of the art, but with significantly reduced model size and complexity. Specifically, an optimization-based network pruning method is applied to a state of the art frame interpolation model, ST-MFNet, which suffers from large model size. The resulting network architecture achieves a 91% reduction in parameter numbers and a 35% increase in speed. The performance of the new network is further enhanced through a teacher-student knowledge distillation training process using a Laplacian distillation loss. The final low complexity model, ST-MFNet Mini, achieves a comparable performance to most existing high-complexity VFI methods, only outperformed by the original ST-MFNet. Our source code is available at https://github.com/crispianm/ST-MFNet-Mini",
            "corpus_id": "256900630",
            "text": "Currently, one of the major challenges in deep learning-based video frame interpolation (VFI) is the large model size and high computational complexity associated with many high performance VFI approaches. In this paper, we present a distillation-based two-stage workflow for obtaining compressed VFI models which perform competitively compared to the state of the art, but with significantly reduced model size and complexity. Specifically, an optimization-based network pruning method is applied to a state of the art frame interpolation model, ST-MFNet, which suffers from large model size. The resulting network architecture achieves a 91% reduction in parameter numbers and a 35% increase in speed. The performance of the new network is further enhanced through a teacher-student knowledge distillation training process using a Laplacian distillation loss. The final low complexity model, ST-MFNet Mini, achieves a comparable performance to most existing high-complexity VFI methods, only outperformed by the original ST-MFNet. Our source code is available at https://github.com/crispianm/ST-MFNet-Mini",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.36181640625
        },
        {
            "paperId": "904761418683af1097576254505506c67dce2d46",
            "corpusId": 259588980,
            "title": "Empowering In-Network Classification in Programmable Switches by Binary Decision Tree and Knowledge Distillation",
            "venue": "IEEE/ACM Transactions on Networking",
            "year": 2024,
            "referenceCount": 62,
            "citationCount": 11,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNET.2023.3287091?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNET.2023.3287091, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1823901871",
                    "name": "Guorui Xie"
                },
                {
                    "authorId": "1930238",
                    "name": "Qing Li"
                },
                {
                    "authorId": "2219436067",
                    "name": "Guanglin Duan"
                },
                {
                    "authorId": "2222546554",
                    "name": "Jiaye Lin"
                },
                {
                    "authorId": "2115459638",
                    "name": "Yutao Dong"
                },
                {
                    "authorId": "50262192",
                    "name": "Yong Jiang"
                },
                {
                    "authorId": "2192855736",
                    "name": "Dan Zhao"
                },
                {
                    "authorId": "20277304",
                    "name": "Yuan Yang"
                }
            ],
            "abstract": "Given the high packet processing efficiency of programmable switches (e.g., P4 switches of Tbps), several works are proposed to offload the decision tree (DT) to P4 switches for in-network classification. Although the DT is suitable for the match-action paradigm in P4 switches, the range match rules used in the DT may not be supported across devices of different P4 standards. Additionally, emerging models including neural networks (NNs) and ensemble models, have shown their superior performance in networking tasks. But their sophisticated operations pose new challenges to the deployment of these models in switches. In this paper, we propose Mousikav2 to address these drawbacks successfully. First, we design a new tree model, i.e., the binary decision tree (BDT). Unlike the DT, our BDT consists of classification rules in the form of bits, which is a good fit for the standard ternary match supported by different hardware/software switches. Second, we introduce a teacher-student knowledge distillation architecture in Mousikav2, which enables the general transfer from other sophisticated models to the BDT. Through this transfer, sophisticated models are indirectly deployed in switches to avoid switch constraints. Finally, a lightweight P4 program is developed to perform classification tasks in switches with the BDT after knowledge distillation. Experiments on three networking tasks and three commodity switches show that Mousikav2 not only improves the classification accuracy by 3.27%, but also reduces the switch stage and memory usage by $2.00\\times $ and 28.67%, respectively. Code is available at https://github.com/xgr19/Mousika.",
            "corpus_id": "259588980",
            "text": "Given the high packet processing efficiency of programmable switches (e.g., P4 switches of Tbps), several works are proposed to offload the decision tree (DT) to P4 switches for in-network classification. Although the DT is suitable for the match-action paradigm in P4 switches, the range match rules used in the DT may not be supported across devices of different P4 standards. Additionally, emerging models including neural networks (NNs) and ensemble models, have shown their superior performance in networking tasks. But their sophisticated operations pose new challenges to the deployment of these models in switches. In this paper, we propose Mousikav2 to address these drawbacks successfully. First, we design a new tree model, i.e., the binary decision tree (BDT). Unlike the DT, our BDT consists of classification rules in the form of bits, which is a good fit for the standard ternary match supported by different hardware/software switches. Second, we introduce a teacher-student knowledge distillation architecture in Mousikav2, which enables the general transfer from other sophisticated models to the BDT. Through this transfer, sophisticated models are indirectly deployed in switches to avoid switch constraints. Finally, a lightweight P4 program is developed to perform classification tasks in switches with the BDT after knowledge distillation. Experiments on three networking tasks and three commodity switches show that Mousikav2 not only improves the classification accuracy by 3.27%, but also reduces the switch stage and memory usage by $2.00\\times $ and 28.67%, respectively. Code is available at https://github.com/xgr19/Mousika.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1463623046875
        },
        {
            "paperId": "fc7f940a739db8e9c385283825bd034b0a1fbefd",
            "corpusId": 275336107,
            "title": "Knowledge distillation with adapted weight",
            "venue": "Statistics (Berlin)",
            "year": 2025,
            "referenceCount": 67,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2501.02705",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.02705, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2236594746",
                    "name": "Sirong Wu"
                },
                {
                    "authorId": "2241568221",
                    "name": "Xi Luo"
                },
                {
                    "authorId": "2338350097",
                    "name": "Junjie Liu"
                },
                {
                    "authorId": "1708351",
                    "name": "Yuhui Deng"
                }
            ],
            "abstract": "Although large models have shown a strong capacity to solve large-scale problems in many areas including natural language and computer vision, their voluminous parameters are hard to deploy in a real-time system due to computational and energy constraints. Addressing this, knowledge distillation through Teacher-Student architecture offers a sustainable pathway to compress the knowledge of large models into more manageable sizes without significantly compromising performance. To enhance the robustness and interpretability of this framework, it is critical to understand how individual training data impact model performance, which is an area that remains underexplored. We propose the Knowledge Distillation with Adaptive Influence Weight (KD-AIF) framework which leverages influence functions from robust statistics to assign weights to training data, grounded in the four key SAFE principles: Sustainability, Accuracy, Fairness, and Explainability. This novel approach not only optimizes distillation but also increases transparency by revealing the significance of different data. The exploration of various update mechanisms within the KD-AIF framework further elucidates its potential to significantly improve learning efficiency and generalization in student models, marking a step toward more explainable and deployable Large Models. KD-AIF is effective in knowledge distillation while also showing exceptional performance in semi-supervised learning with outperforms existing baselines and methods in multiple benchmarks (CIFAR-100, CIFAR-10-4k, SVHN-1k, and GLUE).",
            "corpus_id": "275336107",
            "text": "Although large models have shown a strong capacity to solve large-scale problems in many areas including natural language and computer vision, their voluminous parameters are hard to deploy in a real-time system due to computational and energy constraints. Addressing this, knowledge distillation through Teacher-Student architecture offers a sustainable pathway to compress the knowledge of large models into more manageable sizes without significantly compromising performance. To enhance the robustness and interpretability of this framework, it is critical to understand how individual training data impact model performance, which is an area that remains underexplored. We propose the Knowledge Distillation with Adaptive Influence Weight (KD-AIF) framework which leverages influence functions from robust statistics to assign weights to training data, grounded in the four key SAFE principles: Sustainability, Accuracy, Fairness, and Explainability. This novel approach not only optimizes distillation but also increases transparency by revealing the significance of different data. The exploration of various update mechanisms within the KD-AIF framework further elucidates its potential to significantly improve learning efficiency and generalization in student models, marking a step toward more explainable and deployable Large Models. KD-AIF is effective in knowledge distillation while also showing exceptional performance in semi-supervised learning with outperforms existing baselines and methods in multiple benchmarks (CIFAR-100, CIFAR-10-4k, SVHN-1k, and GLUE).",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.607421875
        },
        {
            "paperId": "e2295dcfbe26b449f5fb373dd7bae24860b9f3ab",
            "corpusId": 276482116,
            "title": "MambaLiteSR: Image Super-Resolution with Low-Rank Mamba Using Knowledge Distillation",
            "venue": "IEEE International Symposium on Quality Electronic Design",
            "year": 2025,
            "referenceCount": 65,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.14090, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2346327200",
                    "name": "Romina Aalishah"
                },
                {
                    "authorId": "2180067662",
                    "name": "Mozhgan Navardi"
                },
                {
                    "authorId": "2393902",
                    "name": "T. Mohsenin"
                }
            ],
            "abstract": "Generative Artificial Intelligence (AI) has gained significant attention in recent years, revolutionizing various applications across industries. Among these, advanced vision models for image super-resolution are in high demand, particularly for deployment on edge devices where real-time processing is crucial. However, deploying such models on edge devices is challenging due to limited computing power and memory. In this paper, we present MambaLiteSR, a novel lightweight image Super-Resolution (SR) model that utilizes the architecture of Vision Mamba. It integrates State Space Blocks and a reconstruction module for efficient feature extraction. To optimize efficiency without affecting performance, MambaLiteSR employs knowledge distillation, transferring essential information from a larger Mamba-based teacher model to a smaller student model through hyperparameter tuning. Through a mathematical analysis of model parameters and their impact on the Peak Signal-to-Noise Ratio (PSNR), we identify key factors and adjust them accordingly. Our comprehensive evaluation shows that MambaLiteSR outperforms state of the art edge SR methods by reducing power consumption while maintaining competitive PSNR and SSIM scores across benchmark datasets such as Set5, Set14, and BSD100. It also reduces the power usage during training by adopting low-rank approximation. Moreover, MambaLiteSR reduces the total number of parameters without degrading performance, enabling the efficient deployment of generative AI models on resource-constrained devices. Deployment on the embedded NVIDIA Jetson Orin Nano confirms the superior balance of MambaLiteSR size, latency, and resource efficiency. The experimental results show that MambaLiteSR achieves performance comparable to both the baseline and other edge models while using 15% fewer parameters than the baseline. It also improves the power consumption by up to 58% compared to state-of-the-art SR edge models, all while maintaining low energy consumption during training.",
            "corpus_id": "276482116",
            "text": "Generative Artificial Intelligence (AI) has gained significant attention in recent years, revolutionizing various applications across industries. Among these, advanced vision models for image super-resolution are in high demand, particularly for deployment on edge devices where real-time processing is crucial. However, deploying such models on edge devices is challenging due to limited computing power and memory. In this paper, we present MambaLiteSR, a novel lightweight image Super-Resolution (SR) model that utilizes the architecture of Vision Mamba. It integrates State Space Blocks and a reconstruction module for efficient feature extraction. To optimize efficiency without affecting performance, MambaLiteSR employs knowledge distillation, transferring essential information from a larger Mamba-based teacher model to a smaller student model through hyperparameter tuning. Through a mathematical analysis of model parameters and their impact on the Peak Signal-to-Noise Ratio (PSNR), we identify key factors and adjust them accordingly. Our comprehensive evaluation shows that MambaLiteSR outperforms state of the art edge SR methods by reducing power consumption while maintaining competitive PSNR and SSIM scores across benchmark datasets such as Set5, Set14, and BSD100. It also reduces the power usage during training by adopting low-rank approximation. Moreover, MambaLiteSR reduces the total number of parameters without degrading performance, enabling the efficient deployment of generative AI models on resource-constrained devices. Deployment on the embedded NVIDIA Jetson Orin Nano confirms the superior balance of MambaLiteSR size, latency, and resource efficiency. The experimental results show that MambaLiteSR achieves performance comparable to both the baseline and other edge models while using 15% fewer parameters than the baseline. It also improves the power consumption by up to 58% compared to state-of-the-art SR edge models, all while maintaining low energy consumption during training.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.26416015625
        },
        {
            "paperId": "38a632a79753c9832c58c4e5ac68d6a63f20d762",
            "corpusId": 269478065,
            "title": "Frequency Domain Alignment for Knowledge Distillation in Clothing Classification",
            "venue": "2023 2nd International Conference on Artificial Intelligence, Human-Computer Interaction and Robotics (AIHCIR)",
            "year": 2023,
            "referenceCount": 19,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/AIHCIR61661.2023.00038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/AIHCIR61661.2023.00038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2299021762",
                    "name": "Guangyu Tang"
                },
                {
                    "authorId": "2299755807",
                    "name": "Chenghu Du"
                },
                {
                    "authorId": "2299105351",
                    "name": "Xiaokun Wu"
                }
            ],
            "abstract": "Knowledge distillation is a powerful approach designed to enhance the performance of lightweight student models by transferring the representation features of a more robust teacher network into the student network. Previous research primarily focuses on matching the feature maps of the teacher and student networks within the spatial domain. However, due to architectural discrepancies between networks, two spatial domain feature maps with identical shape often encapsulate different semantic information at corresponding positions. This has a significant impact on fine-grained classification tasks such as clothing classification. To mitigate the issue of information asymmetry in feature map matching, we introduce a novel method based on frequency domain information alignment. In particular, we extract the magnitude spectrum from the student feature map and the phase spectrum from the teacher feature map. Through specific operations, we generate a student feature map imbued with the positional information of the teacher's feature map. This aligned student feature map can then be used for subsequent spatial matching, adhering to the core assumptions of one-to-one distillation methodologies. We are carrying out comprehensive experiments that demonstrate our method's efficacy, achieving state-of-the-art results on the Clothing1M and Deepfashion datasets.",
            "corpus_id": "269478065",
            "text": "Knowledge distillation is a powerful approach designed to enhance the performance of lightweight student models by transferring the representation features of a more robust teacher network into the student network. Previous research primarily focuses on matching the feature maps of the teacher and student networks within the spatial domain. However, due to architectural discrepancies between networks, two spatial domain feature maps with identical shape often encapsulate different semantic information at corresponding positions. This has a significant impact on fine-grained classification tasks such as clothing classification. To mitigate the issue of information asymmetry in feature map matching, we introduce a novel method based on frequency domain information alignment. In particular, we extract the magnitude spectrum from the student feature map and the phase spectrum from the teacher feature map. Through specific operations, we generate a student feature map imbued with the positional information of the teacher's feature map. This aligned student feature map can then be used for subsequent spatial matching, adhering to the core assumptions of one-to-one distillation methodologies. We are carrying out comprehensive experiments that demonstrate our method's efficacy, achieving state-of-the-art results on the Clothing1M and Deepfashion datasets.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5703125
        },
        {
            "paperId": "a000c9c90ebc0b1550787d7ffcde8f49c2b5d045",
            "corpusId": 266732576,
            "title": "Improved Knowledge Distillation via Teacher Assistants for Sentiment Analysis",
            "venue": "IEEE Symposium Series on Computational Intelligence",
            "year": 2023,
            "referenceCount": 20,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/SSCI52147.2023.10371965?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SSCI52147.2023.10371965, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277550525",
                    "name": "Ximing Dong"
                },
                {
                    "authorId": "2277474698",
                    "name": "Olive Huang"
                },
                {
                    "authorId": "1692320",
                    "name": "P. Thulasiraman"
                },
                {
                    "authorId": "2346798",
                    "name": "Aniket Mahanti"
                }
            ],
            "abstract": "Bidirectional Encoder Representations from Transformers (BERT) has achieved state-of-the-art results on various NLP tasks. However, the size of BERT makes application in time-sensitive scenarios challenging. There are lines of research compressing BERT through different techniques and Knowledge Distillation (KD) is the most popular. Nevertheless, more recent studies challenge the effectiveness of KD from an arbitrarily large teacher model. So far, research on the negative impact of the teacher-student gap on the effectiveness of knowledge transfer has been confined mainly to computer vision. Additionally, those researches were limited to distillations between teachers and students with similar model architectures. To fill the gap in the literature, we implemented a teacher assistant (TA) model lying between a fine-tuned BERT model and non-transformer-based machine learning models, including CNN and Bi-LSTM, for sentiment analysis. We have shown that teaching-assistant-facilitated KD outperformed traditional KD while maintaining a competitive inference efficiency. In particular, a well-designed CNN model could retain 97% of BERT's performance while being 1410x smaller for sentiment analysis. We have also found that BERT is not necessarily a better teacher model than non-transformer-based neural networks.",
            "corpus_id": "266732576",
            "text": "Bidirectional Encoder Representations from Transformers (BERT) has achieved state-of-the-art results on various NLP tasks. However, the size of BERT makes application in time-sensitive scenarios challenging. There are lines of research compressing BERT through different techniques and Knowledge Distillation (KD) is the most popular. Nevertheless, more recent studies challenge the effectiveness of KD from an arbitrarily large teacher model. So far, research on the negative impact of the teacher-student gap on the effectiveness of knowledge transfer has been confined mainly to computer vision. Additionally, those researches were limited to distillations between teachers and students with similar model architectures. To fill the gap in the literature, we implemented a teacher assistant (TA) model lying between a fine-tuned BERT model and non-transformer-based machine learning models, including CNN and Bi-LSTM, for sentiment analysis. We have shown that teaching-assistant-facilitated KD outperformed traditional KD while maintaining a competitive inference efficiency. In particular, a well-designed CNN model could retain 97% of BERT's performance while being 1410x smaller for sentiment analysis. We have also found that BERT is not necessarily a better teacher model than non-transformer-based neural networks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.53564453125
        }
    ],
    "quotes": {
        "cost": 0.20562300000000003,
        "quotes": [
            {
                "idx": 0,
                "key": "[208513309 | Kang et al. | 2019 | Citations: 44]",
                "snippets": "This is partly because a large gap in model capacity between student and teacher hinders learning process of KD as discussed in (Mirzadeh et al. 2019), and the simple objective function to fit the representations of the teacher given by model averaging is not effective to take full advantage of teacher models. In other words, the limited capacity in the student network becomes a bottleneck of KD, which implies that increasing capacity of student models would be beneficial to reduce the performance gap between teacher and student.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 535,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "This is partly because a large gap in model capacity between student and teacher hinders learning process of KD as discussed in (Mirzadeh et al. 2019), and the simple objective function to fit the representations of the teacher given by model averaging is not effective to take full advantage of teacher models. In other words, the limited capacity in the student network becomes a bottleneck of KD, which implies that increasing capacity of student models would be beneficial to reduce the performance gap between teacher and student."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[228376532 | Yuan et al. | 2020 | Citations: 122]",
                "snippets": "Surprisingly, a stronger teacher model may not necessarily lead to a better student model. As shown in Table 1 (Sun et al. (2019)), the RoBERTa-Base model performs better than the BERT-Base model on the MRPC and MNLI-mm tasks. However, the student model using three-layer transformer BERT distilled from the weaker teacher model performs better on the same tasks than the same student model distilled from the stronger teacher model. One possible reason is that the effectiveness of distillation may be bounded by the capability of the student model. A simple student model with fewer parameters may not be able to approximate a very complex teacher model, since the complex teacher model may capture finer-grained patterns in data and cause the student model to overfit in some parts of the data and under some other parts. To achieve good distillation, we have to choose teacher models matching capacities of student models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 740,
                        "end": 1666,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Surprisingly, a stronger teacher model may not necessarily lead to a better student model. As shown in Table 1 (Sun et al. (2019)), the RoBERTa-Base model performs better than the BERT-Base model on the MRPC and MNLI-mm tasks. However, the student model using three-layer transformer BERT distilled from the weaker teacher model performs better on the same tasks than the same student model distilled from the stronger teacher model. One possible reason is that the effectiveness of distillation may be bounded by the capability of the student model. A simple student model with fewer parameters may not be able to approximate a very complex teacher model, since the complex teacher model may capture finer-grained patterns in data and cause the student model to overfit in some parts of the data and under some other parts. To achieve good distillation, we have to choose teacher models matching capacities of student models."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[229679667 | Passban et al. | 2020 | Citations: 122]",
                "snippets": "Experimental results show that intermediate layer matching could be quite effective, but in our study we realized that it may suffer from two shortcomings:\n\n\u2022 If n m, multiple layers in T have to be ignored for distillation but we know that those layers consist of precious information for which we spend expensive resources to learn. This issue is referred to as the skip problem in this paper.\n\n\u2022 Moreover, it seems the way teacher layers are kept/skipped is somewhat arbitrary as there is no particular strategy behind it. Before training, we lack enough knowledge to judge which subset of teacher layers contributes more to the distillation process, so there is a good chance of skipping significant layers if we pick them in an arbitrary fashion. Finding the best subset of layers to distill from requires an exhaustive search or an expert in the field to signify connections. We refer to this issue as the search problem.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Problem Definition",
                        "pdf_hash": "",
                        "start": 1354,
                        "end": 2281,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Experimental results show that intermediate layer matching could be quite effective, but in our study we realized that it may suffer from two shortcomings:\n\n\u2022 If n m, multiple layers in T have to be ignored for distillation but we know that those layers consist of precious information for which we spend expensive resources to learn. This issue is referred to as the skip problem in this paper.\n\n\u2022 Moreover, it seems the way teacher layers are kept/skipped is somewhat arbitrary as there is no particular strategy behind it. Before training, we lack enough knowledge to judge which subset of teacher layers contributes more to the distillation process, so there is a good chance of skipping significant layers if we pick them in an arbitrary fashion. Finding the best subset of layers to distill from requires an exhaustive search or an expert in the field to signify connections. We refer to this issue as the search problem."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[235262724 | Asadian et al. | 2021 | Citations: 1]",
                "snippets": "However, when there is a large difference between the model complexities of teacher and student (i.e., capacity gap), knowledge distillation loses its strength in transferring knowledge from the teacher to the student, thus training a weaker student.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "However, when there is a large difference between the model complexities of teacher and student (i.e., capacity gap), knowledge distillation loses its strength in transferring knowledge from the teacher to the student, thus training a weaker student.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[235694419 | Mazumder et al. | 2021 | Citations: 2]",
                "snippets": "When the teacher and student architectures are the same, the knowledge distillation process is referred to as self-distillation. The authors in [20] demonstrate that self-distillation improves the test set performance of the network. The distillation process increases the generalization ability of the network without requiring additional labeled data for training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Self-Distillation",
                        "pdf_hash": "",
                        "start": 303,
                        "end": 669,
                        "sentence_offsets": [
                            {
                                "start": 303,
                                "end": 431
                            },
                            {
                                "start": 432,
                                "end": 536
                            },
                            {
                                "start": 537,
                                "end": 669
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "When the teacher and student architectures are the same, the knowledge distillation process is referred to as self-distillation. The authors in [20] demonstrate that self-distillation improves the test set performance of the network. The distillation process increases the generalization ability of the network without requiring additional labeled data for training."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[237605152 | Li et al. | 2021 | Citations: 49]",
                "snippets": "We surprisingly find that while the BERT LARGE teacher clearly outperforms the small BERT BASE teacher model, the student model distilled by the BERT BASE teacher achieves better performance on all three datasets. This phenomenon is counter-intuitive as a larger teacher is supposed to provide better supervision signal for the student model. We think that there are two possible factors regarding the size of teacher model that leading to the deteriorated performance:\n\n(1) The predicted logits of the teacher model become less soft as the teacher model becomes larger and more confident about its prediction (Guo et al.,  The teacher model is BERT BASE with 12 layers. 2017; Desai and Durrett, 2020), which decreases the effect of knowledge transfer via the soft targets. We find that a smaller \u03c4 also leads to a decreased performance of the student model, indicating the the less-softened teacher prediction will decrease the student performance. 2 (2) The capacity gap between the teacher and student model increases as the teacher becomes larger. The competency of the student model can not match that of the large teacher model, which weakens the performance of KD.\n\nTo explore the combined influence of these factors, we distill student models with different layers and plot the performance gain compared to directly training the student model without distillation in Figure 2. It can be found that by decreasing the student size, the better supervision from teacher model boosts the performance, while the two counteractive factors dominate as the gap becomes much larger, decreasing the performance gain. We notice that this phenomenon is also observed by Mirzadeh et al. (2020) in computer vision tasks using convolutional networks, showing that it is a widespread issue and",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Bigger Teacher Not Always Raises Better Student",
                        "pdf_hash": "",
                        "start": 595,
                        "end": 2379,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We surprisingly find that while the BERT LARGE teacher clearly outperforms the small BERT BASE teacher model, the student model distilled by the BERT BASE teacher achieves better performance on all three datasets. This phenomenon is counter-intuitive as a larger teacher is supposed to provide better supervision signal for the student model. We think that there are two possible factors regarding the size of teacher model that leading to the deteriorated performance:\n\n(1) The predicted logits of the teacher model become less soft as the teacher model becomes larger and more confident about its prediction (Guo et al.,  The teacher model is BERT BASE with 12 layers. 2017; Desai and Durrett, 2020), which decreases the effect of knowledge transfer via the soft targets. We find that a smaller \u03c4 also leads to a decreased performance of the student model, indicating the the less-softened teacher prediction will decrease the student performance. 2 (2) The capacity gap between the teacher and student model increases as the teacher becomes larger. The competency of the student model can not match that of the large teacher model, which weakens the performance of KD.\n\nTo explore the combined influence of these factors, we distill student models with different layers and plot the performance gain compared to directly training the student model without distillation in Figure 2. It can be found that by decreasing the student size, the better supervision from teacher model boosts the performance, while the two counteractive factors dominate as the gap becomes much larger, decreasing the performance gain. We notice that this phenomenon is also observed by Mirzadeh et al. (2020) in computer vision tasks using convolutional networks, showing that it is a widespread issue and"
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[245650327 | Boschini et al. | 2022 | Citations: 141]",
                "snippets": "While Hinton et al. originally proposed to distillate large teachers -possibly ensembles -into smaller students, further studies revealed additional interesting properties about this technique. In particular, Furlanello et al. (Furlanello et al., 2018) show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[4110009 | Furlanello et al. | 2018 | Citations: 1034]": "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction."
                },
                "metadata": [
                    {
                        "section_title": "Self-Distillation",
                        "pdf_hash": "",
                        "start": 148,
                        "end": 547,
                        "sentence_offsets": [
                            {
                                "start": 148,
                                "end": 341
                            },
                            {
                                "start": 342,
                                "end": 547
                            }
                        ],
                        "ref_mentions": [
                            "4110009"
                        ],
                        "quote": "While Hinton et al. originally proposed to distillate large teachers -possibly ensembles -into smaller students, further studies revealed additional interesting properties about this technique. In particular, Furlanello et al. (Furlanello et al., 2018) show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[247446679 | Gong et al. | 2022 | Citations: 29]",
                "snippets": "For both directions, the student model matches or outperforms its teacher. Usually, in knowledge distillation, the student model gets closer to, but is still weaker than, its teacher model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Which model is a good teacher?",
                        "pdf_hash": "",
                        "start": 1989,
                        "end": 2178,
                        "sentence_offsets": [
                            {
                                "start": 1986,
                                "end": 2063
                            },
                            {
                                "start": 2064,
                                "end": 2178
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For both directions, the student model matches or outperforms its teacher. Usually, in knowledge distillation, the student model gets closer to, but is still weaker than, its teacher model."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[251135436 | Galke et al. | 2021 | Citations: 2]",
                "snippets": "When student and teacher models have a different architecture, we call that cross-architecture distillation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 652,
                        "end": 760,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "When student and teacher models have a different architecture, we call that cross-architecture distillation."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[258832674 | Li et al. | 2023 | Citations: 20]",
                "snippets": "When training on real datasets, it is common to use a relatively large teacher model to train the student model, such as distilling ResNet34 to ResNet18. In general, smaller teacher models often fail to achieve satisfactory distillation performance compared to larger teacher models. However, when working with synthetic datasets, we observe the opposite phenomenon: relatively weak teacher models can actually achieve better distillation performance than strong ones, as shown in Fig. 3 and Fig. 4. Interestingly, we found that as the capacity of the teacher model increases, a significant drop in performance is observed. Specifically, when training ResNet34 on the synthetic dataset, using ResNet18 as the teacher model leads to a 3% improvement in performance compared to using ResNet50 as the teacher model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Relatively weak classifiers are better teachers.",
                        "pdf_hash": "",
                        "start": 344,
                        "end": 1156,
                        "sentence_offsets": [
                            {
                                "start": 344,
                                "end": 497
                            },
                            {
                                "start": 498,
                                "end": 627
                            },
                            {
                                "start": 628,
                                "end": 967
                            },
                            {
                                "start": 968,
                                "end": 1156
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "When training on real datasets, it is common to use a relatively large teacher model to train the student model, such as distilling ResNet34 to ResNet18. In general, smaller teacher models often fail to achieve satisfactory distillation performance compared to larger teacher models. However, when working with synthetic datasets, we observe the opposite phenomenon: relatively weak teacher models can actually achieve better distillation performance than strong ones, as shown in Fig. 3 and Fig. 4. Interestingly, we found that as the capacity of the teacher model increases, a significant drop in performance is observed. Specifically, when training ResNet34 on the synthetic dataset, using ResNet18 as the teacher model leads to a 3% improvement in performance compared to using ResNet50 as the teacher model."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[259378480 | Gao | 2023 | Citations: 0]",
                "snippets": "The use of Simple Knowledge Distillation (SimKD) allows the compression of powerful teacher models into lightweight student models without sacrificing performance. This is achieved by reusing the discriminative classifier of the teacher model for student inference and training the student encoder by feature alignment with a single loss. A projector was also developed to help match the student encoder with the teacher classifier, making the technique applicable to a variety of teacher and student architectures. Experiments show that with the addition of the projector, the technique achieves state-of-the-art results at the cost of a reduced compression ratio.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Simple knowledge distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 665,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 163
                            },
                            {
                                "start": 164,
                                "end": 338
                            },
                            {
                                "start": 339,
                                "end": 515
                            },
                            {
                                "start": 516,
                                "end": 665
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The use of Simple Knowledge Distillation (SimKD) allows the compression of powerful teacher models into lightweight student models without sacrificing performance. This is achieved by reusing the discriminative classifier of the teacher model for student inference and training the student encoder by feature alignment with a single loss. A projector was also developed to help match the student encoder with the teacher classifier, making the technique applicable to a variety of teacher and student architectures. Experiments show that with the addition of the projector, the technique achieves state-of-the-art results at the cost of a reduced compression ratio."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[262084420 | Capogrosso et al. | 2023 | Citations: 43]",
                "snippets": "self-distillation is a special case of online distillation where the teacher and student networks have the same architecture (Yun et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[214727822 | Yun et al. | 2020 | Citations: 281]": "Deep neural networks with millions of parameters may suffer from poor generalization due to overfitting. To mitigate the issue, we propose a new regularization method that penalizes the predictive distribution between similar samples. In particular, we distill the predictive distribution between different samples of the same label during training. This results in regularizing the dark knowledge (i.e., the knowledge on wrong predictions) of a single network (i.e., a self-knowledge distillation) by forcing it to produce more meaningful and consistent predictions in a class-wise manner. Consequently, it mitigates overconfident predictions and reduces intra-class variations. Our experimental results on various image classification tasks demonstrate that the simple yet powerful method can significantly improve not only the generalization ability but also the calibration performance of modern convolutional neural networks."
                },
                "metadata": [
                    {
                        "section_title": "3) Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1927,
                        "end": 2057,
                        "sentence_offsets": [
                            {
                                "start": 1918,
                                "end": 2057
                            }
                        ],
                        "ref_mentions": [
                            "214727822"
                        ],
                        "quote": "self-distillation is a special case of online distillation where the teacher and student networks have the same architecture (Yun et al., 2020)."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[263620565 | Gholami et al. | 2023 | Citations: 12]",
                "snippets": "There's also a limit to how much a smaller student model can learn from a large teacher model. If the student model's capacity is too small, it may not effectively learn the teacher's knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Student Model",
                        "pdf_hash": "",
                        "start": 10,
                        "end": 204,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 104
                            },
                            {
                                "start": 105,
                                "end": 204
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "There's also a limit to how much a smaller student model can learn from a large teacher model. If the student model's capacity is too small, it may not effectively learn the teacher's knowledge."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[267061245 | Wan et al. | 2024 | Citations: 72]",
                "snippets": "In traditional knowledge distillation, the student models are typically constrained to be smaller in size than the teachers. In our scenario, however, there are no limitations on the size of the target model. Second, traditional knowledge distillation often results in the student models lagging behind the teachers in performance after distillation. In contrast, we anticipate that after the fusion, the target model will surpass any of the source models in performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 1316,
                        "end": 1787,
                        "sentence_offsets": [
                            {
                                "start": 1309,
                                "end": 1440
                            },
                            {
                                "start": 1441,
                                "end": 1524
                            },
                            {
                                "start": 1525,
                                "end": 1666
                            },
                            {
                                "start": 1667,
                                "end": 1787
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In traditional knowledge distillation, the student models are typically constrained to be smaller in size than the teachers. In our scenario, however, there are no limitations on the size of the target model. Second, traditional knowledge distillation often results in the student models lagging behind the teachers in performance after distillation. In contrast, we anticipate that after the fusion, the target model will surpass any of the source models in performance."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[267364776 | Yao et al. | 2024 | Citations: 1]",
                "snippets": "Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[270094801 | Wu et al. | 2024 | Citations: 1]",
                "snippets": "Knowledge distillation is commonly employed to compress neural networks, reducing the inference costs and memory footprint. In the scenario of homogenous architecture, feature-based methods have been widely validated for their effectiveness. However, in scenarios where the teacher and student models are of heterogeneous architectures, the inherent differences in feature representation significantly degrade the performance of these methods.\n\nExisting knowledge distillation methods primarily employ logit [1,2,3,4] or intermediate feature maps [5,6,7]8] as the medium for knowledge transfer...While these methods often excel in knowledge distillation with homogeneous architectures due to the intrinsic similarity of feature representations, they face serious setback in heterogeneous settings (Hao et al., 2023).The substantial differences in meta-architecture, input format, and spatial representation between heterogeneous models preclude the use of simple similarity metrics for aligning stage-wise feature representations.Moreover, mimicing the local details in feature representations may lead to detrimental guidance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[265871679 | Hao et al. | 2023 | Citations: 68]": "Knowledge distillation~(KD) has proven to be a highly effective approach for enhancing model performance through a teacher-student training scheme. However, most existing distillation methods are designed under the assumption that the teacher and student models belong to the same model family, particularly the hint-based approaches. By using centered kernel alignment (CKA) to compare the learned features between heterogeneous teacher and student models, we observe significant feature divergence. This divergence illustrates the ineffectiveness of previous hint-based methods in cross-architecture distillation. To tackle the challenge in distilling heterogeneous models, we propose a simple yet effective one-for-all KD framework called OFA-KD, which significantly improves the distillation performance between heterogeneous architectures. Specifically, we project intermediate features into an aligned latent space such as the logits space, where architecture-specific information is discarded. Additionally, we introduce an adaptive target enhancement scheme to prevent the student from being disturbed by irrelevant information. Extensive experiments with various architectures, including CNN, Transformer, and MLP, demonstrate the superiority of our OFA-KD framework in enabling distillation between heterogeneous architectures. Specifically, when equipped with our OFA-KD, the student models achieve notable performance improvements, with a maximum gain of 8.0% on the CIFAR-100 dataset and 0.7% on the ImageNet-1K dataset. PyTorch code and checkpoints can be found at https://github.com/Hao840/OFAKD."
                },
                "metadata": [
                    {
                        "quote": "Knowledge distillation is commonly employed to compress neural networks, reducing the inference costs and memory footprint. In the scenario of homogenous architecture, feature-based methods have been widely validated for their effectiveness. However, in scenarios where the teacher and student models are of heterogeneous architectures, the inherent differences in feature representation significantly degrade the performance of these methods.\n\nExisting knowledge distillation methods primarily employ logit [1,2,3,4] or intermediate feature maps [5,6,7]8] as the medium for knowledge transfer",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1758,
                        "end": 2275,
                        "sentence_offsets": [
                            {
                                "start": 1758,
                                "end": 1963
                            },
                            {
                                "start": 1963,
                                "end": 2177
                            },
                            {
                                "start": 2177,
                                "end": 2274
                            }
                        ],
                        "ref_mentions": [
                            "265871679"
                        ],
                        "quote": "While these methods often excel in knowledge distillation with homogeneous architectures due to the intrinsic similarity of feature representations, they face serious setback in heterogeneous settings (Hao et al., 2023).The substantial differences in meta-architecture, input format, and spatial representation between heterogeneous models preclude the use of simple similarity metrics for aligning stage-wise feature representations.Moreover, mimicing the local details in feature representations may lead to detrimental guidance."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[270257777 | Zhang et al. | 2024 | Citations: 8]",
                "snippets": "Second, the capacity gap between the student model and LLM teachers becomes significantly larger compared to the previous instances when a relatively smaller teacher model was employed.This disparity exacerbates the student model's limited ability to fully match the teacher LLM's output distribution.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1403,
                        "end": 1704,
                        "sentence_offsets": [
                            {
                                "start": 1403,
                                "end": 1588
                            },
                            {
                                "start": 1588,
                                "end": 1704
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Second, the capacity gap between the student model and LLM teachers becomes significantly larger compared to the previous instances when a relatively smaller teacher model was employed.This disparity exacerbates the student model's limited ability to fully match the teacher LLM's output distribution."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[270737867 | Huang et al. | 2024 | Citations: 0]",
                "snippets": "Existing methods (e.g., (Kim et al., 2018)(Passalis et al., 2020)64,82]) often prioritize student model performance at the cost of introducing complex architectures.This increased complexity hinders the adoption of KD (Gou et al., 2020) and negatively impacts subsequent fine-tuning processes, such as identifying the causes of underperformance, devising improvement strategies, and executing them effectively (Wang et al., 2020)(Zhang et al., 2022).Conversely, \"simpler\" student models can increase interpretability but also widen the capacity gap between the student and teacher models, making it challenging to transfer the desired knowledge effectively [56].Therefore, striking an appropriate balance between high student effectiveness and suitable interpretability is crucial for successful knowledge distillation and fine-tuning processes.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215745611 | Wang et al. | 2020 | Citations: 700]": "Deep neural models, in recent years, have been successful in almost every field, even solving the most complex problem statements. However, these models are huge in size with millions (and even billions) of parameters, demanding heavy computation power and failing to be deployed on edge devices. Besides, the performance boost is highly dependent on redundant labeled data. To achieve faster speeds and to handle the problems caused by the lack of labeled data, knowledge distillation (KD) has been proposed to transfer information learned from one model to another. KD is often characterized by the so-called \u2018Student-Teacher\u2019 (S-T) learning framework and has been broadly applied in model compression and knowledge transfer. This paper is about KD and S-T learning, which are being actively studied in recent years. First, we aim to provide explanations of what KD is and how/why it works. Then, we provide a comprehensive survey on the recent progress of KD methods together with S-T frameworks typically used for vision tasks. In general, we investigate some fundamental questions that have been driving this research area and thoroughly generalize the research progress and technical details. Additionally, we systematically analyze the research status of KD in vision applications. Finally, we discuss the potentials and open challenges of existing methods and prospect the future directions of KD and S-T learning.",
                    "[218487294 | Passalis et al. | 2020 | Citations: 139]": "Knowledge Distillation (KD) methods are capable of transferring the knowledge encoded in a large and complex teacher into a smaller and faster student. Early methods were usually limited to transferring the knowledge only between the last layers of the networks, while latter approaches were capable of performing multi-layer KD, further increasing the accuracy of the student. However, despite their improved performance, these methods still suffer from several limitations that restrict both their efficiency and flexibility. First, existing KD methods typically ignore that neural networks undergo through different learning phases during the training process, which often requires different types of supervision for each one. Furthermore, existing multi-layer KD methods are usually unable to effectively handle networks with significantly different architectures (heterogeneous KD). In this paper we propose a novel KD method that works by modeling the information flow through the various layers of the teacher model and then train a student model to mimic this information flow. The proposed method is capable of overcoming the aforementioned limitations by using an appropriate supervision scheme during the different phases of the training process, as well as by designing and training an appropriate auxiliary teacher model that acts as a proxy model capable of \u201cexplaining\u201d the way the teacher works to the student. The effectiveness of the proposed method is demonstrated using four image datasets and several different evaluation setups.",
                    "[219559263 | Gou et al. | 2020 | Citations: 2984]": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.",
                    "[251643827 | Zhang et al. | 2022 | Citations: 35]": "Compared to traditional learning from scratch, knowledge distillation sometimes makes the DNN achieve superior performance. In this paper, we provide a new perspective to explain the success of knowledge distillation based on the information theory, i.e., quantifying knowledge points encoded in intermediate layers of a DNN for classification. To this end, we consider the signal processing in a DNN as a layer-wise process of discarding information. A knowledge point is referred to as an input unit, the information of which is discarded much less than that of other input units. Thus, we propose three hypotheses for knowledge distillation based on the quantification of knowledge points. 1. The DNN learning from knowledge distillation encodes more knowledge points than the DNN learning from scratch. 2. Knowledge distillation makes the DNN more likely to learn different knowledge points simultaneously. In comparison, the DNN learning from scratch tends to encode various knowledge points sequentially. 3. The DNN learning from knowledge distillation is often more stably optimized than the DNN learning from scratch. To verify the above hypotheses, we design three types of metrics with annotations of foreground objects to analyze feature representations of the DNN, i.e., the quantity and the quality of knowledge points, the learning speed of different knowledge points, and the stability of optimization directions. In experiments, we diagnosed various DNNs on different classification tasks, including image classification, 3D point cloud classification, binary sentiment classification, and question answering, which verified the above hypotheses.",
                    "[3608236 | Kim et al. | 2018 | Citations: 551]": "Many researchers have sought ways of model compression to reduce the size of a deep neural network (DNN) with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods, a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper, we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher's knowledge and to translate it for the student. This is done by two convolutional modules, which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods."
                },
                "metadata": [
                    {
                        "section_title": "FORMALIZING DESIGN CONSTRAINTS FOR INFICOND 3.1 Design Challenges",
                        "pdf_hash": "",
                        "start": 459,
                        "end": 1224,
                        "sentence_offsets": [
                            {
                                "start": 459,
                                "end": 590
                            },
                            {
                                "start": 590,
                                "end": 829
                            },
                            {
                                "start": 829,
                                "end": 1041
                            },
                            {
                                "start": 1041,
                                "end": 1224
                            }
                        ],
                        "ref_mentions": [
                            "3608236",
                            "218487294",
                            "219559263",
                            "215745611",
                            "251643827"
                        ],
                        "quote": "Existing methods (e.g., (Kim et al., 2018)(Passalis et al., 2020)64,82]) often prioritize student model performance at the cost of introducing complex architectures.This increased complexity hinders the adoption of KD (Gou et al., 2020) and negatively impacts subsequent fine-tuning processes, such as identifying the causes of underperformance, devising improvement strategies, and executing them effectively (Wang et al., 2020)(Zhang et al., 2022).Conversely, \"simpler\" student models can increase interpretability but also widen the capacity gap between the student and teacher models, making it challenging to transfer the desired knowledge effectively [56].Therefore, striking an appropriate balance between high student effectiveness and suitable interpretability is crucial for successful knowledge distillation and fine-tuning processes."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[271956980 | Yu | 2024 | Citations: 1]",
                "snippets": "In addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled (Mirzadeh et al., 2019). We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[212908749 | Mirzadeh et al. | 2019 | Citations: 1081]": "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."
                },
                "metadata": [
                    {
                        "section_title": "B. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1248,
                        "end": 1647,
                        "sentence_offsets": [
                            {
                                "start": 1248,
                                "end": 1451
                            },
                            {
                                "start": 1452,
                                "end": 1564
                            },
                            {
                                "start": 1565,
                                "end": 1647
                            }
                        ],
                        "ref_mentions": [
                            "212908749"
                        ],
                        "quote": "In addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled (Mirzadeh et al., 2019). We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[272368391 | Xu et al. | 2024 | Citations: 35]",
                "snippets": "This approach involves the student model learning solely from the outputs of the teacher model, without access to its internal mechanics or parameters. It is particularly advantageous when the teacher model's details are proprietary or when the architectures of the teacher and student models differ markedly. For instance, Gu et al. (2023) demonstrated that black-box KD could effectively train models using only the output data from LLM APIs like ChatGPT. The student model trains to emulate the teacher's output distribution based on input-output pairs, a process that, while effective, limits learning to external behaviors without tapping into the teacher's deeper internal states...In contrast, White-box Knowledge Distillation allows the student model to access the internal states and workings of the teacher, facilitating a deeper and more precise learning process. This method enables the student to mimic not just the outputs but also the internal state distributions of the teacher, enhancing learning efficacy and depth. The increased access to the teacher's detailed workings helps guide the student's learning, resulting in more accurate and robust models. However, this technique requires a careful alignment of model architectures to ensure effective knowledge transfer and is generally more complex to implement.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 370,
                        "end": 1055,
                        "sentence_offsets": [
                            {
                                "start": 333,
                                "end": 521
                            },
                            {
                                "start": 522,
                                "end": 679
                            },
                            {
                                "start": 680,
                                "end": 827
                            },
                            {
                                "start": 828,
                                "end": 1056
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This approach involves the student model learning solely from the outputs of the teacher model, without access to its internal mechanics or parameters. It is particularly advantageous when the teacher model's details are proprietary or when the architectures of the teacher and student models differ markedly. For instance, Gu et al. (2023) demonstrated that black-box KD could effectively train models using only the output data from LLM APIs like ChatGPT. The student model trains to emulate the teacher's output distribution based on input-output pairs, a process that, while effective, limits learning to external behaviors without tapping into the teacher's deeper internal states"
                    },
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1094,
                        "end": 1737,
                        "sentence_offsets": [
                            {
                                "start": 1057,
                                "end": 1280
                            },
                            {
                                "start": 1281,
                                "end": 1439
                            },
                            {
                                "start": 1440,
                                "end": 1577
                            },
                            {
                                "start": 1578,
                                "end": 1736
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In contrast, White-box Knowledge Distillation allows the student model to access the internal states and workings of the teacher, facilitating a deeper and more precise learning process. This method enables the student to mimic not just the outputs but also the internal state distributions of the teacher, enhancing learning efficacy and depth. The increased access to the teacher's detailed workings helps guide the student's learning, resulting in more accurate and robust models. However, this technique requires a careful alignment of model architectures to ensure effective knowledge transfer and is generally more complex to implement."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[272753230 | Ballout et al. | 2024 | Citations: 3]",
                "snippets": "This limitation underscores the inherent challenge in knowledge distillation and model scaling. While our method effectively transfers knowledge from a large to a small model, the reduced capacity of the smaller model limits its ability to fully replicate the performance of its larger counterpart. This performance discrepancy highlights the trade-offs involved in model downsizing, where gains in efficiency and deployability often come at the cost of reduced accuracy and overall capability.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Limitation",
                        "pdf_hash": "",
                        "start": 447,
                        "end": 941,
                        "sentence_offsets": [
                            {
                                "start": 447,
                                "end": 542
                            },
                            {
                                "start": 543,
                                "end": 745
                            },
                            {
                                "start": 746,
                                "end": 941
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This limitation underscores the inherent challenge in knowledge distillation and model scaling. While our method effectively transfers knowledge from a large to a small model, the reduced capacity of the smaller model limits its ability to fully replicate the performance of its larger counterpart. This performance discrepancy highlights the trade-offs involved in model downsizing, where gains in efficiency and deployability often come at the cost of reduced accuracy and overall capability."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[273375639 | Li et al. | 2024 | Citations: 0]",
                "snippets": "Most existing KD approaches focus on similar-architecture distillation (Romero et al., 2014)(Tian et al., 2019)Liu et al., 2023) (called SAKD), i.e., optional teachers are restricted to a limited scope with structures similar to the student model. However, this homogeneous distillation presents two principal limitations: (1) Limited Potential: Compared to the broader range of arbitrary teachers (including homogeneous and heterogeneous ones), the restricted scope of teachers in SAKD may fail to include the optimal knowledge necessary to enhance the performance of certain students. For instance, as OFA (Hao et al., 2023) demonstrated, distilling knowledge from a heterogeneous ViT-Base to ResNet50 yields superior student performance compared to using a ResNet152 as the homogeneous teacher. (2) Limited Flexibility: The emergence of new models (Liu et al., 2022; Preprint  Tolstikhin et al., 2021) or the scarcity of perfectly tuned homogeneous teachers in domain-specific tasks (Ronneberger et al., 2015)(Li et al., 2024) poses significant challenges in obtaining suitable homogeneous teachers, thereby impeding the applicability of SAKD.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204838340 | Tian et al. | 2019 | Citations: 1054]": "Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation. Code: this http URL",
                    "[268554202 | Li et al. | 2024 | Citations: 22]": "This paper investigates the effective utilization of unlabeled data for large-area cross-view gee-localization (CVGL), encompassing both unsupervised and semi-supervised settings. Common approaches to CVGL rely on ground-satellite image pairs and employ label-driven supervised training. However, the cost of collecting precise cross-view image pairs hinders the deployment of CVGL in real-life scenarios. Without the pairs, CVGL will be more challenging to handle the significant imaging and spatial gaps between ground and satellite images. To this end, we propose an unsupervised framework including a cross-view projection to guide the model for retrieving initial pseudo-labels and a fast re-ranking mechanism to refine the pseudo-labels by leveraging the fact that \u201cthe perfectly paired ground-satellite image is located in a unique and identical scene\u201d. The framework exhibits competitive performance compared with supervised works on three open-source benchmarks. Our code and models will be released on https://github.com/liguopeng0923/UCVGL.",
                    "[2723173 | Romero et al. | 2014 | Citations: 3899]": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.",
                    "[3719281 | Ronneberger et al. | 2015 | Citations: 77405]": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net ."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 744,
                        "end": 1888,
                        "sentence_offsets": [
                            {
                                "start": 744,
                                "end": 990
                            },
                            {
                                "start": 991,
                                "end": 1329
                            },
                            {
                                "start": 1330,
                                "end": 1540
                            },
                            {
                                "start": 1541,
                                "end": 1888
                            }
                        ],
                        "ref_mentions": [
                            "2723173",
                            "204838340",
                            "3719281",
                            "268554202"
                        ],
                        "quote": "Most existing KD approaches focus on similar-architecture distillation (Romero et al., 2014)(Tian et al., 2019)Liu et al., 2023) (called SAKD), i.e., optional teachers are restricted to a limited scope with structures similar to the student model. However, this homogeneous distillation presents two principal limitations: (1) Limited Potential: Compared to the broader range of arbitrary teachers (including homogeneous and heterogeneous ones), the restricted scope of teachers in SAKD may fail to include the optimal knowledge necessary to enhance the performance of certain students. For instance, as OFA (Hao et al., 2023) demonstrated, distilling knowledge from a heterogeneous ViT-Base to ResNet50 yields superior student performance compared to using a ResNet152 as the homogeneous teacher. (2) Limited Flexibility: The emergence of new models (Liu et al., 2022; Preprint  Tolstikhin et al., 2021) or the scarcity of perfectly tuned homogeneous teachers in domain-specific tasks (Ronneberger et al., 2015)(Li et al., 2024) poses significant challenges in obtaining suitable homogeneous teachers, thereby impeding the applicability of SAKD."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[273963558 | Fuente et al. | 2024 | Citations: 0]",
                "snippets": "Self-distillation, where the teacher and student share the same architecture (Ray, 2021), (Gou et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[219559263 | Gou et al. | 2020 | Citations: 2984]": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."
                },
                "metadata": [
                    {
                        "section_title": "3) Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 691,
                        "end": 779,
                        "sentence_offsets": [
                            {
                                "start": 511,
                                "end": 779
                            }
                        ],
                        "ref_mentions": [
                            "248507674",
                            "219559263"
                        ],
                        "quote": "Self-distillation, where the teacher and student share the same architecture (Ray, 2021), (Gou et al., 2020)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[276533262 | Wang et al. | 2025 | Citations: 0]",
                "snippets": "In the case of heterogeneous architectures between teacher and student models (Hao et al., 2023), the features of these models reside in different latent feature spaces, making it challenging to ensure effective alignment of the learned features. Consequently, directly matching these unrelated features is not only unproductive but may also hinder the student model's performance. Furthermore, feature-based distillation methods focus more on local regions, and this localized attention may be insufficient for effectively transferring knowledge from the teacher model to the student model in knowledge distillation [15]. The knowledge embedded in the teacher model is often too complex for the student model to fully absorb and process [16]. Although teacher models are generally more complex and capable of capturing more knowledge, the main challenge lies in distilling this knowledge into a form that is accessible and beneficial for the student model [16].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[265871679 | Hao et al. | 2023 | Citations: 68]": "Knowledge distillation~(KD) has proven to be a highly effective approach for enhancing model performance through a teacher-student training scheme. However, most existing distillation methods are designed under the assumption that the teacher and student models belong to the same model family, particularly the hint-based approaches. By using centered kernel alignment (CKA) to compare the learned features between heterogeneous teacher and student models, we observe significant feature divergence. This divergence illustrates the ineffectiveness of previous hint-based methods in cross-architecture distillation. To tackle the challenge in distilling heterogeneous models, we propose a simple yet effective one-for-all KD framework called OFA-KD, which significantly improves the distillation performance between heterogeneous architectures. Specifically, we project intermediate features into an aligned latent space such as the logits space, where architecture-specific information is discarded. Additionally, we introduce an adaptive target enhancement scheme to prevent the student from being disturbed by irrelevant information. Extensive experiments with various architectures, including CNN, Transformer, and MLP, demonstrate the superiority of our OFA-KD framework in enabling distillation between heterogeneous architectures. Specifically, when equipped with our OFA-KD, the student models achieve notable performance improvements, with a maximum gain of 8.0% on the CIFAR-100 dataset and 0.7% on the ImageNet-1K dataset. PyTorch code and checkpoints can be found at https://github.com/Hao840/OFAKD."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 108,
                        "end": 1056,
                        "sentence_offsets": [
                            {
                                "start": 108,
                                "end": 340
                            },
                            {
                                "start": 341,
                                "end": 475
                            },
                            {
                                "start": 476,
                                "end": 716
                            },
                            {
                                "start": 717,
                                "end": 837
                            },
                            {
                                "start": 838,
                                "end": 1056
                            }
                        ],
                        "ref_mentions": [
                            "265871679"
                        ],
                        "quote": "In the case of heterogeneous architectures between teacher and student models (Hao et al., 2023), the features of these models reside in different latent feature spaces, making it challenging to ensure effective alignment of the learned features. Consequently, directly matching these unrelated features is not only unproductive but may also hinder the student model's performance. Furthermore, feature-based distillation methods focus more on local regions, and this localized attention may be insufficient for effectively transferring knowledge from the teacher model to the student model in knowledge distillation [15]. The knowledge embedded in the teacher model is often too complex for the student model to fully absorb and process [16]. Although teacher models are generally more complex and capable of capturing more knowledge, the main challenge lies in distilling this knowledge into a form that is accessible and beneficial for the student model [16]."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[276776153 | Medina et al. | 2025 | Citations: 0]",
                "snippets": "However, one limitation of traditional KD is the performance drop when there is a large capacity gap between the teacher and student models because the student cannot effectively represent key features of the teacher. This limitation has been addressed by various methods (Passban et al., 2020)16,(Yim et al., 2017)(Zhang et al., 2019) that modify distillation algorithms or employ multi-stage learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[229679667 | Passban et al. | 2020 | Citations: 122]": "Knowledge distillation is considered as a training and compression strategy in which two neural networks, namely a teacher and a student, are coupled together during training. The teacher network is supposed to be a trustworthy predictor and the student tries to mimic its predictions. Usually, a student with a lighter architecture is selected so we can achieve compression and yet deliver high-quality results. In such a setting, distillation only happens for final predictions whereas the student could also benefit from teacher\u2019s supervision for internal components.\n\nMotivated by this, we studied the problem of distillation for intermediate layers. Since there might not be a one-to-one alignment between student and teacher layers, existing techniques skip some teacher layers and only distill from a subset of them. This shortcoming directly impacts quality, so we instead propose a combinatorial technique which relies on attention. Our model fuses teacher-side information and takes each layer\u2019s significance into consideration, then it performs distillation between combined teacher layers and those of the student. Using our technique, we distilled a 12-layer BERT (Devlin et al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE tasks (Wang et al. 2018). Experimental results show that our combinatorial approach is able to outperform other existing techniques.",
                    "[159041406 | Zhang et al. | 2019 | Citations: 865]": "Convolutional neural networks have been widely deployed in various application scenarios. In order to extend the applications' boundaries to some accuracy-crucial domains, researchers have been investigating approaches to boost accuracy through either deeper or wider network structures, which brings with them the exponential increment of the computational and storage cost, delaying the responding time. In this paper, we propose a general training framework named self distillation, which notably enhances the performance (accuracy) of convolutional neural networks through shrinking the size of the network rather than aggrandizing it. Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. The networks are firstly divided into several sections. Then the knowledge in the deeper portion of the networks is squeezed into the shallow ones. Experiments further prove the generalization of the proposed self distillation framework: enhancement of accuracy at average level is 2.65%, varying from 0.61% in ResNeXt as minimum to 4.07% in VGG19 as maximum. In addition, it can also provide flexibility of depth-wise scalable inference on resource-limited edge devices. Our codes have been released on github."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 467,
                        "end": 820,
                        "sentence_offsets": [
                            {
                                "start": 467,
                                "end": 684
                            },
                            {
                                "start": 685,
                                "end": 820
                            }
                        ],
                        "ref_mentions": [
                            "229679667",
                            "206596723",
                            "159041406"
                        ],
                        "quote": "However, one limitation of traditional KD is the performance drop when there is a large capacity gap between the teacher and student models because the student cannot effectively represent key features of the teacher. This limitation has been addressed by various methods (Passban et al., 2020)16,(Yim et al., 2017)(Zhang et al., 2019) that modify distillation algorithms or employ multi-stage learning."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[277652299 | Adhane et al. | 2024 | Citations: 0]",
                "snippets": "The Student's performance often declines when there is a large architecture (capacity) gap between the Teacher and the Student (Mirzadeh et al., 2019)(Stanton et al., 2021). The drop in the Student's performance may stem from either its own challenges in learning relevant features or the overwhelming knowledge of the Teacher. To investigate this issue, we employ two distillation strategies in our experiments using ResNet-101 as the Teacher and ResNet-18 as the Student, which have a significant capacity disparity. In the first approach, we conduct direct distillation from ResNet-101 to ResNet-18. The second approach introduces an intermediate \"Teacher assistant\" (Son et al., 2020) to help bridge the capacity gap between ResNet-101 and ResNet-18...Figure 7 compares the saliency maps of the distilled features learned by two Students: ResNet-18 directly distilled from ResNet-101 (R18-R101) and ResNet-18 distilled from ResNet-101 through Teacher assistant ResNet-50 (R18-R50-R101). The saliency maps, visualised using UniCAM, reveal that the Teacher assistant helps learn more relevant features that highlight the object parts. In contrast, R18-R101 learns some irrelevant features and misses the salient features for the gt prediction.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[212908749 | Mirzadeh et al. | 2019 | Citations: 1081]": "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.",
                    "[221802641 | Son et al. | 2020 | Citations: 119]": "With the success of deep neural networks, knowledge distillation which guides the learning of a small student network from a large teacher network is being actively studied for model compression and transfer learning. However, few studies have been performed to resolve the poor learning issue of the student network when the student and teacher model sizes significantly differ. In this paper, we propose a densely guided knowledge distillation using multiple teacher assistants that gradually decreases the model size to efficiently bridge the large gap between the teacher and student networks. To stimulate more efficient learning of the student network, we guide each teacher assistant to every other smaller teacher assistants iteratively. Specifically, when teaching a smaller teacher assistant at the next step, the existing larger teacher assistants from the previous step are used as well as the teacher network. Moreover, we design stochastic teaching where, for each mini-batch, a teacher or teacher assistants are randomly dropped. This acts as a regularizer to improve the efficiency of teaching of the student network. Thus, the student can always learn salient distilled knowledge from the multiple sources. We verified the effectiveness of the proposed method for a classification task using CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant performance improvements with various backbone architectures such as ResNet, WideResNet, and VGG.1",
                    "[235390933 | Stanton et al. | 2021 | Citations: 222]": "Knowledge distillation is a popular technique for training a small student network to emulate a larger teacher model, such as an ensemble of networks. We show that while knowledge distillation can improve student generalization, it does not typically work as it is commonly understood: there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student has the capacity to perfectly match the teacher. We identify difficulties in optimization as a key reason for why the student is unable to match the teacher. We also show how the details of the dataset used for distillation play a role in how closely the student matches the teacher -- and that more closely matching the teacher paradoxically does not always lead to better student generalization."
                },
                "metadata": [
                    {
                        "section_title": "Exploring the capacity gap impact",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 701,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 135
                            },
                            {
                                "start": 136,
                                "end": 289
                            },
                            {
                                "start": 290,
                                "end": 480
                            },
                            {
                                "start": 481,
                                "end": 564
                            },
                            {
                                "start": 565,
                                "end": 702
                            }
                        ],
                        "ref_mentions": [
                            "212908749",
                            "235390933",
                            "221802641"
                        ],
                        "quote": "The Student's performance often declines when there is a large architecture (capacity) gap between the Teacher and the Student (Mirzadeh et al., 2019)(Stanton et al., 2021). The drop in the Student's performance may stem from either its own challenges in learning relevant features or the overwhelming knowledge of the Teacher. To investigate this issue, we employ two distillation strategies in our experiments using ResNet-101 as the Teacher and ResNet-18 as the Student, which have a significant capacity disparity. In the first approach, we conduct direct distillation from ResNet-101 to ResNet-18. The second approach introduces an intermediate \"Teacher assistant\" (Son et al., 2020) to help bridge the capacity gap between ResNet-101 and ResNet-18"
                    },
                    {
                        "section_title": "Exploring the capacity gap impact",
                        "pdf_hash": "",
                        "start": 1532,
                        "end": 2022,
                        "sentence_offsets": [
                            {
                                "start": 1532,
                                "end": 1766
                            },
                            {
                                "start": 1767,
                                "end": 1912
                            },
                            {
                                "start": 1913,
                                "end": 2021
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Figure 7 compares the saliency maps of the distilled features learned by two Students: ResNet-18 directly distilled from ResNet-101 (R18-R101) and ResNet-18 distilled from ResNet-101 through Teacher assistant ResNet-50 (R18-R50-R101). The saliency maps, visualised using UniCAM, reveal that the Teacher assistant helps learn more relevant features that highlight the object parts. In contrast, R18-R101 learns some irrelevant features and misses the salient features for the gt prediction."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[277954831 | Aslam et al. | 2025 | Citations: 0]",
                "snippets": "Advances in self-distillation have shown that when knowledge is distilled from a teacher to a student using the same deep learning (DL) architecture, the student performance can surpass the teacher particularly when the network is overparameterized and the teacher is trained with early stopping.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Advances in self-distillation have shown that when knowledge is distilled from a teacher to a student using the same deep learning (DL) architecture, the student performance can surpass the teacher particularly when the network is overparameterized and the teacher is trained with early stopping.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[278602421 | Alif et al. | 2025 | Citations: 0]",
                "snippets": "These methods transfer knowledge from a large teacher to a smaller student model. However, if the student is significantly less complex it may fail to capture and replicate the teacher's intricate outputs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 679,
                        "end": 884,
                        "sentence_offsets": [
                            {
                                "start": 679,
                                "end": 760
                            },
                            {
                                "start": 761,
                                "end": 884
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "These methods transfer knowledge from a large teacher to a smaller student model. However, if the student is significantly less complex it may fail to capture and replicate the teacher's intricate outputs."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.024729,
        "cot": "For this query about advantages and limitations of size-matched or identically-architected teacher and student models in knowledge distillation, I'll organize the information into logical dimensions that address different aspects of the question.\n\nFirst, I'll create an \"Introduction\" section to provide background on knowledge distillation and the different setups (same-architecture vs. different-architecture).\n\nThen, I'll organize the content into several key dimensions:\n\n1. \"Same-Architecture Knowledge Distillation (Self-Distillation)\": This dimension will explore the concept of self-distillation where teacher and student models have identical architectures. This should be in synthesis format to coherently explain the concept, benefits, and research findings.\n\n2. \"Advantages of Size-Matched or Same-Architecture Models\": This dimension will list the concrete benefits when using teachers and students of similar capacity. A list format is appropriate to clearly itemize distinct advantages.\n\n3. \"Limitations of Size-Matched or Same-Architecture Models\": This dimension will present the drawbacks or constraints of using teachers and students of similar capacity. A list format works well to clearly present distinct limitations.\n\n4. \"Impact of Capacity Gap on Knowledge Distillation\": This dimension will synthesize information about how the performance gap between teacher and student relates to their architecture differences. Synthesis format is best to explain the complex relationships.\n\n5. \"Alternative Approaches and Solutions\": This dimension will present strategies to overcome limitations, including techniques like teacher assistants and other modifications. A synthesis format will help connect these related concepts.\n\nThese dimensions collectively address the user's query about advantages, limitations, and performance impacts of size-matched teacher-student setups in knowledge distillation.",
        "plan": {
            "Introduction to Knowledge Distillation (synthesis)": [],
            "Same-Architecture Knowledge Distillation (Self-Distillation) (synthesis)": [
                4,
                6,
                11,
                22,
                26
            ],
            "Advantages of Size-Matched or Same-Architecture Models (list)": [
                0,
                1,
                3,
                7,
                9,
                18
            ],
            "Limitations of Size-Matched or Same-Architecture Models (list)": [
                14,
                21
            ],
            "Impact of Capacity Gap on Knowledge Distillation (synthesis)": [
                0,
                1,
                2,
                3,
                5,
                12,
                13,
                16,
                17,
                20,
                23,
                24,
                25,
                27
            ],
            "Alternative Approaches and Solutions (synthesis)": [
                8,
                10,
                15,
                19,
                25
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Knowledge Distillation",
                "tldr": "Knowledge distillation is a technique where a smaller student model learns from a larger teacher model by mimicking its outputs. This approach enables the transfer of knowledge from complex models to simpler ones, which can then perform with similar accuracy while requiring fewer computational resources. (LLM Memory)",
                "text": "\nKnowledge distillation is a model compression technique first formalized by Hinton et al. in 2015, which involves transferring knowledge from a large, complex model (the teacher) to a smaller, more efficient model (the student). The core idea is that the student model learns not just from hard labels in the training data, but also from the \"soft targets\" or probability distributions produced by the teacher model. These soft targets contain richer information about the relationships between classes than one-hot encoded ground truth labels, often revealing how the teacher model generalizes and what it has learned about the similarities between different categories. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe traditional knowledge distillation process involves training a large teacher model to high accuracy, then using this teacher to guide the training of a smaller student model. The student model is typically trained with a combined loss function that includes both a standard cross-entropy loss against the ground truth labels and a distillation loss that measures the divergence between the student's predictions and the teacher's soft outputs. This approach allows the student to mimic the teacher's behavior while maintaining a smaller footprint in terms of parameters and computational requirements. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nWhile the conventional setup involves a significant size difference between teacher and student, there has been growing interest in scenarios where the teacher and student have identical or similar architectures, which is often referred to as self-distillation or same-architecture knowledge distillation. This approach challenges the traditional notion that knowledge transfer requires a disparity in model capacity. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Same-Architecture Knowledge Distillation (Self-Distillation)",
                "tldr": "Self-distillation is a specialized form of knowledge distillation where the teacher and student models share identical architectures. This approach challenges conventional wisdom by showing that knowledge transfer can occur effectively between models of the same capacity, often resulting in student models that outperform their teachers. (8 sources)",
                "text": "\nSelf-distillation represents a distinct variation of knowledge distillation where both the teacher and student networks share the same architecture <Paper corpusId=\"235694419\" paperTitle=\"(Mazumder et al., 2021)\" isShortName></Paper> <Paper corpusId=\"262084420\" paperTitle=\"(Capogrosso et al., 2023)\" isShortName></Paper> <Paper corpusId=\"214727822\" paperTitle=\"(Yun et al., 2020)\" isShortName></Paper>. While Hinton's original knowledge distillation framework emphasized transferring knowledge from larger teacher models to smaller student models, self-distillation demonstrates that knowledge transfer can occur effectively between models of identical capacity <Paper corpusId=\"245650327\" paperTitle=\"(Boschini et al., 2022)\" isShortName></Paper>.\n\nA particularly intriguing finding is that students in self-distillation scenarios can actually outperform their teachers, especially in cases where networks are overparameterized and the teacher model is trained with early stopping <Paper corpusId=\"277954831\" paperTitle=\"(Aslam et al., 2025)\" isShortName></Paper> <Paper corpusId=\"245650327\" paperTitle=\"(Boschini et al., 2022)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>. Furlanello et al. introduced the concept of \"Born-Again Networks\" (BANs), demonstrating that students with identical parameterization to their teachers could achieve superior performance across both computer vision and language modeling tasks <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>.\n\nThe self-distillation process enhances model generalization without requiring additional labeled training data <Paper corpusId=\"235694419\" paperTitle=\"(Mazumder et al., 2021)\" isShortName></Paper>. This improvement occurs because self-distillation effectively regularizes the \"dark knowledge\" (knowledge about incorrect predictions) within a single network by encouraging more consistent and meaningful predictions within each class <Paper corpusId=\"214727822\" paperTitle=\"(Yun et al., 2020)\" isShortName></Paper>. The process can be conducted in multiple rounds, with each successive round potentially improving performance further <Paper corpusId=\"245650327\" paperTitle=\"(Boschini et al., 2022)\" isShortName></Paper>.\n\nSelf-distillation has become an important technique in model compression and acceleration research, offering a way to improve model performance without changing the architecture <Paper corpusId=\"273963558\" paperTitle=\"(Fuente et al., 2024)\" isShortName></Paper> <Paper corpusId=\"219559263\" paperTitle=\"(Gou et al., 2020)\" isShortName></Paper>. This approach has proven particularly valuable in scenarios where computational resources are limited but model performance cannot be compromised.",
                "citations": [
                    {
                        "id": "(Mazumder et al., 2021)",
                        "snippets": [
                            "When the teacher and student architectures are the same, the knowledge distillation process is referred to as self-distillation. The authors in [20] demonstrate that self-distillation improves the test set performance of the network. The distillation process increases the generalization ability of the network without requiring additional labeled data for training."
                        ],
                        "paper": {
                            "corpus_id": 235694419,
                            "title": "Fair Visual Recognition in Limited Data Regime using Self-Supervision and Self-Distillation",
                            "authors": [
                                {
                                    "authorId": "31222412",
                                    "name": "Pratik Mazumder"
                                },
                                {
                                    "authorId": "144377059",
                                    "name": "Pravendra Singh"
                                },
                                {
                                    "authorId": "145460361",
                                    "name": "Vinay P. Namboodiri"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                            "n_citations": 2
                        },
                        "score": 0.80712890625
                    },
                    {
                        "id": "(Capogrosso et al., 2023)",
                        "snippets": [
                            "self-distillation is a special case of online distillation where the teacher and student networks have the same architecture (Yun et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 262084420,
                            "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning",
                            "authors": [
                                {
                                    "authorId": "2135267479",
                                    "name": "Luigi Capogrosso"
                                },
                                {
                                    "authorId": "1396330675",
                                    "name": "Federico Cunico"
                                },
                                {
                                    "authorId": "1780197",
                                    "name": "D. Cheng"
                                },
                                {
                                    "authorId": "2243336023",
                                    "name": "Franco Fummi"
                                },
                                {
                                    "authorId": "2238815087",
                                    "name": "Marco Cristani"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 43
                        },
                        "score": 0.69580078125
                    },
                    {
                        "id": "(Yun et al., 2020)",
                        "snippets": [
                            "Deep neural networks with millions of parameters may suffer from poor generalization due to overfitting. To mitigate the issue, we propose a new regularization method that penalizes the predictive distribution between similar samples. In particular, we distill the predictive distribution between different samples of the same label during training. This results in regularizing the dark knowledge (i.e., the knowledge on wrong predictions) of a single network (i.e., a self-knowledge distillation) by forcing it to produce more meaningful and consistent predictions in a class-wise manner. Consequently, it mitigates overconfident predictions and reduces intra-class variations. Our experimental results on various image classification tasks demonstrate that the simple yet powerful method can significantly improve not only the generalization ability but also the calibration performance of modern convolutional neural networks."
                        ],
                        "paper": {
                            "corpus_id": 214727822,
                            "title": "Regularizing Class-Wise Predictions via Self-Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "66863443",
                                    "name": "Sukmin Yun"
                                },
                                {
                                    "authorId": "2109073979",
                                    "name": "Jongjin Park"
                                },
                                {
                                    "authorId": "3436470",
                                    "name": "Kimin Lee"
                                },
                                {
                                    "authorId": "143720148",
                                    "name": "Jinwoo Shin"
                                }
                            ],
                            "year": 2020,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 281
                        },
                        "score": 0
                    },
                    {
                        "id": "(Boschini et al., 2022)",
                        "snippets": [
                            "While Hinton et al. originally proposed to distillate large teachers -possibly ensembles -into smaller students, further studies revealed additional interesting properties about this technique. In particular, Furlanello et al. (Furlanello et al., 2018) show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student."
                        ],
                        "paper": {
                            "corpus_id": 245650327,
                            "title": "Class-Incremental Continual Learning Into the eXtended DER-Verse",
                            "authors": [
                                {
                                    "authorId": "51096265",
                                    "name": "Matteo Boschini"
                                },
                                {
                                    "authorId": "2123319338",
                                    "name": "Lorenzo Bonicelli"
                                },
                                {
                                    "authorId": "1429191945",
                                    "name": "Pietro Buzzega"
                                },
                                {
                                    "authorId": "51119730",
                                    "name": "Angelo Porrello"
                                },
                                {
                                    "authorId": "2175529",
                                    "name": "S. Calderara"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 141
                        },
                        "score": 0.69140625
                    },
                    {
                        "id": "(Aslam et al., 2025)",
                        "snippets": [
                            "Advances in self-distillation have shown that when knowledge is distilled from a teacher to a student using the same deep learning (DL) architecture, the student performance can surpass the teacher particularly when the network is overparameterized and the teacher is trained with early stopping."
                        ],
                        "paper": {
                            "corpus_id": 277954831,
                            "title": "Learning from Stochastic Teacher Representations Using Student-Guided Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2057319769",
                                    "name": "Muhammad Haseeb Aslam"
                                },
                                {
                                    "authorId": "2356582101",
                                    "name": "Clara Martinez"
                                },
                                {
                                    "authorId": "3048367",
                                    "name": "Marco Pedersoli"
                                },
                                {
                                    "authorId": "2263129961",
                                    "name": "A. Koerich"
                                },
                                {
                                    "authorId": "2269469250",
                                    "name": "Ali Etemad"
                                },
                                {
                                    "authorId": "2256991724",
                                    "name": "Eric Granger"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.6591796875
                    },
                    {
                        "id": "(Furlanello et al., 2018)",
                        "snippets": [
                            "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction."
                        ],
                        "paper": {
                            "corpus_id": 4110009,
                            "title": "Born Again Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2067208583",
                                    "name": "Tommaso Furlanello"
                                },
                                {
                                    "authorId": "32219137",
                                    "name": "Zachary Chase Lipton"
                                },
                                {
                                    "authorId": "143902495",
                                    "name": "Michael Tschannen"
                                },
                                {
                                    "authorId": "7326223",
                                    "name": "L. Itti"
                                },
                                {
                                    "authorId": "2047844",
                                    "name": "Anima Anandkumar"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1034
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fuente et al., 2024)",
                        "snippets": [
                            "Self-distillation, where the teacher and student share the same architecture (Ray, 2021), (Gou et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 273963558,
                            "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network",
                            "authors": [
                                {
                                    "authorId": "2330192141",
                                    "name": "Ra'ul de la Fuente"
                                },
                                {
                                    "authorId": "2047941908",
                                    "name": "Luciano Radrig\u00e1n"
                                },
                                {
                                    "authorId": "2330183718",
                                    "name": "Anibal S Morales"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.66796875
                    },
                    {
                        "id": "(Gou et al., 2020)",
                        "snippets": [
                            "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."
                        ],
                        "paper": {
                            "corpus_id": 219559263,
                            "title": "Knowledge Distillation: A Survey",
                            "authors": [
                                {
                                    "authorId": "38978232",
                                    "name": "Jianping Gou"
                                },
                                {
                                    "authorId": "2425630",
                                    "name": "B. Yu"
                                },
                                {
                                    "authorId": "144555237",
                                    "name": "S. Maybank"
                                },
                                {
                                    "authorId": "143719920",
                                    "name": "D. Tao"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Journal of Computer Vision",
                            "n_citations": 2984
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Advantages of Size-Matched or Same-Architecture Models",
                "tldr": "Size-matched or identical-architecture knowledge distillation setups address the capacity gap problem that can hinder effective knowledge transfer. These approaches can lead to better student performance, sometimes even surpassing the teacher model. (7 sources)",
                "text": "\n- **Better knowledge transfer efficiency**: When there is a large capacity gap between teacher and student models, knowledge distillation can lose its effectiveness in transferring knowledge, resulting in weaker student performance <Paper corpusId=\"235262724\" paperTitle=\"(Asadian et al., 2021)\" isShortName></Paper>. Using size-matched or similar-capacity models helps overcome this limitation.\n\n- **Capacity matching for optimal distillation**: The effectiveness of distillation may be bounded by the capability of the student model. A simple student with fewer parameters may not be able to approximate a very complex teacher model, making it essential to choose teacher models that match the capacities of student models <Paper corpusId=\"228376532\" paperTitle=\"(Yuan et al., 2020)\" isShortName></Paper>.\n\n- **Potential to outperform teachers**: In self-distillation scenarios, student models can match or even outperform their teachers, which is unusual in traditional knowledge distillation where students typically remain weaker than their teachers <Paper corpusId=\"247446679\" paperTitle=\"(Gong et al., 2022)\" isShortName></Paper>.\n\n- **Avoiding overfitting to complex patterns**: Very complex teacher models may capture finer-grained patterns in data, causing the student model to overfit in some areas while underfitting in others. Size-matched models can mitigate this problem <Paper corpusId=\"228376532\" paperTitle=\"(Yuan et al., 2020)\" isShortName></Paper>.\n\n- **Better performance with synthetic data**: When working with synthetic datasets, relatively weak teacher models can achieve better distillation performance than stronger ones. As teacher model capacity increases beyond what's needed, a significant drop in performance can be observed <Paper corpusId=\"258832674\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\n- **Improved optimization dynamics**: The limited capacity in student networks can become a bottleneck for knowledge distillation, implying that increasing the capacity of student models would be beneficial to reduce the performance gap between teacher and student <Paper corpusId=\"208513309\" paperTitle=\"(Kang et al., 2019)\" isShortName></Paper>.\n\n- **Building powerful teachers without changing capacity**: An effective approach is to build powerful teacher models without changing the capacity (width and depth) of the student model, allowing for optimal performance transfer between teacher and student <Paper corpusId=\"271956980\" paperTitle=\"(Yu, 2024)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Asadian et al., 2021)",
                        "snippets": [
                            "However, when there is a large difference between the model complexities of teacher and student (i.e., capacity gap), knowledge distillation loses its strength in transferring knowledge from the teacher to the student, thus training a weaker student."
                        ],
                        "paper": {
                            "corpus_id": 235262724,
                            "title": "Distilling Knowledge via Intermediate Classifiers",
                            "authors": [
                                {
                                    "authorId": "2051713799",
                                    "name": "Aryan Asadian"
                                },
                                {
                                    "authorId": "1403711294",
                                    "name": "Amirali Salehi-Abari"
                                }
                            ],
                            "year": 2021,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.65771484375
                    },
                    {
                        "id": "(Yuan et al., 2020)",
                        "snippets": [
                            "Surprisingly, a stronger teacher model may not necessarily lead to a better student model. As shown in Table 1 (Sun et al. (2019)), the RoBERTa-Base model performs better than the BERT-Base model on the MRPC and MNLI-mm tasks. However, the student model using three-layer transformer BERT distilled from the weaker teacher model performs better on the same tasks than the same student model distilled from the stronger teacher model. One possible reason is that the effectiveness of distillation may be bounded by the capability of the student model. A simple student model with fewer parameters may not be able to approximate a very complex teacher model, since the complex teacher model may capture finer-grained patterns in data and cause the student model to overfit in some parts of the data and under some other parts. To achieve good distillation, we have to choose teacher models matching capacities of student models."
                        ],
                        "paper": {
                            "corpus_id": 228376532,
                            "title": "Reinforced Multi-Teacher Selection for Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "40247395",
                                    "name": "Fei Yuan"
                                },
                                {
                                    "authorId": "24962156",
                                    "name": "Linjun Shou"
                                },
                                {
                                    "authorId": "145525190",
                                    "name": "J. Pei"
                                },
                                {
                                    "authorId": "5617558",
                                    "name": "Wutao Lin"
                                },
                                {
                                    "authorId": "50175330",
                                    "name": "Ming Gong"
                                },
                                {
                                    "authorId": "1832664242",
                                    "name": "Yan Fu"
                                },
                                {
                                    "authorId": "71790825",
                                    "name": "Daxin Jiang"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 122
                        },
                        "score": 0.708984375
                    },
                    {
                        "id": "(Gong et al., 2022)",
                        "snippets": [
                            "For both directions, the student model matches or outperforms its teacher. Usually, in knowledge distillation, the student model gets closer to, but is still weaker than, its teacher model."
                        ],
                        "paper": {
                            "corpus_id": 247446679,
                            "title": "CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification",
                            "authors": [
                                {
                                    "authorId": "145802952",
                                    "name": "Yuan Gong"
                                },
                                {
                                    "authorId": "40570741",
                                    "name": "Sameer Khurana"
                                },
                                {
                                    "authorId": "2110769738",
                                    "name": "Andrew Rouditchenko"
                                },
                                {
                                    "authorId": "145898106",
                                    "name": "James R. Glass"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 29
                        },
                        "score": 0.76025390625
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "When training on real datasets, it is common to use a relatively large teacher model to train the student model, such as distilling ResNet34 to ResNet18. In general, smaller teacher models often fail to achieve satisfactory distillation performance compared to larger teacher models. However, when working with synthetic datasets, we observe the opposite phenomenon: relatively weak teacher models can actually achieve better distillation performance than strong ones, as shown in Fig. 3 and Fig. 4. Interestingly, we found that as the capacity of the teacher model increases, a significant drop in performance is observed. Specifically, when training ResNet34 on the synthetic dataset, using ResNet18 as the teacher model leads to a 3% improvement in performance compared to using ResNet50 as the teacher model."
                        ],
                        "paper": {
                            "corpus_id": 258832674,
                            "title": "Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?",
                            "authors": [
                                {
                                    "authorId": "2146248526",
                                    "name": "Zheng Li"
                                },
                                {
                                    "authorId": "2116210564",
                                    "name": "Yuxuan Li"
                                },
                                {
                                    "authorId": "2284827556",
                                    "name": "Penghai Zhao"
                                },
                                {
                                    "authorId": "2067622132",
                                    "name": "Renjie Song"
                                },
                                {
                                    "authorId": "2144439048",
                                    "name": "Xiang Li"
                                },
                                {
                                    "authorId": "2146236917",
                                    "name": "Jian Yang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.67724609375
                    },
                    {
                        "id": "(Kang et al., 2019)",
                        "snippets": [
                            "This is partly because a large gap in model capacity between student and teacher hinders learning process of KD as discussed in (Mirzadeh et al. 2019), and the simple objective function to fit the representations of the teacher given by model averaging is not effective to take full advantage of teacher models. In other words, the limited capacity in the student network becomes a bottleneck of KD, which implies that increasing capacity of student models would be beneficial to reduce the performance gap between teacher and student."
                        ],
                        "paper": {
                            "corpus_id": 208513309,
                            "title": "Towards Oracle Knowledge Distillation with Neural Architecture Search",
                            "authors": [
                                {
                                    "authorId": "2111623202",
                                    "name": "Minsoo Kang"
                                },
                                {
                                    "authorId": "8511875",
                                    "name": "Jonghwan Mun"
                                },
                                {
                                    "authorId": "40030651",
                                    "name": "Bohyung Han"
                                }
                            ],
                            "year": 2019,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 44
                        },
                        "score": 0.70263671875
                    },
                    {
                        "id": "(Yu, 2024)",
                        "snippets": [
                            "In addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled (Mirzadeh et al., 2019). We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student."
                        ],
                        "paper": {
                            "corpus_id": 271956980,
                            "title": "Bring the Power of Diffusion Model to Defect Detection",
                            "authors": [
                                {
                                    "authorId": "2317142609",
                                    "name": "Xuyi Yu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.7265625
                    },
                    {
                        "id": "(Mirzadeh et al., 2019)",
                        "snippets": [
                            "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."
                        ],
                        "paper": {
                            "corpus_id": 212908749,
                            "title": "Improved Knowledge Distillation via Teacher Assistant",
                            "authors": [
                                {
                                    "authorId": "145156788",
                                    "name": "Seyed Iman Mirzadeh"
                                },
                                {
                                    "authorId": "1682124",
                                    "name": "Mehrdad Farajtabar"
                                },
                                {
                                    "authorId": "2112839418",
                                    "name": "Ang Li"
                                },
                                {
                                    "authorId": "153898744",
                                    "name": "Nir Levine"
                                },
                                {
                                    "authorId": "2063980545",
                                    "name": "Akihiro Matsukawa"
                                },
                                {
                                    "authorId": "144600887",
                                    "name": "H. Ghasemzadeh"
                                }
                            ],
                            "year": 2019,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 1081
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Limitations of Size-Matched or Same-Architecture Models",
                "tldr": "While size-matched knowledge distillation offers advantages, it also introduces significant constraints by limiting the diversity of potential teacher models and reducing flexibility in practical applications. These limitations can prevent students from accessing optimal knowledge transfer and restrict deployment in domain-specific or evolving model landscapes. (6 sources)",
                "text": "\n- **Limited knowledge diversity**: Restricting teachers to similar architectures as students (SAKD - similar-architecture knowledge distillation) can fail to include the optimal knowledge needed to enhance student performance. Heterogeneous teachers sometimes provide better knowledge transfer than homogeneous ones, as demonstrated when distilling from a ViT-Base to ResNet50 yields superior results compared to using a ResNet152 teacher. <Paper corpusId=\"273375639\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>\n\n- **Reduced flexibility in teacher selection**: The emergence of new model architectures or scarcity of well-tuned homogeneous teachers for domain-specific tasks creates significant challenges in finding suitable same-architecture teachers, limiting the practical applicability of similar-architecture knowledge distillation. <Paper corpusId=\"273375639\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"3719281\" paperTitle=\"(Ronneberger et al., 2015)\" isShortName></Paper> <Paper corpusId=\"268554202\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Potential weakening of representational differences**: In anomaly detection contexts, using identical structures for student-teacher networks may weaken the representational discrepancy for anomalies, reducing the effectiveness of the approach for detecting unusual patterns. <Paper corpusId=\"267364776\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>\n\n- **Architectural constraints**: Size-matched distillation limits exploration of architectural innovations that could benefit the student model, such as deeper but thinner networks that might generalize better while running faster. <Paper corpusId=\"2723173\" paperTitle=\"(Romero et al., 2014)\" isShortName></Paper>\n\n- **Suboptimal for representation learning**: Same-architecture distillation may not fully capture the structural knowledge of the teacher network, potentially ignoring important representational information that could be better transferred through alternative approaches like contrastive learning. <Paper corpusId=\"204838340\" paperTitle=\"(Tian et al., 2019)\" isShortName></Paper>\n\n- **Trade-off with normal data performance**: While different structures between teacher and student can improve anomaly detection, they may increase the likelihood of divergent performance on normal data, creating a challenging trade-off. <Paper corpusId=\"267364776\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Most existing KD approaches focus on similar-architecture distillation (Romero et al., 2014)(Tian et al., 2019)Liu et al., 2023) (called SAKD), i.e., optional teachers are restricted to a limited scope with structures similar to the student model. However, this homogeneous distillation presents two principal limitations: (1) Limited Potential: Compared to the broader range of arbitrary teachers (including homogeneous and heterogeneous ones), the restricted scope of teachers in SAKD may fail to include the optimal knowledge necessary to enhance the performance of certain students. For instance, as OFA (Hao et al., 2023) demonstrated, distilling knowledge from a heterogeneous ViT-Base to ResNet50 yields superior student performance compared to using a ResNet152 as the homogeneous teacher. (2) Limited Flexibility: The emergence of new models (Liu et al., 2022; Preprint  Tolstikhin et al., 2021) or the scarcity of perfectly tuned homogeneous teachers in domain-specific tasks (Ronneberger et al., 2015)(Li et al., 2024) poses significant challenges in obtaining suitable homogeneous teachers, thereby impeding the applicability of SAKD."
                        ],
                        "paper": {
                            "corpus_id": 273375639,
                            "title": "TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant",
                            "authors": [
                                {
                                    "authorId": "2108494287",
                                    "name": "Guopeng Li"
                                },
                                {
                                    "authorId": "2280102525",
                                    "name": "Qiang Wang"
                                },
                                {
                                    "authorId": "2266750866",
                                    "name": "Ke Yan"
                                },
                                {
                                    "authorId": "2266754413",
                                    "name": "Shouhong Ding"
                                },
                                {
                                    "authorId": "2287700762",
                                    "name": "Yuan Gao"
                                },
                                {
                                    "authorId": "2286883466",
                                    "name": "Gui-Song Xia"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.76806640625
                    },
                    {
                        "id": "(Ronneberger et al., 2015)",
                        "snippets": [
                            "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net ."
                        ],
                        "paper": {
                            "corpus_id": 3719281,
                            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
                            "authors": [
                                {
                                    "authorId": "1737326",
                                    "name": "O. Ronneberger"
                                },
                                {
                                    "authorId": "152702479",
                                    "name": "P. Fischer"
                                },
                                {
                                    "authorId": "1710872",
                                    "name": "T. Brox"
                                }
                            ],
                            "year": 2015,
                            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                            "n_citations": 77405
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al._1, 2024)",
                        "snippets": [
                            "This paper investigates the effective utilization of unlabeled data for large-area cross-view gee-localization (CVGL), encompassing both unsupervised and semi-supervised settings. Common approaches to CVGL rely on ground-satellite image pairs and employ label-driven supervised training. However, the cost of collecting precise cross-view image pairs hinders the deployment of CVGL in real-life scenarios. Without the pairs, CVGL will be more challenging to handle the significant imaging and spatial gaps between ground and satellite images. To this end, we propose an unsupervised framework including a cross-view projection to guide the model for retrieving initial pseudo-labels and a fast re-ranking mechanism to refine the pseudo-labels by leveraging the fact that \"the perfectly paired ground-satellite image is located in a unique and identical scene\". The framework exhibits competitive performance compared with supervised works on three open-source benchmarks. Our code and models will be released on https://github.com/liguopeng0923/UCVGL."
                        ],
                        "paper": {
                            "corpus_id": 268554202,
                            "title": "Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization",
                            "authors": [
                                {
                                    "authorId": "2108494287",
                                    "name": "Guopeng Li"
                                },
                                {
                                    "authorId": "2292410067",
                                    "name": "Ming Qian"
                                },
                                {
                                    "authorId": "2259003019",
                                    "name": "Gui-Song Xia"
                                }
                            ],
                            "year": 2024,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 22
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yao et al., 2024)",
                        "snippets": [
                            "Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data."
                        ],
                        "paper": {
                            "corpus_id": 267364776,
                            "title": "Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection",
                            "authors": [
                                {
                                    "authorId": "2151789151",
                                    "name": "Liyi Yao"
                                },
                                {
                                    "authorId": "2282156422",
                                    "name": "Shaobing Gao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.734375
                    },
                    {
                        "id": "(Romero et al., 2014)",
                        "snippets": [
                            "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network."
                        ],
                        "paper": {
                            "corpus_id": 2723173,
                            "title": "FitNets: Hints for Thin Deep Nets",
                            "authors": [
                                {
                                    "authorId": "2069136633",
                                    "name": "Adriana Romero"
                                },
                                {
                                    "authorId": "2482072",
                                    "name": "Nicolas Ballas"
                                },
                                {
                                    "authorId": "3127597",
                                    "name": "Samira Ebrahimi Kahou"
                                },
                                {
                                    "authorId": "3186079",
                                    "name": "Antoine Chassang"
                                },
                                {
                                    "authorId": "143706039",
                                    "name": "C. Gatta"
                                },
                                {
                                    "authorId": "1751762",
                                    "name": "Yoshua Bengio"
                                }
                            ],
                            "year": 2014,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 3899
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tian et al., 2019)",
                        "snippets": [
                            "Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation. Code: this http URL"
                        ],
                        "paper": {
                            "corpus_id": 204838340,
                            "title": "Contrastive Representation Distillation",
                            "authors": [
                                {
                                    "authorId": "2476765",
                                    "name": "Yonglong Tian"
                                },
                                {
                                    "authorId": "1707347",
                                    "name": "Dilip Krishnan"
                                },
                                {
                                    "authorId": "2094770",
                                    "name": "Phillip Isola"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 1054
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Impact of Capacity Gap on Knowledge Distillation",
                "tldr": "The capacity gap between teacher and student models significantly affects knowledge distillation effectiveness, with overly large disparities leading to performance degradation. Research shows that as this gap widens, students struggle to absorb complex teacher knowledge, creating a fundamental limitation that can be addressed through intermediate models or capacity-matched architectures. (15 sources)",
                "text": "\nKnowledge distillation faces a critical challenge when there exists a substantial difference in model complexity between teacher and student networks. This phenomenon, known as the capacity gap problem, can significantly diminish the effectiveness of knowledge transfer <Paper corpusId=\"235262724\" paperTitle=\"(Asadian et al., 2021)\" isShortName></Paper> <Paper corpusId=\"208513309\" paperTitle=\"(Kang et al., 2019)\" isShortName></Paper>. When the gap becomes too large, the student model simply cannot effectively approximate the intricate patterns captured by the more complex teacher, resulting in weaker overall performance <Paper corpusId=\"278602421\" paperTitle=\"(Alif et al., 2025)\" isShortName></Paper>.\n\nThis limitation manifests in several ways. First, a student with fewer parameters may be fundamentally incapable of representing the finer-grained patterns captured by more complex teacher models <Paper corpusId=\"228376532\" paperTitle=\"(Yuan et al., 2020)\" isShortName></Paper>. This constraint becomes particularly evident with modern large language models, where the capacity disparity between teacher LLMs and smaller student models has grown dramatically, exacerbating the student's ability to match the teacher's output distribution <Paper corpusId=\"270257777\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nCounterintuitively, research has demonstrated that stronger teacher models do not necessarily produce better student models. In some cases, students distilled from weaker teachers actually outperform those trained with stronger teachers <Paper corpusId=\"237605152\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>. This paradoxical finding can be attributed to two factors: first, larger teachers tend to produce less soft logits that are harder to mimic; second, the increased capacity gap weakens the knowledge distillation process <Paper corpusId=\"237605152\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\nThe capacity gap also creates architectural challenges. In networks with significantly different depths, intermediate layer matching becomes problematic as multiple teacher layers must be skipped or combined, potentially losing valuable information <Paper corpusId=\"229679667\" paperTitle=\"(Passban et al., 2020)\" isShortName></Paper>. This issue is further complicated when dealing with heterogeneous architectures, where features from different model families reside in different latent spaces, making direct matching ineffective or even detrimental <Paper corpusId=\"276533262\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"265871679\" paperTitle=\"(Hao et al., 2023)\" isShortName></Paper>.\n\nTo address these limitations, researchers have proposed several approaches. One promising strategy involves introducing intermediate-sized \"teacher assistant\" models to bridge the gap between large teachers and small students <Paper corpusId=\"277652299\" paperTitle=\"(Adhane et al., 2024)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper> <Paper corpusId=\"221802641\" paperTitle=\"(Son et al., 2020)\" isShortName></Paper>. Experimental evidence shows that this approach helps students learn more relevant features by providing a smoother knowledge transfer pathway <Paper corpusId=\"277652299\" paperTitle=\"(Adhane et al., 2024)\" isShortName></Paper>.\n\nThe fundamental tension in knowledge distillation lies in balancing model interpretability with performance. Simpler student models increase interpretability but widen the capacity gap, making knowledge transfer less effective <Paper corpusId=\"270737867\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"215745611\" paperTitle=\"(Wang et al., 2020)\" isShortName></Paper>. This trade-off highlights the inherent challenge in model downsizing \u2013 gains in efficiency and deployability often come at the cost of reduced accuracy and capability <Paper corpusId=\"272753230\" paperTitle=\"(Ballout et al., 2024)\" isShortName></Paper>.\n\nWhen designing knowledge distillation approaches, researchers must carefully consider the capacity relationship between teacher and student models. The limited capacity in student networks can become a bottleneck for knowledge distillation, suggesting that increasing student capacity would be beneficial to reduce the performance gap <Paper corpusId=\"208513309\" paperTitle=\"(Kang et al., 2019)\" isShortName></Paper>. Alternatively, building powerful teacher models without increasing their width and depth beyond what the student can absorb offers another effective strategy <Paper corpusId=\"228376532\" paperTitle=\"(Yuan et al., 2020)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Asadian et al., 2021)",
                        "snippets": [
                            "However, when there is a large difference between the model complexities of teacher and student (i.e., capacity gap), knowledge distillation loses its strength in transferring knowledge from the teacher to the student, thus training a weaker student."
                        ],
                        "paper": {
                            "corpus_id": 235262724,
                            "title": "Distilling Knowledge via Intermediate Classifiers",
                            "authors": [
                                {
                                    "authorId": "2051713799",
                                    "name": "Aryan Asadian"
                                },
                                {
                                    "authorId": "1403711294",
                                    "name": "Amirali Salehi-Abari"
                                }
                            ],
                            "year": 2021,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.65771484375
                    },
                    {
                        "id": "(Kang et al., 2019)",
                        "snippets": [
                            "This is partly because a large gap in model capacity between student and teacher hinders learning process of KD as discussed in (Mirzadeh et al. 2019), and the simple objective function to fit the representations of the teacher given by model averaging is not effective to take full advantage of teacher models. In other words, the limited capacity in the student network becomes a bottleneck of KD, which implies that increasing capacity of student models would be beneficial to reduce the performance gap between teacher and student."
                        ],
                        "paper": {
                            "corpus_id": 208513309,
                            "title": "Towards Oracle Knowledge Distillation with Neural Architecture Search",
                            "authors": [
                                {
                                    "authorId": "2111623202",
                                    "name": "Minsoo Kang"
                                },
                                {
                                    "authorId": "8511875",
                                    "name": "Jonghwan Mun"
                                },
                                {
                                    "authorId": "40030651",
                                    "name": "Bohyung Han"
                                }
                            ],
                            "year": 2019,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 44
                        },
                        "score": 0.70263671875
                    },
                    {
                        "id": "(Alif et al., 2025)",
                        "snippets": [
                            "These methods transfer knowledge from a large teacher to a smaller student model. However, if the student is significantly less complex it may fail to capture and replicate the teacher's intricate outputs."
                        ],
                        "paper": {
                            "corpus_id": 278602421,
                            "title": "DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images",
                            "authors": [
                                {
                                    "authorId": "2316207082",
                                    "name": "Sadman Sakib Alif"
                                },
                                {
                                    "authorId": "2316207342",
                                    "name": "Nasim Anzum Promise"
                                },
                                {
                                    "authorId": "2361268094",
                                    "name": "Fiaz Al Abid"
                                },
                                {
                                    "authorId": "9748590",
                                    "name": "Aniqua Nusrat Zereen"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.69189453125
                    },
                    {
                        "id": "(Yuan et al., 2020)",
                        "snippets": [
                            "Surprisingly, a stronger teacher model may not necessarily lead to a better student model. As shown in Table 1 (Sun et al. (2019)), the RoBERTa-Base model performs better than the BERT-Base model on the MRPC and MNLI-mm tasks. However, the student model using three-layer transformer BERT distilled from the weaker teacher model performs better on the same tasks than the same student model distilled from the stronger teacher model. One possible reason is that the effectiveness of distillation may be bounded by the capability of the student model. A simple student model with fewer parameters may not be able to approximate a very complex teacher model, since the complex teacher model may capture finer-grained patterns in data and cause the student model to overfit in some parts of the data and under some other parts. To achieve good distillation, we have to choose teacher models matching capacities of student models."
                        ],
                        "paper": {
                            "corpus_id": 228376532,
                            "title": "Reinforced Multi-Teacher Selection for Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "40247395",
                                    "name": "Fei Yuan"
                                },
                                {
                                    "authorId": "24962156",
                                    "name": "Linjun Shou"
                                },
                                {
                                    "authorId": "145525190",
                                    "name": "J. Pei"
                                },
                                {
                                    "authorId": "5617558",
                                    "name": "Wutao Lin"
                                },
                                {
                                    "authorId": "50175330",
                                    "name": "Ming Gong"
                                },
                                {
                                    "authorId": "1832664242",
                                    "name": "Yan Fu"
                                },
                                {
                                    "authorId": "71790825",
                                    "name": "Daxin Jiang"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 122
                        },
                        "score": 0.708984375
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "Second, the capacity gap between the student model and LLM teachers becomes significantly larger compared to the previous instances when a relatively smaller teacher model was employed.This disparity exacerbates the student model's limited ability to fully match the teacher LLM's output distribution."
                        ],
                        "paper": {
                            "corpus_id": 270257777,
                            "title": "PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs",
                            "authors": [
                                {
                                    "authorId": "2288852433",
                                    "name": "Rongzhi Zhang"
                                },
                                {
                                    "authorId": "2266463492",
                                    "name": "Jiaming Shen"
                                },
                                {
                                    "authorId": "2239381730",
                                    "name": "Tianqi Liu"
                                },
                                {
                                    "authorId": "2266420540",
                                    "name": "Haorui Wang"
                                },
                                {
                                    "authorId": "2266819166",
                                    "name": "Zhen Qin"
                                },
                                {
                                    "authorId": "2304748093",
                                    "name": "Feng Han"
                                },
                                {
                                    "authorId": "2239559694",
                                    "name": "Jialu Liu"
                                },
                                {
                                    "authorId": "2282531735",
                                    "name": "Simon Baumgartner"
                                },
                                {
                                    "authorId": "1815447",
                                    "name": "Michael Bendersky"
                                },
                                {
                                    "authorId": "2305503379",
                                    "name": "Chao Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.708984375
                    },
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "We surprisingly find that while the BERT LARGE teacher clearly outperforms the small BERT BASE teacher model, the student model distilled by the BERT BASE teacher achieves better performance on all three datasets. This phenomenon is counter-intuitive as a larger teacher is supposed to provide better supervision signal for the student model. We think that there are two possible factors regarding the size of teacher model that leading to the deteriorated performance:\n\n(1) The predicted logits of the teacher model become less soft as the teacher model becomes larger and more confident about its prediction (Guo et al.,  The teacher model is BERT BASE with 12 layers. 2017; Desai and Durrett, 2020), which decreases the effect of knowledge transfer via the soft targets. We find that a smaller \u03c4 also leads to a decreased performance of the student model, indicating the the less-softened teacher prediction will decrease the student performance. 2 (2) The capacity gap between the teacher and student model increases as the teacher becomes larger. The competency of the student model can not match that of the large teacher model, which weakens the performance of KD.\n\nTo explore the combined influence of these factors, we distill student models with different layers and plot the performance gain compared to directly training the student model without distillation in Figure 2. It can be found that by decreasing the student size, the better supervision from teacher model boosts the performance, while the two counteractive factors dominate as the gap becomes much larger, decreasing the performance gain. We notice that this phenomenon is also observed by Mirzadeh et al. (2020) in computer vision tasks using convolutional networks, showing that it is a widespread issue and"
                        ],
                        "paper": {
                            "corpus_id": 237605152,
                            "title": "Dynamic Knowledge Distillation for Pre-trained Language Models",
                            "authors": [
                                {
                                    "authorId": "49192881",
                                    "name": "Lei Li"
                                },
                                {
                                    "authorId": "2149202150",
                                    "name": "Yankai Lin"
                                },
                                {
                                    "authorId": "1906099",
                                    "name": "Shuhuai Ren"
                                },
                                {
                                    "authorId": "50492525",
                                    "name": "Peng Li"
                                },
                                {
                                    "authorId": "2108485135",
                                    "name": "Jie Zhou"
                                },
                                {
                                    "authorId": "11774802",
                                    "name": "Xu Sun"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 49
                        },
                        "score": 0.65234375
                    },
                    {
                        "id": "(Passban et al., 2020)",
                        "snippets": [
                            "Experimental results show that intermediate layer matching could be quite effective, but in our study we realized that it may suffer from two shortcomings:\n\n\u2022 If n m, multiple layers in T have to be ignored for distillation but we know that those layers consist of precious information for which we spend expensive resources to learn. This issue is referred to as the skip problem in this paper.\n\n\u2022 Moreover, it seems the way teacher layers are kept/skipped is somewhat arbitrary as there is no particular strategy behind it. Before training, we lack enough knowledge to judge which subset of teacher layers contributes more to the distillation process, so there is a good chance of skipping significant layers if we pick them in an arbitrary fashion. Finding the best subset of layers to distill from requires an exhaustive search or an expert in the field to signify connections. We refer to this issue as the search problem."
                        ],
                        "paper": {
                            "corpus_id": 229679667,
                            "title": "ALP-KD: Attention-Based Layer Projection for Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "5062230",
                                    "name": "Peyman Passban"
                                },
                                {
                                    "authorId": "2000863903",
                                    "name": "Yimeng Wu"
                                },
                                {
                                    "authorId": "1924511",
                                    "name": "Mehdi Rezagholizadeh"
                                },
                                {
                                    "authorId": "2115900360",
                                    "name": "Qun Liu"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 122
                        },
                        "score": 0.7919921875
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "In the case of heterogeneous architectures between teacher and student models (Hao et al., 2023), the features of these models reside in different latent feature spaces, making it challenging to ensure effective alignment of the learned features. Consequently, directly matching these unrelated features is not only unproductive but may also hinder the student model's performance. Furthermore, feature-based distillation methods focus more on local regions, and this localized attention may be insufficient for effectively transferring knowledge from the teacher model to the student model in knowledge distillation [15]. The knowledge embedded in the teacher model is often too complex for the student model to fully absorb and process [16]. Although teacher models are generally more complex and capable of capturing more knowledge, the main challenge lies in distilling this knowledge into a form that is accessible and beneficial for the student model [16]."
                        ],
                        "paper": {
                            "corpus_id": 276533262,
                            "title": "Decoupled Classifier Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2195337296",
                                    "name": "Hairui Wang"
                                },
                                {
                                    "authorId": "2346974967",
                                    "name": "Mengjie Dong"
                                },
                                {
                                    "authorId": "2181805784",
                                    "name": "Guifu Zhu"
                                },
                                {
                                    "authorId": "50024555",
                                    "name": "Ya Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "PLoS ONE",
                            "n_citations": 0
                        },
                        "score": 0.72998046875
                    },
                    {
                        "id": "(Hao et al., 2023)",
                        "snippets": [
                            "Knowledge distillation~(KD) has proven to be a highly effective approach for enhancing model performance through a teacher-student training scheme. However, most existing distillation methods are designed under the assumption that the teacher and student models belong to the same model family, particularly the hint-based approaches. By using centered kernel alignment (CKA) to compare the learned features between heterogeneous teacher and student models, we observe significant feature divergence. This divergence illustrates the ineffectiveness of previous hint-based methods in cross-architecture distillation. To tackle the challenge in distilling heterogeneous models, we propose a simple yet effective one-for-all KD framework called OFA-KD, which significantly improves the distillation performance between heterogeneous architectures. Specifically, we project intermediate features into an aligned latent space such as the logits space, where architecture-specific information is discarded. Additionally, we introduce an adaptive target enhancement scheme to prevent the student from being disturbed by irrelevant information. Extensive experiments with various architectures, including CNN, Transformer, and MLP, demonstrate the superiority of our OFA-KD framework in enabling distillation between heterogeneous architectures. Specifically, when equipped with our OFA-KD, the student models achieve notable performance improvements, with a maximum gain of 8.0% on the CIFAR-100 dataset and 0.7% on the ImageNet-1K dataset. PyTorch code and checkpoints can be found at https://github.com/Hao840/OFAKD."
                        ],
                        "paper": {
                            "corpus_id": 265871679,
                            "title": "One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2147215540",
                                    "name": "Zhiwei Hao"
                                },
                                {
                                    "authorId": "2148899357",
                                    "name": "Jianyuan Guo"
                                },
                                {
                                    "authorId": "3826388",
                                    "name": "Kai Han"
                                },
                                {
                                    "authorId": "103603255",
                                    "name": "Yehui Tang"
                                },
                                {
                                    "authorId": "2270807097",
                                    "name": "Han Hu"
                                },
                                {
                                    "authorId": "2108702980",
                                    "name": "Yunhe Wang"
                                },
                                {
                                    "authorId": "2271675319",
                                    "name": "Chang Xu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 68
                        },
                        "score": 0
                    },
                    {
                        "id": "(Adhane et al., 2024)",
                        "snippets": [
                            "The Student's performance often declines when there is a large architecture (capacity) gap between the Teacher and the Student (Mirzadeh et al., 2019)(Stanton et al., 2021). The drop in the Student's performance may stem from either its own challenges in learning relevant features or the overwhelming knowledge of the Teacher. To investigate this issue, we employ two distillation strategies in our experiments using ResNet-101 as the Teacher and ResNet-18 as the Student, which have a significant capacity disparity. In the first approach, we conduct direct distillation from ResNet-101 to ResNet-18. The second approach introduces an intermediate \"Teacher assistant\" (Son et al., 2020) to help bridge the capacity gap between ResNet-101 and ResNet-18",
                            "Figure 7 compares the saliency maps of the distilled features learned by two Students: ResNet-18 directly distilled from ResNet-101 (R18-R101) and ResNet-18 distilled from ResNet-101 through Teacher assistant ResNet-50 (R18-R50-R101). The saliency maps, visualised using UniCAM, reveal that the Teacher assistant helps learn more relevant features that highlight the object parts. In contrast, R18-R101 learns some irrelevant features and misses the salient features for the gt prediction."
                        ],
                        "paper": {
                            "corpus_id": 277652299,
                            "title": "On Explaining Knowledge Distillation: Measuring and Visualising the Knowledge Transfer Process",
                            "authors": [
                                {
                                    "authorId": "7827164",
                                    "name": "Gereziher W. Adhane"
                                },
                                {
                                    "authorId": "2178547",
                                    "name": "Mohammad Mahdi Dehshibi"
                                },
                                {
                                    "authorId": "2313639114",
                                    "name": "Dennis Vetter"
                                },
                                {
                                    "authorId": "2284862027",
                                    "name": "David Masip"
                                },
                                {
                                    "authorId": "2313639123",
                                    "name": "Gemma Roig"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.78271484375
                    },
                    {
                        "id": "(Mirzadeh et al., 2019)",
                        "snippets": [
                            "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."
                        ],
                        "paper": {
                            "corpus_id": 212908749,
                            "title": "Improved Knowledge Distillation via Teacher Assistant",
                            "authors": [
                                {
                                    "authorId": "145156788",
                                    "name": "Seyed Iman Mirzadeh"
                                },
                                {
                                    "authorId": "1682124",
                                    "name": "Mehrdad Farajtabar"
                                },
                                {
                                    "authorId": "2112839418",
                                    "name": "Ang Li"
                                },
                                {
                                    "authorId": "153898744",
                                    "name": "Nir Levine"
                                },
                                {
                                    "authorId": "2063980545",
                                    "name": "Akihiro Matsukawa"
                                },
                                {
                                    "authorId": "144600887",
                                    "name": "H. Ghasemzadeh"
                                }
                            ],
                            "year": 2019,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 1081
                        },
                        "score": 0
                    },
                    {
                        "id": "(Son et al., 2020)",
                        "snippets": [
                            "With the success of deep neural networks, knowledge distillation which guides the learning of a small student network from a large teacher network is being actively studied for model compression and transfer learning. However, few studies have been performed to resolve the poor learning issue of the student network when the student and teacher model sizes significantly differ. In this paper, we propose a densely guided knowledge distillation using multiple teacher assistants that gradually decreases the model size to efficiently bridge the large gap between the teacher and student networks. To stimulate more efficient learning of the student network, we guide each teacher assistant to every other smaller teacher assistants iteratively. Specifically, when teaching a smaller teacher assistant at the next step, the existing larger teacher assistants from the previous step are used as well as the teacher network. Moreover, we design stochastic teaching where, for each mini-batch, a teacher or teacher assistants are randomly dropped. This acts as a regularizer to improve the efficiency of teaching of the student network. Thus, the student can always learn salient distilled knowledge from the multiple sources. We verified the effectiveness of the proposed method for a classification task using CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant performance improvements with various backbone architectures such as ResNet, WideResNet, and VGG.1"
                        ],
                        "paper": {
                            "corpus_id": 221802641,
                            "title": "Densely Guided Knowledge Distillation using Multiple Teacher Assistants",
                            "authors": [
                                {
                                    "authorId": "1557388434",
                                    "name": "Wonchul Son"
                                },
                                {
                                    "authorId": "107598106",
                                    "name": "Jaemin Na"
                                },
                                {
                                    "authorId": "34600044",
                                    "name": "Wonjun Hwang"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 119
                        },
                        "score": 0
                    },
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "Existing methods (e.g., (Kim et al., 2018)(Passalis et al., 2020)64,82]) often prioritize student model performance at the cost of introducing complex architectures.This increased complexity hinders the adoption of KD (Gou et al., 2020) and negatively impacts subsequent fine-tuning processes, such as identifying the causes of underperformance, devising improvement strategies, and executing them effectively (Wang et al., 2020)(Zhang et al., 2022).Conversely, \"simpler\" student models can increase interpretability but also widen the capacity gap between the student and teacher models, making it challenging to transfer the desired knowledge effectively [56].Therefore, striking an appropriate balance between high student effectiveness and suitable interpretability is crucial for successful knowledge distillation and fine-tuning processes."
                        ],
                        "paper": {
                            "corpus_id": 270737867,
                            "title": "InFiConD: Interactive No-code Fine-tuning with Concept-based Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "8669194",
                                    "name": "Jinbin Huang"
                                },
                                {
                                    "authorId": "2003575022",
                                    "name": "Wenbin He"
                                },
                                {
                                    "authorId": "52158581",
                                    "name": "Liangke Gou"
                                },
                                {
                                    "authorId": "2265644812",
                                    "name": "Liu Ren"
                                },
                                {
                                    "authorId": "2256515852",
                                    "name": "C. Bryan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.734375
                    },
                    {
                        "id": "(Wang et al., 2020)",
                        "snippets": [
                            "Deep neural models, in recent years, have been successful in almost every field, even solving the most complex problem statements. However, these models are huge in size with millions (and even billions) of parameters, demanding heavy computation power and failing to be deployed on edge devices. Besides, the performance boost is highly dependent on redundant labeled data. To achieve faster speeds and to handle the problems caused by the lack of labeled data, knowledge distillation (KD) has been proposed to transfer information learned from one model to another. KD is often characterized by the so-called \u2018Student-Teacher\u2019 (S-T) learning framework and has been broadly applied in model compression and knowledge transfer. This paper is about KD and S-T learning, which are being actively studied in recent years. First, we aim to provide explanations of what KD is and how/why it works. Then, we provide a comprehensive survey on the recent progress of KD methods together with S-T frameworks typically used for vision tasks. In general, we investigate some fundamental questions that have been driving this research area and thoroughly generalize the research progress and technical details. Additionally, we systematically analyze the research status of KD in vision applications. Finally, we discuss the potentials and open challenges of existing methods and prospect the future directions of KD and S-T learning."
                        ],
                        "paper": {
                            "corpus_id": 215745611,
                            "title": "Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks",
                            "authors": [
                                {
                                    "authorId": "2144734901",
                                    "name": "Lin Wang"
                                },
                                {
                                    "authorId": "51182421",
                                    "name": "Kuk-Jin Yoon"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 700
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ballout et al., 2024)",
                        "snippets": [
                            "This limitation underscores the inherent challenge in knowledge distillation and model scaling. While our method effectively transfers knowledge from a large to a small model, the reduced capacity of the smaller model limits its ability to fully replicate the performance of its larger counterpart. This performance discrepancy highlights the trade-offs involved in model downsizing, where gains in efficiency and deployability often come at the cost of reduced accuracy and overall capability."
                        ],
                        "paper": {
                            "corpus_id": 272753230,
                            "title": "Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights",
                            "authors": [
                                {
                                    "authorId": "1491169373",
                                    "name": "Mohamad Ballout"
                                },
                                {
                                    "authorId": "1751765",
                                    "name": "U. Krumnack"
                                },
                                {
                                    "authorId": "2238205582",
                                    "name": "Gunther Heidemann"
                                },
                                {
                                    "authorId": "1743582",
                                    "name": "Kai-Uwe K\u00fchnberger"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Applications of Natural Language to Data Bases",
                            "n_citations": 3
                        },
                        "score": 0.92626953125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Alternative Approaches and Solutions",
                "tldr": "Researchers have developed various strategies to overcome the challenges of capacity gaps in knowledge distillation, including cross-architecture approaches, intermediate teacher assistants, and feature alignment techniques that create common representation spaces between heterogeneous models. (8 sources)",
                "text": "\nWhen faced with the limitations of size-matched or similar-architecture knowledge distillation, researchers have developed several alternative approaches to enhance knowledge transfer effectiveness. One key strategy is cross-architecture distillation, which involves teacher and student models with different architectures <Paper corpusId=\"251135436\" paperTitle=\"(Galke et al., 2021)\" isShortName></Paper>. While this approach introduces new challenges, it also provides greater flexibility in teacher selection and potentially better knowledge transfer.\n\nA promising solution to address the capacity gap problem is the use of intermediate \"teacher assistant\" models that bridge the knowledge transfer between large teachers and small students. This multi-step knowledge distillation approach helps students learn more relevant features by creating a smoother knowledge transfer pathway <Paper corpusId=\"277652299\" paperTitle=\"(Adhane et al., 2024)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>. Experiments comparing direct distillation from ResNet-101 to ResNet-18 versus using a ResNet-50 teacher assistant showed that the latter approach helped students learn more salient features while avoiding irrelevant ones <Paper corpusId=\"277652299\" paperTitle=\"(Adhane et al., 2024)\" isShortName></Paper> <Paper corpusId=\"221802641\" paperTitle=\"(Son et al., 2020)\" isShortName></Paper>.\n\nAnother important distinction in knowledge distillation approaches is between black-box and white-box methodologies. Black-box knowledge distillation involves the student learning solely from the teacher's outputs without access to internal mechanisms, which is particularly useful when teacher models are proprietary or architecturally distinct. In contrast, white-box distillation allows students to access the teacher's internal states, enabling deeper and more precise learning, though requiring careful architectural alignment <Paper corpusId=\"272368391\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>.\n\nFor heterogeneous architectures, traditional feature-based methods often perform poorly due to differences in feature representation. These substantial differences in meta-architecture, input format, and spatial representation prevent the use of simple similarity metrics for aligning stage-wise feature representations <Paper corpusId=\"270094801\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. To address this challenge, researchers have developed approaches like OFA-KD, which projects intermediate features into an aligned latent space where architecture-specific information is discarded, significantly improving distillation performance between heterogeneous architectures <Paper corpusId=\"270094801\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265871679\" paperTitle=\"(Hao et al., 2023)\" isShortName></Paper>.\n\nSimple Knowledge Distillation (SimKD) represents another innovative approach that allows compression of powerful teacher models into lightweight students without sacrificing performance. This technique reuses the teacher's discriminative classifier for student inference while training the student encoder through feature alignment with a single loss. By incorporating a projector to match the student encoder with the teacher classifier, SimKD achieves state-of-the-art results across various teacher-student architecture combinations, though at the cost of a reduced compression ratio <Paper corpusId=\"259378480\" paperTitle=\"(Gao, 2023)\" isShortName></Paper>.\n\nThese alternative approaches demonstrate that effective knowledge transfer is possible even between models with significant capacity differences, providing practical solutions to overcome the limitations of size-matched or identical-architecture knowledge distillation.",
                "citations": [
                    {
                        "id": "(Galke et al., 2021)",
                        "snippets": [
                            "When student and teacher models have a different architecture, we call that cross-architecture distillation."
                        ],
                        "paper": {
                            "corpus_id": 251135436,
                            "title": "General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings",
                            "authors": [
                                {
                                    "authorId": "11220282",
                                    "name": "Lukas Galke"
                                },
                                {
                                    "authorId": "2127384933",
                                    "name": "Isabelle Cuber"
                                },
                                {
                                    "authorId": "2115007899",
                                    "name": "Christophe Meyer"
                                },
                                {
                                    "authorId": "2127389543",
                                    "name": "Henrik Ferdinand Nolscher"
                                },
                                {
                                    "authorId": "2127393042",
                                    "name": "Angelina Sonderecker"
                                },
                                {
                                    "authorId": "1753135",
                                    "name": "A. Scherp"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE International Joint Conference on Neural Network",
                            "n_citations": 2
                        },
                        "score": 0.74755859375
                    },
                    {
                        "id": "(Adhane et al., 2024)",
                        "snippets": [
                            "The Student's performance often declines when there is a large architecture (capacity) gap between the Teacher and the Student (Mirzadeh et al., 2019)(Stanton et al., 2021). The drop in the Student's performance may stem from either its own challenges in learning relevant features or the overwhelming knowledge of the Teacher. To investigate this issue, we employ two distillation strategies in our experiments using ResNet-101 as the Teacher and ResNet-18 as the Student, which have a significant capacity disparity. In the first approach, we conduct direct distillation from ResNet-101 to ResNet-18. The second approach introduces an intermediate \"Teacher assistant\" (Son et al., 2020) to help bridge the capacity gap between ResNet-101 and ResNet-18",
                            "Figure 7 compares the saliency maps of the distilled features learned by two Students: ResNet-18 directly distilled from ResNet-101 (R18-R101) and ResNet-18 distilled from ResNet-101 through Teacher assistant ResNet-50 (R18-R50-R101). The saliency maps, visualised using UniCAM, reveal that the Teacher assistant helps learn more relevant features that highlight the object parts. In contrast, R18-R101 learns some irrelevant features and misses the salient features for the gt prediction."
                        ],
                        "paper": {
                            "corpus_id": 277652299,
                            "title": "On Explaining Knowledge Distillation: Measuring and Visualising the Knowledge Transfer Process",
                            "authors": [
                                {
                                    "authorId": "7827164",
                                    "name": "Gereziher W. Adhane"
                                },
                                {
                                    "authorId": "2178547",
                                    "name": "Mohammad Mahdi Dehshibi"
                                },
                                {
                                    "authorId": "2313639114",
                                    "name": "Dennis Vetter"
                                },
                                {
                                    "authorId": "2284862027",
                                    "name": "David Masip"
                                },
                                {
                                    "authorId": "2313639123",
                                    "name": "Gemma Roig"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.78271484375
                    },
                    {
                        "id": "(Mirzadeh et al., 2019)",
                        "snippets": [
                            "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."
                        ],
                        "paper": {
                            "corpus_id": 212908749,
                            "title": "Improved Knowledge Distillation via Teacher Assistant",
                            "authors": [
                                {
                                    "authorId": "145156788",
                                    "name": "Seyed Iman Mirzadeh"
                                },
                                {
                                    "authorId": "1682124",
                                    "name": "Mehrdad Farajtabar"
                                },
                                {
                                    "authorId": "2112839418",
                                    "name": "Ang Li"
                                },
                                {
                                    "authorId": "153898744",
                                    "name": "Nir Levine"
                                },
                                {
                                    "authorId": "2063980545",
                                    "name": "Akihiro Matsukawa"
                                },
                                {
                                    "authorId": "144600887",
                                    "name": "H. Ghasemzadeh"
                                }
                            ],
                            "year": 2019,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 1081
                        },
                        "score": 0
                    },
                    {
                        "id": "(Son et al., 2020)",
                        "snippets": [
                            "With the success of deep neural networks, knowledge distillation which guides the learning of a small student network from a large teacher network is being actively studied for model compression and transfer learning. However, few studies have been performed to resolve the poor learning issue of the student network when the student and teacher model sizes significantly differ. In this paper, we propose a densely guided knowledge distillation using multiple teacher assistants that gradually decreases the model size to efficiently bridge the large gap between the teacher and student networks. To stimulate more efficient learning of the student network, we guide each teacher assistant to every other smaller teacher assistants iteratively. Specifically, when teaching a smaller teacher assistant at the next step, the existing larger teacher assistants from the previous step are used as well as the teacher network. Moreover, we design stochastic teaching where, for each mini-batch, a teacher or teacher assistants are randomly dropped. This acts as a regularizer to improve the efficiency of teaching of the student network. Thus, the student can always learn salient distilled knowledge from the multiple sources. We verified the effectiveness of the proposed method for a classification task using CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant performance improvements with various backbone architectures such as ResNet, WideResNet, and VGG.1"
                        ],
                        "paper": {
                            "corpus_id": 221802641,
                            "title": "Densely Guided Knowledge Distillation using Multiple Teacher Assistants",
                            "authors": [
                                {
                                    "authorId": "1557388434",
                                    "name": "Wonchul Son"
                                },
                                {
                                    "authorId": "107598106",
                                    "name": "Jaemin Na"
                                },
                                {
                                    "authorId": "34600044",
                                    "name": "Wonjun Hwang"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 119
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "This approach involves the student model learning solely from the outputs of the teacher model, without access to its internal mechanics or parameters. It is particularly advantageous when the teacher model's details are proprietary or when the architectures of the teacher and student models differ markedly. For instance, Gu et al. (2023) demonstrated that black-box KD could effectively train models using only the output data from LLM APIs like ChatGPT. The student model trains to emulate the teacher's output distribution based on input-output pairs, a process that, while effective, limits learning to external behaviors without tapping into the teacher's deeper internal states",
                            "In contrast, White-box Knowledge Distillation allows the student model to access the internal states and workings of the teacher, facilitating a deeper and more precise learning process. This method enables the student to mimic not just the outputs but also the internal state distributions of the teacher, enhancing learning efficacy and depth. The increased access to the teacher's detailed workings helps guide the student's learning, resulting in more accurate and robust models. However, this technique requires a careful alignment of model architectures to ensure effective knowledge transfer and is generally more complex to implement."
                        ],
                        "paper": {
                            "corpus_id": 272368391,
                            "title": "On-Device Language Models: A Comprehensive Review",
                            "authors": [
                                {
                                    "authorId": "2316519813",
                                    "name": "Jiajun Xu"
                                },
                                {
                                    "authorId": "2294674012",
                                    "name": "Zhiyuan Li"
                                },
                                {
                                    "authorId": "2294845809",
                                    "name": "Wei Chen"
                                },
                                {
                                    "authorId": "2316514278",
                                    "name": "Qun Wang"
                                },
                                {
                                    "authorId": "2319809164",
                                    "name": "Xin Gao"
                                },
                                {
                                    "authorId": "2364055424",
                                    "name": "Qi Cai"
                                },
                                {
                                    "authorId": "2319410023",
                                    "name": "Ziyuan Ling"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 35
                        },
                        "score": 0.76025390625
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "Knowledge distillation is commonly employed to compress neural networks, reducing the inference costs and memory footprint. In the scenario of homogenous architecture, feature-based methods have been widely validated for their effectiveness. However, in scenarios where the teacher and student models are of heterogeneous architectures, the inherent differences in feature representation significantly degrade the performance of these methods.\n\nExisting knowledge distillation methods primarily employ logit [1,2,3,4] or intermediate feature maps [5,6,7]8] as the medium for knowledge transfer",
                            "While these methods often excel in knowledge distillation with homogeneous architectures due to the intrinsic similarity of feature representations, they face serious setback in heterogeneous settings (Hao et al., 2023).The substantial differences in meta-architecture, input format, and spatial representation between heterogeneous models preclude the use of simple similarity metrics for aligning stage-wise feature representations.Moreover, mimicing the local details in feature representations may lead to detrimental guidance."
                        ],
                        "paper": {
                            "corpus_id": 270094801,
                            "title": "Aligning in a Compact Space: Contrastive Knowledge Distillation between Heterogeneous Architectures",
                            "authors": [
                                {
                                    "authorId": "2243295129",
                                    "name": "Hongjun Wu"
                                },
                                {
                                    "authorId": "2303803900",
                                    "name": "Li Xiao"
                                },
                                {
                                    "authorId": "2303860883",
                                    "name": "Xingkuo Zhang"
                                },
                                {
                                    "authorId": "2303653169",
                                    "name": "Yining Miao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.80126953125
                    },
                    {
                        "id": "(Hao et al., 2023)",
                        "snippets": [
                            "Knowledge distillation~(KD) has proven to be a highly effective approach for enhancing model performance through a teacher-student training scheme. However, most existing distillation methods are designed under the assumption that the teacher and student models belong to the same model family, particularly the hint-based approaches. By using centered kernel alignment (CKA) to compare the learned features between heterogeneous teacher and student models, we observe significant feature divergence. This divergence illustrates the ineffectiveness of previous hint-based methods in cross-architecture distillation. To tackle the challenge in distilling heterogeneous models, we propose a simple yet effective one-for-all KD framework called OFA-KD, which significantly improves the distillation performance between heterogeneous architectures. Specifically, we project intermediate features into an aligned latent space such as the logits space, where architecture-specific information is discarded. Additionally, we introduce an adaptive target enhancement scheme to prevent the student from being disturbed by irrelevant information. Extensive experiments with various architectures, including CNN, Transformer, and MLP, demonstrate the superiority of our OFA-KD framework in enabling distillation between heterogeneous architectures. Specifically, when equipped with our OFA-KD, the student models achieve notable performance improvements, with a maximum gain of 8.0% on the CIFAR-100 dataset and 0.7% on the ImageNet-1K dataset. PyTorch code and checkpoints can be found at https://github.com/Hao840/OFAKD."
                        ],
                        "paper": {
                            "corpus_id": 265871679,
                            "title": "One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2147215540",
                                    "name": "Zhiwei Hao"
                                },
                                {
                                    "authorId": "2148899357",
                                    "name": "Jianyuan Guo"
                                },
                                {
                                    "authorId": "3826388",
                                    "name": "Kai Han"
                                },
                                {
                                    "authorId": "103603255",
                                    "name": "Yehui Tang"
                                },
                                {
                                    "authorId": "2270807097",
                                    "name": "Han Hu"
                                },
                                {
                                    "authorId": "2108702980",
                                    "name": "Yunhe Wang"
                                },
                                {
                                    "authorId": "2271675319",
                                    "name": "Chang Xu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 68
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gao, 2023)",
                        "snippets": [
                            "The use of Simple Knowledge Distillation (SimKD) allows the compression of powerful teacher models into lightweight student models without sacrificing performance. This is achieved by reusing the discriminative classifier of the teacher model for student inference and training the student encoder by feature alignment with a single loss. A projector was also developed to help match the student encoder with the teacher classifier, making the technique applicable to a variety of teacher and student architectures. Experiments show that with the addition of the projector, the technique achieves state-of-the-art results at the cost of a reduced compression ratio."
                        ],
                        "paper": {
                            "corpus_id": 259378480,
                            "title": "Review of Recent Distillation Studies",
                            "authors": [
                                {
                                    "authorId": "2147417425",
                                    "name": "Min Gao"
                                }
                            ],
                            "year": 2023,
                            "venue": "MATEC Web of Conferences",
                            "n_citations": 0
                        },
                        "score": 0.68310546875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.136098
    }
}